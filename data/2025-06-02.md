<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.CV](#cs.CV) [Total: 118]
- [cs.LG](#cs.LG) [Total: 141]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CL](#cs.CL) [Total: 76]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.SE](#cs.SE) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [stat.ML](#stat.ML) [Total: 15]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 9]
- [eess.AS](#eess.AS) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [nlin.AO](#nlin.AO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking](https://arxiv.org/abs/2505.23930)
*Naomi Omeonga wa Kayembe*

Main category: cs.CY

TL;DR: 论文提出了一种区分问题寻找与问题解决的新框架，强调人类智能的独特性和AI的局限性，并通过混合方法分析公众对AI的讨论。


<details>
  <summary>Details</summary>
Motivation: 澄清人类智能与AI的区别，强调人类在目标设定和情感基础方面的优势。

Method: 结合定性分析和定量指标，对157个YouTube视频进行主题分析。

Result: 公众对AI既着迷又焦虑，讨论主题包括隐私、失业、伦理等。

Conclusion: 主张增强情感和数字素养，推动以人为中心的AI发展。

Abstract: This study introduces a novel conceptual framework distinguishing
problem-seeking from problem-solving to clarify the unique features of human
intelligence in contrast to AI. Problem-seeking refers to the embodied,
emotionally grounded process by which humans identify and set goals, while
problem-solving denotes the execution of strategies aimed at achieving such
predefined objectives. The framework emphasizes that while AI excels at
efficiency and optimization, it lacks the orientation derived from experiential
grounding and the embodiment flexibility intrinsic to human cognition. To
empirically explore this distinction, the research analyzes metadata from 157
YouTube videos discussing AI. Conducting a thematic analysis combining
qualitative insights with keyword-based quantitative metrics, this
mixed-methods approach uncovers recurring themes in public discourse, including
privacy, job displacement, misinformation, optimism, and ethical concerns. The
results reveal a dual sentiment: public fascination with AI's capabilities
coexists with anxiety and skepticism about its societal implications. The
discussion critiques the orthogonality thesis, which posits that intelligence
is separable from goal content, and instead argues that human intelligence
integrates goal-setting and goal-pursuit. It underscores the centrality of
embodied cognition in human reasoning and highlights how AI's limitations come
from its current reliance on computational processing. The study advocates for
enhancing emotional and digital literacy to foster responsible AI engagement.
It calls for reframing public discourse to recognize AI as a tool that augments
-- rather than replaces -- human intelligence. By positioning problem seeking
at the core of cognition and as a critical dimension of intelligence, this
research offers new perspectives on ethically aligned and human-centered AI
development.

</details>


### [2] [Bridging the Gap: Enhancing Digital Accessibility for Medicaid Populations in Telehealth Adoption](https://arxiv.org/abs/2505.24035)
*Vishnu Ramineni,Aditya Gupta,Balakrishna Pothineni,Isan Sahoo,Shivareddy Devarapalli,Balaji Shesharao Ingole*

Main category: cs.CY

TL;DR: 论文探讨了医疗补助人群在远程医疗中的数字可及性问题，提出了通过包容性设计、AI技术和政策支持解决障碍的方法。


<details>
  <summary>Details</summary>
Motivation: 医疗补助人群在远程医疗中面临互联网连接差、数字平台认知不足和技术支持缺乏等问题，研究旨在解决这些障碍。

Method: 采用包容性设计、AI辅助技术和政策支持，提升远程医疗的可及性和效果。

Result: 研究提出了增强数字可及性的解决方案，有助于减少医疗差异并改善患者结果。

Conclusion: 通过多方合作和技术与政策支持，可以构建一个以患者为中心、包容性强的远程医疗系统。

Abstract: The swift evolution of telehealth has revolutionized how medical
professionals deliver healthcare services and boost convenience and
accessibility. Yet, the Medicaid population encounters several impediments in
utilizing facilities especially owing to poor internet connectivity, less
awareness about digital platforms, and a shortage of assistive technologies.
The paper aims to explicate key factors behind digital accessibility for
Medicaid populations and expounds robust solutions to eradicate these
challenges. Through inclusive design ideas, AI-assisted technologies, and
all-encompassing policies by the concerned authorities, healthcare
professionals can enhance usability and efficacy and thus better serve the
needy. This revolution not only enhances convenience but also expands access,
mainly for underserved groups such as rural populations or those with mobility
issues, thereby ensuring inclusivity and flexibility in the healthcare domain.
Besides, the paper highlights the vitality of collaboration between healthcare
professionals, policymakers, and tech developers in unveiling the accessibility
and usability impediments. What else helps in minimizing healthcare differences
and enhancing patient outcomes is guaranteeing equitable access to telehealth
for Medicaid beneficiaries. The paper systematically offers major
recommendations to increase digital accessibility in telehealth, thereby
creating a patient-oriented and all-encompassing healthcare system.

</details>


### [3] [Evaluating Gemini in an arena for learning](https://arxiv.org/abs/2505.24477)
*LearnLM Team,Abhinit Modi,Aditya Srikanth Veerubhotla,Aliya Rysbek,Andrea Huber,Ankit Anand,Avishkar Bhoopchand,Brett Wiltshire,Daniel Gillick,Daniel Kasenberg,Eleni Sgouritsa,Gal Elidan,Hengrui Liu,Holger Winnemoeller,Irina Jurenka,James Cohan,Jennifer She,Julia Wilkowski,Kaiz Alarakyia,Kevin R. McKee,Komal Singh,Lisa Wang,Markus Kunesch,Miruna Pîslar,Niv Efron,Parsa Mahmoudieh,Pierre-Alexandre Kamienny,Sara Wiltberger,Shakir Mohamed,Shashank Agarwal,Shubham Milind Phal,Sun Jae Lee,Theofilos Strinopoulos,Wei-Jen Ko,Yael Gold-Zamir,Yael Haramaty,Yannis Assael*

Main category: cs.CY

TL;DR: 论文提出了一个评估AI教育模型的基准测试，并通过盲测对比发现Gemini 2.5 Pro在支持学习目标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估AI教育模型的通用基准，研究旨在填补这一空白。

Method: 通过“学习竞技场”实验，让教育者和专家盲测对比多个领先AI模型。

Result: Gemini 2.5 Pro在73.2%的对比中胜出，并在教学原则中表现突出。

Conclusion: Gemini 2.5 Pro是当前最适合教育场景的AI模型。

Abstract: Artificial intelligence (AI) is poised to transform education, but the
research community lacks a robust, general benchmark to evaluate AI models for
learning. To assess state-of-the-art support for educational use cases, we ran
an "arena for learning" where educators and pedagogy experts conduct blind,
head-to-head, multi-turn comparisons of leading AI models. In particular, $N =
189$ educators drew from their experience to role-play realistic learning use
cases, interacting with two models sequentially, after which $N = 206$ experts
judged which model better supported the user's learning goals. The arena
evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7
Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro
in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini
2.5 Pro also demonstrated markedly higher performance across key principles of
good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading
model for learning.

</details>


### [4] [Generative Knowledge Production Pipeline Driven by Academic Influencers](https://arxiv.org/abs/2505.24681)
*Katalin Feher,Marton Demeter*

Main category: cs.CY

TL;DR: 研究分析了生成式AI对学术知识生产的影响，提出了一种平衡原创性与伦理的生成式出版流程，并探讨了学术网红在其中的中介作用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI改变了知识生产、验证和传播方式，引发了学术诚信和可信度问题，需探索如何平衡创新与伦理。

Method: 分析了53个学术网红视频（覆盖530万观众），识别出一种结构化的、可实施的生成式出版流程。

Result: 生成式AI能自动化出版流程并促进知识生产民主化，但也挑战传统科学规范；学术网红是连接实践与政策的关键中介。

Conclusion: 研究提出了生成式出版流程和政策框架，支持学者、教育者和政策制定者推动负责任的知识生产，并优化学术工作流程。

Abstract: Generative AI transforms knowledge production, validation, and dissemination,
raising academic integrity and credibility concerns. This study examines 53
academic influencer videos that reached 5.3 million viewers to identify an
emerging, structured, implementation-ready pipeline balancing originality,
ethical compliance, and human-AI collaboration despite the disruptive impacts.
Findings highlight generative AI's potential to automate publication workflows
and democratize participation in knowledge production while challenging
traditional scientific norms. Academic influencers emerge as key intermediaries
in this paradigm shift, connecting bottom-up practices with institutional
policies to improve adaptability. Accordingly, the study proposes a generative
publication production pipeline and a policy framework for co-intelligence
adaptation and reinforcing credibility-centered standards in AI-powered
research. These insights support scholars, educators, and policymakers in
understanding AI's transformative impact by advocating responsible and
innovation-driven knowledge production. Additionally, they reveal pathways for
automating best practices, optimizing scholarly workflows, and fostering
creativity in academic research and publication.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [5] [ISMAF: Intrinsic-Social Modality Alignment and Fusion for Multimodal Rumor Detection](https://arxiv.org/abs/2505.24176)
*Zihao Yu,Xiang Li,Jing Zhang*

Main category: cs.MM

TL;DR: 本文提出了一种新颖的ISMAF框架，用于多模态谣言检测，通过跨模态一致性对齐和自适应融合机制，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的谣言传播速度快，现有方法忽视了内在模态与社交模态的复杂交互，导致检测效果受限。

Method: ISMAF框架采用跨模态一致性对齐策略、互学习方法及自适应融合机制，整合文本、图像和社交上下文信息。

Result: 在英中多模态数据集上，ISMAF表现优于现有最先进模型。

Conclusion: ISMAF通过有效融合多模态信息，为谣言检测提供了更全面的解决方案。

Abstract: The rapid dissemination of rumors on social media highlights the urgent need
for automatic detection methods to safeguard societal trust and stability.
While existing multimodal rumor detection models primarily emphasize capturing
consistency between intrinsic modalities (e.g., news text and images), they
often overlook the intricate interplay between intrinsic and social modalities.
This limitation hampers the ability to fully capture nuanced relationships that
are crucial for a comprehensive understanding. Additionally, current methods
struggle with effectively fusing social context with textual and visual
information, resulting in fragmented interpretations. To address these
challenges, this paper proposes a novel Intrinsic-Social Modality Alignment and
Fusion (ISMAF) framework for multimodal rumor detection. ISMAF first employs a
cross-modal consistency alignment strategy to align complex interactions
between intrinsic and social modalities. It then leverages a mutual learning
approach to facilitate collaborative refinement and integration of
complementary information across modalities. Finally, an adaptive fusion
mechanism is incorporated to dynamically adjust the contribution of each
modality, tackling the complexities of three-modality fusion. Extensive
experiments on both English and Chinese real-world multimedia datasets
demonstrate that ISMAF consistently outperforms state-of-the-art models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [6] [Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.24113)
*Pengcheng Dai,Yuanqiu Mo,Wenwu Yu,Wei Ren*

Main category: cs.MA

TL;DR: 本文提出了一种分布式神经策略梯度算法，用于解决网络化多智能体强化学习问题，通过创新的神经网络设计提升表达能力和协作效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法因线性函数逼近表达能力不足，导致性能受限，本文旨在通过分布式神经网络设计提升协作效果。

Method: 算法包含分布式评论家步骤和分散式行动者步骤，分别用于协作评估联合策略和更新局部策略参数。

Result: 理论分析证明了算法的全局收敛性，仿真实验验证了其在机器人路径规划中的有效性。

Conclusion: 所提算法在表达能力和协作效率上优于现有方法，适用于复杂多智能体任务。

Abstract: This paper studies the networked multi-agent reinforcement learning (NMARL)
problem, where the objective of agents is to collaboratively maximize the
discounted average cumulative rewards. Different from the existing methods that
suffer from poor expression due to linear function approximation, we propose a
distributed neural policy gradient algorithm that features two innovatively
designed neural networks, specifically for the approximate Q-functions and
policy functions of agents. This distributed neural policy gradient algorithm
consists of two key components: the distributed critic step and the
decentralized actor step. In the distributed critic step, agents receive the
approximate Q-function parameters from their neighboring agents via a
time-varying communication networks to collaboratively evaluate the joint
policy. In contrast, in the decentralized actor step, each agent updates its
local policy parameter solely based on its own approximate Q-function. In the
convergence analysis, we first establish the global convergence of agents for
the joint policy evaluation in the distributed critic step. Subsequently, we
rigorously demonstrate the global convergence of the overall distributed neural
policy gradient algorithm with respect to the objective function. Finally, the
effectiveness of the proposed algorithm is demonstrated by comparing it with a
centralized algorithm through simulation in the robot path planning
environment.

</details>


### [7] [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/abs/2505.24239)
*Sana Ebrahimi,Mohsen Dehghankar,Abolfazl Asudeh*

Main category: cs.MA

TL;DR: 提出了一种基于可信度评分的抗对抗多智能体LLM框架，通过迭代游戏建模协作问答过程，逐步学习智能体的可信度评分，有效减少对抗性影响并增强多智能体合作的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在多个领域表现出强大能力，但易受对抗性和低性能智能体的影响，需解决这一问题。

Method: 将协作问答过程建模为迭代游戏，智能体通过通信贡献最终输出，系统基于智能体的历史贡献逐步学习可信度评分。

Result: 实验表明，该系统在多种任务和设置下能有效减少对抗性影响，提升多智能体合作的鲁棒性，即使在对抗性占多数的情况下。

Conclusion: 提出的可信度评分框架为多智能体LLM系统提供了一种通用的抗对抗解决方案，显著提升了系统的稳定性和性能。

Abstract: While multi-agent LLM systems show strong capabilities in various domains,
they are highly vulnerable to adversarial and low-performing agents. To resolve
this issue, in this paper, we introduce a general and adversary-resistant
multi-agent LLM framework based on credibility scoring. We model the
collaborative query-answering process as an iterative game, where the agents
communicate and contribute to a final system output. Our system associates a
credibility score that is used when aggregating the team outputs. The
credibility scores are learned gradually based on the past contributions of
each agent in query answering. Our experiments across multiple tasks and
settings demonstrate our system's effectiveness in mitigating adversarial
influence and enhancing the resilience of multi-agent cooperation, even in the
adversary-majority settings.

</details>


### [8] [R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.24265)
*Harsh Goel,Mohammad Omama,Behdad Chalaki,Vaishnav Tadiparthi,Ehsan Moradi Pari,Sandeep Chinchali*

Main category: cs.MA

TL;DR: R3DM提出了一种基于角色的多智能体强化学习框架，通过最大化角色、轨迹和未来行为的互信息，提升协调能力，在SMAC环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于角色的MARL方法仅从智能体的过去经验中推导角色，忽略了角色对未来行为的影响，导致协调效果不佳。

Method: R3DM通过对比学习从过去轨迹中推导中间角色，利用动态模型促进不同角色的未来行为多样性，优化互信息目标。

Result: 在SMAC和SMACv2环境中，R3DM将胜率提升高达20%，优于现有MARL方法。

Conclusion: R3DM通过角色与未来行为的关联，显著提升了多智能体协调能力，为复杂任务提供了有效解决方案。

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in large-scale traffic control, autonomous vehicles, and robotics. Drawing
inspiration from biological systems where roles naturally emerge to enable
coordination, role-based MARL methods have been proposed to enhance cooperation
learning for complex tasks. However, existing methods exclusively derive roles
from an agent's past experience during training, neglecting their influence on
its future trajectories. This paper introduces a key insight: an agent's role
should shape its future behavior to enable effective coordination. Hence, we
propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel
role-based MARL framework that learns emergent roles by maximizing the mutual
information between agents' roles, observed trajectories, and expected future
behaviors. R3DM optimizes the proposed objective through contrastive learning
on past trajectories to first derive intermediate roles that shape intrinsic
rewards to promote diversity in future behaviors across different roles through
a learned dynamics model. Benchmarking on SMAC and SMACv2 environments
demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving
multi-agent coordination to increase win rates by up to 20%.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [9] [FinRipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness](https://arxiv.org/abs/2505.23826)
*Yuanjian Xu,Jianing Hao,Kunsheng Tang,Jingnan Chen,Anxian Liu,Peng Liu,Guang Zhang*

Main category: cs.SI

TL;DR: FinRipple是一个结合金融理论和强化学习的框架，用于分析金融市场中的涟漪效应，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统事件研究受限于静态分析和简化假设，无法捕捉金融市场的复杂涟漪效应。

Method: FinRipple通过时间变化的知识图谱和资产定价理论，结合强化学习，使大语言模型能够预测涟漪效应。

Result: 实验表明，FinRipple为涟漪效应预测提供了有效的解决方案。

Conclusion: FinRipple填补了金融领域涟漪效应预测的空白，具有重要应用价值。

Abstract: Financial markets exhibit complex dynamics where localized events trigger
ripple effects across entities. Previous event studies, constrained by static
single-company analyses and simplistic assumptions, fail to capture these
ripple effects. While large language models (LLMs) offer emergent reasoning
capabilities, their direct application falters due to structural market
unawareness and limited capacity to analyze ripple effects. We propose
FinRipple, an elegant framework that empowers LLMs with the ability to analyze
ripple effects through financial theory-guided large-scale reinforcement
learning. We begin by relaxing the assumptions of previous methods,
incorporating a time-varying knowledge graph to accurately represent market
structure. By seamlessly integrating classical asset pricing theory, we align
the LLM with the market, enabling it to predict ripple effects. To the best of
our knowledge, we are the first to provide a standardized definition of ripple
effect prediction, a task that is extremely important yet unexplored in the
financial domain. Extensive experiments demonstrate that FinRipple provides a
promising solution to this task.

</details>


### [10] [Why Academics Are Leaving Twitter for Bluesky](https://arxiv.org/abs/2505.24801)
*Dorian Quelle,Frederic Denker,Prashant Garg,Alexandre Bovet*

Main category: cs.SI

TL;DR: 研究分析了30万学术用户从Twitter/X迁移到Bluesky的行为，发现18%的学者迁移，迁移率受学科、政治表达和Twitter参与度影响，但不受传统学术指标影响。信息源对迁移的影响远大于受众，且影响在一周内呈指数衰减。简单传染驱动了大部分迁移行为。


<details>
  <summary>Details</summary>
Motivation: 探讨学术用户从Twitter/X迁移到Bluesky的模式及其影响因素，揭示网络外部性、方向性影响和平台迁移的理论。

Method: 结合文献计量数据、纵向社交媒体活动和新颖的跨平台身份匹配管道，使用时间变化的Cox模型和匹配对设计分析迁移行为。

Result: 18%的学者迁移，信息源对迁移的影响显著大于受众，简单传染驱动了大部分迁移行为。重建更多Twitter网络的用户在Bluesky上更活跃。

Conclusion: 信息源在克服切换成本中起核心作用，研究为网络外部性和平台迁移理论提供了新见解。

Abstract: We analyse the migration of 300,000 academic users from Twitter/X to Bluesky
between 2023 and early 2025, combining rich bibliometric data, longitudinal
social-media activity, and a novel cross-platform identity-matching pipeline.
We show that 18% of scholars in our sample transitioned, with transition rates
varying sharply by discipline, political expression, and Twitter engagement but
not by traditional academic metrics. Using time-varying Cox models and a
matched-pairs design, we isolate genuine peer influence from homophily. We
uncover a striking asymmetry whereby information sources drive migration far
more powerfully than audience, with this influence decaying exponentially
within a week. We further develop an ego-level contagion classifier, revealing
that simple contagion drives two-thirds of all exits, shock-driven bursts
account for 16%, and complex contagion plays a marginal role. Finally, we show
that scholars who rebuild a higher fraction of their former Twitter networks on
Bluesky remain significantly more active and engaged. Our findings provide new
insights onto theories of network externalities, directional influence, and
platform migration, highlighting information sources' central role in
overcoming switching costs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
*Christopher D. Rosin*

Main category: cs.AI

TL;DR: 论文提出了一种基于推理大语言模型（LLMs）的代码生成方法CPro1，用于解决组合设计领域中的开放性问题，成功解决了多个长期未解的实例。


<details>
  <summary>Details</summary>
Motivation: 组合设计领域存在许多开放性问题，传统方法难以解决。通过结合推理LLMs和代码生成，可以更高效地探索这些问题的解决方案。

Method: CPro1利用推理LLMs生成搜索启发式方法，结合文本定义和验证器，自动调整超参数并提供执行反馈。

Result: CPro1成功解决了16个组合设计问题中的7个，包括3个新解决实例，并解决了2025年文献中的多个开放性问题。

Conclusion: 推理LLMs在组合设计问题中表现出色，CPro1方法为解决复杂数学问题提供了新思路。

Abstract: Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [12] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
*Mengkang Hu,Yuhang Zhou,Wendong Fan,Yuzhou Nie,Bowei Xia,Tao Sun,Ziyu Ye,Zhaoxuan Jin,Yingru Li,Qiguang Chen,Zeyu Zhang,Yifeng Wang,Qianshuo Ye,Bernard Ghanem,Ping Luo,Guohao Li*

Main category: cs.AI

TL;DR: Workforce是一个分层多智能体框架，通过模块化架构实现跨领域任务转移，结合优化的训练方法OWL，显著提升了多领域任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的多智能体系统在跨领域任务转移时需要完全重新设计和训练的问题。

Method: 采用模块化架构，包括领域无关的Planner、协调器Coordinator和领域专用的Workers，结合OWL优化训练方法。

Result: 在GAIA基准测试中达到69.70%的准确率，优于商业系统，32B模型在挑战性任务中表现接近GPT-4o。

Conclusion: Workforce为下一代通用AI助手奠定了基础，实现了可扩展的泛化和模块化领域转移。

Abstract: Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [13] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/abs/2505.23946)
*Yuanzhe Liu,Ryan Deng,Tim Kaler,Xuhao Chen,Charles E. Leiserson,Yao Ma,Jie Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于“课程”的多LLM协作框架，通过共享学习经验提升团队性能，证明小模型团队可以超越大模型和其他协作方法。


<details>
  <summary>Details</summary>
Motivation: 观察到不同LLM在不同任务中表现各异，但互补优势未知，因此探索如何通过团队协作提升性能。

Method: 设计了课程征集-存储-选择机制，让LLM团队在协作过程中互相学习成功与失败经验。

Result: 实验表明，采用课程协作的小LLM团队性能优于单个大LLM和其他多LLM协作方法。

Conclusion: 通过共享学习经验，小模型团队可以高效协作，超越大模型，为多LLM协作提供了新思路。

Abstract: Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [14] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
*Sania Nayab,Marco Simoni,Giulio Rossolini*

Main category: cs.AI

TL;DR: 本文提出了一种利用知识图谱（KGs）系统生成虚假三元组的方法，并通过大型语言模型（LLMs）生成具有不同可信度的虚假信息。同时，研究了LLMs在区分真实与虚假信息方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展加剧了虚假信息的传播，对社会、公共舆论和国家安全构成威胁。需要探索结构化、可扩展的虚假信息生成方法以评估这些威胁。

Method: 利用知识图谱的结构特性（如实体间距离和谓词）生成虚假三元组，并指导LLMs生成不同可信度的虚假信息。

Result: 该方法生成的虚假信息对人类检测具有挑战性，同时揭示了当前LLM检测方法的局限性。

Conclusion: 需要改进检测策略并深入研究生成模型的固有偏见。

Abstract: The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [15] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/abs/2505.23950)
*Boyuan Chen,Donghai Hong,Jiaming Ji,Jiacheng Zheng,Bowen Dong,Jiayi Zhou,Kaile Wang,Juntao Dai,Xuyao Wang,Wenqi Chen,Qirui Zheng,Wenxin Li,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 该论文提出了InterMT数据集，用于研究多轮多模态交互中缺失的能力，并通过人类反馈和专家标注提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）缺乏复杂的多轮多模态交互能力，而人类学习依赖于与环境的持续多模态交互。

Method: 构建InterMT数据集，包含15.6k提示、52.6k多轮对话实例和32.4k人类标注偏好对，并引入工具增强的MLLMs生成多轮QA实例。

Result: 提出了InterMT-Bench评估工具，展示了数据集在法官调解等应用中的实用性，并揭示了多轮扩展规律。

Conclusion: 开源InterMT数据集旨在推动MLLMs向更接近人类智能的多轮多模态交互能力发展。

Abstract: As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [16] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/abs/2505.23982)
*Jerry Junyang Cheung,Shiyao Shen,Yuchen Zhuang,Yinghao Li,Rampi Ramprasad,Chao Zhang*

Main category: cs.AI

TL;DR: MSQA是一个用于评估大型语言模型（LLMs）在材料科学领域知识和复杂推理能力的基准测试，包含1,757个研究生水平问题。实验显示，专有LLMs表现最佳，开源LLMs次之，领域特定LLMs表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLMs在材料科学领域知识和复杂推理能力的基准测试，MSQA填补了这一空白。

Method: MSQA包含两种问题格式（详细解释和二元判断），涵盖七个材料科学子领域，测试LLMs的精确知识和多步推理能力。

Result: 专有LLMs准确率最高达84.5%，开源LLMs约60.5%，领域特定LLMs表现较差。

Conclusion: MSQA是首个同时评估LLMs在材料科学领域事实知识和推理能力的基准测试，揭示了当前LLMs的局限性。

Abstract: Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [17] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)
*Mingyang Mao,Mariela M. Perez-Cabarcas,Utteja Kallakuri,Nicholas R. Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.AI

TL;DR: Multi-RAG是一个多模态检索增强生成系统，旨在通过整合多源信息流（如视频、音频和文本）为人类提供动态适应性辅助，减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和智能代理在人类生活中的普及，需要减轻人类在动态、信息丰富场景中的认知负担。

Method: 提出Multi-RAG系统，通过多模态信息理解和推理，结合视频、音频和文本数据，提升情境理解能力。

Result: 在MMBench-Video数据集上表现优于现有开源视频大语言模型和大型视觉语言模型，且资源消耗更低。

Conclusion: Multi-RAG展示了作为未来动态人机适应性辅助系统的高效实用潜力。

Abstract: To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [18] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/abs/2505.24036)
*Amel Gader,Alsayed Algergawy*

Main category: cs.AI

TL;DR: GenIC是一个基于大语言模型的两步生成式实例补全框架，通过属性预测和链接预测完成知识图谱补全任务，实验表明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 知识图谱补全任务中，现代知识库常包含实体描述和类型，这些信息可为推断缺失事实提供有价值的情境。

Method: GenIC框架分为两步：属性预测（多标签分类任务）和链接预测（生成式序列到序列任务）。

Result: 在三个数据集上的实验结果表明，该方法优于现有基线。

Conclusion: GenIC通过利用实体描述和大语言模型的能力，提出了一种有效的知识图谱实例补全方法。

Abstract: Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [19] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/abs/2505.24037)
*Qiao Xiao,Alan Ansell,Boqian Wu,Lu Yin,Mykola Pechenizkiy,Shiwei Liu,Decebal Constantin Mocanu*

Main category: cs.AI

TL;DR: 提出了一种名为SEFT的新方法，专门用于稀疏大语言模型（LLMs）的微调，通过动态调整稀疏拓扑结构，在保持稀疏性的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在部署时面临计算资源需求高的问题，而现有的稀疏化和微调方法难以在高稀疏度下保持性能。

Method: SEFT通过动态演化稀疏拓扑结构，采用权重丢弃与增长策略，结合敏感性驱动的剪枝标准，实现任务特定的适应。

Result: 实验表明，SEFT在多种LLMs和基准测试中表现优于现有方法，同时具有更高的内存和时间效率。

Conclusion: SEFT为稀疏LLMs的微调提供了一种高效且性能优越的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [20] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文探讨了如何通过检索增强生成（RAG）提升大型视觉语言模型（LVLM）的性能，研究了检索、重排序和生成阶段的优化方法，并提出了一种统一的自主框架，最终实现了5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态任务中表现突出，但仍受限于静态训练数据、幻觉问题和无法验证最新外部证据的缺陷，影响了其在动态现实应用中的表现。RAG提供了一种解决方案。

Method: 论文系统分析了多模态RAG流程，包括检索阶段的模态配置和策略、重排序阶段的去偏和相关性提升方法，以及生成阶段的检索候选整合策略，并提出了一种自主框架。

Result: 通过优化RAG流程，论文实现了平均5%的性能提升，且无需微调。

Conclusion: RAG能有效提升LVLM的性能，自主框架进一步优化了证据选择和生成过程，为动态应用提供了实用解决方案。

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [21] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/abs/2505.24181)
*Guanghao Li,Wenhao Jiang,Mingfeng Chen,Yan Li,Hao Yu,Shuting Dong,Tao Ren,Ming Tang,Chun Yuan*

Main category: cs.AI

TL;DR: Flow CoT提出了一种新的推理范式，通过渐进式潜在认知状态建模递归推理，结合SCOUT框架实现无需预训练的轻量级微调，显著提升了推理性能和解释质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有CoT方法依赖中间推理步骤导致的扩展性和泛化性不足问题，以及递归推理方法缺乏理论框架和需要昂贵预训练的局限性。

Method: 提出Flow CoT范式，将递归推理建模为潜在认知状态的渐进轨迹；设计SCOUT框架，通过渐进蒸馏和跨注意力回顾模块实现轻量级微调。

Result: 在八个推理基准测试中，SCOUT显著提升了准确性和解释质量，最高增益达1.8%。定性分析显示其支持逐步深化推理。

Conclusion: SCOUT验证了Flow CoT的有效性，为LLMs提供了一种可扩展的推理增强框架。

Abstract: Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [22] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/abs/2505.24197)
*Bhrij Patel,Ashish Jagmohan,Aditya Vempaty*

Main category: cs.AI

TL;DR: 论文提出了一种直接从演示中学习API功能的新方法，适用于缺乏文档的场景，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前API文档常缺失、过时或不一致，阻碍了可靠通用代理的开发，因此需要一种无需依赖文档的学习方法。

Method: 通过收集专家代理和自我探索的API演示，研究演示数量、LLM生成的摘要和评估对任务成功率的影响。

Result: 实验表明，提供显式函数调用和自然语言批评显著提高了任务成功率，但直接从演示学习仍具挑战性。

Conclusion: 论文指出了文档无关、自我改进的API代理的关键挑战，为未来工作提供了方向。

Abstract: Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [23] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/abs/2505.24201)
*Xu He,Di Wu,Yan Zhai,Kun Sun*

Main category: cs.AI

TL;DR: 该论文提出了一种针对多智能体系统（MAS）的系统级异常检测框架，结合结构建模和运行时行为监控，以解决现有防护机制的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的多智能体系统（MAS）面临多方面的安全和可靠性挑战，现有防护机制仅能提供部分保护，无法应对系统性或多点故障。

Method: 提出了一种基于图的框架，将智能体交互建模为动态执行图，并引入可插拔的SentinelAgent进行运行时监控和干预。

Result: 通过案例研究验证了框架的有效性，能够检测单点故障、提示注入、多智能体共谋及潜在攻击路径。

Conclusion: 该研究为构建更可信、可监控和安全的基于智能体的AI生态系统奠定了基础。

Abstract: The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [24] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208)
*Wenhan Yang,Spencer Stice,Ali Payani,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型（VLM）的安全性因模态间隙而下降，提出一种预训练正则化方法显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在生成安全输出方面表现不佳，尤其是与纯文本模型相比，模态间隙被认为是主要原因。

Method: 通过分析模态间隙与安全性的关系，提出一种预训练正则化方法以减少模态间隙。

Result: 实验表明，该方法在LLaVA v1.5等模型上显著降低不安全率（最高16.3%），且不影响性能。

Conclusion: 减少模态间隙是提升VLM安全性的有效途径，并可增强现有防御方法。

Abstract: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [25] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/abs/2505.24226)
*Yibo Zhao,Jiapeng Zhu,Ye Guo,Kangkang He,Xiang Li*

Main category: cs.AI

TL;DR: E^2GraphRAG是一种高效的图基RAG框架，通过双向索引和自适应检索策略，显著提升了索引和检索效率，同时保持问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG方法（如GraphRAG）效率低下且依赖手动预定义查询模式，限制了实际应用。

Method: 在索引阶段，E^2GraphRAG使用大语言模型构建摘要树，并结合SpaCy构建实体图；通过双向索引捕获实体与文档块的多对多关系。检索阶段采用自适应策略，根据图结构选择局部或全局模式。

Result: 实验表明，E^2GraphRAG的索引速度比GraphRAG快10倍，检索速度比LightRAG快100倍，同时保持竞争力的问答性能。

Conclusion: E^2GraphRAG通过优化索引和检索流程，显著提升了效率和效果，具有实际应用潜力。

Abstract: Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [26] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/abs/2505.24230)
*Murari Ambati*

Main category: cs.AI

TL;DR: ProofNet++是一个神经符号框架，结合了大型语言模型（LLMs）与形式化证明验证和自我纠正机制，提升了自动定理证明的准确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的系统存在逻辑步骤虚构和推理不可验证的问题，ProofNet++旨在解决这些限制。

Method: 通过结合符号证明树监督、使用验证器作为奖励函数的强化学习循环和迭代自我纠正模块。

Result: 在miniF2F、Lean的mathlib和HOL Light上的实验表明，ProofNet++显著提高了证明的准确性、正确性和形式化可验证性。

Conclusion: 论文提供了验证器引导的强化学习框架的收敛性和稳定性理论分析，并开源了数据集和代码库。

Abstract: We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [27] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/abs/2505.24258)
*Vishal Pallagani,Nitin Gupta,John Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: FABLE是一个评估大语言模型（LLM）数据流理解能力的基准测试，涵盖多种数据流分析任务，结果显示推理模型准确性高但速度慢，通用和代码模型表现接近随机。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言和编程语言中表现优异，但其在数据流推理能力方面缺乏系统评估，因此需要开发基准测试来填补这一空白。

Method: FABLE基准测试结合了八种经典数据流分析任务，并在三个实际领域（烹饪食谱、旅行路线、自动化计划）中实例化，包含2400个问答对。评估了三种LLM：推理模型、通用模型和代码模型。

Result: 推理模型准确性最高但推理速度慢20倍以上，通用和代码模型表现接近随机。

Conclusion: FABLE为系统评估数据流推理提供了首个诊断基准，并为开发具有更强程序理解能力的模型提供了见解。

Abstract: Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [28] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/abs/2505.24260)
*Mingyi He,Yuebing Liang,Shenhao Wang,Yunhan Zheng,Qingyi Wang,Dingyi Zhuang,Li Tian,Jinhua Zhao*

Main category: cs.AI

TL;DR: 提出了一种分步生成式城市设计框架，结合多模态扩散模型与人类专业知识，提升设计过程的适应性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI方法通常采用端到端流程，缺乏对实际设计中迭代性的考虑，且控制有限。

Method: 将设计过程分为三个阶段（路网与土地利用规划、建筑布局规划、详细规划与渲染），利用多模态扩散模型生成初步设计，并由人类设计师审核和优化。

Result: 实验表明，该框架在保真度、合规性和多样性方面优于基线模型和端到端方法。

Conclusion: 多模态扩散模型和分步生成方法有助于保留人类控制权并支持迭代优化，为人机协作的城市设计奠定了基础。

Abstract: Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [29] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/abs/2505.24273)
*Hongyi James Cai,Junlin Wang,Xiaoyin Chen,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: 研究探讨了监督微调（SFT）和强化学习（RL）在大语言模型（LLMs）推理能力中的作用，特别是回溯对推理改进的贡献及其最优使用程度。


<details>
  <summary>Details</summary>
Motivation: 尽管RL能内化搜索策略并支持长链思维（CoT）推理，但回溯的具体贡献及其最优使用方式尚不明确。

Method: 在八个推理任务上系统研究SFT和RL的交互作用，并通过构建合成数据集控制回溯步骤数量进行实验。

Result: 研究发现：1）长CoT与回溯通常能提升RL训练效果；2）更复杂问题需要更多回溯步骤；3）RL更关注结构而非内容正确性。

Conclusion: 研究结果为设计优化LLMs推理训练策略提供了实用见解。

Abstract: Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [30] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
*Yueqi Zhang,Peiwen Yuan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: 论文提出了一种解决人类与AI对话中引用问题的框架，通过引入跨度条件生成和轻量级训练方法QuAda，实现了对引用文本的精准处理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型缺乏显式机制处理对话中的引用文本，限制了其在实际应用中的表现。

Method: 提出了跨度条件生成框架，并开发了QuAda方法，通过动态调整注意力机制来增强或抑制对引用文本的关注。

Result: 实验表明，QuAda适用于多种场景并能泛化到未见主题，提供了一种高效的即插即用解决方案。

Conclusion: QuAda为引用感知对话提供了一种轻量级且有效的解决方案，具有广泛的应用潜力。

Abstract: Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [31] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/abs/2505.24306)
*Kechen Li,Yaotian Tao,Ximing Wen,Quanwei Sun,Zifei Gong,Chang Xu,Xizhe Zhang,Tianbo Ji*

Main category: cs.AI

TL;DR: 论文提出了一种结合传统算法与LLMs的混合提示技术AoT，并通过GridRoute基准评估其在路径规划任务中的表现，结果显示AoT显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs的独立推理能力，而忽略了其与传统算法的协同潜力，论文旨在填补这一空白。

Method: 提出GridRoute基准和混合提示技术AoT，结合传统算法指导LLMs进行路径规划。

Result: AoT显著提升了LLMs在不同规模网格环境中的性能，尤其在复杂环境中表现更优。

Conclusion: AoT为路径规划问题提供了一种有前景的解决方案，展示了LLMs与传统算法结合的价值。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [32] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/abs/2505.24422)
*Zhenghua Pan,Yong Wang*

Main category: cs.AI

TL;DR: 本文探讨了知识中否定的理解与分类，提出了三种否定类型（矛盾否定、对立否定和中介否定），并建立了数学基础SCOI和LCOI，证明了其操作性质和形式推理关系。


<details>
  <summary>Details</summary>
Motivation: 研究知识处理中否定的基本问题，从哲学、逻辑和语言学等多领域分析否定的特性，以区分矛盾和对立概念。

Method: 基于矛盾与对立的区分，提出三种否定类型，并引入SCOI和LCOI作为数学基础，证明其性质和推理关系。

Result: 成功定义了三种否定类型，并建立了SCOI和LCOI的数学框架，证明了其操作性质和形式推理关系。

Conclusion: 通过SCOI和LCOI为知识中的否定提供了系统的数学基础，为未来研究奠定了基础。

Abstract: In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [33] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/abs/2505.24426)
*David Gamez*

Main category: cs.AI

TL;DR: 论文提出了一种基于预测能力的通用智能度量方法，通过预测准确性和环境复杂性来衡量智能。


<details>
  <summary>Details</summary>
Motivation: 当前对智能的定义和度量缺乏共识，无法比较人类、动物和AI的智能水平。

Method: 提出基于预测能力的智能度量方法，结合预测准确性和Kolmogorov复杂性。

Result: 实验验证了该方法在虚拟迷宫和时间序列预测中的可行性。

Conclusion: 该通用智能度量方法为比较人类、动物和AI的智能提供了新起点。

Abstract: Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [34] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/abs/2505.24442)
*Zhentao Xie,Chengcheng Han,Jinxin Shi,Wenjun Cui,Xin Zhao,Xingjiao Wu,Jiabao Zhao*

Main category: cs.AI

TL;DR: 论文提出Residual Mixture-of-Agents (RMoA)，通过残差连接优化多智能体系统的效率和可靠性，减少计算开销和信息损失。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在大型语言模型上表现出强大能力，但仍受限于高计算开销、信息丢失和鲁棒性问题。

Method: 设计基于嵌入的多样性选择机制，引入残差提取和聚合代理，以及自适应终止机制。

Result: RMoA在多个任务上实现最优性能，同时显著降低计算开销。

Conclusion: RMoA通过残差学习和创新机制，有效提升了多智能体系统的效率和可靠性。

Abstract: Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [35] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/abs/2505.24458)
*Tianlong Yu,Chenghang Ye,Zheyu Yang,Ziyi Zhou,Cui Tang,Zui Tao,Jun Zhang,Kailong Wang,Liting Zhou,Yang Yang,Ting Bi*

Main category: cs.AI

TL;DR: SEAR数据集是一个多模态资源，用于研究通过增强现实（AR）和多模态大语言模型（LLM）策划的社会工程（SE）攻击。数据集包含180个标注对话和60名参与者的模拟对抗场景，揭示了SEAR攻击的高效性（如93.3%的钓鱼链接点击率）。


<details>
  <summary>Details</summary>
Motivation: 研究AR和LLM驱动的社会工程攻击的威胁，提供多模态数据以支持防御框架的设计和攻击检测。

Method: 收集60名参与者在模拟对抗场景中的180个标注对话，包括AR捕捉的视觉/音频线索、环境背景和社交媒体资料，辅以主观信任评估。

Result: SEAR攻击表现出高效性（93.3%钓鱼链接点击率，85%通话接受率）和信任劫持能力（76.7%信任提升）。

Conclusion: SEAR数据集为研究AR驱动的社会工程攻击提供了重要资源，支持防御框架设计和多模态对抗操纵的理解，同时确保伦理合规。

Abstract: The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [36] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
*Vasilije Markovic,Lazar Obradovic,Laszlo Hajdu,Jovan Pavlovic*

Main category: cs.AI

TL;DR: 论文研究了将大语言模型（LLMs）与知识图谱（KGs）结合时，系统性超参数优化的重要性，并通过实验展示了针对性调优的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探讨在检索增强生成系统中，超参数优化对性能的影响，填补这一领域的未充分研究空白。

Method: 使用Cognee框架，在三个多跳QA基准数据集上优化与分块、图谱构建、检索和提示相关的参数，并通过多种指标评估性能。

Result: 实验表明，针对性调优能带来显著性能提升，但提升效果因数据集和指标而异，凸显了调优的价值和标准评估的局限性。

Conclusion: 未来进展不仅依赖于架构改进，还需更清晰的优化和评估框架，以应对复杂模块化系统的挑战。

Abstract: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [37] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/abs/2505.24493)
*Xin Jing,Jiadong Wang,Iosif Tsangko,Andreas Triantafyllopoulos,Björn W. Schuller*

Main category: cs.AI

TL;DR: 论文提出使用GPT-4o自动标注语音情感数据，解决了人工标注成本高且不一致的问题，并构建了MELT数据集，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）中人工标注成本高且不一致，而大语言模型（LLMs）在文本标注中表现优异，但其在语音情感数据标注中的应用尚未充分研究。

Method: 利用GPT-4o仅通过文本提示对《老友记》多模态数据集进行标注，生成MELT数据集，并通过微调自监督学习模型验证其效果。

Result: MELT数据集在语音情感识别任务中表现一致且有效，主观实验也显示性能提升。

Conclusion: GPT-4o能够高效生成准确的语音情感标注，MELT数据集为SER研究提供了新的资源。

Abstract: Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [38] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/abs/2505.24597)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.AI

TL;DR: NextLocMoE是一个基于大语言模型和双层次Mixture-of-Experts设计的新框架，用于解决现有方法在捕捉位置多功能语义和用户行为动态性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉真实世界位置的多功能语义和用户群体的异质行为动态。

Method: NextLocMoE采用双层次MoE设计，包括嵌入级别的Location Semantics MoE和Transformer骨干中的Personalized MoE，并结合历史感知路由机制。

Result: 在多个真实城市数据集上的实验表明，NextLocMoE在预测准确性、跨域泛化能力和可解释性方面表现优异。

Conclusion: NextLocMoE通过双层次MoE设计和历史感知路由机制，显著提升了位置预测的性能和适应性。

Abstract: Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [39] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/abs/2505.24601)
*Zekun Wang,Ethan L. Haarer,Nicki Barari,Christopher J. MacLellan*

Main category: cs.AI

TL;DR: 论文提出了一种神经符号对的概念，结合了神经和符号方法，并通过共同的知识表示链接。通过分类网络（一种区分网络）实现，展示了符号方法在少量数据和计算下高效学习分类网络，而神经方法在资源充足时能获得更高准确性。


<details>
  <summary>Details</summary>
Motivation: 结合神经和符号方法的优势，构建一种能够根据需求灵活切换的神经符号对，为未来更深入的神经符号计算集成奠定基础。

Method: 提出分类网络作为知识表示，构建神经符号对，并分别评估符号方法和神经方法在学习和准确性上的表现。

Result: 符号方法在少量数据和计算下更高效，神经方法在资源充足时准确性更高，两者可根据需求互换使用。

Conclusion: 该研究为未来神经符号计算的更深入集成提供了基础，展示了灵活结合两种方法的潜力。

Abstract: We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [40] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/abs/2505.24622)
*Ben Griffin,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 论文提出了一种轻量级集成框架，结合LLM生成的YES/NO问题，构建透明决策系统，显著提升初创公司成功预测的精确度。


<details>
  <summary>Details</summary>
Motivation: 预测初创公司成功需要既准确又可解释的模型，传统方法难以兼顾。

Method: 通过筛选、排序和基于阈值的投票机制，将LLM生成的弱启发式问题集成为强预测器。

Result: 在10%成功率的测试集上，精确度达50%（比随机选择高5倍），加入专家启发式后提升至54%。

Conclusion: 结合LLM推理与人类洞察的简单可解释集成方法，适用于高风险决策领域如VC。

Abstract: Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [41] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/abs/2505.24655)
*Frederike Lübeck,Jonas Wildberger,Frederik Träuble,Maximilian Mordig,Sergios Gatidis,Andreas Krause,Bernhard Schölkopf*

Main category: cs.AI

TL;DR: AdaCVD是一种基于大语言模型的心血管疾病风险预测框架，优于现有模型，能够灵活处理患者信息并适应新人群。


<details>
  <summary>Details</summary>
Motivation: 现有心血管疾病风险预测模型过于简化患者信息，且对分布变化敏感，无法满足临床实际需求。

Method: 基于UK Biobank超过50万参与者的数据，对大语言模型进行微调，开发了AdaCVD框架。

Result: AdaCVD在基准测试中表现优于现有风险评分和标准机器学习方法，并在不同亚组中表现稳健。

Conclusion: AdaCVD为灵活、AI驱动的临床决策支持工具提供了可行路径，适用于动态医疗环境。

Abstract: Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [42] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/abs/2505.24784)
*Conor Heins,Toon Van de Maele,Alexander Tschantz,Hampus Linander,Dimitrije Markovic,Tommaso Salvatori,Corrado Pezzato,Ozan Catal,Ran Wei,Magnus Koudahl,Marco Perin,Karl Friston,Tim Verbelen,Christopher Buckley*

Main category: cs.AI

TL;DR: AXIOM结合了主动推理的核心先验与深度强化学习的泛化能力，提出了一种在低数据环境下高效学习的新架构。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在数据效率上的不足，同时保留主动推理的贝叶斯优势。

Method: 整合对象中心动态的核心先验，在线扩展生成模型结构，并通过贝叶斯模型简化优化。

Result: AXIOM在仅10,000次交互步骤内掌握多种游戏，参数少且无需梯度优化。

Conclusion: AXIOM成功结合了贝叶斯方法的高效性与深度强化学习的泛化能力。

Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [43] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)
*Patrick Tser Jern Kon,Jiachen Liu,Xinyi Zhu,Qiuyi Ding,Jingjia Peng,Jiarong Xing,Yibo Huang,Yiming Qiu,Jayanth Srinivasa,Myungjin Lee,Mosharaf Chowdhury,Matei Zaharia,Ang Chen*

Main category: cs.AI

TL;DR: EXP-Bench是一个新基准，用于评估AI代理在完整AI研究实验中的表现，涵盖假设制定、实验设计、执行和结果分析。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理难以处理复杂、端到端的研究实验，阻碍了AI研究的自动化进展。

Method: 设计半自动化流程，从顶级AI论文中提取实验细节，构建461个任务。

Result: 领先的LLM代理在部分实验环节得分20-35%，但完整实验成功率仅0.5%。

Conclusion: EXP-Bench为未来AI代理提升研究实验能力提供了关键工具。

Abstract: Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [44] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
*Jingyan Shen,Jiarui Yao,Rui Yang,Yifan Sun,Feng Luo,Rui Pan,Tong Zhang,Han Zhao*

Main category: cs.AI

TL;DR: MiCRo是一个两阶段框架，通过上下文感知混合建模和动态路由策略，解决了传统Bradley-Terry模型无法捕捉多样化人类偏好的问题，显著提升了LLM的个性化对齐能力。


<details>
  <summary>Details</summary>
Motivation: 传统Bradley-Terry模型假设全局奖励函数，无法捕捉人类偏好的多样性和异质性，限制了LLM的个性化和多元化对齐。

Method: MiCRo采用两阶段方法：1) 上下文感知混合建模捕捉多样化偏好；2) 动态路由策略根据上下文调整混合权重，实现高效偏好适应。

Result: 实验表明，MiCRo能有效捕捉多样化人类偏好，显著提升下游个性化任务表现。

Conclusion: MiCRo为LLM的个性化对齐提供了一种高效且可扩展的解决方案，无需依赖精细标注。

Abstract: Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [45] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo,Zhaoyi Li,Jiacheng Liu,Jiacheng Cui,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.AI

TL;DR: 论文介绍了Open CaptchaWorld，一个专门用于评估多模态LLM代理在动态CAPTCHA任务中表现的基准平台，结果显示当前代理性能远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: CAPTCHAs是部署网络代理的关键瓶颈，但现有代理在多步交互任务中的能力尚未充分测试。

Method: 提出Open CaptchaWorld基准平台，涵盖20种CAPTCHA类型，并引入CAPTCHA Reasoning Depth指标量化解决步骤。

Result: 人类成功率93.3%，而最佳MLLM代理仅40.0%，显示当前代理性能不足。

Conclusion: Open CaptchaWorld是诊断多模态代理局限性的重要工具，有助于开发更强大的推理系统。

Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [46] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga,Sergio Flesca,Filippo Furfaro,Luigi Pontieri,Francesco Scala*

Main category: cs.AI

TL;DR: 论文提出了一种结合神经符号方法的高效数据处理方法，用于解决事件与业务活动映射不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 现代企业和组织需要监控和分析过程痕迹，但在事件与业务活动映射不确定时，传统方法可能效率低下且计算量大。

Method: 采用神经符号方法，结合序列标记模型和基于AAF的推理器，优化事件解释。

Result: 实验证明该方法能有效利用有限标注数据，并通过先验知识提升性能。

Conclusion: 该方法在数据标注和计算资源受限的场景下具有显著优势，支持绿色AI发展。

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
*Jianyang Gu,Samuel Stevens,Elizabeth G Campolongo,Matthew J Thompson,Net Zhang,Jiaman Wu,Andrei Kopanev,Zheda Mai,Alexander E. White,James Balhoff,Wasila Dahdul,Daniel Rubenstein,Hilmar Lapp,Tanya Berger-Wolf,Wei-Lun Chao,Yu Su*

Main category: cs.CV

TL;DR: 论文通过大规模对比视觉语言训练，在生物视觉模型中发现了涌现行为，并利用TreeOfLife-200M数据集训练BioCLIP 2模型，展示了其在多种生物视觉任务中的高准确性。


<details>
  <summary>Details</summary>
Motivation: 探索大规模基础模型在生物视觉领域的涌现行为，验证其能否超越初始训练目标。

Method: 使用TreeOfLife-200M数据集训练BioCLIP 2模型，通过对比学习目标区分不同物种。

Result: BioCLIP 2在栖息地分类和性状预测等任务中表现优异，其嵌入空间展现了与功能和生态意义对齐的涌现特性。

Conclusion: 大规模训练数据促进了生物意义嵌入空间的形成，涌现特性随数据规模增加而显著。

Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors,
learning new capabilities beyond their initial training objectives. We find
such emergent behaviors in biological vision models via large-scale contrastive
vision-language training. To achieve this, we first curate TreeOfLife-200M,
comprising 214 million images of living organisms, the largest and most diverse
biological organism image dataset to date. We then train BioCLIP 2 on
TreeOfLife-200M to distinguish different species. Despite the narrow training
objective, BioCLIP 2 yields extraordinary accuracy when applied to various
biological visual tasks such as habitat classification and trait prediction. We
identify emergent properties in the learned embedding space of BioCLIP 2. At
the inter-species level, the embedding distribution of different species aligns
closely with functional and ecological meanings (e.g., beak sizes and
habitats). At the intra-species level, instead of being diminished, the
intra-species variations (e.g., life stages and sexes) are preserved and better
separated in subspaces orthogonal to inter-species distinctions. We provide
formal proof and analyses to explain why hierarchical supervision and
contrastive objectives encourage these emergent properties. Crucially, our
results reveal that these properties become increasingly significant with
larger-scale training data, leading to a biologically meaningful embedding
space.

</details>


### [48] [Generating Fit Check Videos with a Handheld Camera](https://arxiv.org/abs/2505.23886)
*Bowei Chen,Brian Curless,Ira Kemelmacher-Shlizerman,Steven M. Seitz*

Main category: cs.CV

TL;DR: 提出了一种通过手持移动设备捕捉全身视频的便捷方法，仅需两张静态照片和IMU运动参考即可合成类似动作的视频。


<details>
  <summary>Details</summary>
Motivation: 传统全身视频捕捉需要固定摄像头和精心构图，不便且耗时。

Method: 基于视频扩散模型，结合无参数帧生成策略和多参考注意力机制，利用前后自拍照合成视频。

Result: 实现了新场景中的渲染，光照和阴影一致，视频生成更真实。

Conclusion: 该方法为便捷的全身视频捕捉提供了有效解决方案。

Abstract: Self-captured full-body videos are popular, but most deployments require
mounted cameras, carefully-framed shots, and repeated practice. We propose a
more convenient solution that enables full-body video capture using handheld
mobile devices. Our approach takes as input two static photos (front and back)
of you in a mirror, along with an IMU motion reference that you perform while
holding your mobile phone, and synthesizes a realistic video of you performing
a similar target motion. We enable rendering into a new scene, with consistent
illumination and shadows. We propose a novel video diffusion-based model to
achieve this. Specifically, we propose a parameter-free frame generation
strategy, as well as a multi-reference attention mechanism, that effectively
integrate appearance information from both the front and back selfies into the
video diffusion model. Additionally, we introduce an image-based fine-tuning
strategy to enhance frame sharpness and improve the generation of shadows and
reflections, achieving a more realistic human-scene composition.

</details>


### [49] [Cora: Correspondence-aware image editing using few step diffusion](https://arxiv.org/abs/2505.23907)
*Amirhossein Almohammadi,Aryan Mikaeili,Sauradip Nag,Negar Hassanpour,Andrea Tagliasacchi,Ali Mahdavi-Amiri*

Main category: cs.CV

TL;DR: Cora是一种基于扩散模型的图像编辑框架，通过语义对应和噪声校正实现高质量编辑，解决了现有方法在结构变化和纹理保留上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在处理非刚性变形、对象修改或内容生成时存在纹理不相关或关键属性丢失的问题。

Method: Cora引入语义对应的噪声校正和插值注意力图，通过结构对齐实现纹理传递和新内容生成。

Result: 实验表明，Cora在保持结构、纹理和身份方面优于其他方法，用户研究也证实其效果更优。

Conclusion: Cora在复杂图像编辑任务中表现出色，提供了内容生成与保留的平衡控制。

Abstract: Image editing is an important task in computer graphics, vision, and VFX,
with recent diffusion-based methods achieving fast and high-quality results.
However, edits requiring significant structural changes, such as non-rigid
deformations, object modifications, or content generation, remain challenging.
Existing few step editing approaches produce artifacts such as irrelevant
texture or struggle to preserve key attributes of the source image (e.g.,
pose). We introduce Cora, a novel editing framework that addresses these
limitations by introducing correspondence-aware noise correction and
interpolated attention maps. Our method aligns textures and structures between
the source and target images through semantic correspondence, enabling accurate
texture transfer while generating new content when necessary. Cora offers
control over the balance between content generation and preservation. Extensive
experiments demonstrate that, quantitatively and qualitatively, Cora excels in
maintaining structure, textures, and identity across diverse edits, including
pose changes, object addition, and texture refinements. User studies confirm
that Cora delivers superior results, outperforming alternatives.

</details>


### [50] [Interactive Video Generation via Domain Adaptation](https://arxiv.org/abs/2505.24253)
*Ishaan Rawal,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种改进交互式视频生成（IVG）的方法，通过掩码归一化和时间内在扩散先验解决现有方法中的感知质量下降和初始化差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本条件的扩散模型在交互式视频生成中难以控制运动元素（如物体轨迹），且现有训练无关方法通过注意力掩码引导轨迹会导致感知质量下降。

Method: 提出掩码归一化以缓解注意力掩码引起的内部协变量偏移，并引入时间内在扩散先验以解决初始化噪声与IVG条件不匹配的问题。

Result: 实验表明，掩码归一化和时间内在去噪在感知质量和轨迹控制上优于现有IVG技术。

Conclusion: 本文的方法显著提升了交互式视频生成的性能，为未来研究提供了新思路。

Abstract: Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.

</details>


### [51] [Representational Difference Explanations](https://arxiv.org/abs/2505.23917)
*Neehar Kondapaneni,Oisin Mac Aodha,Pietro Perona*

Main category: cs.CV

TL;DR: 提出了一种名为RDX的方法，用于发现和可视化两种学习表示之间的差异，支持更直接和可解释的模型比较。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释AI（XAI）方法在模型比较方面效果不佳，科学分析中比较是基础，但缺乏有效工具。

Method: 提出Representational Differences Explanations（RDX）方法，通过比较已知概念差异的模型验证其有效性。

Result: RDX在ImageNet和iNaturalist数据集上成功揭示了有意义的表示差异和数据的微妙模式，优于现有XAI技术。

Conclusion: RDX填补了机器学习中模型比较工具的空白，提供了一种有效且可解释的对比模型表示的方法。

Abstract: We propose a method for discovering and visualizing the differences between
two learned representations, enabling more direct and interpretable model
comparisons. We validate our method, which we call Representational Differences
Explanations (RDX), by using it to compare models with known conceptual
differences and demonstrate that it recovers meaningful distinctions where
existing explainable AI (XAI) techniques fail. Applied to state-of-the-art
models on challenging subsets of the ImageNet and iNaturalist datasets, RDX
reveals both insightful representational differences and subtle patterns in the
data. Although comparison is a cornerstone of scientific analysis, current
tools in machine learning, namely post hoc XAI methods, struggle to support
model comparison effectively. Our work addresses this gap by introducing an
effective and explainable tool for contrasting model representations.

</details>


### [52] [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/abs/2505.23922)
*David Ma,Huaqing Yuan,Xingjian Wang,Qianbo Zang,Tianci Liu,Xinyang He,Yanbin Wei,Jiawei Guo,Ni Jiahui,Zhenzhu Yang,Meng Cao,Shanghaoran Quan,Yizhi Li,Wangchunshu Zhou,Jiaheng Liu,Wenhao Huang,Ge Zhang,Shiwen Ni,Xiaojie Jin*

Main category: cs.CV

TL;DR: ScaleLong是一个新的基准测试，用于评估多模态大语言模型（MLLM）在长视频理解中的多时间尺度表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能直接比较模型在同一视频内容中不同时间尺度（如秒、分钟、小时）上的表现，ScaleLong旨在解决这一问题。

Method: ScaleLong通过在同一视频中嵌入针对四个时间尺度（秒、分钟、小时等）的问题，设计了一个多时间尺度的评估框架。

Result: 评估23个MLLM显示，模型表现呈U型曲线，最短和最长时间尺度表现较好，中间尺度表现较差。视觉标记容量的增加能提升所有时间尺度的推理能力。

Conclusion: ScaleLong为长视频理解提供了一个细粒度的多时间尺度基准，推动了MLLM能力的进步。

Abstract: Although long-video understanding demands that models capture hierarchical
temporal information -- from clip (seconds) and shot (tens of seconds) to event
(minutes) and story (hours) -- existing benchmarks either neglect this
multi-scale design or scatter scale-specific questions across different videos,
preventing direct comparison of model performance across timescales on the same
content. To address this, we introduce ScaleLong, the first benchmark to
disentangle these factors by embedding questions targeting four hierarchical
timescales -- clip (seconds), shot (tens of seconds), event (minutes), and
story (hours) -- all within the same video content. This within-content
multi-timescale questioning design enables direct comparison of model
performance across timescales on identical videos. ScaleLong features 269 long
videos (avg.\ 86\,min) from 5 main categories and 36 sub-categories, with 4--8
carefully designed questions, including at least one question for each
timescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with
higher accuracy at the shortest and longest timescales and a dip at
intermediate levels. Furthermore, ablation studies show that increased visual
token capacity consistently enhances reasoning across all timescales. ScaleLong
offers a fine-grained, multi-timescale benchmark for advancing MLLM
capabilities in long-video understanding. The code and dataset are available
https://github.com/multimodal-art-projection/ScaleLong.

</details>


### [53] [Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts](https://arxiv.org/abs/2505.23926)
*Xuweiyi Chen,Wentao Zhou,Aruni RoyChowdhury,Zezhou Cheng*

Main category: cs.CV

TL;DR: Point-MoE提出了一种混合专家架构，用于解决3D点云理解中的跨域泛化问题，无需依赖域标签即可自动分配专家，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据因来源多样（如不同传感器和场景）导致域异质性，传统方法难以实现统一模型的规模化训练。

Method: 提出Point-MoE架构，采用简单的top-k路由策略，自动分配专家处理不同域数据。

Result: Point-MoE在混合域数据上表现优于传统方法，并能更好地泛化到未见域。

Conclusion: Point-MoE为3D理解提供了一种可扩展的解决方案，通过模型自动发现数据中的结构，而非依赖人工标注或域监督。

Abstract: While scaling laws have transformed natural language processing and computer
vision, 3D point cloud understanding has yet to reach that stage. This can be
attributed to both the comparatively smaller scale of 3D datasets, as well as
the disparate sources of the data itself. Point clouds are captured by diverse
sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,
outdoor), each introducing unique scanning patterns, sampling densities, and
semantic biases. Such domain heterogeneity poses a major barrier towards
training unified models at scale, especially under the realistic constraint
that domain labels are typically inaccessible at inference time. In this work,
we propose Point-MoE, a Mixture-of-Experts architecture designed to enable
large-scale, cross-domain generalization in 3D perception. We show that
standard point cloud backbones degrade significantly in performance when
trained on mixed-domain data, whereas Point-MoE with a simple top-k routing
strategy can automatically specialize experts, even without access to domain
labels. Our experiments demonstrate that Point-MoE not only outperforms strong
multi-domain baselines but also generalizes better to unseen domains. This work
highlights a scalable path forward for 3D understanding: letting the model
discover structure in diverse 3D data, rather than imposing it via manual
curation or domain supervision.

</details>


### [54] [Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review](https://arxiv.org/abs/2505.23952)
*Adriano Fragomeni,Dima Damen,Michael Wray*

Main category: cs.CV

TL;DR: 本文综述了81篇关于文本到视频检索的研究，重点探讨了利用辅助信息（如视觉属性、时空上下文和文本描述）提升检索性能的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的文本到视频检索方法仅依赖视频和文本模态的对齐，但辅助信息可以弥补模态间的语义鸿沟，提升检索效果。

Method: 综述分析了81篇论文，总结了利用辅助信息的方法，包括视觉属性、时空上下文和文本描述等。

Result: 研究展示了辅助信息在提升检索性能上的显著效果，并列举了基准数据集上的最新成果。

Conclusion: 未来研究应进一步探索如何利用辅助信息优化检索性能，并提出了可能的研究方向。

Abstract: Text-to-Video (T2V) retrieval aims to identify the most relevant item from a
gallery of videos based on a user's text query. Traditional methods rely solely
on aligning video and text modalities to compute the similarity and retrieve
relevant items. However, recent advancements emphasise incorporating auxiliary
information extracted from video and text modalities to improve retrieval
performance and bridge the semantic gap between these modalities. Auxiliary
information can include visual attributes, such as objects; temporal and
spatial context; and textual descriptions, such as speech and rephrased
captions. This survey comprehensively reviews 81 research papers on
Text-to-Video retrieval that utilise such auxiliary information. It provides a
detailed analysis of their methodologies; highlights state-of-the-art results
on benchmark datasets; and discusses available datasets and their auxiliary
information. Additionally, it proposes promising directions for future
research, focusing on different ways to further enhance retrieval performance
using this information.

</details>


### [55] [MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification](https://arxiv.org/abs/2505.23961)
*Rafi Hassan Chowdhury,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 提出了一种轻量级的Vision Transformer方法，用于芒果叶病害分类，计算高效且适用于低端设备，准确率达99.43%。


<details>
  <summary>Details</summary>
Motivation: 芒果种植因病害导致经济损失，现有深度学习方法计算成本高，需设计高效解决方案。

Method: 采用轻量级Vision Transformer和自注意力机制，结合运行时增强技术。

Result: 在MangoLeafBD数据集上达到99.43%准确率，模型尺寸、参数量和计算量均优于现有方法。

Conclusion: 该方法高效且性能优越，适用于芒果叶病害分类的实际应用。

Abstract: Ensuring food safety is critical due to its profound impact on public health,
economic stability, and global supply chains. Cultivation of Mango, a major
agricultural product in several South Asian countries, faces high financial
losses due to different diseases, affecting various aspects of the entire
supply chain. While deep learning-based methods have been explored for mango
leaf disease classification, there remains a gap in designing solutions that
are computationally efficient and compatible with low-end devices. In this
work, we propose a lightweight Vision Transformer-based pipeline with a
self-attention mechanism to classify mango leaf diseases, achieving
state-of-the-art performance with minimal computational overhead. Our approach
leverages global attention to capture intricate patterns among disease types
and incorporates runtime augmentation for enhanced performance. Evaluation on
the MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing
methods in terms of model size, parameter count, and FLOPs count.

</details>


### [56] [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/abs/2505.23977)
*Yichen Feng,Zhangchen Xu,Fengqing Jiang,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.CV

TL;DR: 论文提出了一种名为VisualSphinx的大规模合成视觉逻辑推理训练数据，通过规则到图像的合成流程生成数据，提升了视觉语言模型在逻辑推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）缺乏大规模且结构良好的训练数据，限制了其在多模态推理和逻辑一致性决策中的表现。

Method: 提出VisualSphinx数据集，采用规则到图像的合成流程，从种子问题中提取并扩展谜题规则，生成具有基础答案的图像合成代码。

Result: 实验表明，使用VisualSphinx训练的VLM在逻辑推理任务中表现更优，且其推理能力还能提升其他任务（如代数、算术和几何推理）的表现。

Conclusion: VisualSphinx填补了VLM训练数据的空白，显著提升了模型的逻辑推理能力，并具有广泛的应用潜力。

Abstract: Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

</details>


### [57] [DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets](https://arxiv.org/abs/2505.23980)
*Bayu Adhi Tama,Mansa Krishna,Homayra Alam,Mostafa Cham,Omar Faruque,Gong Cheng,Jianwu Wang,Mathieu Morlighem,Vandana Janeja*

Main category: cs.CV

TL;DR: DeepTopoNet是一种深度学习框架，通过动态损失平衡机制整合雷达和BedMachine数据，提高格陵兰冰下地形的重建精度。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰下地形的准确重建对预测冰盖质量损失和海平面上升至关重要，但现有观测数据稀疏且复杂，导致模型不确定性高。

Method: 提出DeepTopoNet框架，结合雷达和BedMachine数据，利用动态损失平衡机制和CNN架构，优化地形重建。

Result: 在Upernavik Isstrøm区域测试中，模型表现优于基线方法，实现了高精度的冰下地形重建。

Conclusion: DeepTopoNet展示了深度学习在填补观测空白中的潜力，为冰下地形推断提供了可扩展的高效解决方案。

Abstract: Understanding Greenland's subglacial topography is critical for projecting
the future mass loss of the ice sheet and its contribution to global sea-level
rise. However, the complex and sparse nature of observational data,
particularly information about the bed topography under the ice sheet,
significantly increases the uncertainty in model projections. Bed topography is
traditionally measured by airborne ice-penetrating radar that measures the ice
thickness directly underneath the aircraft, leaving data gap of tens of
kilometers in between flight lines. This study introduces a deep learning
framework, which we call as DeepTopoNet, that integrates radar-derived ice
thickness observations and BedMachine Greenland data through a novel dynamic
loss-balancing mechanism. Among all efforts to reconstruct bed topography,
BedMachine has emerged as one of the most widely used datasets, combining mass
conservation principles and ice thickness measurements to generate
high-resolution bed elevation estimates. The proposed loss function adaptively
adjusts the weighting between radar and BedMachine data, ensuring robustness in
areas with limited radar coverage while leveraging the high spatial resolution
of BedMachine predictions i.e. bed estimates. Our approach incorporates
gradient-based and trend surface features to enhance model performance and
utilizes a CNN architecture designed for subgrid-scale predictions. By
systematically testing on the Upernavik Isstr{\o}m) region, the model achieves
high accuracy, outperforming baseline methods in reconstructing subglacial
terrain. This work demonstrates the potential of deep learning in bridging
observational gaps, providing a scalable and efficient solution to inferring
subglacial topography.

</details>


### [58] [DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment](https://arxiv.org/abs/2505.24002)
*Vaishnav Ramesh,Junliang Liu,Haining Wang,Md Jahidul Islam*

Main category: cs.CV

TL;DR: 论文提出了一种结合深度引导跨注意力和Transformer-CNN桥接的NR-IQA方法，显著提升了图像质量评估的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决无参考图像质量评估（NR-IQA）中因缺乏人类主观感知导致的泛化能力不足问题。

Method: 提出Depth-CAR机制和TCB模块，融合深度信息与多模态特征，优化特征选择。

Result: DGIQA模型在合成和真实数据集上达到SOTA性能，尤其在跨数据集和自然失真评估中表现突出。

Conclusion: Depth-CAR和TCB的结合显著提升了NR-IQA的泛化能力和评估效果。

Abstract: A long-held challenge in no-reference image quality assessment (NR-IQA)
learning from human subjective perception is the lack of objective
generalization to unseen natural distortions. To address this, we integrate a
novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which
distills scene depth and spatial features into a structure-aware representation
for improved NR-IQA. This brings in the knowledge of object saliency and
relative contrast of the scene for more discriminative feature learning.
Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse
high-level global contextual dependencies from a transformer backbone with
local spatial features captured by a set of hierarchical CNN (convolutional
neural network) layers. We implement TCB and Depth-CAR as multimodal
attention-based projection functions to select the most informative features,
which also improve training time and inference efficiency. Experimental results
demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)
performance on both synthetic and authentic benchmark datasets. More
importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well
as in assessing natural image distortions such as low-light effects, hazy
conditions, and lens flares.

</details>


### [59] [Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model](https://arxiv.org/abs/2505.24007)
*Nokimul Hasan Arif,Shadman Rabby,Md Hefzul Hossain Papon,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 论文提出了一种基于集成预处理的框架，通过自适应选择输入过滤方法（降噪、边缘增强或原始输入），显著减少大语言模型（LLMs）的视觉幻觉问题，无需修改模型架构或训练流程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在视觉输入不一致时产生的幻觉问题影响其可靠性，当前研究多关注事后修正或模型微调，而预处理技术的研究较少。

Method: 提出了一种集成预处理框架，根据问题类型自适应选择输入过滤方法（NR、EE或org），以减少幻觉。

Result: 在`HaloQuest'数据集上，该方法将幻觉率降低了44.3%，通过SelfCheckGPT的NLI分数衡量。

Conclusion: 研究表明，智能输入预处理可显著提升LLM响应的真实性，为更可靠的多模态系统提供了新思路。

Abstract: Visual hallucinations in Large Language Models (LLMs), where the model
generates responses that are inconsistent with the visual input, pose a
significant challenge to their reliability, particularly in contexts where
precise and trustworthy outputs are critical. Current research largely
emphasizes post-hoc correction or model-specific fine-tuning strategies, with
limited exploration of preprocessing techniques to address hallucination issues
at the input stage. This study presents a novel ensemble-based preprocessing
framework that adaptively selects the most appropriate filtering approach --
noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the
type of question posed, resulting into reduced hallucination without requiring
any modifications to the underlying model architecture or training pipeline.
Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal
reasoning on visually complex inputs, our method achieves a 44.3% reduction in
hallucination rates, as measured by Natural Language Inference (NLI) scores
using SelfCheckGPT. This demonstrates that intelligent input conditioning alone
can significantly enhance factual grounding in LLM responses. The findings
highlight the importance of adaptive preprocessing techniques in mitigating
hallucinations, paving the way for more reliable multimodal systems capable of
addressing real-world challenges.

</details>


### [60] [Multi-Group Proportional Representation for Text-to-Image Models](https://arxiv.org/abs/2505.24023)
*Sangwon Jung,Alex Oesterling,Claudio Mayrink Verdun,Sajani Vithana,Taesup Moon,Flavio P. Calmon*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过多群体比例表示（MPR）指标，系统衡量文本到图像（T2I）生成模型对交叉群体的代表性，并开发了优化算法以改善生成平衡性。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能加剧刻板印象或忽视少数群体，但目前缺乏系统方法来衡量和控制生成图像中的代表性危害。

Method: 引入MPR指标评估生成图像中不同群体的最差偏差，并开发算法优化模型以平衡生成。

Result: 实验表明MPR能有效衡量交叉群体的代表性，且作为训练目标可改善生成平衡性而不降低质量。

Conclusion: MPR为衡量和优化T2I模型的代表性提供了灵活且有效的工具。

Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from
textual descriptions. As these models proliferate, they expose new concerns
about their ability to represent diverse demographic groups, propagate
stereotypes, and efface minority populations. Despite growing attention to the
"safe" and "responsible" design of artificial intelligence (AI), there is no
established methodology to systematically measure and control representational
harms in image generation. This paper introduces a novel framework to measure
the representation of intersectional groups in images generated by T2I models
by applying the Multi-Group Proportional Representation (MPR) metric. MPR
evaluates the worst-case deviation of representation statistics across given
population groups in images produced by a generative model, allowing for
flexible and context-specific measurements based on user requirements. We also
develop an algorithm to optimize T2I models for this metric. Through
experiments, we demonstrate that MPR can effectively measure representation
statistics across multiple intersectional groups and, when used as a training
objective, can guide models toward a more balanced generation across
demographic groups while maintaining generation quality.

</details>


### [61] [DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/abs/2505.24025)
*Chenbin Pan,Wenbin He,Zhengzhong Tu,Liu Ren*

Main category: cs.CV

TL;DR: 论文提出了DINO-R1，首次尝试通过强化学习提升视觉基础模型的上下文推理能力，并引入GRQO训练策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的推理能力已通过强化学习得到显著提升，但视觉基础模型（如DINO系列）的推理能力尚未充分探索。

Method: 提出DINO-R1，采用GRQO训练策略，结合KL正则化稳定训练，并整合视觉提示编码器和查询选择机制。

Result: 在COCO、LVIS和ODinW数据集上，DINO-R1显著优于监督微调基线，表现出强大的泛化能力。

Conclusion: DINO-R1为视觉基础模型的推理能力提供了有效解决方案，展示了强化学习在视觉任务中的潜力。

Abstract: The recent explosive interest in the reasoning capabilities of large language
models, such as DeepSeek-R1, has demonstrated remarkable success through
reinforcement learning-based fine-tuning frameworks, exemplified by methods
like Group Relative Policy Optimization (GRPO). However, such reasoning
abilities remain underexplored and notably absent in vision foundation models,
including representation models like the DINO series. In this work, we propose
\textbf{DINO-R1}, the first such attempt to incentivize visual in-context
reasoning capabilities of vision foundation models using reinforcement
learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query
Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly
designed for query-based representation models, which computes query-level
rewards based on group-normalized alignment quality. We also apply
KL-regularization to stabilize the objectness distribution to reduce the
training instability. This joint optimization enables dense and expressive
supervision across queries while mitigating overfitting and distributional
drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models
that integrate a visual prompt encoder and a visual-guided query selection
mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that
DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving
strong generalization in both open-vocabulary and closed-set visual prompting
scenarios.

</details>


### [62] [MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking](https://arxiv.org/abs/2505.24026)
*Numair Nadeem,Muhammad Hamza Asad,Saeed Anwar,Abdul Bais*

Main category: cs.CV

TL;DR: MaskAdapt通过结合RGB图像和深度数据，提出了一种新的无监督域适应方法，显著提高了作物与杂草的语义分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖像素级标注且难以适应新领域，尤其是作物与杂草视觉混淆时。

Method: 整合RGB与深度数据，利用深度梯度解决纹理模糊，并通过几何感知掩码策略增强空间上下文学习。

Result: 在真实农业数据集上，MaskAdapt优于现有方法，分割mIOU显著提升。

Conclusion: MaskAdapt为农业场景中的语义分割提供了一种高效且鲁棒的解决方案。

Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm
management; however, most existing methods depend on labor intensive
pixel-level annotations. A further challenge arises when models trained on one
field (source domain) fail to generalize to new fields (target domain) due to
domain shifts, such as variations in lighting, camera setups, soil composition,
and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by
enabling adaptation without target-domain labels, but current UDA methods
struggle with occlusions and visual blending between crops and weeds, leading
to misclassifications in real-world conditions. To overcome these limitations,
we introduce MaskAdapt, a novel approach that enhances segmentation accuracy
through multimodal contextual learning by integrating RGB images with features
derived from depth data. By computing depth gradients from depth maps, our
method captures spatial transitions that help resolve texture ambiguities.
These gradients, through a cross-attention mechanism, refines RGB feature
representations, resulting in sharper boundary delineation. In addition, we
propose a geometry-aware masking strategy that applies horizontal, vertical,
and stochastic masks during training. This encourages the model to focus on the
broader spatial context for robust visual recognition. Evaluations on real
agricultural datasets demonstrate that MaskAdapt consistently outperforms
existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation
mean Intersection over Union (mIOU) across diverse field conditions.

</details>


### [63] [SIM: A mapping framework for built environment auditing based on street view imagery](https://arxiv.org/abs/2505.24076)
*Huan Ning,Zhenlong Li,Manzhu Yu,Wenpeng Yin*

Main category: cs.CV

TL;DR: 论文提出了一种开源街景映射框架，通过三种管道自动测量和映射目标对象，提升建筑环境审计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统建筑环境审计依赖耗时且昂贵的人工调查，而基于街景图像和深度学习的自动化方法缺乏通用框架。

Method: 开发了开源街景映射框架，包含三种管道：地面物体宽度测量、已知尺寸物体的3D定位和直径测量。

Result: 通过道路宽度测量、停车标志定位和行道树直径测量三个案例展示了框架的实用性。

Conclusion: 该框架为建筑环境审计提供了自动化工具，显著提升了生产力和准确性。

Abstract: Built environment auditing refers to the systematic documentation and
assessment of urban and rural spaces' physical, social, and environmental
characteristics, such as walkability, road conditions, and traffic lights. It
is used to collect data for the evaluation of how built environments impact
human behavior, health, mobility, and overall urban functionality.
Traditionally, built environment audits were conducted using field surveys and
manual observations, which were time-consuming and costly. The emerging street
view imagery, e.g., Google Street View, has become a widely used data source
for conducting built environment audits remotely. Deep learning and computer
vision techniques can extract and classify objects from street images to
enhance auditing productivity. Before meaningful analysis, the detected objects
need to be geospatially mapped for accurate documentation. However, the mapping
methods and tools based on street images are underexplored, and there are no
universal frameworks or solutions yet, imposing difficulties in auditing the
street objects. In this study, we introduced an open source street view mapping
framework, providing three pipelines to map and measure: 1) width measurement
for ground objects, such as roads; 2) 3D localization for objects with a known
dimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,
street trees). These pipelines can help researchers, urban planners, and other
professionals automatically measure and map target objects, promoting built
environment auditing productivity and accuracy. Three case studies, including
road width measurement, stop sign localization, and street tree diameter
measurement, are provided in this paper to showcase pipeline usage.

</details>


### [64] [ComposeAnything: Composite Object Priors for Text-to-Image Generation](https://arxiv.org/abs/2505.24086)
*Zeeshan Khan,Shizhe Chen,Cordelia Schmid*

Main category: cs.CV

TL;DR: ComposeAnything 是一个新框架，通过生成 2.5D 语义布局和改进扩散模型的初始化，提升复杂文本到图像生成的构图能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像（T2I）模型在处理复杂对象排列时表现不佳，尤其是 3D 定位和构图质量。

Method: 利用 LLM 生成 2.5D 语义布局（含深度信息），并以此作为扩散模型的初始化先验，通过对象先验强化和空间控制去噪改进生成。

Result: 在 T2I-CompBench 和 NSR-1K 基准测试中表现优于现有方法，尤其在 2D/3D 空间排列和高对象数量场景下。

Conclusion: ComposeAnything 能够生成高质量且忠实于文本的图像，显著提升了复杂构图的生成能力。

Abstract: Generating images from text involving complex and novel object arrangements
remains a significant challenge for current text-to-image (T2I) models.
Although prior layout-based methods improve object arrangements using spatial
constraints with 2D layouts, they often struggle to capture 3D positioning and
sacrifice quality and coherence. In this work, we introduce ComposeAnything, a
novel framework for improving compositional image generation without retraining
existing T2I models. Our approach first leverages the chain-of-thought
reasoning abilities of LLMs to produce 2.5D semantic layouts from text,
consisting of 2D object bounding boxes enriched with depth information and
detailed captions. Based on this layout, we generate a spatial and depth aware
coarse composite of objects that captures the intended composition, serving as
a strong and interpretable prior that replaces stochastic noise initialization
in diffusion-based T2I models. This prior guides the denoising process through
object prior reinforcement and spatial-controlled denoising, enabling seamless
generation of compositional objects and coherent backgrounds, while allowing
refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art
methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D
spatial arrangements, high object counts, and surreal compositions. Human
evaluations further demonstrate that our model generates high-quality images
with compositions that faithfully reflect the text.

</details>


### [65] [Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors](https://arxiv.org/abs/2505.24103)
*Peiran Xu,Yadong Mu*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督功能定位方法，通过伪标签和基础模型改进功能学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于类激活图，适用于语义分割但不适合定位动作和功能，因此需要更有效的方法。

Method: 利用基础模型生成伪标签，并引入标签精炼、细粒度特征对齐和轻量推理模块。

Result: 实验表明，模型性能显著优于现有方法。

Conclusion: 通过结合基础模型的语义知识，成功缩小了对象与动作之间的差距。

Abstract: In this work, we focus on the task of weakly supervised affordance grounding,
where a model is trained to identify affordance regions on objects using
human-object interaction images and egocentric object images without dense
labels. Previous works are mostly built upon class activation maps, which are
effective for semantic segmentation but may not be suitable for locating
actions and functions. Leveraging recent advanced foundation models, we develop
a supervised training pipeline based on pseudo labels. The pseudo labels are
generated from an off-the-shelf part segmentation model, guided by a mapping
from affordance to part names. Furthermore, we introduce three key enhancements
to the baseline model: a label refining stage, a fine-grained feature alignment
process, and a lightweight reasoning module. These techniques harness the
semantic knowledge of static objects embedded in off-the-shelf foundation
models to improve affordance learning, effectively bridging the gap between
objects and actions. Extensive experiments demonstrate that the performance of
the proposed model has achieved a breakthrough improvement over existing
methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .

</details>


### [66] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/abs/2505.24108)
*Alina Devkota,Annahita Amireskandari,Joel Palko,Shyam Thakkar,Donald Adjeroh,Xiajun Jiang,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.CV

TL;DR: 提出了一种联邦学习框架，用于训练胃肠内窥镜成像的基础模型，解决了医疗数据隐私问题，并在多个下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗数据隐私限制导致基础模型训练困难，联邦学习提供了一种解决方案。

Method: 采用联邦学习框架，评估多种算法在无任务标签情况下的适用性，实验涵盖同质和异质设置。

Result: 训练的基础模型在分类、检测和分割任务中均表现优异。

Conclusion: 该联邦学习方法在保护隐私的同时有效提升了基础模型的性能。

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [67] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/abs/2505.24120)
*Ai Jian,Weijie Qiu,Xiaokun Wang,Peiyu Wang,Yunzhuo Hao,Jiangbo Pei,Yichen Wei,Yi Peng,Xuchen Song*

Main category: cs.CV

TL;DR: CSVQA是一个专门用于评估科学推理能力的多模态基准测试，包含1,378个跨STEM领域的问题-答案对，强调领域知识和视觉证据整合。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准测试缺乏对科学推理能力的评估，CSVQA填补了这一空白。

Method: 设计了包含真实科学内容的视觉问答基准，并提出了严格的评估协议，验证模型预测是否基于有效推理步骤。

Result: 对15个VLM的评估显示性能差异显著，最高准确率仅49.6%。

Conclusion: CSVQA揭示了VLM在科学推理能力上的不足，呼吁进一步研究提升。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in
multimodal understanding, yet their capabilities for scientific reasoning
remains inadequately assessed. Current multimodal benchmarks predominantly
evaluate generic image comprehension or text-driven reasoning, lacking
authentic scientific contexts that require domain-specific knowledge
integration with visual evidence analysis. To fill this gap, we present CSVQA,
a diagnostic multimodal benchmark specifically designed for evaluating
scientific reasoning through domain-grounded visual question answering.Our
benchmark features 1,378 carefully constructed question-answer pairs spanning
diverse STEM disciplines, each demanding domain knowledge, integration of
visual evidence, and higher-order reasoning. Compared to prior multimodal
benchmarks, CSVQA places greater emphasis on real-world scientific content and
complex reasoning.We additionally propose a rigorous evaluation protocol to
systematically assess whether model predictions are substantiated by valid
intermediate reasoning steps based on curated explanations. Our comprehensive
evaluation of 15 VLMs on this benchmark reveals notable performance
disparities, as even the top-ranked proprietary model attains only 49.6\%
accuracy.This empirical evidence underscores the pressing need for advancing
scientific reasoning capabilities in VLMs. Our CSVQA is released at
https://huggingface.co/datasets/Skywork/CSVQA.

</details>


### [68] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/abs/2505.24139)
*Yichen Xie,Runsheng Xu,Tong He,Jyh-Jing Hwang,Katie Luo,Jingwei Ji,Hubert Lin,Letian Chen,Yiren Lu,Zhaoqi Leng,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: S4-Driver是一种基于多模态大语言模型的自监督运动规划算法，通过稀疏体积策略将2D视觉表示转换为3D空间，无需微调视觉编码器，性能优于现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的端到端运动规划方法通常在2D图像空间预训练，而非自动驾驶所需的3D空间，导致性能不足。

Method: 提出S4-Driver算法，利用稀疏体积策略将PaLI模型的视觉表示从透视视图转换为3D空间，整合多视角和多帧输入。

Result: 在nuScenes和Waymo数据集上验证，S4-Driver性能优于现有监督方法，且无需人工标注，具有良好扩展性。

Conclusion: S4-Driver展示了自监督方法在3D运动规划中的潜力，尤其在处理大规模未标注数据时表现优异。

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [69] [The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models](https://arxiv.org/abs/2505.24141)
*Jiashuai Liu,Yingjia Shang,Yingkang Zhan,Di Zhang,Yi Niu,Dong Wei,Xian Wu,Zeyu Gao,Chen Li,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文首次系统研究了病理学基础模型在全切片图像分析中对对抗攻击的脆弱性，提出了一种无标签攻击框架，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着病理学基础模型在研究和临床决策支持系统中的广泛应用，其安全性问题日益突出，但其对抗攻击的脆弱性尚未被充分研究。

Method: 提出了一种基于局部扰动全局影响的攻击框架，修订了四种经典白盒攻击方法，并重新定义了扰动预算。实验在三个代表性模型、五个数据集和六个下游任务中进行。

Result: 实验表明，仅修改每张切片0.1%的补丁即可导致下游任务准确率下降高达20%。

Conclusion: 研究为病理学基础模型的对抗鲁棒性和可靠部署奠定了基础，并公开了代码。

Abstract: With the widespread adoption of pathology foundation models in both research
and clinical decision support systems, exploring their security has become a
critical concern. However, despite their growing impact, the vulnerability of
these models to adversarial attacks remains largely unexplored. In this work,
we present the first systematic investigation into the security of pathology
foundation models for whole slide image~(WSI) analysis against adversarial
attacks. Specifically, we introduce the principle of \textit{local perturbation
with global impact} and propose a label-free attack framework that operates
without requiring access to downstream task labels. Under this attack
framework, we revise four classical white-box attack methods and redefine the
perturbation budget based on the characteristics of WSI. We conduct
comprehensive experiments on three representative pathology foundation models
across five datasets and six downstream tasks. Despite modifying only 0.1\% of
patches per slide with imperceptible noise, our attack leads to downstream
accuracy degradation that can reach up to 20\% in the worst cases. Furthermore,
we analyze key factors that influence attack success, explore the relationship
between patch-level vulnerability and semantic content, and conduct a
preliminary investigation into potential defence strategies. These findings lay
the groundwork for future research on the adversarial robustness and reliable
deployment of pathology foundation models. Our code is publicly available at:
https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.

</details>


### [70] [Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction](https://arxiv.org/abs/2505.24156)
*Chenyou Fan,Fangzheng Yan,Chenjia Bai,Jiepeng Wang,Chi Zhang,Zhen Wang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于文本到视频模型的双臂操作策略，通过两阶段训练（文本到光流和光流到视频）减少语言模糊性，降低对机器人数据的需求。


<details>
  <summary>Details</summary>
Motivation: 双臂操作策略因动作空间大和协调需求而难以泛化，现有方法因数据稀缺和单臂与双臂差异而效果不佳。

Method: 微调文本到视频模型，分两阶段训练文本到光流和光流到视频模型，光流作为中间变量减少语言模糊性。

Result: 在仿真和真实实验中验证了方法的有效性，显著减少了对低层次动作数据的依赖。

Conclusion: 提出的两阶段方法有效解决了双臂操作策略的泛化问题，降低了数据需求。

Abstract: Learning a generalizable bimanual manipulation policy is extremely
challenging for embodied agents due to the large action space and the need for
coordinated arm movements. Existing approaches rely on Vision-Language-Action
(VLA) models to acquire bimanual policies. However, transferring knowledge from
single-arm datasets or pre-trained VLA models often fails to generalize
effectively, primarily due to the scarcity of bimanual data and the fundamental
differences between single-arm and bimanual manipulation. In this paper, we
propose a novel bimanual foundation policy by fine-tuning the leading
text-to-video models to predict robot trajectories and training a lightweight
diffusion policy for action generation. Given the lack of embodied knowledge in
text-to-video models, we introduce a two-stage paradigm that fine-tunes
independent text-to-flow and flow-to-video models derived from a pre-trained
text-to-video model. Specifically, optical flow serves as an intermediate
variable, providing a concise representation of subtle movements between
images. The text-to-flow model predicts optical flow to concretize the intent
of language instructions, and the flow-to-video model leverages this flow for
fine-grained video prediction. Our method mitigates the ambiguity of language
in single-stage text-to-video prediction and significantly reduces the
robot-data requirement by avoiding direct use of low-level actions. In
experiments, we collect high-quality manipulation data for real dual-arm robot,
and the results of simulation and real-world experiments demonstrate the
effectiveness of our method.

</details>


### [71] [Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders](https://arxiv.org/abs/2505.24158)
*Bo Fang,Wenhao Wu,Qiangqiang Wu,Yuxin Song,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出Nar-KFC模块，通过关键帧选择和文本叙事插入，解决长视频理解中视觉令牌过多与语言模型上下文限制的矛盾。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中，视觉令牌过多与语言模型上下文限制的矛盾导致传统方法效率低下或内容不相关。

Method: 1. 将关键帧选择建模为整数二次规划问题，优化查询相关性和帧多样性；2. 引入非关键帧生成的文本叙事，弥补稀疏采样的时间不连续性。

Result: 在多个长视频基准测试中，Nar-KFC显著提升了多模态大语言模型的性能。

Conclusion: Nar-KFC是一种高效且内容感知的长视频压缩策略，有效结合了视觉和文本模态。

Abstract: Employing Multimodal Large Language Models (MLLMs) for long video
understanding remains a challenging problem due to the dilemma between the
substantial number of video frames (i.e., visual tokens) versus the limited
context length of language models. Traditional uniform sampling often leads to
selection of irrelevant content, while post-training MLLMs on thousands of
frames imposes a substantial computational burden. In this paper, we propose
threading keyframes with narratives (Nar-KFC), a plug-and-play module to
facilitate effective and efficient long video perception. Nar-KFC generally
involves two collaborative steps. First, we formulate the keyframe selection
process as an integer quadratic programming problem, jointly optimizing
query-relevance and frame-diversity. To avoid its computational complexity, a
customized greedy search strategy is designed as an efficient alternative.
Second, to mitigate the temporal discontinuity caused by sparse keyframe
sampling, we further introduce interleaved textual narratives generated from
non-keyframes using off-the-shelf captioners. These narratives are inserted
between keyframes based on their true temporal order, forming a coherent and
compact representation. Nar-KFC thus serves as a temporal- and content-aware
compression strategy that complements visual and textual modalities.
Experimental results on multiple long-video benchmarks demonstrate that Nar-KFC
significantly improves the performance of popular MLLMs. Code will be made
publicly available.

</details>


### [72] [Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry](https://arxiv.org/abs/2505.24162)
*Isaac Aguirre,Ivan Sipiran*

Main category: cs.CV

TL;DR: 提出一种无需训练的零样本3D对称性检测方法，利用DINOv2等基础视觉模型的视觉特征，通过渲染视图提取特征并反向投影到原始几何体上，实验表明其优于传统几何方法和学习型方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用基础视觉模型解决复杂的3D几何问题（如对称性检测），而无需训练数据。

Method: 从3D对象的渲染视图中提取视觉特征，反向投影到原始几何体上，利用对称不变性识别反射对称平面。

Result: 在ShapeNet子集上的实验显示，该方法优于传统几何方法和学习型方法，且无需训练数据。

Conclusion: 基础视觉模型可用于解决复杂3D几何问题，如对称性检测，且无需训练数据。

Abstract: We present a simple yet effective training-free approach for zero-shot 3D
symmetry detection that leverages visual features from foundation vision models
such as DINOv2. Our method extracts features from rendered views of 3D objects
and backprojects them onto the original geometry. We demonstrate the symmetric
invariance of these features and use them to identify reflection-symmetry
planes through a proposed algorithm. Experiments on a subset of ShapeNet
demonstrate that our approach outperforms both traditional geometric methods
and learning-based approaches without requiring any training data. Our work
demonstrates how foundation vision models can help in solving complex 3D
geometric problems such as symmetry detection.

</details>


### [73] [Pretraining Deformable Image Registration Networks with Random Images](https://arxiv.org/abs/2505.24167)
*Junyu Chen,Shuwen Wei,Yihao Liu,Aaron Carass,Yong Du*

Main category: cs.CV

TL;DR: 通过随机生成图像训练深度神经网络，提出一种用于医学图像配准的基础模型预训练策略，提高了配准精度并减少了对领域特定数据的需求。


<details>
  <summary>Details</summary>
Motivation: 探索无需医学图像即可训练深度神经网络的方法，以提升医学图像配准的效率和性能。

Method: 利用随机生成的图像及其噪声和对比度特性作为代理任务，预训练基础模型。

Result: 预训练策略提高了配准精度，减少了对领域特定数据的需求，并加速了下游训练的收敛。

Conclusion: 该方法为医学图像配准提供了一种高效且数据需求较低的预训练策略。

Abstract: Recent advances in deep learning-based medical image registration have shown
that training deep neural networks~(DNNs) does not necessarily require medical
images. Previous work showed that DNNs trained on randomly generated images
with carefully designed noise and contrast properties can still generalize well
to unseen medical data. Building on this insight, we propose using registration
between random images as a proxy task for pretraining a foundation model for
image registration. Empirical results show that our pretraining strategy
improves registration accuracy, reduces the amount of domain-specific data
needed to achieve competitive performance, and accelerates convergence during
downstream training, thereby enhancing computational efficiency.

</details>


### [74] [DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?](https://arxiv.org/abs/2505.24173)
*Tianhong Zhou,Yin Xu,Yingtao Zhu,Chuxi Xiao,Haiyang Bian,Lei Wei,Xuegong Zhang*

Main category: cs.CV

TL;DR: DrVD-Bench是一个多模态临床视觉推理基准，用于评估视觉语言模型（VLMs）是否真正模拟人类临床推理，而非仅模仿表面模式。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能系统评估VLMs是否真正模拟人类临床推理，因此需要开发一个更严谨的评估框架。

Method: DrVD-Bench包含三个模块：视觉证据理解、推理轨迹评估和报告生成评估，共7,789个图像-问题对，覆盖20种任务类型和17种诊断类别。

Result: 测试19种VLMs发现，随着推理复杂性增加，性能显著下降，部分模型虽表现出类似人类推理的迹象，但仍依赖表面相关性。

Conclusion: DrVD-Bench为开发可信赖的临床VLMs提供了结构化评估框架。

Abstract: Vision-language models (VLMs) exhibit strong zero-shot generalization on
natural images and show early promise in interpretable medical image analysis.
However, existing benchmarks do not systematically evaluate whether these
models truly reason like human clinicians or merely imitate superficial
patterns. To address this gap, we propose DrVD-Bench, the first multimodal
benchmark for clinical visual reasoning. DrVD-Bench consists of three modules:
Visual Evidence Comprehension, Reasoning Trajectory Assessment, and Report
Generation Evaluation, comprising a total of 7,789 image-question pairs. Our
benchmark covers 20 task types, 17 diagnostic categories, and five imaging
modalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is
explicitly structured to reflect the clinical reasoning workflow from modality
recognition to lesion identification and diagnosis. We benchmark 19 VLMs,
including general-purpose and medical-specific, open-source and proprietary
models, and observe that performance drops sharply as reasoning complexity
increases. While some models begin to exhibit traces of human-like reasoning,
they often still rely on shortcut correlations rather than grounded visual
understanding. DrVD-Bench offers a rigorous and structured evaluation framework
to guide the development of clinically trustworthy VLMs.

</details>


### [75] [Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT](https://arxiv.org/abs/2505.24182)
*Zhuobai Dong,Junchao Yi,Ziyuan Zheng,Haochen Han,Xiangxi Zheng,Alex Jinpeng Wang,Fangming Liu,Linjie Li*

Main category: cs.CV

TL;DR: MVPBench是一个评估多模态大语言模型视觉物理推理能力的基准，发现当前模型在复杂场景中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在视觉物理推理中的局限性，尤其是对基本物理规律、空间关系和因果效应的理解不足。

Method: 引入MVPBench基准，通过视觉链式推理（CoT）评估模型，并设计图一致性指标验证推理路径的物理逻辑。

Result: 实验显示，即使最先进的模型在视觉推理准确性和图文对齐方面表现不佳，且RL微调可能损害空间推理能力。

Conclusion: 当前模型的视觉物理推理能力有限，需重新思考微调方法以提高性能。

Abstract: Understanding the physical world - governed by laws of motion, spatial
relations, and causality - poses a fundamental challenge for multimodal large
language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o
demonstrate impressive perceptual and reasoning capabilities, our investigation
reveals these models struggle profoundly with visual physical reasoning,
failing to grasp basic physical laws, spatial interactions, and causal effects
in complex scenes. More importantly, they often fail to follow coherent
reasoning chains grounded in visual evidence, especially when multiple steps
are needed to arrive at the correct answer. To rigorously evaluate this
capability, we introduce MVPBench, a curated benchmark designed to rigorously
evaluate visual physical reasoning through the lens of visual chain-of-thought
(CoT). Each example features interleaved multi-image inputs and demands not
only the correct final answer but also a coherent, step-by-step reasoning path
grounded in evolving visual cues. This setup mirrors how humans reason through
real-world physical processes over time. To ensure fine-grained evaluation, we
introduce a graph-based CoT consistency metric that verifies whether the
reasoning path of model adheres to valid physical logic. Additionally, we
minimize shortcut exploitation from text priors, encouraging models to rely on
visual understanding. Experimental results reveal a concerning trend: even
cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text
alignment in physical domains. Surprisingly, RL-based post-training alignment -
commonly believed to improve visual reasoning performance - often harms spatial
reasoning, suggesting a need to rethink current fine-tuning practices.

</details>


### [76] [Boosting All-in-One Image Restoration via Self-Improved Privilege Learning](https://arxiv.org/abs/2505.24207)
*Gang Wu,Junjun Jiang,Kui Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: SIPL通过利用自身输出作为伪特权信号，实现测试时的自优化，显著提升图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 解决统一图像恢复模型在多样化和混合退化任务中的优化不稳定性和任务间冲突问题。

Method: 提出SIPL范式，利用特权信息在训练和推理阶段，通过Proxy Fusion模块和特权字典实现自优化。

Result: 在多种图像恢复基准测试中显著提升性能，如PSNR提高+4.58 dB和+1.28 dB。

Conclusion: SIPL是一种高效且广泛适用的方法，可无缝集成到多种架构中，显著提升图像恢复效果。

Abstract: Unified image restoration models for diverse and mixed degradations often
suffer from unstable optimization dynamics and inter-task conflicts. This paper
introduces Self-Improved Privilege Learning (SIPL), a novel paradigm that
overcomes these limitations by innovatively extending the utility of privileged
information (PI) beyond training into the inference stage. Unlike conventional
Privilege Learning, where ground-truth-derived guidance is typically discarded
after training, SIPL empowers the model to leverage its own preliminary outputs
as pseudo-privileged signals for iterative self-refinement at test time.
Central to SIPL is Proxy Fusion, a lightweight module incorporating a learnable
Privileged Dictionary. During training, this dictionary distills essential
high-frequency and structural priors from privileged feature representations.
Critically, at inference, the same learned dictionary then interacts with
features derived from the model's initial restoration, facilitating a
self-correction loop. SIPL can be seamlessly integrated into various backbone
architectures, offering substantial performance improvements with minimal
computational overhead. Extensive experiments demonstrate that SIPL
significantly advances the state-of-the-art on diverse all-in-one image
restoration benchmarks. For instance, when integrated with the PromptIR model,
SIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation
tasks and +1.28 dB on diverse five-task benchmarks, underscoring its
effectiveness and broad applicability. Codes are available at our project page
https://github.com/Aitical/SIPL.

</details>


### [77] [STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models](https://arxiv.org/abs/2505.24210)
*Zheng Tan,Weizhen Wang,Andrea L. Bertozzi,Ernest K. Ryu*

Main category: cs.CV

TL;DR: 提出了一种名为STORK的新型训练自由、结构无关的扩散模型ODE求解器，适用于中NFE范围（20-50），在图像生成质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在高保真图像和视频生成中表现优异，但高质量生成通常需要大量函数评估（NFE），导致采样速度慢。现有研究多关注低NFE范围（<10），而中NFE范围（20-50）在实际应用中更为常见，但相关研究较少。

Method: 提出STORK方法，基于一类刚性ODE求解器，结合泰勒展开适应，适用于任何扩散模型采样，包括噪声和流匹配模型。

Result: 在20-50 NFE范围内，STORK在无条件像素级生成和条件潜空间生成任务中（如Stable Diffusion 3.5和SANA）表现出更高的生成质量（FID得分更低）。

Conclusion: STORK是一种高效且通用的扩散模型ODE求解器，填补了中NFE范围研究的空白，并在实际应用中表现出优越性能。

Abstract: Diffusion models (DMs) have demonstrated remarkable performance in
high-fidelity image and video generation. Because high-quality generations with
DMs typically require a large number of function evaluations (NFEs), resulting
in slow sampling, there has been extensive research successfully reducing the
NFE to a small range (<10) while maintaining acceptable image quality. However,
many practical applications, such as those involving Stable Diffusion 3.5,
FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve
superior results, and, despite the practical relevance, research on the
effective sampling within this mid-NFE regime remains underexplored. In this
work, we propose a novel, training-free, and structure-independent DM ODE
solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,
based on a class of stiff ODE solvers with a Taylor expansion adaptation.
Unlike prior work such as DPM-Solver, which is dependent on the semi-linear
structure of the DM ODE, STORK is applicable to any DM sampling, including
noise-based and flow matching-based models. Within the 20-50 NFE range, STORK
achieves improved generation quality, as measured by FID scores, across
unconditional pixel-level generation and conditional latent-space generation
tasks using models like Stable Diffusion 3.5 and SANA. Code is available at
https://github.com/ZT220501/STORK.

</details>


### [78] [Benchmarking Foundation Models for Zero-Shot Biometric Tasks](https://arxiv.org/abs/2505.24214)
*Redwan Sony,Parisa Farmanifard,Hamzeh Alzwairy,Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 论文提出了一种评估视觉语言模型（VLMs）和多模态大语言模型（MLLMs）在六种生物识别任务中的零样本和小样本性能的基准测试，展示了这些模型在生物识别领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型（如VLMs和MLLMs）在生物识别任务中的应用潜力，填补现有研究的空白。

Method: 使用41种公开可用的VLMs和MLLMs，评估其在六种生物识别任务（如人脸验证、虹膜识别等）中的表现，并测试简单分类器头的效果。

Result: 模型在多项任务中表现优异，例如人脸验证在LFW数据集上达到96.77%的TMR（FMR为1%），虹膜识别在IITD-R-Full数据集上达到97.55%的TMR（FMR为1%）。

Conclusion: 预训练模型在生物识别任务中具有显著潜力，为实现通用人工智能的长期目标提供了支持。

Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs)
and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of
artificial intelligence, enabling remarkable generalization across diverse
tasks with minimal or no supervision. Yet, their potential in biometric
recognition and analysis remains relatively underexplored. In this work, we
introduce a comprehensive benchmark that evaluates the zero-shot and few-shot
performance of state-of-the-art publicly available VLMs and MLLMs across six
biometric tasks spanning the face and iris modalities: face verification, soft
biometric attribute prediction (gender and race), iris recognition,
presentation attack detection (PAD), and face manipulation detection (morphs
and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments
show that embeddings from these foundation models can be used for diverse
biometric tasks with varying degrees of success. For example, in the case of
face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a
False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)
dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1
percent FMR on the IITD-R-Full dataset was 97.55 percent without any
fine-tuning. Further, we show that applying a simple classifier head to these
embeddings can help perform DeepFake detection for faces, Presentation Attack
Detection (PAD) for irides, and extract soft biometric attributes like gender
and ethnicity from faces with reasonably high accuracy. This work reiterates
the potential of pretrained models in achieving the long-term vision of
Artificial General Intelligence.

</details>


### [79] [Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation](https://arxiv.org/abs/2505.24216)
*Prasanna Reddy Pulakurthi,Majid Rabbani,Jamison Heard,Sohail Dianat,Celso M. de Melo,Raghuveer Rao*

Main category: cs.CV

TL;DR: 论文提出了一种新的无源域自适应方法（SFDA），通过引入Shuffle PatchMix（SPM）增强技术和伪标签重加权策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决无源域自适应中因缺乏源数据导致的性能下降问题，特别是在小数据集上容易过拟合和伪标签噪声较大的情况下。

Method: 采用SPM技术对图像块进行随机混洗和混合，生成多样化的增强数据；同时设计了一种重加权策略，优先选择可靠的伪标签以减少噪声影响。

Result: 在PACS、VisDA-C和DomainNet-126三个基准测试中取得最优结果，PACS上单目标和多目标设置分别提升7.3%和7.2%。

Conclusion: SPM和伪标签重加权的结合为SFDA设立了新的性能基准，代码已开源。

Abstract: This work investigates Source-Free Domain Adaptation (SFDA), where a model
adapts to a target domain without access to source data. A new augmentation
technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are
introduced to enhance performance. SPM shuffles and blends image patches to
generate diverse and challenging augmentations, while the reweighting strategy
prioritizes reliable pseudo-labels to mitigate label noise. These techniques
are particularly effective on smaller datasets like PACS, where overfitting and
pseudo-label noise pose greater risks. State-of-the-art results are achieved on
three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,
improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target
and multi-target settings, respectively, while gains of 2.8% and 0.7% are
attained on DomainNet-126 and VisDA-C. This combination of advanced
augmentation and robust pseudo-label reweighting establishes a new benchmark
for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM

</details>


### [80] [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](https://arxiv.org/abs/2505.24222)
*Fangyikang Wang,Hubery Yin,Lei Qian,Yinan Li,Shaobin Zhuang,Huminhao Zhu,Yilin Zhang,Yanlong Tang,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Levenberg-Marquardt-Langevin (LML)的新方法，通过低秩近似和阻尼机制高效利用扩散Hessian几何，显著提升了扩散模型（DMs）的图像生成质量，且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型采样技术主要依赖一阶Langevin动力学，而利用二阶Hessian几何提升采样质量在MCMC中常见，但直接应用于高维DMs会导致计算复杂度高。本文旨在解决这一问题。

Method: 提出LML方法，通过低秩近似扩散Hessian几何和引入阻尼机制，避免了显式的高复杂度计算，从而高效提升采样质量。

Result: 实验表明，LML方法在多个预训练DMs上显著提升了图像生成质量，且计算开销可忽略。

Conclusion: LML方法通过高效利用Hessian几何，为扩散模型采样提供了一种计算高效且性能优越的解决方案。

Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of
generating images via learning the noised score function of data distribution.
Current DM sampling techniques typically rely on first-order Langevin dynamics
at each noise level, with efforts concentrated on refining inter-level
denoising strategies. While leveraging additional second-order Hessian geometry
to enhance the sampling quality of Langevin is a common practice in Markov
chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in
high-dimensional DMs lead to quadratic-complexity computational costs,
rendering them non-scalable. In this work, we introduce a novel
Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion
Hessian geometry in a training-free manner, drawing inspiration from the
celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces
two key innovations: (1) A low-rank approximation of the diffusion Hessian,
leveraging the DMs' inherent structure and circumventing explicit
quadratic-complexity computations; (2) A damping mechanism to stabilize the
approximated Hessian. This LML approximated Hessian geometry enables the
diffusion sampling to execute more accurate steps and improve the image
generation quality. We further conduct a theoretical analysis to substantiate
the approximation error bound of low-rank approximation and the convergence
property of the damping mechanism. Extensive experiments across multiple
pretrained DMs validate that the LML method significantly improves image
generation quality, with negligible computational overhead.

</details>


### [81] [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
*Haibo Jin,Peiyan Zhang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: 研究发现，链式思维（CoT）提示可能降低大型语言模型（LLM）的归纳推理能力，而非增强。通过诊断任务和理论框架，揭示了CoT的三种失败模式，并提出结构化干预措施以提升准确性。


<details>
  <summary>Details</summary>
Motivation: 探索链式思维（CoT）提示是否真正提升大型语言模型的归纳推理能力，并揭示其潜在问题。

Method: 设计了四种诊断性游戏任务（如国际象棋、德州扑克等），结合理论框架分析CoT的失败模式，并提出结构化干预措施。

Result: CoT提示可能降低模型的归纳推理性能，而非提升。通过结构化干预，可以在不重新训练的情况下提高准确性。

Conclusion: 有效的CoT推理不仅需要更多步骤，还需确保这些步骤结构合理。结构化干预是提升归纳推理能力的关键。

Abstract: Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
sparse examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.

</details>


### [82] [Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models](https://arxiv.org/abs/2505.24227)
*Ying Yang,Jie Zhang,Xiao Lv,Di Lin,Tao Xiang,Qing Guo*

Main category: cs.CV

TL;DR: LightD是一个通过语义引导的重新光照生成自然对抗样本的框架，用于视觉与语言预训练（VLP）模型，解决了现有方法在VLP任务中生成不自然或无效样本的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法主要针对分类任务，难以适应VLP模型，导致攻击效果不佳或样本不自然。

Method: LightD利用ChatGPT提出上下文感知的初始光照参数，并结合预训练的重新光照模型（IC-light）进行多样化光照调整，同时通过梯度优化增强攻击效果。

Result: LightD在图像描述和视觉问答等任务中，对多种VLP模型表现出高效和优越的攻击效果。

Conclusion: LightD通过语义引导的重新光照，成功生成了自然且有效的对抗样本，为VLP模型的对抗攻击提供了新思路。

Abstract: While adversarial attacks on vision-and-language pretraining (VLP) models
have been explored, generating natural adversarial samples crafted through
realistic and semantically meaningful perturbations remains an open challenge.
Existing methods, primarily designed for classification tasks, struggle when
adapted to VLP models due to their restricted optimization spaces, leading to
ineffective attacks or unnatural artifacts. To address this, we propose
\textbf{LightD}, a novel framework that generates natural adversarial samples
for VLP models via semantically guided relighting. Specifically, LightD
leverages ChatGPT to propose context-aware initial lighting parameters and
integrates a pretrained relighting model (IC-light) to enable diverse lighting
adjustments. LightD expands the optimization space while ensuring perturbations
align with scene semantics. Additionally, gradient-based optimization is
applied to the reference lighting image to further enhance attack effectiveness
while maintaining visual naturalness. The effectiveness and superiority of the
proposed LightD have been demonstrated across various VLP models in tasks such
as image captioning and visual question answering.

</details>


### [83] [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
*Haibo Jin,Peiyan Zhang,Peiran Wang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: 论文提出了一个统一的理论框架，将越狱攻击和幻觉问题分别建模为令牌级和注意力级优化，并验证了两者在损失收敛和梯度行为上的相似性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型（LFMs）存在幻觉和越狱攻击两种漏洞，通常被孤立研究，但作者发现针对一种漏洞的防御会影响另一种，暗示两者存在深层联系。

Method: 提出统一理论框架，将越狱攻击建模为令牌级优化，幻觉建模为注意力级优化，并通过实验验证损失收敛和梯度行为的一致性。

Result: 在LLaVA-1.5和MiniGPT-4上验证了优化趋势和梯度对齐，并发现针对一种漏洞的缓解技术也能降低另一种漏洞的成功率。

Conclusion: 揭示了LFMs的共享失败模式，建议鲁棒性策略应同时针对两种漏洞。

Abstract: Large foundation models (LFMs) are susceptible to two distinct
vulnerabilities: hallucinations and jailbreak attacks. While typically studied
in isolation, we observe that defenses targeting one often affect the other,
hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as
token-level optimization and hallucinations as attention-level optimization.
Within this framework, we establish two key propositions: (1) \textit{Similar
Loss Convergence} - the loss functions for both vulnerabilities converge
similarly when optimizing for target-specific outputs; and (2) \textit{Gradient
Consistency in Attention Redistribution} - both exhibit consistent gradient
behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,
showing consistent optimization trends and aligned gradients. Leveraging this
connection, we demonstrate that mitigation techniques for hallucinations can
reduce jailbreak success rates, and vice versa. Our findings reveal a shared
failure mode in LFMs and suggest that robustness strategies should jointly
address both vulnerabilities.

</details>


### [84] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/abs/2505.24238)
*Bowen Dong,Minheng Ni,Zitong Huang,Guanglei Yang,Wangmeng Zuo,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一个名为{\dataset}的基准测试，用于区分多模态大语言模型（MLLMs）中的感知诱导幻觉和推理诱导幻觉，并提出了{\method}方法以减少推理幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态幻觉限制了MLLMs的正确性，但现有基准无法区分感知和推理诱导的幻觉，阻碍了模型诊断和改进。

Method: 提出{\dataset}基准测试，通过构建问题隔离推理幻觉，并引入多粒度评估指标。同时提出{\method}方法，结合课程强化微调和协作提示推理以减少幻觉。

Result: 研究发现模型规模、数据规模和训练阶段显著影响幻觉程度，当前MLLMs在空间推理能力上有限，问题类型与幻觉模式相关。{\method}有效减少了逻辑幻觉。

Conclusion: {\dataset}和{\method}为MLLMs的幻觉问题提供了诊断和改进工具，未来可针对特定幻觉模式进一步优化。

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [85] [LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework](https://arxiv.org/abs/2505.24245)
*Xin Kang,Zihan Zheng,Lei Chu,Yue Gao,Jiahao Li,Hao Pan,Xuejin Chen,Yan Lu*

Main category: cs.CV

TL;DR: LTM3D是一个结合扩散模型和自回归模型的3D形状生成框架，通过条件分布建模和前缀学习提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合扩散模型和自回归模型用于3D形状生成时存在挑战，LTM3D旨在解决这一问题。

Method: LTM3D采用条件分布建模主干，结合掩码自编码器和扩散模型，并引入前缀学习和潜在令牌重建模块。

Result: 实验表明，LTM3D在图像和文本条件生成任务中表现优于现有方法，生成形状的提示保真度和结构准确性更高。

Conclusion: LTM3D为多模态、多表示的3D生成提供了一个通用框架。

Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D
shape generation that integrates the strengths of diffusion and auto-regressive
(AR) models. While diffusion-based methods effectively model continuous latent
spaces and AR models excel at capturing inter-token dependencies, combining
these paradigms for 3D shape generation remains a challenge. To address this,
LTM3D features a Conditional Distribution Modeling backbone, leveraging a
masked autoencoder and a diffusion model to enhance token dependency learning.
Additionally, we introduce Prefix Learning, which aligns condition tokens with
shape latent tokens during generation, improving flexibility across modalities.
We further propose a Latent Token Reconstruction module with
Reconstruction-Guided Sampling to reduce uncertainty and enhance structural
fidelity in generated shapes. Our approach operates in token space, enabling
support for multiple 3D representations, including signed distance fields,
point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on
image- and text-conditioned shape generation tasks demonstrate that LTM3D
outperforms existing methods in prompt fidelity and structural accuracy while
offering a generalizable framework for multi-modal, multi-representation 3D
generation.

</details>


### [86] [50 Years of Automated Face Recognition](https://arxiv.org/abs/2505.24247)
*Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文回顾了50年来人脸识别技术的发展，从早期几何统计方法到现代深度学习模型，分析了数据集、损失函数、神经网络设计等关键创新，并探讨了训练数据对模型泛化的影响。尽管取得了显著进展，但仍存在挑战，如可扩展性、多模态融合等。


<details>
  <summary>Details</summary>
Motivation: 总结人脸识别技术的发展历程，分析关键技术进步及其影响，并指出未来研究方向。

Method: 通过历史回顾和技术分析，结合数据集、损失函数、神经网络设计等方面的创新，评估模型性能。

Result: 现代人脸识别系统在NIST FRVT评估中达到0.13%的误识别率，但仍面临可扩展性等挑战。

Conclusion: 尽管技术取得重大突破，未来仍需解决可扩展性、多模态融合等问题，推动人脸识别进一步发展。

Abstract: Over the past 50 years, automated face recognition has evolved from
rudimentary, handcrafted systems into sophisticated deep learning models that
rival and often surpass human performance. This paper chronicles the history
and technological progression of FR, from early geometric and statistical
methods to modern deep neural architectures leveraging massive real and
AI-generated datasets. We examine key innovations that have shaped the field,
including developments in dataset, loss function, neural network design and
feature fusion. We also analyze how the scale and diversity of training data
influence model generalization, drawing connections between dataset growth and
benchmark improvements. Recent advances have achieved remarkable milestones:
state-of-the-art face verification systems now report False Negative
Identification Rates of 0.13% against a 12.4 million gallery in NIST FRVT
evaluations for 1:N visa-to-border matching. While recent advances have enabled
remarkable accuracy in high- and low-quality face scenarios, numerous
challenges persist. While remarkable progress has been achieved, several open
research problems remain. We outline critical challenges and promising
directions for future face recognition research, including scalability,
multi-modal fusion, synthetic identity generation, and explainable systems.

</details>


### [87] [Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization](https://arxiv.org/abs/2505.24249)
*Qingyao Tian,Huai Liao,Xinyan Huang,Bingyu Yang,Hongbin Liu*

Main category: cs.CV

TL;DR: PANSv2是一个基于视觉的6-DOF支气管镜定位框架，通过整合深度估计、标志点检测和中心线约束，解决了现有方法在泛化性和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有支气管镜定位方法在跨病例泛化性和视觉退化下的鲁棒性上表现不佳，PANSv2旨在解决这些问题。

Method: PANSv2结合深度估计、标志点检测和中心线约束，利用预训练的内窥镜基础模型EndoOmni和EndoMamba，并引入自动重新初始化模块。

Result: 在10个病例的数据集上，PANSv2的跟踪成功率最高，绝对轨迹误差低于5mm的比例提升了18.1%。

Conclusion: PANSv2在泛化性和鲁棒性上表现优异，具有临床应用的潜力。

Abstract: Vision-based 6-DOF bronchoscopy localization offers a promising solution for
accurate and cost-effective interventional guidance. However, existing methods
struggle with 1) limited generalization across patient cases due to scarce
labeled data, and 2) poor robustness under visual degradation, as bronchoscopy
procedures frequently involve artifacts such as occlusions and motion blur that
impair visual information. To address these challenges, we propose PANSv2, a
generalizable and robust bronchoscopy localization framework. Motivated by PANS
that leverages multiple visual cues for pose likelihood measurement, PANSv2
integrates depth estimation, landmark detection, and centerline constraints
into a unified pose optimization framework that evaluates pose probability and
solves for the optimal bronchoscope pose. To further enhance generalization
capabilities, we leverage the endoscopic foundation model EndoOmni for depth
estimation and the video foundation model EndoMamba for landmark detection,
incorporating both spatial and temporal analyses. Pretrained on diverse
endoscopic datasets, these models provide stable and transferable visual
representations, enabling reliable performance across varied bronchoscopy
scenarios. Additionally, to improve robustness to visual degradation, we
introduce an automatic re-initialization module that detects tracking failures
and re-establishes pose using landmark detections once clear views are
available. Experimental results on bronchoscopy dataset encompassing 10 patient
cases show that PANSv2 achieves the highest tracking success rate, with an
18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)
compared to existing methods, showing potential towards real clinical usage.

</details>


### [88] [Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames](https://arxiv.org/abs/2505.24257)
*Sahithya Ravi,Gabriel Sarch,Vibhav Vineet,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.CV

TL;DR: Disjoint-3DQA是一个评估视觉语言模型（VLMs）在非共视帧中对物体空间关系推理能力的基准测试，发现现有模型与人类表现差距显著，且时间间隔越大性能下降越明显。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估VLMs在长时间跨度的空间推理能力，尤其是在物体非共视情况下的表现。

Method: 通过Disjoint-3DQA基准测试，对七种先进VLMs进行评估，并分析不同输入（如轨迹、鸟瞰图、3D坐标）对性能的影响。

Result: 模型表现比人类低28%，时间间隔增大时性能下降更显著（60%至30%）。提供3D坐标可显著提升性能20%，而其他输入改进有限。

Conclusion: Disjoint-3DQA揭示了VLMs在构建和维护3D场景表示上的瓶颈，为未来视觉、语言与具身AI研究提供了明确挑战。

Abstract: An embodied AI assistant operating on egocentric video must integrate spatial
cues across time - for instance, determining where an object A, glimpsed a few
moments ago lies relative to an object B encountered later. We introduce
Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs
by posing questions about object pairs that are not co-visible in the same
frame. We evaluated seven state-of-the-art VLMs and found that models lag
behind human performance by 28%, with steeper declines in accuracy (60% to 30
%) as the temporal gap widens. Our analysis further reveals that providing
trajectories or bird's-eye-view projections to VLMs results in only marginal
improvements, whereas providing oracle 3D coordinates leads to a substantial
20% performance increase. This highlights a core bottleneck of multi-frame VLMs
in constructing and maintaining 3D scene representations over time from visual
signals. Disjoint-3DQA therefore sets a clear, measurable challenge for
long-horizon spatial reasoning and aims to catalyze future research at the
intersection of vision, language, and embodied AI.

</details>


### [89] [LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization](https://arxiv.org/abs/2505.24282)
*Zirui Shang,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: 论文提出通过LLMs扩展语言查询以提供更详细的动作边界线索，并通过建模边界概率分数来减少边界不确定性对训练的影响。


<details>
  <summary>Details</summary>
Motivation: 语言查询通常缺乏动作边界的具体细节，导致训练数据中的边界不确定性增加。

Method: 1. 使用LLMs生成动作边界的文本描述以扩展查询；2. 建模动作边界的概率分数，结合语义相似性和时间距离。

Result: 实验结果表明该方法有效提升了语言驱动动作定位的性能。

Conclusion: 该方法模型无关，可轻松集成到现有模型中，显著减少边界不确定性对训练的影响。

Abstract: Language-driven action localization in videos requires not only semantic
alignment between language query and video segment, but also prediction of
action boundaries.
  However, the language query primarily describes the main content of an action
and usually lacks specific details of action start and end boundaries, which
increases the subjectivity of manual boundary annotation and leads to boundary
uncertainty in training data.
  In this paper, on one hand, we propose to expand the original query by
generating textual descriptions of the action start and end boundaries through
LLMs, which can provide more detailed boundary cues for localization and thus
reduce the impact of boundary uncertainty.
  On the other hand, to enhance the tolerance to boundary uncertainty during
training, we propose to model probability scores of action boundaries by
calculating the semantic similarities between frames and the expanded query as
well as the temporal distances between frames and the annotated boundary
frames. They can provide more consistent boundary supervision, thus improving
the stability of training.
  Our method is model-agnostic and can be seamlessly and easily integrated into
any existing models of language-driven action localization in an off-the-shelf
manner. Experimental results on several datasets demonstrate the effectiveness
of our method.

</details>


### [90] [EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding](https://arxiv.org/abs/2505.24287)
*Ege Özsoy,Arda Mamur,Felix Tristram,Chantal Pellegrini,Magdalena Wysocki,Benjamin Busam,Nassir Navab*

Main category: cs.CV

TL;DR: EgoExOR是首个结合第一人称和第三人称视角的手术室数据集，提供多模态数据支持临床感知任务。


<details>
  <summary>Details</summary>
Motivation: 手术室需要高精度的感知模型以提高安全性和效率，现有数据集视角单一，无法全面支持需求。

Method: EgoExOR整合了穿戴式眼镜的自我中心数据和RGB-D相机的外部数据，包含详细场景图标注。

Result: 评估了两种先进模型的性能，并提出了新的基线模型，展示了多模态和多视角数据的优势。

Conclusion: EgoExOR为手术室感知研究提供了新的基础和资源。

Abstract: Operating rooms (ORs) demand precise coordination among surgeons, nurses, and
equipment in a fast-paced, occlusion-heavy environment, necessitating advanced
perception models to enhance safety and efficiency. Existing datasets either
provide partial egocentric views or sparse exocentric multi-view context, but
do not explore the comprehensive combination of both. We introduce EgoExOR, the
first OR dataset and accompanying benchmark to fuse first-person and
third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two
emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally
Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand
tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D
cameras, and ultrasound imagery. Its detailed scene graph annotations, covering
36 entities and 22 relations (568,235 triplets), enable robust modeling of
clinical interactions, supporting tasks like action recognition and
human-centric perception. We evaluate the surgical scene graph generation
performance of two adapted state-of-the-art models and offer a new baseline
that explicitly leverages EgoExOR's multimodal and multi-perspective signals.
This new dataset and benchmark set a new foundation for OR perception, offering
a rich, multimodal resource for next-generation clinical perception.

</details>


### [91] [Category-aware EEG image generation based on wavelet transform and contrast semantic loss](https://arxiv.org/abs/2505.24301)
*Enshang Zhang,Zhicheng Zhang,Takashi Hanakawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的EEG信号编码器，结合离散小波变换和门控机制，用于从EEG信号中提取视觉刺激相关特征，并通过预训练的扩散模型重建视觉刺激。实验结果表明，该方法在语义对齐和分类准确率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实现脑机接口的关键步骤是从EEG信号中重建视觉刺激，但现有方法在语义层面的定量分析存在局限性。

Method: 提出了一种结合离散小波变换和门控机制的Transformer编码器，通过特征对齐和类别感知融合损失提取EEG信号特征，并利用预训练的扩散模型重建视觉刺激。

Result: 在THINGS-EEG数据集上的实验显示，模型在语义对齐和分类准确率上显著提升，单被试最高准确率达43%。

Conclusion: 该方法在将神经活动转化为视觉表征方面表现出色，为脑机接口的实现提供了有效工具。

Abstract: Reconstructing visual stimuli from EEG signals is a crucial step in realizing
brain-computer interfaces. In this paper, we propose a transformer-based EEG
signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating
mechanism. Guided by the feature alignment and category-aware fusion losses,
this encoder is used to extract features related to visual stimuli from EEG
signals. Subsequently, with the aid of a pre-trained diffusion model, these
features are reconstructed into visual stimuli. To verify the effectiveness of
the model, we conducted EEG-to-image generation and classification tasks using
the THINGS-EEG dataset. To address the limitations of quantitative analysis at
the semantic level, we combined WordNet-based classification and semantic
similarity metrics to propose a novel semantic-based score, emphasizing the
ability of our model to transfer neural activities into visual representations.
Experimental results show that our model significantly improves semantic
alignment and classification accuracy, which achieves a maximum single-subject
accuracy of 43\%, outperforming other state-of-the-art methods. The source code
and supplementary material is available at
https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main.

</details>


### [92] [Progressive Class-level Distillation](https://arxiv.org/abs/2505.24310)
*Jiayan Li,Jun Li,Zhourui Zhang,Jianhua Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为渐进式类别级蒸馏（PCD）的方法，通过分阶段蒸馏解决传统知识蒸馏中低概率类别信息被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，高置信度类别主导蒸馏过程，导致低概率类别的区分信息被忽视，知识传递不充分。

Method: PCD方法通过排名教师-学生logits差异确定蒸馏优先级，分阶段进行双向蒸馏（从细到粗和从粗到细），实现充分的知识传递。

Result: 在公开基准数据集上的实验表明，PCD在分类和检测任务上均优于现有方法。

Conclusion: PCD通过渐进式分阶段蒸馏，有效解决了传统知识蒸馏中的不足，提升了知识传递的全面性。

Abstract: In knowledge distillation (KD), logit distillation (LD) aims to transfer
class-level knowledge from a more powerful teacher network to a small student
model via accurate teacher-student alignment at the logits level. Since
high-confidence object classes usually dominate the distillation process,
low-probability classes which also contain discriminating information are
downplayed in conventional methods, leading to insufficient knowledge transfer.
To address this issue, we propose a simple yet effective LD method termed
Progressive Class-level Distillation (PCD). In contrast to existing methods
which perform all-class ensemble distillation, our PCD approach performs
stage-wise distillation for step-by-step knowledge transfer. More specifically,
we perform ranking on teacher-student logits difference for identifying
distillation priority from scratch, and subsequently divide the entire LD
process into multiple stages. Next, bidirectional stage-wise distillation
incorporating fine-to-coarse progressive learning and reverse coarse-to-fine
refinement is conducted, allowing comprehensive knowledge transfer via
sufficient logits alignment within separate class groups in different
distillation stages. Extension experiments on public benchmarking datasets
demonstrate the superiority of our method compared to state-of-the-arts for
both classification and detection tasks.

</details>


### [93] [InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing](https://arxiv.org/abs/2505.24315)
*Jinlu Zhang,Yixin Chen,Zan Wang,Jie Yang,Yizhou Wang,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了一种零样本3D人机交互生成框架，利用预训练模型解决开放集对象生成问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从文本生成新颖人机交互（HOI）时面临挑战，尤其是开放集对象。

Method: 结合大型语言模型（LLM）推理人机关系，利用预训练2D扩散模型解析对象，并通过多视角SDS生成初始姿态，最后优化细节。

Result: 实验验证了方法的有效性，尤其在细粒度交互和处理开放集对象方面优于现有工作。

Conclusion: 该框架无需特定数据集训练，成功解决了开放集3D HOI生成的挑战。

Abstract: Recent advances in 3D human-aware generation have made significant progress.
However, existing methods still struggle with generating novel Human Object
Interaction (HOI) from text, particularly for open-set objects. We identify
three main challenges of this task: precise human-object relation reasoning,
affordance parsing for any object, and detailed human interaction pose
synthesis aligning description and object geometry. In this work, we propose a
novel zero-shot 3D HOI generation framework without training on specific
datasets, leveraging the knowledge from large-scale pre-trained models.
Specifically, the human-object relations are inferred from large language
models (LLMs) to initialize object properties and guide the optimization
process. Then we utilize a pre-trained 2D image diffusion model to parse unseen
objects and extract contact points, avoiding the limitations imposed by
existing 3D asset knowledge. The initial human pose is generated by sampling
multiple hypotheses through multi-view SDS based on the input text and object
geometry. Finally, we introduce a detailed optimization to generate
fine-grained, precise, and natural interaction, enforcing realistic 3D contact
between the 3D object and the involved body parts, including hands in grasping.
This is achieved by distilling human-level feedback from LLMs to capture
detailed human-object relations from the text instruction. Extensive
experiments validate the effectiveness of our approach compared to prior works,
particularly in terms of the fine-grained nature of interactions and the
ability to handle open-set 3D objects.

</details>


### [94] [STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising](https://arxiv.org/abs/2505.24327)
*Jingjing Liu,Jiashun Jin,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为STAR-Net的遥感图像去噪方法，结合低秩先验和非局部自相似性，并通过ADMM引导的深度展开网络自动学习参数，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法缺乏物理信息模型的整合且解释性不足，同时传统方法对非局部自相似性关注不足且参数调优繁琐。

Method: 提出STAR-Net和其稀疏变体STAR-Net-S，利用低秩先验捕捉非局部自相似性，并通过ADMM引导的深度展开网络自动学习参数。

Result: 在合成和真实数据集上，STAR-Net和STAR-Net-S优于现有遥感图像去噪方法。

Conclusion: STAR-Net结合了模型和深度学习的优势，解决了传统方法的不足，表现出色。

Abstract: Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named sparse tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
sparse variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.

</details>


### [95] [DisTime: Distribution-based Time Representation for Video Large Language Models](https://arxiv.org/abs/2505.24329)
*Yingsen Zeng,Zepeng Huang,Yujie Zhong,Chengjian Feng,Jie Hu,Lin Ma,Yang Liu*

Main category: cs.CV

TL;DR: DisTime是一个轻量级框架，通过连续时间嵌入和分布时间解码器提升Video-LLMs的时间定位能力，并创建了InternVid-TG数据集。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在时间定位上存在离散时间表示和数据集不足的问题，需要更精确的时间理解方法。

Method: DisTime采用可学习令牌生成连续时间嵌入，结合分布时间解码器和编码器，并通过自动化标注生成InternVid-TG数据集。

Result: DisTime在三个时间敏感任务中表现最佳，并在Video QA任务中保持竞争力。

Conclusion: DisTime有效解决了时间定位问题，并通过新数据集提升了性能。

Abstract: Despite advances in general video understanding, Video Large Language Models
(Video-LLMs) face challenges in precise temporal localization due to discrete
time representations and limited temporally aware datasets. Existing methods
for temporal expression either conflate time with text-based numerical values,
add a series of dedicated temporal tokens, or regress time using specialized
temporal grounding heads. To address these issues, we introduce DisTime, a
lightweight framework designed to enhance temporal comprehension in Video-LLMs.
DisTime employs a learnable token to create a continuous temporal embedding
space and incorporates a Distribution-based Time Decoder that generates
temporal probability distributions, effectively mitigating boundary ambiguities
and maintaining temporal continuity. Additionally, the Distribution-based Time
Encoder re-encodes timestamps to provide time markers for Video-LLMs. To
overcome temporal granularity limitations in existing datasets, we propose an
automated annotation paradigm that combines the captioning capabilities of
Video-LLMs with the localization expertise of dedicated temporal models. This
leads to the creation of InternVid-TG, a substantial dataset with 1.25M
temporally grounded events across 179k videos, surpassing ActivityNet-Caption
by 55 times. Extensive experiments demonstrate that DisTime achieves
state-of-the-art performance across benchmarks in three time-sensitive tasks
while maintaining competitive performance in Video QA tasks. Code and data are
released at https://github.com/josephzpng/DisTime.

</details>


### [96] [KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices](https://arxiv.org/abs/2505.24334)
*Uzair Khan,Franco Fummi,Luigi Capogrosso*

Main category: cs.CV

TL;DR: KairosAD是一种基于MobileSAM的轻量级异常检测方法，适用于资源受限的嵌入式设备，参数减少78%，推理速度快4倍，性能与现有模型相当。


<details>
  <summary>Details</summary>
Motivation: 解决现有异常检测模型在中小企业生产线上部署时计算资源需求高、不实用的问题。

Method: 利用Mobile Segment Anything Model (MobileSAM)进行图像异常检测。

Result: 在MVTec-AD和ViSA数据集上表现优异，参数减少78%，推理速度快4倍，AUROC性能相当。

Conclusion: KairosAD成功部署于嵌入式设备，并在实际生产线上验证，代码开源。

Abstract: In the era of intelligent manufacturing, anomaly detection has become
essential for maintaining quality control on modern production lines. However,
while many existing models show promising performance, they are often too
large, computationally demanding, and impractical to deploy on
resource-constrained embedded devices that can be easily installed on the
production lines of Small and Medium Enterprises (SMEs). To bridge this gap, we
present KairosAD, a novel supervised approach that uses the power of the Mobile
Segment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD
has been evaluated on the two well-known industrial anomaly detection datasets,
i.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer
parameters and boasts a 4x faster inference time compared to the leading
state-of-the-art model, while maintaining comparable AUROC performance. We
deployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA
Jetson AGX. Finally, KairosAD was successfully installed and tested on the real
production line of the Industrial Computer Engineering Laboratory (ICE Lab) at
the University of Verona. The code is available at
https://github.com/intelligolabs/KairosAD.

</details>


### [97] [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
*Gilles Quentin Hacheme,Girmaw Abebe Tadesse,Caleb Robinson,Akram Zaytar,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: GeoVision Labeler (GVL) 是一个严格零样本分类框架，通过视觉大语言模型生成图像描述，再通过传统大语言模型映射到用户定义类别，实现灵活分类。


<details>
  <summary>Details</summary>
Motivation: 解决地理空间图像分类中标注数据稀缺的问题，提供无需任务特定预训练的零样本分类方案。

Method: GVL 使用视觉大语言模型生成图像描述，再通过传统大语言模型映射到用户定义类别；对于复杂任务，采用递归聚类和分层分类策略。

Result: 在 SpaceNet v7 上达到 93.2% 的零样本准确率，并在复杂多类任务中表现竞争性。

Conclusion: GVL 是一个模块化、可解释的零样本分类框架，适用于多种地理空间应用，并已开源。

Abstract: Classifying geospatial imagery remains a major bottleneck for applications
such as disaster response and land-use monitoring-particularly in regions where
annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that
claim zero-shot classification capabilities for satellite imagery nonetheless
rely on task-specific pretraining and adaptation to reach competitive
performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot
classification framework: a vision Large Language Model (vLLM) generates rich,
human-readable image descriptions, which are then mapped to user-defined
classes by a conventional Large Language Model (LLM). This modular, and
interpretable pipeline enables flexible image classification for a large range
of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,
and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary
Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class
classification tasks (UC Merced, RESISC45), we implemented a recursive
LLM-driven clustering to form meta-classes at successive depths, followed by
hierarchical classification-first resolving coarse groups, then finer
distinctions-to deliver competitive zero-shot performance. GVL is open-sourced
at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in
real-world geospatial workflows.

</details>


### [98] [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/abs/2505.24342)
*Fanhang Man,Xiaoyue Chen,Huandong Wang,Baining Zhao,Han Li,Xinlei Chen,Yong Li*

Main category: cs.CV

TL;DR: K-EVER²是一个知识增强的视觉情感推理与检索框架，通过多模态对齐整合外部情感知识，无需手工标签即可实现跨图像类型的稳健情感预测。


<details>
  <summary>Details</summary>
Motivation: 图像中的情感线索抽象、重叠且复杂，现有视觉语言模型（VLM）因监督有限和情感基础稀疏而难以建模和解释这些情感。

Method: 提出K-EVER²框架，结构化视觉情感线索并整合外部情感知识，通过多模态对齐实现无监督情感预测。

Result: 在Emotion6、EmoSet和M-Disaster基准测试中，K-EVER²显著优于CNN和VLM基线，特定情感准确率提升19%，平均准确率提升12.3%。

Conclusion: K-EVER²为视觉内容的情感理解提供了可扩展且通用的解决方案。

Abstract: Understanding what emotions images evoke in their viewers is a foundational
goal in human-centric visual computing. While recent advances in
vision-language models (VLMs) have shown promise for visual emotion analysis
(VEA), several key challenges remain unresolved. Emotional cues in images are
often abstract, overlapping, and entangled, making them difficult to model and
interpret. Moreover, VLMs struggle to align these complex visual patterns with
emotional semantics due to limited supervision and sparse emotional grounding.
Finally, existing approaches lack structured affective knowledge to resolve
ambiguity and ensure consistent emotional reasoning across diverse visual
domains.
  To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}},
a knowledge-enhanced framework for emotion reasoning and retrieval. Our
approach introduces a semantically structured formulation of visual emotion
cues and integrates external affective knowledge through multimodal alignment.
Without relying on handcrafted labels or direct emotion supervision,
K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions
across heterogeneous image types.
  We validate our framework on three representative benchmarks, Emotion6,
EmoSet, and M-Disaster, covering social media imagery, human-centric scenes,
and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms
strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for
specific emotions and a \textbf{12.3\% average accuracy gain} across all
emotion categories. Our results demonstrate a scalable and generalizable
solution for advancing emotional understanding of visual content.

</details>


### [99] [VUDG: A Dataset for Video Understanding Domain Generalization](https://arxiv.org/abs/2505.24346)
*Ziyi Wang,Zhi Gao,Boxuan Yu,Zirui Dai,Yuxiang Song,Qingyuan Lu,Jin Chen,Xinxiao Wu*

Main category: cs.CV

TL;DR: 论文提出了视频理解领域泛化（VUDG）数据集，用于评估视频理解模型在域偏移下的表现，并展示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解研究忽视了真实世界中的域偏移问题，导致领域泛化（DG）在视频理解中未被充分探索。

Method: 提出了VUDG数据集，包含11个不同领域的视频，覆盖三种域偏移类型，并采用多专家渐进标注框架为视频标注多选和开放式问答对。

Result: 实验表明，大多数模型（包括最先进的LVLMs）在域偏移下性能下降，揭示了当前模型对数据分布偏移的鲁棒性差异。

Conclusion: VUDG为未来领域泛化视频理解研究提供了宝贵资源。

Abstract: Video understanding has made remarkable progress in recent years, largely
driven by advances in deep models and the availability of large-scale annotated
datasets. However, existing works typically ignore the inherent domain shifts
encountered in real-world video applications, leaving domain generalization
(DG) in video understanding underexplored. Hence, we propose Video
Understanding Domain Generalization (VUDG), a novel dataset designed
specifically for evaluating the DG performance in video understanding. VUDG
contains videos from 11 distinct domains that cover three types of domain
shifts, and maintains semantic similarity across different domains to ensure
fair and meaningful evaluation. We propose a multi-expert progressive
annotation framework to annotate each video with both multiple-choice and
open-ended question-answer pairs. Extensive experiments on 9 representative
large video-language models (LVLMs) and several traditional video question
answering methods show that most models (including state-of-the-art LVLMs)
suffer performance degradation under domain shifts. These results highlight the
challenges posed by VUDG and the difference in the robustness of current models
to data distribution shifts. We believe VUDG provides a valuable resource for
prompting future research in domain generalization video understanding.

</details>


### [100] [Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation](https://arxiv.org/abs/2505.24361)
*Roger Ferrod,Cássio F. Dantas,Luigi Di Caro,Dino Ienco*

Main category: cs.CV

TL;DR: CroDiNo-KD提出了一种新的跨模态知识蒸馏框架，通过解耦表示、对比学习和数据增强改进RGBD语义分割。


<details>
  <summary>Details</summary>
Motivation: 解决传统跨模态知识蒸馏框架在教师架构选择和蒸馏过程中的局限性，提升单模态模型的性能。

Method: 利用解耦表示、对比学习和解耦数据增强，同时学习RGB和深度单模态模型。

Result: 在三个RGBD数据集上验证了CroDiNo-KD的优越性，优于现有方法。

Conclusion: CroDiNo-KD展示了跨模态知识蒸馏的新思路，挑战了传统的师生范式。

Abstract: Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as
robotics, autonomous driving and remote sensing. The combination of these
multi-modal data enhances environmental perception by providing 3D spatial
context, which is absent in standard RGB images. Although RGBD multi-modal data
can be available to train computer vision models, accessing all sensor
modalities during the inference stage may be infeasible due to sensor failures
or resource constraints, leading to a mismatch between data modalities
available during training and inference. Traditional Cross-Modal Knowledge
Distillation (CMKD) frameworks, developed to address this task, are typically
based on a teacher/student paradigm, where a multi-modal teacher distills
knowledge into a single-modality student model. However, these approaches face
challenges in teacher architecture choices and distillation process selection,
thus limiting their adoption in real-world scenarios. To overcome these issues,
we introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on
Knowledge Distillation), a novel cross-modal knowledge distillation framework
for RGBD semantic segmentation. Our approach simultaneously learns
single-modality RGB and Depth models by exploiting disentanglement
representation, contrastive learning and decoupled data augmentation with the
aim to structure the internal manifolds of neural network models through
interaction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets
across diverse domains, considering recent CMKD frameworks as competitors. Our
findings illustrate the quality of CroDiNo-KD, and they suggest reconsidering
the conventional teacher/student paradigm to distill information from
multi-modal data to single-modality neural networks.

</details>


### [101] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/abs/2505.24371)
*Md Intisar Chowdhury,Kittinun Aukkapinyo,Hiroshi Fujimura,Joo Ann Woo,Wasu Wasusatein,Fadoua Ghourabi*

Main category: cs.CV

TL;DR: 提出了一种基于网格的局部和全局区域转录系统（Grid-LoGAT），用于视频问答（VideoQA），通过视觉语言模型（VLM）提取视频帧文本，再通过大语言模型（LLM）生成答案，在隐私保护的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频问答中文本转录质量和隐私保护问题，同时提升局部细节和全局信息的整合能力。

Method: 分两阶段：1）使用VLM从视频帧提取文本；2）使用LLM处理问题生成答案。采用网格化视觉提示提升转录质量。

Result: 在NExT-QA和STAR-QA数据集上分别达到65.9%和50.11%的准确率，优于同类基线模型，并在定位问题上比非网格版本提升24分。

Conclusion: Grid-LoGAT在视频问答中表现优异，尤其在定位任务上显著提升，同时兼顾隐私保护。

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA.

</details>


### [102] [D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding](https://arxiv.org/abs/2505.24372)
*Yichi Zhang,Gongwei Chen,Jun Zhu,Jia Wan*

Main category: cs.CV

TL;DR: D2AF是一种仅使用输入图像的视觉定位标注框架，通过多模态大模型和对象检测模型生成区域-文本对，提升数据量和多样性，并通过过滤方法优化数据质量，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉定位任务需要大规模数据集，但人工标注成本高且限制数据规模和多样性。现有伪标签生成方法依赖人工标注，难以扩展。

Method: 提出D2AF框架，结合多模态大模型和对象检测模型，采用双驱动标注策略生成区域-文本对，并通过一致性分布感知过滤优化数据质量。

Result: 实验表明，增加数据量提升模型性能，但提升程度取决于伪标签对数据分布的扩展效果。D2AF在三个视觉定位任务中取得最优结果。

Conclusion: D2AF通过自动化标注和优化数据质量，克服了数据规模和多样性限制，显著提升了视觉定位模型的性能。

Abstract: Visual Grounding is a task that aims to localize a target region in an image
based on a free-form natural language description. With the rise of Transformer
architectures, there is an increasing need for larger datasets to boost
performance. However, the high cost of manual annotation poses a challenge,
hindering the scale of data and the ability of large models to enhance their
effectiveness. Previous pseudo label generation methods heavily rely on
human-labeled captions of the original dataset, limiting scalability and
diversity. To address this, we propose D2AF, a robust annotation framework for
visual grounding using only input images. This approach overcomes dataset size
limitations and enriches both the quantity and diversity of referring
expressions. Our approach leverages multimodal large models and object
detection models. By implementing dual-driven annotation strategies, we
effectively generate detailed region-text pairs using both closed-set and
open-set approaches. We further conduct an in-depth analysis of data quantity
and data distribution. Our findings demonstrate that increasing data volume
enhances model performance. However, the degree of improvement depends on how
well the pseudo labels broaden the original data distribution. Based on these
insights, we propose a consistency and distribution aware filtering method to
further improve data quality by effectively removing erroneous and redundant
data. This approach effectively eliminates noisy data, leading to improved
performance. Experiments on three visual grounding tasks demonstrate that our
method significantly improves the performance of existing models and achieves
state-of-the-art results.

</details>


### [103] [Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification](https://arxiv.org/abs/2505.24375)
*Maciej Wielgosz,Simon Berg,Heikki Korpunen,Stephan Hoffmann*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于从行车记录仪视频中分类林业操作，使用3D ResNet-50架构，验证F1分数为0.88，精度为0.90。


<details>
  <summary>Details</summary>
Motivation: 减少传统时间研究中的人工工作量，为林业操作监测和效率分析提供可扩展的解决方案。

Method: 采用3D ResNet-50架构，结合PyTorchVideo实现，使用手动标注的现场记录数据集进行训练，并集成标准预处理和数据增强技术。

Result: 模型在验证集上表现优异（F1分数0.88，精度0.90），但存在过拟合问题，需更多数据和类别平衡。

Conclusion: 该方法在林业环境中展示了时空卷积网络的有效性，为未来实时活动识别系统奠定了基础，计划改进包括数据集扩展和嵌入式系统部署。

Abstract: This paper presents a deep learning-based framework for classifying forestry
operations from dashcam video footage. Focusing on four key work elements -
crane-out, cutting-and-to-processing, driving, and processing - the approach
employs a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a
manually annotated dataset of field recordings, the model achieves strong
performance, with a validation F1 score of 0.88 and precision of 0.90. These
results underscore the effectiveness of spatiotemporal convolutional networks
for capturing both motion patterns and appearance in real-world forestry
environments.
  The system integrates standard preprocessing and augmentation techniques to
improve generalization, but overfitting is evident, highlighting the need for
more training data and better class balance. Despite these challenges, the
method demonstrates clear potential for reducing the manual workload associated
with traditional time studies, offering a scalable solution for operational
monitoring and efficiency analysis in forestry.
  This work contributes to the growing application of AI in natural resource
management and sets the foundation for future systems capable of real-time
activity recognition in forest machinery. Planned improvements include dataset
expansion, enhanced regularization, and deployment trials on embedded systems
for in-field use.

</details>


### [104] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/abs/2505.24380)
*Zheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于条带感知空间感知的细粒度鸟类图像分类框架，通过捕捉图像中的长距离空间依赖关系，提升模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 细粒度鸟类图像分类（FBIC）在生态监测和物种识别中具有重要意义，但由于物种大小、成像距离、复杂背景和鸟类姿态多变等因素，传统方法难以稳定提取判别性特征。

Method: 提出了一种结合扩展感知聚合器（EPA）和通道语义编织（CSW）的框架，EPA整合局部纹理和全局结构信息，CSW在通道维度自适应融合长短期信息。

Result: 在CUB-200-2011数据集上的实验表明，该框架显著提升了性能，同时保持了架构的高效性。

Conclusion: 该框架通过条带感知空间感知有效解决了FBIC中的挑战，提升了模型的鲁棒性和可解释性。

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [105] [Leadership Assessment in Pediatric Intensive Care Unit Team Training](https://arxiv.org/abs/2505.24389)
*Liangyang Ouyang,Yuki Sakai,Ryosuke Furuta,Hisataka Nozawa,Hikoro Matsui,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了一种基于自我中心视觉的自动化框架，用于评估PICU团队的领导能力，通过行为线索（如注视对象、眼神交流和对话模式）进行分析。


<details>
  <summary>Details</summary>
Motivation: 评估PICU团队的领导能力是提升团队效率的关键，但传统方法依赖主观评估，缺乏客观性。

Method: 使用Aria Glasses记录视频、音频、注视和头部运动数据，结合REMoDNaV、SAM、YOLO和ChatGPT进行自动化分析。

Result: 实验显示领导能力与行为指标（如注视时间、转换模式和直接指令）显著相关。

Conclusion: 提出的框架能有效评估PICU团队的领导能力，为培训提供客观依据。

Abstract: This paper addresses the task of assessing PICU team's leadership skills by
developing an automated analysis framework based on egocentric vision. We
identify key behavioral cues, including fixation object, eye contact, and
conversation patterns, as essential indicators of leadership assessment. In
order to capture these multimodal signals, we employ Aria Glasses to record
egocentric video, audio, gaze, and head movement data. We collect one-hour
videos of four simulated sessions involving doctors with different roles and
levels. To automate data processing, we propose a method leveraging REMoDNaV,
SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection,
and conversation classification. In the experiments, significant correlations
are observed between leadership skills and behavioral metrics, i.e., the output
of our proposed methods, such as fixation time, transition patterns, and direct
orders in speech. These results indicate that our proposed data collection and
analysis framework can effectively solve skill assessment for training PICU
teams.

</details>


### [106] [S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification](https://arxiv.org/abs/2505.24401)
*Xianheng Ma,Hongchen Tan,Xiuping Liu,Yi Zhang,Huasheng Wang,Jiang Liu,Ying Chen,Hantao Liu*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和脉冲神经网络的低参数高效长序列事件行人重识别模型S3CE-Net，结合空间-时间注意力机制和特征采样策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机在恶劣光照条件下减少背景干扰、保护隐私的优势，研究长序列事件行人重识别任务。

Method: 基于脉冲神经网络构建S3CE-Net，引入空间-时间注意力机制（SSAM）和时空特征采样策略（STFS），实现高效语义关联和特征提取。

Result: 在多个主流数据集上验证了S3CE-Net的优异性能。

Conclusion: S3CE-Net是一种低参数、高效的长序列事件行人重识别模型，具有广泛的应用潜力。

Abstract: In this paper, we leverage the advantages of event cameras to resist harsh
lighting conditions, reduce background interference, achieve high time
resolution, and protect facial information to study the long-sequence
event-based person re-identification (Re-ID) task. To this end, we propose a
simple and efficient long-sequence event Re-ID model, namely the Spike-guided
Spatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better
handle asynchronous event data, we build S3CE-Net based on spiking neural
networks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal
Attention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy
(STFS). The SSAM is designed to carry out semantic interaction and association
in both spatial and temporal dimensions, leveraging the capabilities of SNNs.
The STFS involves sampling spatial feature subsequences and temporal feature
subsequences from the spatiotemporal dimensions, driving the Re-ID model to
perceive broader and more robust effective semantics. Notably, the STFS
introduces no additional parameters and is only utilized during the training
stage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for
long-sequence event-based person Re-ID. Extensive experiments have verified
that our S3CE-Net achieves outstanding performance on many mainstream
long-sequence event-based person Re-ID datasets. Code is available
at:https://github.com/Mhsunshine/SC3E_Net.

</details>


### [107] [Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing](https://arxiv.org/abs/2505.24402)
*Mika Feng,Koichi Ito,Takafumi Aoki,Tetsushi Ohki,Masakatsu Nishigaki*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Transformer（ViT）的人脸反欺骗攻击检测方法，利用ViT的中间特征平衡局部和全局特征，并结合两种数据增强方法提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统可能被恶意攻击者通过照片欺骗，因此需要检测欺骗攻击以确保安全性。

Method: 使用ViT的中间特征计算损失和得分，结合两种数据增强方法（人脸反欺骗数据增强和分块数据增强）。

Result: 在OULU-NPU和SiW数据集上验证了方法的有效性。

Conclusion: 提出的方法能有效检测人脸欺骗攻击，结合数据增强进一步提升了检测精度。

Abstract: Face recognition systems are designed to be robust against changes in head
pose, illumination, and blurring during image capture. If a malicious person
presents a face photo of the registered user, they may bypass the
authentication process illegally. Such spoofing attacks need to be detected
before face recognition. In this paper, we propose a spoofing attack detection
method based on Vision Transformer (ViT) to detect minute differences between
live and spoofed face images. The proposed method utilizes the intermediate
features of ViT, which have a good balance between local and global features
that are important for spoofing attack detection, for calculating loss in
training and score in inference. The proposed method also introduces two data
augmentation methods: face anti-spoofing data augmentation and patch-wise data
augmentation, to improve the accuracy of spoofing attack detection. We
demonstrate the effectiveness of the proposed method through experiments using
the OULU-NPU and SiW datasets.

</details>


### [108] [PCIE_Interaction Solution for Ego4D Social Interaction Challenge](https://arxiv.org/abs/2505.24404)
*Kanokphan Lertniphonphan,Feng Chen,Junda Xu,Fengbu Lan,Jun Xie,Tao Zhang,Zhepeng Wang*

Main category: cs.CV

TL;DR: 团队提出了PCIE_Interaction解决方案，用于CVPR 2025的Ego4D社交互动挑战，涵盖LAM和TTM任务，通过视觉和音频融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决社交互动检测中视觉和音频信号的融合问题，提升LAM和TTM任务的准确性。

Method: LAM任务使用面部质量增强和集成方法；TTM任务通过视觉质量评分加权融合音频和视觉信号。

Result: 在LAM和TTM任务中分别取得0.81和0.71的mAP。

Conclusion: PCIE_Interaction通过多模态融合有效提升了社交互动检测性能。

Abstract: This report presents our team's PCIE_Interaction solution for the Ego4D
Social Interaction Challenge at CVPR 2025, addressing both Looking At Me (LAM)
and Talking To Me (TTM) tasks. The challenge requires accurate detection of
social interactions between subjects and the camera wearer, with LAM relying
exclusively on face crop sequences and TTM combining speaker face crops with
synchronized audio segments. In the LAM track, we employ face quality
enhancement and ensemble methods. For the TTM task, we extend visual
interaction analysis by fusing audio and visual cues, weighted by a visual
quality score. Our approach achieved 0.81 and 0.71 mean average precision (mAP)
on the LAM and TTM challenges leader board. Code is available at
https://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction

</details>


### [109] [IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models](https://arxiv.org/abs/2505.24406)
*Hanting Wang,Tao Jin,Wang Lin,Shulei Wang,Hai Huang,Shengpeng Ji,Zhou Zhao*

Main category: cs.CV

TL;DR: IRBridge框架利用预训练生成模型改进图像修复任务，避免为每种退化类型单独训练模型，提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法需为每种退化类型单独训练桥接模型，计算成本高且性能有限。

Method: 提出过渡方程桥接两个扩散过程，引入IRBridge框架直接利用生成模型。

Result: 在六种图像修复任务中验证了IRBridge的鲁棒性和泛化性能提升。

Conclusion: IRBridge为图像修复提供了一种更灵活、高效的解决方案。

Abstract: Bridge models in image restoration construct a diffusion process from
degraded to clear images. However, existing methods typically require training
a bridge model from scratch for each specific type of degradation, resulting in
high computational costs and limited performance. This work aims to efficiently
leverage pretrained generative priors within existing image restoration bridges
to eliminate this requirement. The main challenge is that standard generative
models are typically designed for a diffusion process that starts from pure
noise, while restoration tasks begin with a low-quality image, resulting in a
mismatch in the state distributions between the two processes. To address this
challenge, we propose a transition equation that bridges two diffusion
processes with the same endpoint distribution. Based on this, we introduce the
IRBridge framework, which enables the direct utilization of generative models
within image restoration bridges, offering a more flexible and adaptable
approach to image restoration. Extensive experiments on six image restoration
tasks demonstrate that IRBridge efficiently integrates generative priors,
resulting in improved robustness and generalization performance. Code will be
available at GitHub.

</details>


### [110] [PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge](https://arxiv.org/abs/2505.24411)
*Feng Chen,Kanokphan Lertniphonphan,Qiancheng Yan,Xiaohui Fan,Jun Xie,Tao Zhang,Zhepeng Wang*

Main category: cs.CV

TL;DR: 团队提出HP-ViT+和时空特征融合方法，在CVPR2025挑战赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决RGB第一人称视频中手部和身体姿态估计的复杂问题，如细微动作和遮挡。

Method: HP-ViT+结合Vision Transformer和CNN，加权融合优化预测；身体姿态采用多模态时空特征融合。

Result: 手部姿态PA-MPJPE 8.31，身体姿态MPJPE 11.25，熟练度估计准确率0.53。

Conclusion: 方法在多个挑战赛中表现优异，验证了技术的有效性。

Abstract: This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D
Pose and Proficiency Estimation Challenges at CVPR2025. Focused on the
intricate task of estimating 21 3D hand joints from RGB egocentric videos,
which are complicated by subtle movements and frequent occlusions, we developed
the Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a
Vision Transformer and a CNN backbone, using weighted fusion to refine the hand
pose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal
spatio-temporal feature integration strategy to address the complexities of
body pose estimation across dynamic contexts. Our methods achieved remarkable
performance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the
Body Pose Challenge, securing championship titles in both competitions. We
extended our pose estimation solutions to the Proficiency Estimation task,
applying core technologies such as transformer-based architectures. This
extension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the
Demonstrator Proficiency Estimation competition.

</details>


### [111] [EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering](https://arxiv.org/abs/2505.24417)
*Runnan Lu,Yuxuan Zhang,Jiaming Liu,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: EasyText是一个基于DiT的多语言文本渲染框架，通过字符定位编码和位置编码插值技术实现精确控制，并在大规模数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在单语言文本渲染上取得进展，但多语言文本渲染仍未被充分探索。本文旨在填补这一空白。

Method: 提出EasyText框架，结合字符定位编码和位置编码插值技术，并使用大规模合成数据集进行预训练和微调。

Result: 实验表明，EasyText在多语言文本渲染、视觉质量和布局感知文本集成方面表现优越。

Conclusion: EasyText为多语言文本渲染提供了一种高效且可控的解决方案。

Abstract: Generating accurate multilingual text with diffusion models has long been
desired but remains challenging. Recent methods have made progress in rendering
text in a single language, but rendering arbitrary languages is still an
unexplored area. This paper introduces EasyText, a text rendering framework
based on DiT (Diffusion Transformer), which connects denoising latents with
multilingual character tokens encoded as character tokens. We propose character
positioning encoding and position encoding interpolation techniques to achieve
controllable and precise text rendering. Additionally, we construct a
large-scale synthetic text image dataset with 1 million multilingual image-text
annotations as well as a high-quality dataset of 20K annotated images, which
are used for pretraining and fine-tuning respectively. Extensive experiments
and evaluations demonstrate the effectiveness and advancement of our approach
in multilingual text rendering, visual quality, and layout-aware text
integration.

</details>


### [112] [Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation](https://arxiv.org/abs/2505.24431)
*Bozhong Zheng,Jinye Gan,Xiaohao Xu,Wenqiao Li,Xiaonan Huang,Na Ni,Yingna Wu*

Main category: cs.CV

TL;DR: PASDF是一种新型3D点云异常检测与修复框架，通过连续、姿态不变的形状表示提升检测精度和修复能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的方法因离散体素化或投影表示导致几何保真度问题，限制了细粒度异常定位。

Method: 提出PASDF框架，结合姿态对齐模块和SDF网络，动态整合姿态信息，学习连续SDF的高保真修复模板。

Result: 在Real3D-AD和Anomaly-ShapeNet上分别达到80.2%和90.0%的AUROC分数，表现优异。

Conclusion: 连续几何表示在3D异常检测和修复中具有显著优势，PASDF框架为相关研究提供了新思路。

Abstract: 3D point cloud anomaly detection is essential for robust vision systems but
is challenged by pose variations and complex geometric anomalies. Existing
patch-based methods often suffer from geometric fidelity issues due to discrete
voxelization or projection-based representations, limiting fine-grained anomaly
localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel
framework that integrates 3D anomaly detection and repair by learning a
continuous, pose-invariant shape representation. PASDF leverages a Pose
Alignment Module for canonicalization and a SDF Network to dynamically
incorporate pose, enabling implicit learning of high-fidelity anomaly repair
templates from the continuous SDF. This facilitates precise pixel-level anomaly
localization through an Anomaly-Aware Scoring Module. Crucially, the continuous
3D representation in PASDF extends beyond detection, facilitating in-situ
anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate
state-of-the-art performance, achieving high object-level AUROC scores of 80.2%
and 90.0%, respectively. These results highlight the effectiveness of
continuous geometric representations in advancing 3D anomaly detection and
facilitating practical anomaly region repair. The code is available at
https://github.com/ZZZBBBZZZ/PASDF to support further research.

</details>


### [113] [SORCE: Small Object Retrieval in Complex Environments](https://arxiv.org/abs/2505.24441)
*Chunxu Liu,Chi Xie,Xiaxu Chen,Wei Li,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了SORCE（复杂环境中的小物体检索），作为文本到图像检索（T2IR）的新子领域，专注于通过文本查询检索复杂图像中的小物体。作者提出了SORCE-1K基准，并发现现有T2IR方法表现不佳，因此提出了一种基于多模态大语言模型（MLLM）和区域提示（ReP）的多嵌入方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有T2IR基准主要关注整体图像语义或显著前景物体，忽略了复杂环境中不显眼的小物体。这些小物体检索在实际应用中至关重要，因此需要专门的研究和基准。

Method: 提出SORCE-1K基准，包含复杂环境图像和描述小物体的文本查询。为解决现有方法的不足，提出基于MLLM和ReP的多嵌入表示方法，为每张图像生成多个嵌入。

Result: 实验结果表明，多嵌入方法在SORCE-1K上显著优于现有T2IR方法，验证了多嵌入表示和文本定制MLLM特征的有效性。

Conclusion: SORCE-1K为小物体检索提供了有效的基准，多嵌入方法和MLLM特征为解决该任务展示了潜力。

Abstract: Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a
given textual query to images in a gallery. Existing benchmarks primarily focus
on textual queries describing overall image semantics or foreground salient
objects, possibly overlooking inconspicuous small objects, especially in
complex environments. Such small object retrieval is crucial, as in real-world
applications, the targets of interest are not always prominent in the image.
Thus, we introduce SORCE (Small Object Retrieval in Complex Environments), a
new subfield of T2IR, focusing on retrieving small objects in complex images
with textual queries. We propose a new benchmark, SORCE-1K, consisting of
images with complex environments and textual queries describing less
conspicuous small objects with minimal contextual cues from other salient
objects. Preliminary analysis on SORCE-1K finds that existing T2IR methods
struggle to capture small objects and encode all the semantics into a single
embedding, leading to poor retrieval performance on SORCE-1K. Therefore, we
propose to represent each image with multiple distinctive embeddings. We
leverage Multimodal Large Language Models (MLLMs) to extract multiple
embeddings for each image instructed by a set of Regional Prompts (ReP).
Experimental results show that our multi-embedding approach through MLLM and
ReP significantly outperforms existing T2IR methods on SORCE-1K. Our
experiments validate the effectiveness of SORCE-1K for benchmarking SORCE
performances, highlighting the potential of multi-embedding representation and
text-customized MLLM features for addressing this task.

</details>


### [114] [Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers](https://arxiv.org/abs/2505.24443)
*Heejo Kong,Sung-Jin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAC的新框架，通过利用多个模型在未标记数据上的预测差异，提升开放集半监督学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习假设标记和未标记数据具有相同的类别分布，但实际中未标记数据常包含异常类别（离群点），导致模型性能下降。现有方法依赖单一模型的预测差异，在标记数据不足时容易失效。

Method: DAC通过训练多个具有不同偏见的模型，利用它们在未标记数据上的预测不一致性来检测离群点。关键贡献是通过单一训练过程构建这些模型。

Result: 该方法在标记数据不足时仍能有效检测离群点，性能优于现有方法。

Conclusion: DAC框架通过利用模型间的预测差异，显著提升了开放集半监督学习的鲁棒性。

Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and
unlabeled data share an identical class distribution, however in practice, this
assumption is easily violated, as unlabeled data often includes unknown class
data, i.e., outliers. The outliers are treated as noise, considerably degrading
the performance of SSL models. To address this drawback, we propose a novel
framework, Diversify and Conquer (DAC), to enhance SSL robustness in the
context of open-set semi-supervised learning. In particular, we note that
existing open-set SSL methods rely on prediction discrepancies between inliers
and outliers from a single model trained on labeled data. This approach can be
easily failed when the labeled data is insufficient, leading to performance
degradation that is worse than naive SSL that do not account for outliers. In
contrast, our approach exploits prediction disagreements among multiple models
that are differently biased towards the unlabeled distribution. By leveraging
the discrepancies arising from training on unlabeled data, our method enables
robust outlier detection even when the labeled data is underspecified. Our key
contribution is constructing a collection of differently biased models through
a single training process. By encouraging divergent heads to be differently
biased towards outliers while making consistent predictions for inliers, we
exploit the disagreement among these heads as a measure to identify unknown
concepts. Our code is available at https://github.com/heejokong/DivCon.

</details>


### [115] [SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking](https://arxiv.org/abs/2505.24466)
*Yingjia Xu,Jinlin Wu,Zhen Chen,Daming Gao,Yang Yang,Zhen Lei,Min Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于文本的人物检索方法，通过结合行人外观和场景上下文信息，提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注外观跨模态检索，忽略了场景中的上下文信息，而这些信息可能对检索有重要补充作用。

Method: 提出了SA-Person框架，分为两阶段：第一阶段对齐文本与行人区域；第二阶段通过SceneRanker结合场景上下文重新排序。

Result: 在SCENEPERSON-13W数据集上验证了框架的有效性。

Conclusion: 结合外观和场景信息的检索方法在复杂场景中表现更优。

Abstract: Text-based person retrieval aims to identify a target individual from a
gallery of images based on a natural language description. It presents a
significant challenge due to the complexity of real-world scenes and the
ambiguity of appearance-related descriptions. Existing methods primarily
emphasize appearance-based cross-modal retrieval, often neglecting the
contextual information embedded within the scene, which can offer valuable
complementary insights for retrieval. To address this, we introduce
SCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich
annotations covering both pedestrian appearance and environmental cues. Based
on this, we propose SA-Person, a two-stage retrieval framework. In the first
stage, it performs discriminative appearance grounding by aligning textual cues
with pedestrian-specific regions. In the second stage, it introduces
SceneRanker, a training-free, scene-aware re-ranking method leveraging
multimodal large language models to jointly reason over pedestrian appearance
and the global scene context. Experiments on SCENEPERSON-13W validate the
effectiveness of our framework in challenging scene-level retrieval scenarios.
The code and dataset will be made publicly available.

</details>


### [116] [SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds](https://arxiv.org/abs/2505.24475)
*Cheng Zeng,Xiatian Qi,Chi Chen,Kai Sun,Wangle Zhang,Yuxuan Liu,Yan Meng,Bisheng Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种用于点云屋顶平面实例分割的两阶段超点生成方法，结合手工特征和新型解码器，显著提升了Transformer模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有超点Transformer在点云屋顶平面分割中性能受限，主要由于低质量超点。研究旨在解决这一问题并提升分割效果。

Method: 提出了满足高质量超点的两个标准及两阶段生成过程，结合手工特征，设计了结合Kolmogorov-Arnold网络和Transformer模块的解码器，并采用传统算法后处理优化预测。

Result: 在自建数据集和修正的RoofN3D数据集上实现了最优性能，且模型对边界标注不敏感，降低了标注负担。

Conclusion: 研究揭示了点云密度、均匀性和精度对分割性能的影响，强调了数据增强策略的重要性。

Abstract: Transformers have been seldom employed in point cloud roof plane instance
segmentation, which is the focus of this study, and existing superpoint
Transformers suffer from limited performance due to the use of low-quality
superpoints. To address this challenge, we establish two criteria that
high-quality superpoints for Transformers should satisfy and introduce a
corresponding two-stage superpoint generation process. The superpoints
generated by our method not only have accurate boundaries, but also exhibit
consistent geometric sizes and shapes, both of which greatly benefit the
feature learning of superpoint Transformers. To compensate for the limitations
of deep learning features when the training set size is limited, we incorporate
multidimensional handcrafted features into the model. Additionally, we design a
decoder that combines a Kolmogorov-Arnold Network with a Transformer module to
improve instance prediction and mask extraction. Finally, our network's
predictions are refined using traditional algorithm-based postprocessing. For
evaluation, we annotated a real-world dataset and corrected annotation errors
in the existing RoofN3D dataset. Experimental results show that our method
achieves state-of-the-art performance on our dataset, as well as both the
original and reannotated RoofN3D datasets. Moreover, our model is not sensitive
to plane boundary annotations during training, significantly reducing the
annotation burden. Through comprehensive experiments, we also identified key
factors influencing roof plane segmentation performance: in addition to roof
types, variations in point cloud density, density uniformity, and 3D point
precision have a considerable impact. These findings underscore the importance
of incorporating data augmentation strategies that account for point cloud
quality to enhance model robustness under diverse and challenging conditions.

</details>


### [117] [Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model](https://arxiv.org/abs/2505.24476)
*Yuting Zhang,Hao Lu,Qingyong Hu,Yin Wang,Kaishen Yuan,Xin Liu,Kaishun Wu*

Main category: cs.CV

TL;DR: Period-LLM是一种多模态大语言模型，旨在提升周期性任务的性能，并通过“从易到难”的范式构建基准测试。


<details>
  <summary>Details</summary>
Motivation: 周期性现象在自然过程中普遍存在，但当前多模态大语言模型在周期性任务上表现不佳。

Method: 采用“从易到难”的范式，逐步提升模型能力，并提出“抵抗逻辑遗忘”优化策略。

Result: 实验证明Period-LLM在周期性任务上优于现有模型。

Conclusion: Period-LLM为多模态周期性任务提供了有效解决方案。

Abstract: Periodic or quasi-periodic phenomena reveal intrinsic characteristics in
various natural processes, such as weather patterns, movement behaviors,
traffic flows, and biological signals. Given that these phenomena span multiple
modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer
promising potential to effectively capture and understand their complex nature.
However, current MLLMs struggle with periodic tasks due to limitations in: 1)
lack of temporal modelling and 2) conflict between short and long periods. This
paper introduces Period-LLM, a multimodal large language model designed to
enhance the performance of periodic tasks across various modalities, and
constructs a benchmark of various difficulty for evaluating the cross-modal
periodic capabilities of large models. Specially, We adopt an "Easy to Hard
Generalization" paradigm, starting with relatively simple text-based tasks and
progressing to more complex visual and multimodal tasks, ensuring that the
model gradually builds robust periodic reasoning capabilities. Additionally, we
propose a "Resisting Logical Oblivion" optimization strategy to maintain
periodic reasoning abilities during semantic alignment. Extensive experiments
demonstrate the superiority of the proposed Period-LLM over existing MLLMs in
periodic tasks. The code is available at
https://github.com/keke-nice/Period-LLM.

</details>


### [118] [ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation](https://arxiv.org/abs/2505.24481)
*Jing Huang,Yongkang Zhao,Yuhan Li,Zhitao Dai,Cheng Chen,Qiying Lai*

Main category: cs.CV

TL;DR: ACM-UNet是一种通用的医学图像分割框架，通过轻量级适配器机制结合预训练的CNN和Mamba模型，解决了结构不匹配问题，并在Synapse和ACDC基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分利用预训练视觉骨干网络（如ResNet、ViT、VMamba）的能力，因结构不匹配导致性能受限。

Method: 提出ACM-UNet，保留UNet简单设计，通过适配器机制整合CNN和Mamba模型，并在解码器中引入多尺度小波变换模块。

Result: 在Synapse数据集上达到85.12% Dice Score和13.89mm HD95，计算效率高（17.93G FLOPs）。

Conclusion: ACM-UNet通过结合CNN和SSM的优势，实现了高性能和高效计算，为医学图像分割提供了有效解决方案。

Abstract: The U-shaped encoder-decoder architecture with skip connections has become a
prevailing paradigm in medical image segmentation due to its simplicity and
effectiveness. While many recent works aim to improve this framework by
designing more powerful encoders and decoders, employing advanced convolutional
neural networks (CNNs) for local feature extraction, Transformers or state
space models (SSMs) such as Mamba for global context modeling, or hybrid
combinations of both, these methods often struggle to fully utilize pretrained
vision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To
bridge this gap, we introduce ACM-UNet, a general-purpose segmentation
framework that retains a simple UNet-like design while effectively
incorporating pretrained CNNs and Mamba models through a lightweight adapter
mechanism. This adapter resolves architectural incompatibilities and enables
the model to harness the complementary strengths of CNNs and SSMs-namely,
fine-grained local detail extraction and long-range dependency modeling.
Additionally, we propose a hierarchical multi-scale wavelet transform module in
the decoder to enhance feature fusion and reconstruction fidelity. Extensive
experiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet
achieves state-of-the-art performance while remaining computationally
efficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the
Synapse dataset with 17.93G FLOPs, showcasing its effectiveness and
scalability. Code is available at: https://github.com/zyklcode/ACM-UNet.

</details>


### [119] [Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing](https://arxiv.org/abs/2505.24489)
*Anasse Boutayeb,Iyad Lahsen-cherif,Ahmed El Khadimi*

Main category: cs.CV

TL;DR: 论文提出了一种基于Deformable-DETR模型的目标检测方法，应用于光学和SAR遥感图像，取得了较高的F1分数。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的目标检测具有重要意义，而基于Transformer的深度学习模型在此任务中表现突出。

Method: 使用Deformable-DETR模型，分别在光学（Pleiades Aircraft数据集）和SAR（SSDD数据集）图像上进行实验。

Result: 10折分层验证显示，模型在光学数据集上F1分数为95.12%，在SSDD上为94.54%，优于其他对比模型。

Conclusion: Deformable-DETR在遥感图像目标检测中表现优异，尤其在光学和SAR图像上具有显著优势。

Abstract: Object detection has recently seen an interesting trend in terms of the most
innovative research work, this task being of particular importance in the field
of remote sensing, given the consistency of these images in terms of
geographical coverage and the objects present. Furthermore, Deep Learning (DL)
models, in particular those based on Transformers, are especially relevant for
visual computing tasks in general, and target detection in particular. Thus,
the present work proposes an application of Deformable-DETR model, a specific
architecture using deformable attention mechanisms, on remote sensing images in
two different modes, especially optical and Synthetic Aperture Radar (SAR). To
achieve this objective, two datasets are used, one optical, which is Pleiades
Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset
(SSDD). The results of a 10-fold stratified validation showed that the proposed
model performed particularly well, obtaining an F1 score of 95.12% for the
optical dataset and 94.54% for SSDD, while comparing these results with several
models detections, especially those based on CNNs and transformers, as well as
those specifically designed to detect different object classes in remote
sensing images.

</details>


### [120] [Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation](https://arxiv.org/abs/2505.24499)
*Ximing Xing,Yandong Guan,Jing Zhang,Dong Xu,Qian Yu*

Main category: cs.CV

TL;DR: Reason-SVG是一个新框架，通过“Drawing-with-Thought”范式增强LLM生成SVG的能力，结合监督微调和强化学习，显著提升SVG生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成SVG时缺乏结构有效性、语义忠实性和视觉连贯性，Reason-SVG旨在解决这一问题。

Method: 采用两阶段训练策略：监督微调激活基础推理能力，强化学习（GRPO）优化生成过程；设计混合奖励函数评估推理和SVG质量。

Result: Reason-SVG显著提升了LLM生成SVG的准确性和视觉吸引力，并引入了SVGX-DwT-10k数据集。

Conclusion: Reason-SVG通过结合DwT、SFT和强化学习，为LLM生成高质量SVG提供了有效解决方案。

Abstract: Generating high-quality Scalable Vector Graphics (SVGs) is challenging for
Large Language Models (LLMs), as it requires advanced reasoning for structural
validity, semantic faithfulness, and visual coherence -- capabilities in which
current LLMs often fall short. In this work, we introduce Reason-SVG, a novel
framework designed to enhance LLM reasoning for SVG generation. Reason-SVG
pioneers the "Drawing-with-Thought" (DwT) paradigm, in which models generate
both SVG code and explicit design rationales, mimicking the human creative
process. Reason-SVG adopts a two-stage training strategy: First, Supervised
Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational
reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group
Relative Policy Optimization (GRPO), empowers the model to generate both DwT
and SVGs rationales through refined, reward-driven reasoning. To facilitate
reasoning-driven SVG generation, we design a Hybrid Reward function that
evaluates the presence and utility of DwT reasoning, along with structural
validity, semantic alignment, and visual quality. We also introduce the
SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each
SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT,
and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance
in generating accurate and visually compelling SVGs, potentially fostering "Aha
moments" in design.

</details>


### [121] [un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP](https://arxiv.org/abs/2505.24517)
*Yinqi Li,Jiahe Zhao,Hong Chang,Ruibing Hou,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种改进CLIP模型的方法un²CLIP，通过利用生成模型unCLIP的能力来捕捉更多图像细节，同时保持与原始文本编码器的对齐。实验表明，该方法在多个任务中显著优于原始CLIP和其他改进方法。


<details>
  <summary>Details</summary>
Motivation: CLIP在区分图像细节和密集预测任务中表现不佳，因此需要改进以更好地捕捉视觉细节。

Method: 通过反转生成模型unCLIP（称为un²CLIP），利用其捕捉图像细节的能力，同时保持与CLIP文本编码器的对齐。

Result: 实验证明，un²CLIP在MMVP-VLM基准测试、开放词汇分割任务和多模态大语言模型任务中显著优于原始CLIP和其他改进方法。

Conclusion: un²CLIP成功提升了CLIP模型的视觉细节捕捉能力，同时保持了其与文本编码器的对齐，为多模态任务提供了更强大的基础模型。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a foundation model
and has been applied to various vision and multimodal tasks. However, recent
works indicate that CLIP falls short in distinguishing detailed differences in
images and shows suboptimal performance on dense-prediction and vision-centric
multimodal tasks. Therefore, this work focuses on improving existing CLIP
models, aiming to capture as many visual details in images as possible. We find
that a specific type of generative models, unCLIP, provides a suitable
framework for achieving our goal. Specifically, unCLIP trains an image
generator conditioned on the CLIP image embedding. In other words, it inverts
the CLIP image encoder. Compared to discriminative models like CLIP, generative
models are better at capturing image details because they are trained to learn
the data distribution of images. Additionally, the conditional input space of
unCLIP aligns with CLIP's original image-text embedding space. Therefore, we
propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this
way, the improved image encoder can gain unCLIP's visual detail capturing
ability while preserving its alignment with the original text encoder
simultaneously. We evaluate our improved CLIP across various tasks to which
CLIP has been applied, including the challenging MMVP-VLM benchmark, the
dense-prediction open-vocabulary segmentation task, and multimodal large
language model tasks. Experiments show that un$^2$CLIP significantly improves
the original CLIP and previous CLIP improvement methods. Code and models will
be available at https://github.com/LiYinqi/un2CLIP.

</details>


### [122] [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/abs/2505.24519)
*Yuqi Zhang,Yuchun Miao,Zuchao Li,Liang Ding*

Main category: cs.CV

TL;DR: AMIA是一种轻量级的防御方法，通过自动屏蔽无关图像块和联合意图分析，显著提升大型视觉语言模型的安全性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 针对大型视觉语言模型（LVLMs）的对抗性攻击和隐藏有害意图问题，提出一种无需重新训练的防御方法。

Method: 1. 自动屏蔽与文本无关的图像块以破坏对抗性扰动；2. 联合意图分析以发现并缓解隐藏的有害意图。

Result: 防御成功率从平均52.4%提升至81.7%，实用性仅下降2%，推理开销较低。

Conclusion: 屏蔽和意图分析是确保安全性与实用性平衡的关键。

Abstract: We introduce AMIA, a lightweight, inference-only defense for Large
Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of
text-irrelevant image patches to disrupt adversarial perturbations, and (2)
conducts joint Intention Analysis to uncover and mitigate hidden harmful
intents before response generation. Without any retraining, AMIA improves
defense success rates across diverse LVLMs and jailbreak benchmarks from an
average of 52.4% to 81.7%, preserves general utility with only a 2% average
accuracy drop, and incurs only modest inference overhead. Ablation confirms
both masking and intention analysis are essential for a robust safety-utility
trade-off.

</details>


### [123] [UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation](https://arxiv.org/abs/2505.24521)
*Yang-Tian Sun,Xin Yu,Zehuan Huang,Yi-Hua Huang,Yuan-Chen Guo,Ziyi Yang,Yan-Pei Cao,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 利用扩散模型先验辅助单目几何估计，通过全局坐标系设计和联合训练提升视频几何属性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注单帧几何属性估计，忽略了扩散模型在帧间一致性上的潜力。

Method: 1) 选择全局坐标系中与视频帧共享对应关系的几何属性；2) 引入位置编码重用的条件方法；3) 联合训练共享对应关系的多几何属性。

Result: 在视频全局几何属性预测中表现优异，可直接用于重建任务，且能泛化到动态场景。

Conclusion: 通过适当设计和微调，扩散模型的帧间一致性可有效提升几何估计的全局一致性。

Abstract: Recently, methods leveraging diffusion model priors to assist monocular
geometric estimation (e.g., depth and normal) have gained significant attention
due to their strong generalization ability. However, most existing works focus
on estimating geometric properties within the camera coordinate system of
individual video frames, neglecting the inherent ability of diffusion models to
determine inter-frame correspondence. In this work, we demonstrate that,
through appropriate design and fine-tuning, the intrinsic consistency of video
generation models can be effectively harnessed for consistent geometric
estimation. Specifically, we 1) select geometric attributes in the global
coordinate system that share the same correspondence with video frames as the
prediction targets, 2) introduce a novel and efficient conditioning method by
reusing positional encodings, and 3) enhance performance through joint training
on multiple geometric attributes that share the same correspondence. Our
results achieve superior performance in predicting global geometric attributes
in videos and can be directly applied to reconstruction tasks. Even when
trained solely on static video data, our approach exhibits the potential to
generalize to dynamic video scenes.

</details>


### [124] [Optimal Density Functions for Weighted Convolution in Learning Models](https://arxiv.org/abs/2505.24527)
*Simone Cammarasana,Giuseppe Patanè*

Main category: cs.CV

TL;DR: 论文提出了一种加权卷积方法，通过最优密度函数调整邻域像素对中心像素的贡献，显著提升了卷积神经网络的精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积对所有邻域像素一视同仁，而加权卷积通过距离调整贡献，有望提高模型精度。

Method: 提出加权卷积框架，分离优化卷积核权重（随机梯度下降）和密度函数（DIRECT-L算法）。

Result: 实验显示，加权卷积在图像去噪任务中损失降低53%，测试精度提升，但执行时间增加11%。

Conclusion: 加权卷积在精度提升方面表现优异，未来将扩展至2D和3D图像任务。

Abstract: The paper introduces the weighted convolution, a novel approach to the
convolution for signals defined on regular grids (e.g., 2D images) through the
application of an optimal density function to scale the contribution of
neighbouring pixels based on their distance from the central pixel. This choice
differs from the traditional uniform convolution, which treats all neighbouring
pixels equally. Our weighted convolution can be applied to convolutional neural
network problems to improve the approximation accuracy. Given a convolutional
network, we define a framework to compute the optimal density function through
a minimisation model. The framework separates the optimisation of the
convolutional kernel weights (using stochastic gradient descent) from the
optimisation of the density function (using DIRECT-L). Experimental results on
a learning model for an image-to-image task (e.g., image denoising) show that
the weighted convolution significantly reduces the loss (up to 53% improvement)
and increases the test accuracy compared to standard convolution. While this
method increases execution time by 11%, it is robust across several
hyperparameters of the learning model. Future work will apply the weighted
convolution to real-case 2D and 3D image convolutional learning problems.

</details>


### [125] [Geospatial Foundation Models to Enable Progress on Sustainable Development Goals](https://arxiv.org/abs/2505.24528)
*Pedram Ghamisi,Weikang Yu,Xiaokang Zhang,Aldino Rizaldy,Jian Wang,Chufeng Zhou,Richard Gloaguen,Gustau Camps-Valls*

Main category: cs.CV

TL;DR: SustainFM是一个基于17个可持续发展目标的基准框架，用于评估地理空间基础模型（FMs）的实际效用及其与可持续发展目标的契合度。研究发现FMs在多样任务中表现优于传统方法，但需关注转移性、泛化能力和能源效率。


<details>
  <summary>Details</summary>
Motivation: 研究地理空间基础模型（FMs）的实际效用及其与全球可持续发展目标的契合度，填补现有研究的空白。

Method: 引入SustainFM框架，通过多样化的任务（如资产财富预测和环境灾害检测）对FMs进行跨学科评估。

Result: FMs在多数任务中优于传统方法，但其评估需包括转移性、泛化能力和能源效率。FMs为可持续发展目标提供了可扩展的解决方案。

Conclusion: 呼吁从模型中心开发转向影响驱动的部署，强调能源效率、领域适应性和伦理考量。

Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have
revolutionized natural language processing and computer vision, and are now
advancing geospatial analysis and Earth Observation (EO). They promise improved
generalization across tasks, scalability, and efficient adaptation with minimal
labeled data. However, despite the rapid proliferation of geospatial FMs, their
real-world utility and alignment with global sustainability goals remain
underexplored. We introduce SustainFM, a comprehensive benchmarking framework
grounded in the 17 Sustainable Development Goals with extremely diverse tasks
ranging from asset wealth prediction to environmental hazard detection. This
study provides a rigorous, interdisciplinary assessment of geospatial FMs and
offers critical insights into their role in attaining sustainability goals. Our
findings show: (1) While not universally superior, FMs often outperform
traditional approaches across diverse tasks and datasets. (2) Evaluating FMs
should go beyond accuracy to include transferability, generalization, and
energy efficiency as key criteria for their responsible use. (3) FMs enable
scalable, SDG-grounded solutions, offering broad utility for tackling complex
sustainability challenges. Critically, we advocate for a paradigm shift from
model-centric development to impact-driven deployment, and emphasize metrics
such as energy efficiency, robustness to domain shifts, and ethical
considerations.

</details>


### [126] [Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts](https://arxiv.org/abs/2505.24541)
*Xin He,Xumeng Han,Longhui Wei,Lingxi Xie,Qi Tian*

Main category: cs.CV

TL;DR: Mixpert是一种高效的多专家视觉架构，通过动态路由机制分配任务，解决了单一视觉编码器在多任务学习中的冲突问题，同时保持了联合优化的优势。


<details>
  <summary>Details</summary>
Motivation: 单一视觉编码器难以处理多样化的视觉任务，且多编码器结构复杂且难以联合优化。Mixpert旨在解决这些问题。

Method: Mixpert采用混合视觉专家架构，结合动态路由机制，为不同任务分配最合适的视觉专家。

Result: 实验表明，Mixpert在多任务学习中显著提升了性能，且计算成本较低。

Conclusion: Mixpert是一种高效且灵活的解决方案，适用于多模态大语言模型中的视觉任务处理。

Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of
complex image information, typically leveraging a vision encoder to perceive
various visual scenarios. However, relying solely on a single vision encoder to
handle diverse task domains proves difficult and inevitably leads to conflicts.
Recent work enhances data perception by directly integrating multiple
domain-specific vision encoders, yet this structure adds complexity and limits
the potential for joint optimization. In this paper, we introduce Mixpert, an
efficient mixture-of-vision-experts architecture that inherits the joint
learning advantages from a single vision encoder while being restructured into
a multi-expert paradigm for task-specific fine-tuning across different visual
tasks. Additionally, we design a dynamic routing mechanism that allocates input
images to the most suitable visual expert. Mixpert effectively alleviates
domain conflicts encountered by a single vision encoder in multi-task learning
with minimal additional computational cost, making it more efficient than
multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,
with experimental results demonstrating substantial performance gains across
various tasks.

</details>


### [127] [Optimal Weighted Convolution for Classification and Denosing](https://arxiv.org/abs/2505.24558)
*Simone Cammarasana,Giuseppe Patanè*

Main category: cs.CV

TL;DR: 提出了一种新型加权卷积算子，通过引入空间密度函数增强传统CNN，提升特征提取能力，兼容现有架构，并在图像分类和去噪任务中表现优于标准卷积。


<details>
  <summary>Details</summary>
Motivation: 传统卷积操作对所有邻域像素赋予相同权重，忽略了空间位置信息，限制了特征提取的灵活性。

Method: 提出加权卷积算子，通过空间密度函数动态调整邻域像素权重，保持参数数量不变，支持多维信号处理，并实现高效计算。

Result: 在CIFAR-100和DIV2K数据集上，加权卷积显著提升性能（如VGG分类准确率从56.89%提升至66.94%，DnCNN去噪PSNR从20.17提升至22.63）。

Conclusion: 加权卷积通过空间密度函数优化特征提取，兼容性强且高效，适用于多种任务和数据类型。

Abstract: We introduce a novel weighted convolution operator that enhances traditional
convolutional neural networks (CNNs) by integrating a spatial density function
into the convolution operator. This extension enables the network to
differentially weight neighbouring pixels based on their relative position to
the reference pixel, improving spatial characterisation and feature extraction.
The proposed operator maintains the same number of trainable parameters and is
fully compatible with existing CNN architectures. Although developed for 2D
image data, the framework is generalisable to signals on regular grids of
arbitrary dimensions, such as 3D volumetric data or 1D time series. We propose
an efficient implementation of the weighted convolution by pre-computing the
density function and achieving execution times comparable to standard
convolution layers. We evaluate our method on two deep learning tasks: image
classification using the CIFAR-100 dataset [KH+09] and image denoising using
the DIV2K dataset [AT17]. Experimental results with state-of-the-art
classification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN
[ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves
performance with respect to standard convolution across different quantitative
metrics. For example, VGG achieves an accuracy of 66.94% with weighted
convolution versus 56.89% with standard convolution on the classification
problem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the
denoising problem. All models were trained on the CINECA Leonardo cluster to
reduce the execution time and improve the tuning of the density function
values. The PyTorch implementation of the weighted convolution is publicly
available at: https://github.com/cammarasana123/weightedConvolution2.0.

</details>


### [128] [Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2505.24567)
*Qinghe Ma,Jian Zhang,Lei Qi,Qian Yu,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 论文提出了一种新场景MiDSS，结合了半监督学习和领域适应，并提出了UST-RUN框架，通过中间域信息提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中，标注数据有限和领域偏移是常见问题，但现有方法通常单独处理。MiDSS场景结合了这两个问题，需要新的解决方案。

Method: 提出了UST-RUN框架，包括UCP构建中间域、SymGD训练策略、TP-RAM渐进式风格融合，以及高质量伪标签样本选择。

Result: 在四个公开数据集上验证了UST-RUN的优越性，特别是在Prostate数据集上Dice分数提升了12.94%。

Conclusion: UST-RUN有效解决了MiDSS问题，通过中间域信息和高质量伪标签实现了显著性能提升。

Abstract: Both limited annotation and domain shift are prevalent challenges in medical
image segmentation. Traditional semi-supervised segmentation and unsupervised
domain adaptation methods address one of these issues separately. However, the
coexistence of limited annotation and domain shift is quite common, which
motivates us to introduce a novel and challenging scenario: Mixed Domain
Semi-supervised medical image Segmentation (MiDSS), where limited labeled data
from a single domain and a large amount of unlabeled data from multiple
domains. To tackle this issue, we propose the UST-RUN framework, which fully
leverages intermediate domain information to facilitate knowledge transfer. We
employ Unified Copy-paste (UCP) to construct intermediate domains, and propose
a Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by
merging pseudo-labels from intermediate samples. Subsequently, we introduce a
Training Process aware Random Amplitude MixUp (TP-RAM) to progressively
incorporate style-transition components into intermediate samples. To generate
more diverse intermediate samples, we further select reliable samples with
high-quality pseudo-labels, which are then mixed with other unlabeled data.
Additionally, we generate sophisticated intermediate samples with high-quality
pseudo-labels for unreliable samples, ensuring effective knowledge transfer for
them. Extensive experiments on four public datasets demonstrate the superiority
of UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the
Prostate dataset. Our code is available at https://github.com/MQinghe/UST-RUN

</details>


### [129] [SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition](https://arxiv.org/abs/2505.24600)
*Omer Nacar,Yasser Al-Habashi,Serry Sibaee,Adel Ammar,Wadii Boulila*

Main category: cs.CV

TL;DR: SARD是一个大规模合成阿拉伯语OCR数据集，旨在填补现有数据集的不足，提供干净、可控的训练环境，并支持多样化的阿拉伯语书籍样式文本处理。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语OCR数据集规模小、多样性不足，且缺乏真实书籍布局的复杂性，限制了现代OCR模型的训练效果。

Method: 通过合成生成843,622个文档图像，包含6.9亿单词，覆盖10种阿拉伯字体，模拟书籍布局，避免真实噪声。

Result: SARD为OCR模型提供了高质量的训练数据，支持传统和深度学习方法，展示了其在处理复杂阿拉伯文本中的潜力。

Conclusion: SARD是开发和评估鲁棒OCR及视觉语言模型的重要资源，能够处理多样化的阿拉伯书籍样式文本。

Abstract: Arabic Optical Character Recognition (OCR) is essential for converting vast
amounts of Arabic print media into digital formats. However, training modern
OCR models, especially powerful vision-language models, is hampered by the lack
of large, diverse, and well-structured datasets that mimic real-world book
layouts. Existing Arabic OCR datasets often focus on isolated words or lines or
are limited in scale, typographic variety, or structural complexity found in
books. To address this significant gap, we introduce SARD (Large-Scale
Synthetic Arabic OCR Dataset). SARD is a massive, synthetically generated
dataset specifically designed to simulate book-style documents. It comprises
843,622 document images containing 690 million words, rendered across ten
distinct Arabic fonts to ensure broad typographic coverage. Unlike datasets
derived from scanned documents, SARD is free from real-world noise and
distortions, offering a clean and controlled environment for model training.
Its synthetic nature provides unparalleled scalability and allows for precise
control over layout and content variation. We detail the dataset's composition
and generation process and provide benchmark results for several OCR models,
including traditional and deep learning approaches, highlighting the challenges
and opportunities presented by this dataset. SARD serves as a valuable resource
for developing and evaluating robust OCR and vision-language models capable of
processing diverse Arabic book-style texts.

</details>


### [130] [GARLIC: GAussian Representation LearnIng for spaCe partitioning](https://arxiv.org/abs/2505.24608)
*Panagiotis Rigas,Panagiotis Drivas,Charalambos Tzamos,Ioannis Chamodrakas,George Ioannakis,Leonidas J. Guibas,Ioannis Z. Emiris*

Main category: cs.CV

TL;DR: GARLIC是一种基于高维高斯分布的新型索引结构，用于高效学习和搜索高维向量空间，结合信息论目标优化，支持快速构建和高精度检索。


<details>
  <summary>Details</summary>
Motivation: 传统的高维向量空间索引方法在速度和精度上存在不足，GARLIC通过高斯分布和渐进优化来解决这一问题。

Method: 利用高斯分布表示空间分区，通过分裂和克隆操作渐进优化高斯参数，结合信息论目标平衡覆盖率和一致性。

Result: 在标准基准测试中，GARLIC在k-NN检索和分类任务中表现优异，构建速度快且精度高，尤其在低训练数据量下仍保持高性能。

Conclusion: GARLIC是一种高效且通用的高维向量索引方法，适用于需要速度和精度的应用场景。

Abstract: We introduce GARLIC (GAussian Representation LearnIng for spaCe
partitioning), a novel indexing structure based on \(N\)-dimensional Gaussians
for efficiently learning high-dimensional vector spaces. Our approach is
inspired from Gaussian splatting techniques, typically used in 3D rendering,
which we adapt for high-dimensional search and classification. We optimize
Gaussian parameters using information-theoretic objectives that balance
coverage, assignment confidence, and structural and semantic consistency. A key
contribution is to progressively refine the representation through split and
clone operations, handling hundreds of dimensions, thus handling varying data
densities. GARLIC offers the fast building times of traditional space
partitioning methods (e.g., under \(\sim5\) min build time for SIFT1M) while
achieving \(\sim50\%\) Recall10@10 in low-candidate regimes. Experimental
results on standard benchmarks demonstrate our method's consistency in (a)
\(k\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by
using about half their probes for the same Recall10@10 in Fashion-MNIST, and
(b) in classification tasks, beating by \(\sim15\%\) accuracy other majority
voting methods. Further, we show strong generalization capabilities,
maintaining high accuracy even with downsampled training data: using just
\(1\%\) of the training data returns \(\sim 45\%\) Recall@1, thus making GARLIC
quite powerful for applications requiring both speed and accuracy.

</details>


### [131] [Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/abs/2505.24625)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: VG LLM是一种直接从视频数据中理解3D空间的新方法，无需额外3D输入，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 提升MLLMs直接从视频数据中理解和推理3D空间的能力，避免依赖复杂的3D数据输入。

Method: 提出VG LLM，通过3D视觉几何编码器从视频序列提取3D先验信息，并与视觉标记结合输入MLLM。

Result: 在3D场景理解和空间推理任务中表现优异，4B模型甚至超越Gemini-1.5-Pro。

Conclusion: VG LLM为直接从视频学习3D理解提供了高效解决方案，性能显著。

Abstract: Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method, the Video-3D Geometry Large
Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that
extracts 3D prior information from video sequences. This information is
integrated with visual tokens and fed into the MLLM. Extensive experiments have
shown that our method has achieved substantial improvements in various tasks
related to 3D scene understanding and spatial reasoning, all directly learned
from video sources. Impressively, our 4B model, which does not rely on explicit
3D data inputs, achieves competitive results compared to existing
state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.

</details>


### [132] [NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation](https://arxiv.org/abs/2505.24634)
*Xuzhi Wang,Wei Feng,Lingdong Kong,Liang Wan*

Main category: cs.CV

TL;DR: 论文提出了一种非均匀圆柱分割网络（NUC-Net），解决了现有体素方法在LiDAR语义分割中的计算成本高和点分布不平衡问题，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素的LiDAR语义分割方法存在计算成本高和点分布不平衡的问题，限制了其效率和性能。

Method: 提出非均匀圆柱分割网络（NUC-Net），采用API方法对径向轴进行非均匀分割，并提出非均匀多尺度聚合方法以增强上下文信息。

Result: 在SemanticKITTI和nuScenes数据集上实现了最先进的性能，训练速度提升4倍，GPU内存减少2倍，推理速度提升3倍。

Conclusion: NUC-Net是一种通用组件，显著提升了LiDAR语义分割的准确性和效率，并通过理论分析验证了其有效性。

Abstract: LiDAR semantic segmentation plays a vital role in autonomous driving.
Existing voxel-based methods for LiDAR semantic segmentation apply uniform
partition to the 3D LiDAR point cloud to form a structured representation based
on cartesian/cylindrical coordinates. Although these methods show impressive
performance, the drawback of existing voxel-based methods remains in two
aspects: (1) it requires a large enough input voxel resolution, which brings a
large amount of computation cost and memory consumption. (2) it does not well
handle the unbalanced point distribution of LiDAR point cloud. In this paper,
we propose a non-uniform cylindrical partition network named NUC-Net to tackle
the above challenges. Specifically, we propose the Arithmetic Progression of
Interval (API) method to non-uniformly partition the radial axis and generate
the voxel representation which is representative and efficient. Moreover, we
propose a non-uniform multi-scale aggregation method to improve contextual
information. Our method achieves state-of-the-art performance on SemanticKITTI
and nuScenes datasets with much faster speed and much less training time. And
our method can be a general component for LiDAR semantic segmentation, which
significantly improves both the accuracy and efficiency of the uniform
counterpart by $4 \times$ training faster and $2 \times$ GPU memory reduction
and $3 \times$ inference speedup. We further provide theoretical analysis
towards understanding why NUC is effective and how point distribution affects
performance. Code is available at
\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.

</details>


### [133] [Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data](https://arxiv.org/abs/2505.24636)
*Marios Glytsos,Panagiotis P. Filntisis,George Retsinas,Petros Maragos*

Main category: cs.CV

TL;DR: PLANTPose是一种基于RGB输入的类别级6D姿态估计框架，通过预测6D姿态和变形参数，适应未见实例的形状变化，无需依赖实例特定数据。


<details>
  <summary>Details</summary>
Motivation: 解决农业中水果和蔬菜形状、大小和纹理的高类内变异性导致的6D姿态估计难题，避免依赖实例特定CAD模型或深度传感器。

Method: PLANTPose框架预测6D姿态和变形参数，利用Stable Diffusion增强合成训练图像的真实感，缩小合成数据与真实世界的差距。

Result: 在包含多种形状、大小和成熟度香蕉的基准测试中，PLANTPose显著优于现有RGB方法MegaPose。

Conclusion: PLANTPose在无需实例特定数据的情况下，有效处理高类内变异性，为农业机器人应用提供实用解决方案。

Abstract: Accurate 6D object pose estimation is essential for robotic grasping and
manipulation, particularly in agriculture, where fruits and vegetables exhibit
high intra-class variability in shape, size, and texture. The vast majority of
existing methods rely on instance-specific CAD models or require depth sensors
to resolve geometric ambiguities, making them impractical for real-world
agricultural applications. In this work, we introduce PLANTPose, a novel
framework for category-level 6D pose estimation that operates purely on RGB
input. PLANTPose predicts both the 6D pose and deformation parameters relative
to a base mesh, allowing a single category-level CAD model to adapt to unseen
instances. This enables accurate pose estimation across varying shapes without
relying on instance-specific data. To enhance realism and improve
generalization, we also leverage Stable Diffusion to refine synthetic training
images with realistic texturing, mimicking variations due to ripeness and
environmental factors and bridging the domain gap between synthetic data and
the real world. Our evaluations on a challenging benchmark that includes
bananas of various shapes, sizes, and ripeness status demonstrate the
effectiveness of our framework in handling large intraclass variations while
maintaining accurate 6D pose predictions, significantly outperforming the
state-of-the-art RGB-based approach MegaPose.

</details>


### [134] [Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models](https://arxiv.org/abs/2505.24638)
*Zahid Hassan Tushar,Adeleke Ademakinwa,Jianwu Wang,Zhibo Zhang,Sanjay Purushotham*

Main category: cs.CV

TL;DR: 论文提出了一种名为CAAC的新型角度不变、基于注意力的深度学习模型，用于更准确地估计云光学厚度（COT），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统独立像素近似（IPA）方法存在显著偏差，而现有深度学习模型对辐射强度变化和角度敏感，导致COT估计不准确。

Method: 提出Cloud-Attention-Net with Angle Coding (CAAC)，利用注意力机制和角度嵌入，结合多角度训练策略，实现角度不变性。

Result: CAAC显著优于现有深度学习模型，将云属性检索误差降低至少九倍。

Conclusion: CAAC通过结合注意力机制和多角度训练，解决了现有方法的局限性，为COT估计提供了更准确的解决方案。

Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing
Earth's climate, weather, and radiation budget. Satellite radiance measurements
enable global COT retrieval, but challenges like 3D cloud effects, viewing
angles, and atmospheric interference must be addressed to ensure accurate
estimation. Traditionally, the Independent Pixel Approximation (IPA) method,
which treats individual pixels independently, has been used for COT estimation.
However, IPA introduces significant bias due to its simplified assumptions.
Recently, deep learning-based models have shown improved performance over IPA
but lack robustness, as they are sensitive to variations in radiance intensity,
distortions, and cloud shadows. These models also introduce substantial errors
in COT estimation under different solar and viewing zenith angles. To address
these challenges, we propose a novel angle-invariant, attention-based deep
model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages
attention mechanisms and angle embeddings to account for satellite viewing
geometry and 3D radiative transfer effects, enabling more accurate retrieval of
COT. Additionally, our multi-angle training strategy ensures angle invariance.
Through comprehensive experiments, we demonstrate that CAAC significantly
outperforms existing state-of-the-art deep learning models, reducing cloud
property retrieval errors by at least a factor of nine.

</details>


### [135] [A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning](https://arxiv.org/abs/2505.24641)
*Chengzhi Wu,Qianliang Huang,Kun Jin,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 提出了一种基于对比跨分支注意力的点云自监督学习框架PoCCA，通过分支间信息交换提升表征学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习框架仅在损失端进行特征对比，分支间无信息交换，限制了表征学习能力。

Method: 引入子分支，允许在损失端前进行分支间信息交换，适用于点云数据。

Result: 在无额外训练数据情况下，学习到的表征在下游任务中达到最优性能。

Conclusion: PoCCA框架通过分支间信息交换显著提升了点云表征学习效果。

Abstract: Contrastive learning is an essential method in self-supervised learning. It
primarily employs a multi-branch strategy to compare latent representations
obtained from different branches and train the encoder. In the case of
multi-modal input, diverse modalities of the same object are fed into distinct
branches. When using single-modal data, the same input undergoes various
augmentations before being fed into different branches. However, all existing
contrastive learning frameworks have so far only performed contrastive
operations on the learned features at the final loss end, with no information
exchange between different branches prior to this stage. In this paper, for
point cloud unsupervised learning without the use of extra training data, we
propose a Contrastive Cross-branch Attention-based framework for Point cloud
data (termed PoCCA), to learn rich 3D point cloud representations. By
introducing sub-branches, PoCCA allows information exchange between different
branches before the loss end. Experimental results demonstrate that in the case
of using no extra training data, the representations learned with our
self-supervised model achieve state-of-the-art performances when used for
downstream tasks on point clouds.

</details>


### [136] [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.24649)
*Huu-Thien Tran,Thanh-Dat Truong,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种新的双射最大似然学习（BIMA）方法，通过归一化流理论减少视觉语言模型中的幻觉问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多个领域广泛应用，但其生成的幻觉问题（即模型响应与视觉内容不符）是一个重要挑战，需要解决以提高系统可信度。

Method: 采用双射最大似然学习（BIMA）方法，基于归一化流理论，优化解码过程以减少幻觉。

Result: BIMA在POPE基准测试中平均F1得分为85.06%，并显著降低了CHAIRS和CHAIRI指标（分别减少7.6%和2.6%）。

Conclusion: BIMA是首批通过双射方法减少大型视觉语言模型幻觉的研究之一，效果显著。

Abstract: Large vision-language models have become widely adopted to advance in various
domains. However, developing a trustworthy system with minimal interpretable
characteristics of large-scale models presents a significant challenge. One of
the most prevalent terms associated with the fallacy functions caused by these
systems is hallucination, where the language model generates a response that
does not correspond to the visual content. To mitigate this problem, several
approaches have been developed, and one prominent direction is to ameliorate
the decoding process. In this paper, we propose a new Bijective Maximum
Likelihood Learning (BIMA) approach to hallucination mitigation using
normalizing flow theories. The proposed BIMA method can efficiently mitigate
the hallucination problem in prevailing vision-language models, resulting in
significant improvements. Notably, BIMA achieves the average F1 score of 85.06%
on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,
respectively. To the best of our knowledge, this is one of the first studies
that contemplates the bijection means to reduce hallucination induced by large
vision-language models.

</details>


### [137] [Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2505.24667)
*Jiahe Chen,Jiahe Ying,Shen Wang,Jianwei Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种解耦竞争框架（DCF），用于解决半监督医学图像分割中的过耦合问题和认知偏差，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域标注样本不足，现有方法（如Mean Teacher和Dual Students）存在过耦合和认知偏差问题，限制了性能。

Method: 提出DCF框架，通过竞争机制动态解耦学生和教师模型，并促进学生间精确知识交换。

Result: 在多个公开数据集上验证，DCF表现优于现有方法。

Conclusion: DCF有效解决了现有方法的局限性，提升了半监督医学图像分割的性能。

Abstract: Confronting the critical challenge of insufficiently annotated samples in
medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a
promising solution. Specifically, most methodologies following the Mean Teacher
(MT) or Dual Students (DS) architecture have achieved commendable results.
However, to date, these approaches face a performance bottleneck due to two
inherent limitations, \textit{e.g.}, the over-coupling problem within MT
structure owing to the employment of exponential moving average (EMA)
mechanism, as well as the severe cognitive bias between two students of DS
structure, both of which potentially lead to reduced efficacy, or even model
collapse eventually. To mitigate these issues, a Decoupled Competitive
Framework (DCF) is elaborated in this work, which utilizes a straightforward
competition mechanism for the update of EMA, effectively decoupling students
and teachers in a dynamical manner. In addition, the seamless exchange of
invaluable and precise insights is facilitated among students, guaranteeing a
better learning paradigm. The DCF introduced undergoes rigorous validation on
three publicly accessible datasets, which encompass both 2D and 3D datasets.
The results demonstrate the superiority of our method over previous
cutting-edge competitors. Code will be available at
https://github.com/JiaheChen2002/DCF.

</details>


### [138] [6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly](https://arxiv.org/abs/2505.24669)
*Chengzhi Wu,Hao Fu,Jan-Philipp Kaiser,Erik Tabuchi Barczak,Julius Pfrommer,Gisela Lanza,Michael Heizmann,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 论文提出了一种多阶段管道方法，用于在制造领域中对螺栓进行6D姿态估计，解决了遮挡和单视图数据获取的限制问题。


<details>
  <summary>Details</summary>
Motivation: 在制造领域中，利用先验知识可以提高6D姿态估计的准确性，尤其是在自动化拆卸任务中。

Method: 提出了一种多阶段管道方法，结合先验知识，从多视角获取数据以解决遮挡问题，并实现完整的螺栓信息获取。

Result: 该方法成功捕获了电机上所有螺栓的6D信息，验证了先验知识在复杂任务中的有效性。

Conclusion: 该研究不仅推动了6D姿态估计领域的发展，还展示了领域特定知识在制造和自动化中的实用性。

Abstract: The accurate estimation of 6D pose remains a challenging task within the
computer vision domain, even when utilizing 3D point cloud data. Conversely, in
the manufacturing domain, instances arise where leveraging prior knowledge can
yield advancements in this endeavor. This study focuses on the disassembly of
starter motors to augment the engineering of product life cycles. A pivotal
objective in this context involves the identification and 6D pose estimation of
bolts affixed to the motors, facilitating automated disassembly within the
manufacturing workflow. Complicating matters, the presence of occlusions and
the limitations of single-view data acquisition, notably when motors are placed
in a clamping system, obscure certain portions and render some bolts
imperceptible. Consequently, the development of a comprehensive pipeline
capable of acquiring complete bolt information is imperative to avoid oversight
in bolt detection. In this paper, employing the task of bolt detection within
the scope of our project as a pertinent use case, we introduce a meticulously
devised pipeline. This multi-stage pipeline effectively captures the 6D
information with regard to all bolts on the motor, thereby showcasing the
effective utilization of prior knowledge in handling this challenging task. The
proposed methodology not only contributes to the field of 6D pose estimation
but also underscores the viability of integrating domain-specific insights to
tackle complex problems in manufacturing and automation.

</details>


### [139] [Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism](https://arxiv.org/abs/2505.24679)
*Evangelos Sariyanidi,Lisa Yankowitz,Robert T. Schultz,John D. Herrington,Birkan Tunc,Jeffrey Cohn*

Main category: cs.CV

TL;DR: 论文提出了一种名为Facial Basis的新编码系统，替代传统的FACS，用于检测面部动作单元（AU），克服了FACS的三大局限性，并在自闭症诊断中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FACS编码过程耗时且昂贵，且现有自动检测方法精度不足，无法全面表征面部表情。

Method: 提出完全无监督的数据驱动编码系统Facial Basis，对应局部化和可解释的3D面部运动，无需人工标注。

Result: Facial Basis在预测自闭症诊断中优于常用AU检测器，且能重建所有可观察运动。

Conclusion: Facial Basis是首个替代FACS的系统，能全面编码面部行为，并开源实现。

Abstract: The Facial Action Coding System (FACS) has been used by numerous studies to
investigate the links between facial behavior and mental health. The laborious
and costly process of FACS coding has motivated the development of machine
learning frameworks for Action Unit (AU) detection. Despite intense efforts
spanning three decades, the detection accuracy for many AUs is considered to be
below the threshold needed for behavioral research. Also, many AUs are excluded
altogether, making it impossible to fulfill the ultimate goal of FACS-the
representation of any facial expression in its entirety. This paper considers
an alternative approach. Instead of creating automated tools that mimic FACS
experts, we propose to use a new coding system that mimics the key properties
of FACS. Specifically, we construct a data-driven coding system called the
Facial Basis, which contains units that correspond to localized and
interpretable 3D facial movements, and overcomes three structural limitations
of automated FACS coding. First, the proposed method is completely
unsupervised, bypassing costly, laborious and variable manual annotation.
Second, Facial Basis reconstructs all observable movement, rather than relying
on a limited repertoire of recognizable movements (as in automated FACS).
Finally, the Facial Basis units are additive, whereas AUs may fail detection
when they appear in a non-additive combination. The proposed method outperforms
the most frequently used AU detector in predicting autism diagnosis from
in-person and remote conversations, highlighting the importance of encoding
facial behavior comprehensively. To our knowledge, Facial Basis is the first
alternative to FACS for deconstructing facial expressions in videos into
localized movements. We provide an open source implementation of the method at
github.com/sariyanidi/FacialBasis.

</details>


### [140] [Learning reusable concepts across different egocentric video understanding tasks](https://arxiv.org/abs/2505.24690)
*Simone Alberto Peirone,Francesca Pistilli,Antonio Alliegro,Tatiana Tommasi,Giuseppe Averta*

Main category: cs.CV

TL;DR: Hier-EgoPack是一个统一框架，旨在通过多任务视角为自主系统提供全面的感知能力。


<details>
  <summary>Details</summary>
Motivation: 人类对视频流的理解是多方面的，而现有系统缺乏这种全面的感知能力。Hier-EgoPack旨在通过任务协同和知识抽象，为机器人提供类似人类的感知能力。

Method: 提出Hier-EgoPack框架，通过创建任务视角的集合，支持下游任务的跨任务知识迁移和技能复用。

Result: 框架能够为机器人提供可携带的技能背包，增强其在多任务中的表现。

Conclusion: Hier-EgoPack为自主系统的全面感知提供了新思路，通过任务协同和知识抽象提升机器人的多任务能力。

Abstract: Our comprehension of video streams depicting human activities is naturally
multifaceted: in just a few moments, we can grasp what is happening, identify
the relevance and interactions of objects in the scene, and forecast what will
happen soon, everything all at once. To endow autonomous systems with such
holistic perception, learning how to correlate concepts, abstract knowledge
across diverse tasks, and leverage tasks synergies when learning novel skills
is essential. In this paper, we introduce Hier-EgoPack, a unified framework
able to create a collection of task perspectives that can be carried across
downstream tasks and used as a potential source of additional insights, as a
backpack of skills that a robot can carry around and use when needed.

</details>


### [141] [Conformal Prediction for Zero-Shot Models](https://arxiv.org/abs/2505.24693)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型在分体共形预测范式下的能力，提出了一种名为Conf-OT的迁移学习方法，通过最优传输问题解决预训练与适应任务之间的领域差距，显著提升了集合效率。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在下游任务中表现出强大的适应性和泛化能力，但其可靠性和不确定性尚未被充分研究。本文旨在填补这一空白。

Method: 采用分体共形预测范式，提出Conf-OT方法，通过最优传输问题解决领域漂移问题，无需额外数据分割。

Result: 在15个数据集和三种非共形评分上，Conf-OT实现了高达20%的相对效率提升，且速度比现有方法快15倍。

Conclusion: Conf-OT方法在保持覆盖保证的同时，有效解决了领域漂移问题，显著提升了模型的效率和速度。

Abstract: Vision-language models pre-trained at large scale have shown unprecedented
adaptability and generalization to downstream tasks. Although its
discriminative potential has been widely explored, its reliability and
uncertainty are still overlooked. In this work, we investigate the capabilities
of CLIP models under the split conformal prediction paradigm, which provides
theoretical guarantees to black-box models based on a small, labeled
calibration set. In contrast to the main body of literature on conformal
predictors in vision classifiers, foundation models exhibit a particular
characteristic: they are pre-trained on a one-time basis on an inaccessible
source domain, different from the transferred task. This domain drift
negatively affects the efficiency of the conformal sets and poses additional
challenges. To alleviate this issue, we propose Conf-OT, a transfer learning
setting that operates transductive over the combined calibration and query
sets. Solving an optimal transport problem, the proposed method bridges the
domain gap between pre-training and adaptation without requiring additional
data splits but still maintaining coverage guarantees. We comprehensively
explore this conformal prediction strategy on a broad span of 15 datasets and
three non-conformity scores. Conf-OT provides consistent relative improvements
of up to 20% on set efficiency while being 15 times faster than popular
transductive approaches.

</details>


### [142] [RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement](https://arxiv.org/abs/2505.24705)
*Raman Jha,Adithya Lenka,Mani Ramanagopal,Aswin Sankaranarayanan,Kaushik Mitra*

Main category: cs.CV

TL;DR: RT-X Net是一种跨注意力网络，融合RGB和热成像图像以提升夜间图像质量，并在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 夜间图像因高噪声和强光源导致质量下降，热成像图像能提供互补信息，但现有方法未能充分利用多模态数据。

Method: 提出RT-X Net，利用自注意力网络提取特征，并通过跨注意力机制融合RGB和热成像图像。

Result: 在LLVIP和V-TIEE数据集上，RT-X Net表现优于现有方法。

Conclusion: RT-X Net有效提升夜间图像质量，并发布了V-TIEE数据集支持相关研究。

Abstract: In nighttime conditions, high noise levels and bright illumination sources
degrade image quality, making low-light image enhancement challenging. Thermal
images provide complementary information, offering richer textures and
structural details. We propose RT-X Net, a cross-attention network that fuses
RGB and thermal images for nighttime image enhancement. We leverage
self-attention networks for feature extraction and a cross-attention mechanism
for fusion to effectively integrate information from both modalities. To
support research in this domain, we introduce the Visible-Thermal Image
Enhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and
thermal images captured under diverse nighttime conditions. Extensive
evaluations on the publicly available LLVIP dataset and our V-TIEE dataset
demonstrate that RT-X Net outperforms state-of-the-art methods in low-light
image enhancement. The code and the V-TIEE can be found here
https://github.com/jhakrraman/rt-xnet.

</details>


### [143] [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/abs/2505.24718)
*Jisheng Dang,Jingze Wu,Teng Wang,Xuanhui Lin,Nannan Zhu,Hongbo Chen,Wei-Shi Zheng,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TW-GRPO通过引入令牌加权机制和密集奖励粒度，改进了GRPO在视觉推理任务中的表现，显著提升了多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在复杂推理任务中存在推理链冗长和奖励机制粗糙的问题，导致效率低下。

Method: 提出TW-GRPO框架，包括令牌加权机制（优先高信息密度令牌）和软奖励训练（多选择QA任务），并采用问题-答案反转的数据增强策略。

Result: 在多个基准测试中表现优异，如CLEVRER准确率提升18.8%，MMVU达到65.8%。

Conclusion: TW-GRPO通过优化推理链和奖励机制，显著提升了视觉推理任务的性能。

Abstract: Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group variance),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.

</details>


### [144] [DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds](https://arxiv.org/abs/2505.24733)
*Jiaxu Zhang,Xianfang Zeng,Xin Chen,Wei Zuo,Gang Yu,Guosheng Lin,Zhigang Tu*

Main category: cs.CV

TL;DR: DreamDance是一个新颖的角色艺术动画框架，通过相机轨迹生成稳定的角色和场景动画。


<details>
  <summary>Details</summary>
Motivation: 解决角色动画中相机轨迹与动态角色结合的挑战。

Method: 分两步：相机感知场景修复和姿态感知视频修复，结合高斯场和DiT模型。

Result: 生成高质量、一致的动态角色动画。

Conclusion: DreamDance有效解决了角色动画中的动态相机问题。

Abstract: This paper presents DreamDance, a novel character art animation framework
capable of producing stable, consistent character and scene motion conditioned
on precise camera trajectories. To achieve this, we re-formulate the animation
task as two inpainting-based steps: Camera-aware Scene Inpainting and
Pose-aware Video Inpainting. The first step leverages a pre-trained image
inpainting model to generate multi-view scene images from the reference art and
optimizes a stable large-scale Gaussian field, which enables coarse background
video rendering with camera trajectories. However, the rendered video is rough
and only conveys scene motion. To resolve this, the second step trains a
pose-aware video inpainting model that injects the dynamic character into the
scene video while enhancing background quality. Specifically, this model is a
DiT-based video generation model with a gating strategy that adaptively
integrates the character's appearance and pose information into the base
background video. Through extensive experiments, we demonstrate the
effectiveness and generalizability of DreamDance, producing high-quality and
consistent character animations with remarkable camera dynamics.

</details>


### [145] [Tackling View-Dependent Semantics in 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.24746)
*Jiazhong Cen,Xudong Zhou,Jiemin Fang,Changsong Wen,Lingxi Xie,Xiaopeng Zhang,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: LaGa（Language Gaussians）通过分解3D场景为对象并构建视角聚合的语义表示，解决了2D与3D语义理解之间的视角依赖性问题，显著提升了3D场景理解的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将2D语义特征简单投影到3D高斯上，忽略了视角依赖语义（view-dependent semantics）这一关键问题。

Method: LaGa通过分解3D场景为对象，聚类语义描述符并基于多视角语义重新加权，构建视角聚合的语义表示。

Result: 在LERF-OVS数据集上，LaGa比之前的SOTA方法提升了18.7%的mIoU。

Conclusion: LaGa有效捕捉了视角依赖语义的关键信息，实现了更全面的3D场景理解。

Abstract: Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D
scene reconstruction from RGB images. Many studies extend this paradigm for
language-driven open-vocabulary scene understanding. However, most of them
simply project 2D semantic features onto 3D Gaussians and overlook a
fundamental gap between 2D and 3D understanding: a 3D object may exhibit
various semantics from different viewpoints--a phenomenon we term
view-dependent semantics. To address this challenge, we propose LaGa (Language
Gaussians), which establishes cross-view semantic connections by decomposing
the 3D scene into objects. Then, it constructs view-aggregated semantic
representations by clustering semantic descriptors and reweighting them based
on multi-view semantics. Extensive experiments demonstrate that LaGa
effectively captures key information from view-dependent semantics, enabling a
more comprehensive understanding of 3D scenes. Notably, under the same
settings, LaGa achieves a significant improvement of +18.7% mIoU over the
previous SOTA on the LERF-OVS dataset. Our code is available at:
https://github.com/SJTU-DeepVisionLab/LaGa.

</details>


### [146] [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/abs/2505.24787)
*Yucheng Zhou,Jiahao Yuan,Qianning Wang*

Main category: cs.CV

TL;DR: 论文介绍了LongBench-T2I，一个专门评估文本到图像生成模型在复杂指令下表现的基准，并提出Plan2Gen框架以提升复杂指令生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在复杂指令（涉及多对象、属性和空间关系）下表现不佳，且缺乏针对复杂指令的评估基准。

Method: 提出LongBench-T2I基准（500个复杂提示）和Plan2Gen框架（利用大语言模型分解复杂指令），并开发自动化评估工具包。

Result: LongBench-T2I提供了全面的评估维度，Plan2Gen框架无需额外训练即可提升复杂指令生成效果。

Conclusion: LongBench-T2I填补了复杂指令评估的空白，Plan2Gen框架为复杂图像生成提供了新思路。

Abstract: Recent advancements in text-to-image (T2I) generation have enabled models to
produce high-quality images from textual descriptions. However, these models
often struggle with complex instructions involving multiple objects,
attributes, and spatial relationships. Existing benchmarks for evaluating T2I
models primarily focus on general text-image alignment and fail to capture the
nuanced requirements of complex, multi-faceted prompts. Given this gap, we
introduce LongBench-T2I, a comprehensive benchmark specifically designed to
evaluate T2I models under complex instructions. LongBench-T2I consists of 500
intricately designed prompts spanning nine diverse visual evaluation
dimensions, enabling a thorough assessment of a model's ability to follow
complex instructions. Beyond benchmarking, we propose an agent framework
(Plan2Gen) that facilitates complex instruction-driven image generation without
requiring additional model training. This framework integrates seamlessly with
existing T2I models, using large language models to interpret and decompose
complex prompts, thereby guiding the generation process more effectively. As
existing evaluation metrics, such as CLIPScore, fail to adequately capture the
nuances of complex instructions, we introduce an evaluation toolkit that
automates the quality assessment of generated images using a set of
multi-dimensional metrics. The data and code are released at
https://github.com/yczhou001/LongBench-T2I.

</details>


### [147] [Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification](https://arxiv.org/abs/2505.24792)
*Xinliu Zhong,Leo Hwa Liang,Angela S. Koh,Yeo Si Yong*

Main category: cs.CV

TL;DR: 提出了一种基于Few-Shot Learning的深度学习网络，用于优化结肠镜图像分析，提高结直肠癌（CRC）的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜检查具有侵入性且依赖高质量图像，但图像相似性高且获取不足，影响诊断准确性。

Method: 结合Few-Shot Learning架构，包括特征提取器、任务插值、关系嵌入和双级路由注意力机制，增强模型对未见图像模式的适应能力。

Result: 在Kvasir数据集上表现优异，准确率达90.1%，F1分数0.891，超越现有方法。

Conclusion: 该方法为结肠镜检查的挑战提供了有效解决方案，通过先进图像分析优化CRC检测。

Abstract: Traditional diagnostic methods like colonoscopy are invasive yet critical
tools necessary for accurately diagnosing colorectal cancer (CRC). Detection of
CRC at early stages is crucial for increasing patient survival rates. However,
colonoscopy is dependent on obtaining adequate and high-quality endoscopic
images. Prolonged invasive procedures are inherently risky for patients, while
suboptimal or insufficient images hamper diagnostic accuracy. These images,
typically derived from video frames, often exhibit similar patterns, posing
challenges in discrimination. To overcome these challenges, we propose a novel
Deep Learning network built on a Few-Shot Learning architecture, which includes
a tailored feature extractor, task interpolation, relational embedding, and a
bi-level routing attention mechanism. The Few-Shot Learning paradigm enables
our model to rapidly adapt to unseen fine-grained endoscopic image patterns,
and the task interpolation augments the insufficient images artificially from
varied instrument viewpoints. Our relational embedding approach discerns
critical intra-image features and captures inter-image transitions between
consecutive endoscopic frames, overcoming the limitations of Convolutional
Neural Networks (CNNs). The integration of a light-weight attention mechanism
ensures a concentrated analysis of pertinent image regions. By training on
diverse datasets, the model's generalizability and robustness are notably
improved for handling endoscopic images. Evaluated on Kvasir dataset, our model
demonstrated superior performance, achieving an accuracy of 90.1\%, precision
of 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current
state-of-the-art methods, presenting a promising solution to the challenges of
invasive colonoscopy by optimizing CRC detection through advanced image
analysis.

</details>


### [148] [CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning](https://arxiv.org/abs/2505.24816)
*Jiangpeng He,Zhihao Duan,Fengqing Zhu*

Main category: cs.CV

TL;DR: CL-LoRA提出了一种双适配器架构，结合任务共享适配器和任务特定适配器，解决了现有方法参数冗余和跨任务知识共享不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的CIL方法为每个新任务创建新适配器，导致参数冗余且无法利用跨任务共享知识。

Method: CL-LoRA采用双适配器架构：任务共享适配器学习跨任务知识，任务特定适配器捕获新任务独特特征。共享适配器使用随机正交矩阵和知识蒸馏，任务特定适配器引入可学习块权重以减少任务间干扰。

Result: CL-LoRA在多个基准测试中表现优异，同时减少了训练和推理计算量。

Conclusion: CL-LoRA为预训练模型的持续学习提供了更高效和可扩展的范式。

Abstract: Class-Incremental Learning (CIL) aims to learn new classes sequentially while
retaining the knowledge of previously learned classes. Recently, pre-trained
models (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown
remarkable performance in rehearsal-free CIL without requiring exemplars from
previous tasks. However, existing adapter-based methods, which incorporate
lightweight learnable modules into PTMs for CIL, create new adapters for each
new task, leading to both parameter redundancy and failure to leverage shared
knowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation
(CL-LoRA), which introduces a novel dual-adapter architecture combining
\textbf{task-shared adapters} to learn cross-task knowledge and
\textbf{task-specific adapters} to capture unique features of each new task.
Specifically, the shared adapters utilize random orthogonal matrices and
leverage knowledge distillation with gradient reassignment to preserve
essential shared knowledge. In addition, we introduce learnable block-wise
weights for task-specific adapters, which mitigate inter-task interference
while maintaining the model's plasticity. We demonstrate CL-LoRA consistently
achieves promising performance under multiple benchmarks with reduced training
and inference computation, establishing a more efficient and scalable paradigm
for continual learning with pre-trained models.

</details>


### [149] [Segmenting France Across Four Centuries](https://arxiv.org/abs/2505.24824)
*Marta López-Rauhut,Hongyu Zhou,Mathieu Aubry,Loic Landrieu*

Main category: cs.CV

TL;DR: 本文介绍了一个新的历史地图数据集，用于大规模、长期的土地利用和土地覆盖演变分析，并评估了三种分割方法的性能。


<details>
  <summary>Details</summary>
Motivation: 历史地图提供了过去几个世纪领土演变的宝贵视角，但现有数据集通常局限于单一地图类型或时期，且标注成本高。本文旨在解决这些问题。

Method: 提出了一个覆盖法国大都市的历史地图数据集，包含18、19和20世纪的地图，并提供现代和历史标注。评估了三种分割方法：全监督模型和两种弱监督模型。

Result: 数据集展示了分割任务的复杂性，包括风格不一致和景观变化。实验表明，弱监督方法在利用现代标注时表现良好。

Conclusion: 本文的数据集和方法为长期环境监测提供了支持，能够揭示几个世纪的景观变化。

Abstract: Historical maps offer an invaluable perspective into territory evolution
across past centuries--long before satellite or remote sensing technologies
existed. Deep learning methods have shown promising results in segmenting
historical maps, but publicly available datasets typically focus on a single
map type or period, require extensive and costly annotations, and are not
suited for nationwide, long-term analyses. In this paper, we introduce a new
dataset of historical maps tailored for analyzing large-scale, long-term land
use and land cover evolution with limited annotations. Spanning metropolitan
France (548,305 km^2), our dataset contains three map collections from the
18th, 19th, and 20th centuries. We provide both comprehensive modern labels and
22,878 km^2 of manually annotated historical labels for the 18th and 19th
century maps. Our dataset illustrates the complexity of the segmentation task,
featuring stylistic inconsistencies, interpretive ambiguities, and significant
landscape changes (e.g., marshlands disappearing in favor of forests). We
assess the difficulty of these challenges by benchmarking three approaches: a
fully-supervised model trained with historical labels, and two
weakly-supervised models that rely only on modern annotations. The latter
either use the modern labels directly or first perform image-to-image
translation to address the stylistic gap between historical and contemporary
maps. Finally, we discuss how these methods can support long-term environment
monitoring, offering insights into centuries of landscape transformation. Our
official project repository is publicly available at
https://github.com/Archiel19/FRAx4.git.

</details>


### [150] [Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning](https://arxiv.org/abs/2505.24837)
*Yinglian Zhu,Haiyang Yu,Qizao Wang,Wei Lu,Xiangyang Xue,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于对比范式的分层多粒度图文对齐框架（Hi-GITA），用于中文字符识别，显著优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 现有中文字符识别方法通常基于自回归和编辑距离后处理，且依赖单级字符表示，无法充分利用中文字符的细粒度语义信息。

Method: 提出多粒度编码器（图像和文本侧）和多粒度融合模块，并引入细粒度解耦图文对比损失，实现多粒度对齐。

Result: 在零样本手写字符和部首识别任务中，准确率提升约20%。

Conclusion: Hi-GITA框架有效利用了中文字符的细粒度语义信息，显著提升了识别性能。

Abstract: Chinese Character Recognition (CCR) is a fundamental technology for
intelligent document processing. Unlike Latin characters, Chinese characters
exhibit unique spatial structures and compositional rules, allowing for the use
of fine-grained semantic information in representation. However, existing
approaches are usually based on auto-regressive as well as edit distance
post-process and typically rely on a single-level character representation. In
this paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning
(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant
fine-grained semantic information of Chinese characters, we propose
multi-granularity encoders on both image and text sides. Specifically, the
Image Multi-Granularity Encoder extracts hierarchical image representations
from character images, capturing semantic cues from localized strokes to
holistic structures. The Text Multi-Granularity Encoder extracts stroke and
radical sequence representations at different levels of granularity. To better
capture the relationships between strokes and radicals, we introduce
Multi-Granularity Fusion Modules on the image and text sides, respectively.
Furthermore, to effectively bridge the two modalities, we further introduce a
Fine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text
representations across multiple granularities. Extensive experiments
demonstrate that our proposed Hi-GITA significantly outperforms existing
zero-shot CCR methods. For instance, it brings about 20% accuracy improvement
in handwritten character and radical zero-shot settings. Code and models will
be released soon.

</details>


### [151] [VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software](https://arxiv.org/abs/2505.24838)
*Brandon Man,Ghadi Nehme,Md Ferdous Alam,Faez Ahmed*

Main category: cs.CV

TL;DR: VideoCAD是一个大规模合成数据集，用于工程UI交互学习，包含41K标注视频，支持长时程和高复杂度任务。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的UI代理数据集和方法难以满足专业工程工具的需求，因此需要开发适用于高精度任务的数据集和方法。

Method: 通过自动化框架生成高保真UI动作数据，构建VideoCAD数据集，并提出VideoCADFormer模型学习CAD交互。

Result: VideoCADFormer在CAD交互学习中表现优于基线模型，同时VideoCAD支持VQA基准测试，揭示了视频UI理解中的关键挑战。

Conclusion: VideoCAD为工程UI交互学习提供了重要资源，并指出了未来研究需解决的多模态、空间推理和长时程依赖等问题。

Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process,
requiring precise, long-horizon user interactions with intricate 3D interfaces.
While recent advances in AI-driven user interface (UI) agents show promise,
most existing datasets and methods focus on short, low-complexity tasks in
mobile or web applications, failing to capture the demands of professional
engineering tools. In this work, we introduce VideoCAD, the first attempt at
engineering UI interaction learning for precision tasks. Specifically, VideoCAD
is a large-scale synthetic dataset consisting of over 41K annotated video
recordings of CAD operations, generated using an automated framework for
collecting high-fidelity UI action data from human-made CAD designs. Compared
to existing datasets, VideoCAD offers an order of magnitude higher complexity
in UI interaction learning for real-world engineering tasks, having up to a 20x
longer time horizon than other datasets. We show two important downstream
applications of VideoCAD: learning UI interactions from professional precision
3D CAD tools and a visual question-answering (VQA) benchmark designed to
evaluate multimodal large language models' (LLM) spatial reasoning and video
understanding abilities. To learn the UI interactions, we propose
VideoCADFormer - a state-of-the-art model in learning CAD interactions directly
from video, which outperforms multiple behavior cloning baselines. Both
VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key
challenges in the current state of video-based UI understanding, including the
need for precise action grounding, multi-modal and spatial reasoning, and
long-horizon dependencies.

</details>


### [152] [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
*Yuwen Tan,Yuan Qing,Boqing Gong*

Main category: cs.CV

TL;DR: 研究发现，许多先进的大语言模型（LLMs）缺乏对视觉世界的层次知识，甚至不了解生物学分类体系。这限制了视觉LLMs的层次视觉理解能力。通过构建约100万道四选一视觉问答任务，发现微调视觉LLM后，LLMs的层次一致性提升更多，表明LLMs是视觉LLMs理解层次视觉概念的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 揭示当前LLMs在视觉层次知识上的不足，并探讨其对视觉LLMs能力的限制。

Method: 使用约100万道基于六种分类体系和四个图像数据集的四选一视觉问答任务，对LLMs和视觉LLMs进行测试和微调。

Result: LLMs缺乏层次知识，微调后其层次一致性提升多于视觉LLMs，表明LLMs是视觉LLMs的瓶颈。

Conclusion: 视觉LLMs无法完全理解层次视觉概念，除非LLMs具备相应的分类知识。

Abstract: This paper reveals that many state-of-the-art large language models (LLMs)
lack hierarchical knowledge about our visual world, unaware of even
well-established biology taxonomies. This shortcoming makes LLMs a bottleneck
for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone
Fish but not Vertebrate). We arrive at these findings using about one million
four-choice visual question answering (VQA) tasks constructed from six
taxonomies and four image datasets. Interestingly, finetuning a vision LLM
using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because
the VQA tasks improve the LLM's hierarchical consistency more than the vision
LLM's. We conjecture that one cannot make vision LLMs understand visual
concepts fully hierarchical until LLMs possess corresponding taxonomy
knowledge.

</details>


### [153] [Reading Recognition in the Wild](https://arxiv.org/abs/2505.24848)
*Charig Yang,Samiul Alam,Shakhrul Iman Siam,Michael J. Proulx,Lambert Mathias,Kiran Somasundaram,Luis Pesqueira,James Fort,Sheroze Sheriffdeen,Omkar Parkhi,Carl Ren,Mi Zhang,Yuning Chai,Richard Newcombe,Hyo Jin Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的阅读识别任务，用于判断用户是否在阅读，并引入了一个大规模多模态数据集（Reading in the Wild），同时提出了一种灵活的Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 为了实现智能眼镜中的情境感知AI，需要记录用户与世界的互动，包括阅读行为。

Method: 利用三种模态（RGB视频、眼动追踪、头部姿态）设计了一个灵活的Transformer模型，支持单模态或多模态输入。

Result: 实验表明这些模态对任务具有相关性和互补性，并验证了数据集在扩展阅读研究方面的实用性。

Conclusion: 该研究为阅读识别提供了新的数据集和方法，支持更广泛和真实的场景应用。

Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism. Code, model, and
data will be public.

</details>


### [154] [ViStoryBench: Comprehensive Benchmark Suite for Story Visualization](https://arxiv.org/abs/2505.24862)
*Cailin Zhuang,Ailin Huang,Wei Cheng,Jingwei Wu,Yaoqi Hu,Jiaqi Liao,Zhewei Huang,Hongyuan Wang,Xinyao Liao,Weiwei Cai,Hengyuan Xu,Xuanyang Zhang,Xianfang Zeng,Gang Yu,Chi Zhang*

Main category: cs.CV

TL;DR: ViStoryBench是一个用于评估故事可视化模型的综合性基准，包含多样化的故事类型和艺术风格，旨在全面测试模型的性能。


<details>
  <summary>Details</summary>
Motivation: 提升故事可视化框架在真实场景中的表现，通过多样化的数据集和多维度评估推动模型改进。

Method: 收集多样化数据集，涵盖不同情节和视觉风格，设计多维度评估指标。

Result: ViStoryBench为研究者提供了全面评估模型性能的工具，帮助识别模型的优缺点。

Conclusion: ViStoryBench是一个结构化、多方面的评估框架，有助于推动故事可视化模型的针对性改进。

Abstract: Story visualization, which aims to generate a sequence of visually coherent
images aligning with a given narrative and reference images, has seen
significant progress with recent advancements in generative models. To further
enhance the performance of story visualization frameworks in real-world
scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We
collect a diverse dataset encompassing various story types and artistic styles,
ensuring models are evaluated across multiple dimensions such as different
plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D
renderings). ViStoryBench is carefully curated to balance narrative structures
and visual elements, featuring stories with single and multiple protagonists to
test models' ability to maintain character consistency. Additionally, it
includes complex plots and intricate world-building to challenge models in
generating accurate visuals. To ensure comprehensive comparisons, our benchmark
incorporates a wide range of evaluation metrics assessing critical aspects.
This structured and multifaceted framework enables researchers to thoroughly
identify both the strengths and weaknesses of different models, fostering
targeted improvements.

</details>


### [155] [TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection](https://arxiv.org/abs/2505.24866)
*Xinqi Xiong,Prakrut Patel,Qingyuan Fan,Amisha Wadhwa,Sarathy Selvam,Xiao Guo,Luchao Qi,Xiaoming Liu,Roni Sengupta*

Main category: cs.CV

TL;DR: TalkingHeadBench是一个新的多模型多生成器基准和数据集，旨在评估最先进的深度伪造检测器在先进生成器上的性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展对媒体、政治和金融等领域构成风险，但现有检测基准未能反映最新进展。

Method: 引入TalkingHeadBench，包含领先学术和商业模型生成的深度伪造视频，并设计协议评估检测器的鲁棒性和泛化能力。

Result: 对多种检测方法（如CNN、视觉Transformer和时间模型）进行基准测试，分析其鲁棒性和泛化能力，并提供错误分析。

Conclusion: TalkingHeadBench旨在推动研究，以应对快速发展的生成技术，促进更鲁棒和泛化的检测模型。

Abstract: The rapid advancement of talking-head deepfake generation fueled by advanced
generative models has elevated the realism of synthetic videos to a level that
poses substantial risks in domains such as media, politics, and finance.
However, current benchmarks for deepfake talking-head detection fail to reflect
this progress, relying on outdated generators and offering limited insight into
model robustness and generalization. We introduce TalkingHeadBench, a
comprehensive multi-model multi-generator benchmark and curated dataset
designed to evaluate the performance of state-of-the-art detectors on the most
advanced generators. Our dataset includes deepfakes synthesized by leading
academic and commercial models and features carefully constructed protocols to
assess generalization under distribution shifts in identity and generator
characteristics. We benchmark a diverse set of existing detection methods,
including CNNs, vision transformers, and temporal models, and analyze their
robustness and generalization capabilities. In addition, we provide error
analysis using Grad-CAM visualizations to expose common failure modes and
detector biases. TalkingHeadBench is hosted on
https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to
all data splits and protocols. Our benchmark aims to accelerate research
towards more robust and generalizable detection models in the face of rapidly
evolving generative techniques.

</details>


### [156] [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/abs/2505.24867)
*Ujjwal Upadhyay,Mukul Ranjan,Zhiqiang Shen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 论文提出了SpookyBench基准测试，揭示了当前视觉语言模型在纯时间序列任务中的局限性，与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间信息模糊时无法捕捉纯时间模式，而人类却能高效识别，这凸显了模型的不足。

Method: 通过构建SpookyBench基准测试，其中信息仅编码在噪声帧的时间序列中，测试模型和人类的表现。

Result: 人类准确率超过98%，而最先进的视觉语言模型准确率为0%，表明模型对空间特征的过度依赖。

Conclusion: 需要新的架构或训练范式以解耦空间和时间处理，SpookyBench的发布旨在推动时间模式识别研究。

Abstract: Recent advances in vision-language models (VLMs) have made impressive strides
in understanding spatio-temporal relationships in videos. However, when spatial
information is obscured, these models struggle to capture purely temporal
patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is
encoded solely in temporal sequences of noise-like frames, mirroring natural
phenomena from biological signaling to covert communication. Interestingly,
while humans can recognize shapes, text, and patterns in these sequences with
over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance
gap highlights a critical limitation: an over-reliance on frame-level spatial
features and an inability to extract meaning from temporal cues. Furthermore,
when trained in data sets with low spatial signal-to-noise ratios (SNR),
temporal understanding of models degrades more rapidly than human perception,
especially in tasks requiring fine-grained temporal reasoning. Overcoming this
limitation will require novel architectures or training paradigms that decouple
spatial dependencies from temporal processing. Our systematic analysis shows
that this issue persists across model scales and architectures. We release
SpookyBench to catalyze research in temporal pattern recognition and bridge the
gap between human and machine video understanding. Dataset and code has been
made available on our project website: https://timeblindness.github.io/.

</details>


### [157] [SiLVR: A Simple Language-based Video Reasoning Framework](https://arxiv.org/abs/2505.24869)
*Ce Zhang,Yan-Bo Lin,Ziyang Wang,Mohit Bansal,Gedas Bertasius*

Main category: cs.CV

TL;DR: SiLVR是一个简单的基于语言的视频推理框架，通过两阶段分解复杂视频理解任务，利用语言表示和强大的LLM推理能力，在多个视频语言任务中取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM在复杂视频语言任务中的推理能力显著落后，SiLVR旨在解决这一问题。

Method: SiLVR将视频转换为语言表示，并通过自适应令牌减少方案处理长上下文多感官输入，利用LLM进行推理。

Result: SiLVR在多个视频语言任务中取得最佳效果，并证明强推理LLM能有效整合多感官信息。

Conclusion: SiLVR框架简单、模块化且无需训练，展示了LLM在视频推理任务中的潜力。

Abstract: Recent advances in test-time optimization have led to remarkable reasoning
capabilities in Large Language Models (LLMs), enabling them to solve highly
complex problems in math and coding. However, the reasoning capabilities of
multimodal LLMs (MLLMs) still significantly lag, especially for complex
video-language tasks. To address this issue, we present SiLVR, a Simple
Language-based Video Reasoning framework that decomposes complex video
understanding into two stages. In the first stage, SiLVR transforms raw video
into language-based representations using multisensory inputs, such as short
clip captions and audio/speech subtitles. In the second stage, language
descriptions are fed into a powerful reasoning LLM to solve complex
video-language understanding tasks. To handle long-context multisensory inputs,
we use an adaptive token reduction scheme, which dynamically determines the
temporal granularity with which to sample the tokens. Our simple, modular, and
training-free video reasoning framework achieves the best-reported results on
Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.
Furthermore, our empirical study focused on video reasoning capabilities shows
that, despite not being explicitly trained on video, strong reasoning LLMs can
effectively aggregate multisensory input information from video, speech, and
audio for complex temporal, causal, long-context, and knowledge acquisition
reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.

</details>


### [158] [GenSpace: Benchmarking Spatially-Aware Image Generation](https://arxiv.org/abs/2505.24870)
*Zehan Wang,Jiayang Xu,Ziang Zhang,Tianyu Pan,Chao Du,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: GenSpace是一个新的基准和评估流程，用于评估图像生成模型的空间感知能力，发现当前模型在3D细节上存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究AI图像生成模型是否具备类似人类的空间感知能力，以及如何更准确地评估其空间表现。

Method: 提出GenSpace基准和专用评估流程，利用视觉基础模型重建3D场景几何，提供更精确的空间忠实度指标。

Result: AI模型能生成视觉吸引人的图像，但在对象放置、关系和测量等3D细节上表现不佳。

Conclusion: 总结了当前图像生成模型在空间感知上的三大局限，并提出了改进方向。

Abstract: Humans can intuitively compose and arrange scenes in the 3D space for
photography. However, can advanced AI image generators plan scenes with similar
3D spatial awareness when creating images from text or image prompts? We
present GenSpace, a novel benchmark and evaluation pipeline to comprehensively
assess the spatial awareness of current image generation models. Furthermore,
standard evaluations using general Vision-Language Models (VLMs) frequently
fail to capture the detailed spatial errors. To handle this challenge, we
propose a specialized evaluation pipeline and metric, which reconstructs 3D
scene geometry using multiple visual foundation models and provides a more
accurate and human-aligned metric of spatial faithfulness. Our findings show
that while AI models create visually appealing images and can follow general
instructions, they struggle with specific 3D details like object placement,
relationships, and measurements. We summarize three core limitations in the
spatial perception of current state-of-the-art image generation models: 1)
Object Perspective Understanding, 2) Egocentric-Allocentric Transformation and
3) Metric Measurement Adherence, highlighting possible directions for improving
spatial intelligence in image generation.

</details>


### [159] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种多模态LLM的RLVR后训练框架，通过优化数据集混合策略提升模型的泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR在多模态LLM中的应用面临数据集多样性和目标冲突的挑战，需优化数据混合策略。

Method: 开发了多模态RLVR框架，提出数据混合策略预测并优化RL微调结果。

Result: 多域RLVR训练结合混合策略显著提升模型性能，最佳混合策略使准确率平均提升5.24%。

Conclusion: 优化的数据混合策略在多模态RLVR中有效提升模型泛化和推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


### [160] [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
*Zilin Xiao,Jaywon Koo,Siru Ouyang,Jefferson Hernandez,Yu Meng,Vicente Ordonez*

Main category: cs.CV

TL;DR: ProxyThinker是一种无需训练的推理技术，通过调整解码动态，使大型模型继承小型视觉推理器的能力，显著提升性能并加速推理。


<details>
  <summary>Details</summary>
Motivation: 强化微调（RFT）训练大型视觉语言模型（LVLM）计算成本高，难以扩展模型规模。

Method: 通过从RFT推理器的输出分布中减去基础模型的输出分布，ProxyThinker修改解码动态，激发自我验证和自我纠正等复杂行为。

Result: ProxyThinker在空间、数学和多学科推理等视觉基准测试中显著提升性能，推理速度提升38倍。

Conclusion: ProxyThinker为大型模型的实用部署提供了高效解决方案。

Abstract: Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.

</details>


### [161] [MiniMax-Remover: Taming Bad Noise Helps Video Object Removal](https://arxiv.org/abs/2505.24873)
*Bojia Zi,Weixuan Peng,Xianbiao Qi,Jianan Wang,Shihao Zhao,Rong Xiao,Kam-Fai Wong*

Main category: cs.CV

TL;DR: MiniMax-Remover是一种新颖的两阶段视频对象去除方法，通过简化预训练视频生成模型并采用极小极大优化策略，显著提高了编辑质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 视频对象去除是视频编辑的关键子任务，但现有方法存在幻觉对象、视觉伪影和推理速度慢的问题。

Method: 第一阶段去除文本输入和交叉注意力层以简化模型；第二阶段通过极小极大优化策略训练模型，提高编辑质量和推理速度。

Result: 方法在仅6个采样步骤下实现最先进的视频对象去除效果，且不依赖CFG，显著提升了推理效率。

Conclusion: MiniMax-Remover在效果和效率上优于现有方法，实验验证了其优越性。

Abstract: Recent advances in video diffusion models have driven rapid progress in video
editing techniques. However, video object removal, a critical subtask of video
editing, remains challenging due to issues such as hallucinated objects and
visual artifacts. Furthermore, existing methods often rely on computationally
expensive sampling procedures and classifier-free guidance (CFG), resulting in
slow inference. To address these limitations, we propose MiniMax-Remover, a
novel two-stage video object removal approach. Motivated by the observation
that text condition is not best suited for this task, we simplify the
pretrained video generation model by removing textual input and cross-attention
layers, resulting in a more lightweight and efficient model architecture in the
first stage. In the second stage, we distilled our remover on successful videos
produced by the stage-1 model and curated by human annotators, using a minimax
optimization strategy to further improve editing quality and inference speed.
Specifically, the inner maximization identifies adversarial input noise ("bad
noise") that makes failure removals, while the outer minimization step trains
the model to generate high-quality removal results even under such challenging
conditions. As a result, our method achieves a state-of-the-art video object
removal results with as few as 6 sampling steps and doesn't rely on CFG,
significantly improving inference efficiency. Extensive experiments demonstrate
the effectiveness and superiority of MiniMax-Remover compared to existing
methods. Codes and Videos are available at: https://minimax-remover.github.io.

</details>


### [162] [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
*Yu Zhang,Yunqi Li,Yifan Yang,Rui Wang,Yuqing Yang,Dai Qi,Jianmin Bao,Dongdong Chen,Chong Luo,Lili Qiu*

Main category: cs.CV

TL;DR: ReasonGen-R1是一个两阶段框架，结合了思维链推理和强化学习，通过文本推理生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 探索思维链推理和强化学习在生成视觉模型中的应用，填补了NLP与视觉生成之间的空白。

Method: 1. 通过监督微调在推理数据集上训练自回归图像生成器；2. 使用Group Relative Policy Optimization（GRPO）算法优化输出。

Result: 在GenEval、DPG和T2I基准测试中，ReasonGen-R1表现优于现有模型。

Conclusion: ReasonGen-R1成功整合了推理和强化学习，提升了图像生成的质量和可控性。

Abstract: Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.

</details>


### [163] [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/abs/2505.24876)
*Tajamul Ashraf,Amal Saqib,Hanan Ghani,Muhra AlMahri,Yuhao Li,Noor Ahsan,Umair Nawaz,Jean Lahoud,Hisham Cholakkal,Mubarak Shah,Philip Torr,Fahad Shahbaz Khan,Rao Muhammad Anwer,Salman Khan*

Main category: cs.CV

TL;DR: Agent-X是一个大规模基准测试，用于评估视觉中心代理在多模态环境中的多步深度推理能力，揭示当前模型在多步视觉任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估视觉中心代理时存在局限性，如单轮查询、单一视觉模态和缺乏多步推理评估框架。

Method: 引入Agent-X基准测试，包含828个任务，涵盖多种视觉环境和工具使用，并提出细粒度的步级评估框架。

Result: 即使最佳模型（如GPT、Gemini和Qwen家族）在多步视觉任务中的成功率也低于50%。

Conclusion: Agent-X揭示了当前LMM在推理和工具使用能力上的瓶颈，为未来研究提供了方向。

Abstract: Deep reasoning is fundamental for solving complex tasks, especially in
vision-centric scenarios that demand sequential, multimodal understanding.
However, existing benchmarks typically evaluate agents with fully synthetic,
single-turn queries, limited visual modalities, and lack a framework to assess
reasoning quality over multiple steps as required in real-world settings. To
address this, we introduce Agent-X, a large-scale benchmark for evaluating
vision-centric agents multi-step and deep reasoning capabilities in real-world,
multimodal settings. Agent- X features 828 agentic tasks with authentic visual
contexts, including images, multi-image comparisons, videos, and instructional
text. These tasks span six major agentic environments: general visual
reasoning, web browsing, security and surveillance, autonomous driving, sports,
and math reasoning. Our benchmark requires agents to integrate tool use with
explicit, stepwise decision-making in these diverse settings. In addition, we
propose a fine-grained, step-level evaluation framework that assesses the
correctness and logical coherence of each reasoning step and the effectiveness
of tool usage throughout the task. Our results reveal that even the
best-performing models, including GPT, Gemini, and Qwen families, struggle to
solve multi-step vision tasks, achieving less than 50% full-chain success.
These findings highlight key bottlenecks in current LMM reasoning and tool-use
capabilities and identify future research directions in vision-centric agentic
reasoning models. Our data and code are publicly available at
https://github.com/mbzuai-oryx/Agent-X

</details>


### [164] [AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion](https://arxiv.org/abs/2505.24877)
*Yangyi Huang,Ye Yuan,Xueting Li,Jan Kautz,Umar Iqbal*

Main category: cs.CV

TL;DR: AdaHuman是一种新框架，从单张图像生成高保真、可动画的3D虚拟形象，通过姿态条件扩散模型和3D高斯溅射优化模块实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成高细节、可动画的3D虚拟形象，AdaHuman旨在解决这一问题。

Method: 结合姿态条件3D联合扩散模型和多视图图像合成，以及3D高斯溅射优化模块。

Result: 在公开基准测试中显著优于现有方法，生成高保真虚拟形象。

Conclusion: AdaHuman为3D虚拟形象生成提供了高效解决方案，代码和模型将公开。

Abstract: Existing methods for image-to-3D avatar generation struggle to produce highly
detailed, animation-ready avatars suitable for real-world applications. We
introduce AdaHuman, a novel framework that generates high-fidelity animatable
3D avatars from a single in-the-wild image. AdaHuman incorporates two key
innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes
consistent multi-view images in arbitrary poses alongside corresponding 3D
Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A
compositional 3DGS refinement module that enhances the details of local body
parts through image-to-image refinement and seamlessly integrates them using a
novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These
components allow AdaHuman to generate highly realistic standardized A-pose
avatars with minimal self-occlusion, enabling rigging and animation with any
input motion. Extensive evaluation on public benchmarks and in-the-wild images
demonstrates that AdaHuman significantly outperforms state-of-the-art methods
in both avatar reconstruction and reposing. Code and models will be publicly
available for research purposes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [165] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/abs/2505.23857)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种基于输出反馈的马尔可夫决策过程（OPMDP）和一种新的演员-评论家算法DATD3，用于处理部分状态信息下的强化学习任务。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的强化学习常面临输出反馈问题，即智能体只能获取部分状态信息，需要一种更有效的框架和算法来解决这一挑战。

Method: 提出了OPMDP框架，并在此基础上设计了DATD3算法，该算法结合深度可分离卷积和多头注意力机制来编码历史观测数据。

Result: 在连续控制任务上的实验表明，DATD3在部分和完全可观测性下均优于现有的基于记忆和循环的基线方法。

Conclusion: OPMDP和DATD3为输出反馈环境下的强化学习提供了有效的解决方案，同时避免了循环模型的不稳定性。

Abstract: Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [166] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun,Qingyong Li,Wen Wang,Yang Liu,Yangli-ao Geng,Boyang Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为LOT Merging的新方法，通过逐层最小化特征漂移，高效合并多任务模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在参数级或任务损失视角下合并模型时存在性能差距或高成本问题，而特征漂移与性能下降密切相关。

Method: 提出LOT Merging，通过凸二次优化问题逐层最小化特征漂移，解析求解线性层和归一化层参数。

Result: 在视觉和视觉语言基准测试中，LOT Merging比现有方法性能提升高达4.4%（ViT-B/32）。

Conclusion: LOT Merging通过最小化特征漂移，实现了高效且高性能的多任务模型合并。

Abstract: Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [167] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/abs/2505.23861)
*Renye Zhang,Mengyun Yang,Qichang Zhao,Jianxin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BiBLDR的双向行为学习策略，用于药物重定位，通过双向行为序列学习捕捉药物与疾病的交互模式，显著提升了冷启动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的药物重定位方法在冷启动场景中表现不佳，因为缺乏新药物与疾病的关联信息。BiBLDR通过双向行为序列学习解决这一问题。

Method: 构建双向行为序列，分两阶段进行：首先构建药物和疾病的原型空间，然后利用这些原型和序列数据预测药物-疾病关联。

Result: 实验表明，BiBLDR在基准数据集上达到最先进性能，尤其在冷启动场景中显著优于现有方法。

Conclusion: BiBLDR通过双向行为学习策略，有效提升了药物重定位的准确性和鲁棒性，尤其在冷启动场景中表现突出。

Abstract: Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [168] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/abs/2505.23863)
*Chang Liu,Bohao Zhao,Jingtao Ding,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: PhyxMamba是一种结合Mamba状态空间模型和物理信息原理的新框架，用于从短期观测中预测混沌系统的长期行为。


<details>
  <summary>Details</summary>
Motivation: 混沌系统的长期预测因对初始条件的敏感性和复杂几何结构而具有挑战性，现有方法依赖长期数据或仅关注短期相关性。

Method: 通过时间延迟嵌入重构吸引子流形，结合生成式训练和多令牌预测，增强物理约束和统计不变性。

Result: 在多种混沌系统中表现优异，能够准确捕捉动力学特征并实现长期预测。

Conclusion: PhyxMamba为观测稀缺条件下的混沌系统预测提供了新方法，具有广泛的应用潜力。

Abstract: Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [169] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/abs/2505.24534)
*Florian Frantzen,Michael T. Schaub*

Main category: cs.LG

TL;DR: HLSAD是一种新颖的方法，用于检测时间演化的单纯复形中的异常，通过利用Hodge Laplacian的谱特性，显著提高了检测复杂结构异常的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统图异常检测方法难以捕捉高阶交互的变化，而这些变化对识别复杂结构异常至关重要。

Method: 利用单纯复形的Hodge Laplacian谱特性建模数据点间的多向交互，结合高阶单纯结构提升性能。

Result: 在合成和真实数据集上的实验表明，HLSAD在检测事件和变化点方面优于现有图方法。

Conclusion: HLSAD通过高阶交互建模，显著提升了异常检测的准确性和效率。

Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [170] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/abs/2505.23864)
*Wei Zhuo,Zhaohuan Zhan,Ziduo Yang,Han Yu*

Main category: cs.LG

TL;DR: FedAux是一个针对图结构数据的联邦学习框架，通过辅助投影向量（APV）对齐和聚合异构本地模型，无需共享原始数据或节点嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决图结构数据联邦学习中的非独立同分布（non-IID）挑战，特别是在每个客户端持有不同子图的场景下。

Method: 每个客户端联合训练本地图神经网络（GNN）和可学习的APV，通过软排序和轻量级1D卷积优化嵌入，服务器利用APV计算客户端相似性并进行加权参数混合。

Result: FedAux在多个图基准测试中显著优于现有基线，尤其在准确性和个性化性能方面。

Conclusion: FedAux通过APV有效捕获客户端特定信息，实现个性化模型的同时保留跨客户端知识转移，理论分析和实验验证了其设计的合理性和优越性。

Abstract: Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [171] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/abs/2505.23865)
*Emanuele Masiero,Vito Trianni,Giuseppe Vizzari,Dimitri Ognibene*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段深度学习框架，用于农业领域的主动探索任务，通过不确定性感知策略提高探索效率。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要高效的自主系统进行作物监测，以减少资源消耗。本文旨在解决大规模环境中的主动探索问题。

Method: 提出了一种基于LSTM的信念模型和DQN代理架构，结合POV可见性掩码，优化信息增益和探索效率。

Result: 实验表明，Double-CNN DQN代理在探索效率上表现最佳，尤其是在大规模环境中。

Conclusion: 不确定性感知策略（如熵、信念状态和可见性跟踪）能实现稳健且可扩展的探索。未来工作包括课程学习和多代理协作。

Abstract: Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [172] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/abs/2505.23866)
*Chengli Tan,Yubo Zhou,Haishan Ye,Guang Dai,Junmin Liu,Zengjie Song,Jiangshe Zhang,Zixiang Zhao,Yunda Hao,Yong Xu*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络在安全关键应用中的校准问题，提出SAM和CSAM方法以减少过自信和校准误差。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在医疗诊断和自动驾驶等安全关键应用中容易过自信且校准不佳，可能导致严重后果。

Method: 提出使用SAM（锐度感知最小化）及其变体CSAM，通过最大化预测分布的熵来改善模型校准。

Result: 实验表明，SAM能有效减少校准误差，CSAM表现更优。

Conclusion: SAM和CSAM是改善深度神经网络校准的有效方法，CSAM表现最佳。

Abstract: Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [173] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/abs/2505.23868)
*Zhaokun Wang,Jinyu Guo,Jingwen Pu,Lingfeng Chen,Hongli Pu,Jie Ou. Libo Qin,Wenhong Tian*

Main category: cs.LG

TL;DR: 提出了一种名为LoPE的噪声鲁棒适应方法，通过非对称LoRA配置增强模型对噪声的鲁棒性，无需数据清洗。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法易受噪声数据干扰，传统噪声处理方法依赖数据预处理或易导致误差累积。

Method: 采用非对称LoRA配置，结合专用噪声专家（LoPE），通过两阶段训练注入噪声并选择性屏蔽噪声专家。

Result: 实验表明，LoPE仅通过低成本噪声注入即可实现高性能和鲁棒性。

Conclusion: LoPE提供了一种无需数据清洗的噪声鲁棒适应方法，显著提升模型性能。

Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [174] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/abs/2505.23870)
*Yixian Shen,Qi Bi,Jia-Hong Huang,Hongyi Zhu,Andy D. Pimentel,Anuj Pathania*

Main category: cs.LG

TL;DR: MaCP是一种高效的自适应余弦投影方法，通过利用余弦投影的能量压缩和去相关特性，显著提升模型效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 针对大规模基础模型微调时参数和内存需求高的问题，提出一种更高效的方法。

Method: 将权重变化投影到离散余弦空间，并按频谱分区选择关键频率成分。

Result: 在单模态和多模态任务中表现优异，计算复杂度和内存需求显著降低。

Conclusion: MaCP在效率和准确性上均优于现有方法，适用于多种任务。

Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [175] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/abs/2505.23871)
*Zeyuan Liu,Zhihe Yang,Jiawei Xu,Rui Yang,Jiafei Lyu,Baoxiang Wang,Yunjian Xu,Xiu Li*

Main category: cs.LG

TL;DR: 论文提出ADG方法，利用扩散模型解决离线强化学习中的数据噪声问题，通过分阶段去噪和数据恢复提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常因传感器或人为输入而包含噪声，现有方法难以处理高维状态空间和多元素同时损坏的情况。扩散模型虽具去噪潜力，但易过拟合噪声样本。

Method: 提出ADG方法，分三步：1) 引入Ambient DDPM从近似分布中学习；2) 利用噪声预测区分干净与损坏数据；3) 用标准DDPM修复损坏数据。

Result: 在MuJoCo等基准测试中，ADG有效减少噪声影响，提升离线强化学习的鲁棒性，达到最优性能。

Conclusion: ADG为离线强化学习中的数据噪声问题提供了一种通用且高效的解决方案，可无缝集成到现有算法中。

Abstract: Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [176] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/abs/2505.23875)
*Peter Samoaa,Marcus Vukojevic,Morteza Haghir Chehreghani,Antonio Longa*

Main category: cs.LG

TL;DR: RelSC是一个新的图回归数据集，基于程序图构建，提供连续目标变量，填补现有基准的多样性不足。


<details>
  <summary>Details</summary>
Motivation: 现有图回归基准集中于分子图和引用网络，缺乏多样性，限制了模型在异构图结构上的泛化能力。

Method: 引入RelSC数据集，包含两种变体：RelSC-H（同构图）和RelSC-M（多关系图），评估多种图神经网络架构。

Result: 结果显示同构和多关系设置下性能差异显著，强调了结构表示的重要性。

Conclusion: RelSC是一个具有挑战性和多样性的基准，有助于推动图回归方法的发展。

Abstract: Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [177] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/abs/2505.23876)
*Polad Geidarov*

Main category: cs.LG

TL;DR: 论文研究了通过解析公式计算权重的多层感知机神经网络在度量识别任务中的表现，实验表明预计算权重的网络训练更快且对训练数据减少更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨预计算权重对神经网络训练效率和鲁棒性的影响，特别是在小规模数据集上的表现。

Method: 通过解析公式计算权重，并在MNIST数据集上比较预计算权重与随机初始化权重的训练效果。

Result: 预计算权重的多层感知机训练速度更快，且对训练数据减少的鲁棒性更强。

Conclusion: 预计算权重是一种有效的神经网络初始化方法，尤其适用于小规模数据集。

Abstract: The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [178] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2505.23878)
*Jing Ma,Chenhao Dang,Mingjie Liao*

Main category: cs.LG

TL;DR: 提出了一种基于actor-critic的在线数据混合方法（AC-ODM），通过辅助网络动态调整域权重并考虑域内交互，显著提升了大型语言模型的预训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有在线数据混合方法（ODM）未考虑域内交互且无法动态调整权重，限制了预训练效率和模型性能。

Method: 使用actor-critic网络动态捕获域权重变化，并通过奖励函数考虑域内交互；利用小代理模型训练的策略直接应用于目标模型的数据采样。

Result: AC-ODM-410M在验证困惑度上比ODM快71%达到最优，零-shot MMLU准确率提升27.5%，HumanEval pass@1性能提高2.23倍。

Conclusion: AC-ODM方法通过动态数据混合和策略迁移，显著提升了预训练效率和模型性能。

Abstract: The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [179] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/abs/2505.23879)
*Caio Cheohen,Vinnícius M. S. Gomes,Manuela L. da Silva*

Main category: cs.LG

TL;DR: 研究开发了一种混合CNN-LSTM深度学习模型，利用刺突蛋白序列和临床数据预测COVID-19严重程度，表现优异。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了准确预测疾病严重程度的重要性，以优化医疗资源分配和患者管理。刺突蛋白的高突变率影响病毒致病性，AI方法为此提供了解决方案。

Method: 从GISAID数据库中获取9,570个刺突蛋白序列，筛选后纳入3,467个样本（2,313例重症，1,154例轻症）。通过特征工程提取序列特征，结合临床数据，训练混合CNN-LSTM模型。

Result: 模型F1分数为82.92%，ROC-AUC为0.9084，精度83.56%，召回率82.85%，分类性能稳健。训练准确率达85%，未出现明显过拟合。

Conclusion: CNN-LSTM混合模型有效预测COVID-19严重程度，展示了AI在基因组监测和精准公共卫生中的潜力，为未来疫情早期预测提供了框架。

Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [180] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang,Sai Bi,Yicong Hong,Kai Zhang,Fujun Luan,Songlin Yang,Kalyan Sunkavalli,William T. Freeman,Hao Tan*

Main category: cs.LG

TL;DR: LaCT（大块测试时训练）通过使用极大块更新（2K至1M令牌）提升硬件利用率和状态容量，适用于多模态任务。


<details>
  <summary>Details</summary>
Motivation: 现有TTT方法因小批量更新导致硬件利用率低，难以处理长上下文数据。

Method: 采用极大块更新（LaCT），提升非线性状态规模和硬件效率，支持复杂优化器。

Result: 在多种任务中验证有效性，包括14B参数视频扩散模型和100万令牌上下文长度。

Conclusion: LaCT为长上下文建模和测试时训练提供了高效解决方案，有望推动相关研究。

Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [181] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/abs/2505.23913)
*Gustavo Sutter Pessurno de Carvalho,Mohammed Abdulrahman,Hao Wang,Sriram Ganapathi Subramanian,Marc St-Aubin,Sharon O'Sullivan,Lawrence Wan,Luis Ricardez-Sandoval,Pascal Poupart,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 提出了一种基于预训练深度生成模型的零样本贝叶斯优化方法，无需代理模型拟合或采集函数优化，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化需要昂贵的代理模型训练和采集函数优化，限制了其效率和应用范围。

Method: 使用预训练的深度生成模型直接从最优点的后验分布中采样，避免了传统优化步骤。

Result: 在真实基准测试中实现了35倍以上的效率提升，支持高效并行和分布式优化。

Conclusion: 该方法为贝叶斯优化提供了一种高效、零样本的解决方案，适用于高吞吐量优化任务。

Abstract: The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [182] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng,Jie Fu*

Main category: cs.LG

TL;DR: 本文研究了基于人类反馈的强化学习（RLHF）在理论上的统计效率，提出了一种模型无关的后验采样算法，并给出了其理论保证。


<details>
  <summary>Details</summary>
Motivation: RLHF在大型语言模型（LLM）对齐人类偏好方面取得了巨大成功，但需要从理论角度研究其统计效率。

Method: 设计了一种基于Thompson采样的模型无关后验采样算法，并采用Bellman eluder（BE）维度作为函数类的复杂度度量。

Result: 算法实现了$O(\sqrt{T})$的遗憾界，并依赖于时间范围、BE维度和函数类的对数括号数。

Conclusion: 通过最大似然估计（MLE）泛化边界，建立了平方Bellman误差的集中不等式，为获得eluder型遗憾界提供了关键支持。

Abstract: Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [183] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/abs/2505.23933)
*Galen Pogoncheff,Michael Beyeler*

Main category: cs.LG

TL;DR: BIRD框架通过匹配师生模型的内部表示结构，有效传递人类对齐行为，显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在微调或任务特定数据不足时，人类对齐行为难以保持的问题。

Method: 采用表示结构蒸馏（BIRD）框架，匹配师生模型的内部表示结构。

Result: 在图像分类任务中，BIRD比基线方法提升鲁棒准确率16%，且教师模型可更小更简单。

Conclusion: BIRD为安全AI系统部署提供了可扩展的对齐种子，教师模型的三项表示特性解释了85%的迁移成功方差。

Abstract: Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [184] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/abs/2505.23939)
*Andrea Mattia Garavagno,Edoardo Ragusa,Antonio Frisoli,Paolo Gastaldo*

Main category: cs.LG

TL;DR: 提出一种在边缘设备上自动设计神经网络的方法，保护隐私敏感数据不外传，适用于医疗和工业物联网。


<details>
  <summary>Details</summary>
Motivation: 解决隐私敏感数据在物联网应用中无法安全使用机器学习的问题，同时满足医疗和工业物联网的需求。

Method: 在物联网网关上运行，为连接的传感器节点设计神经网络，数据不离开本地网络。

Result: 在Visual Wake Words数据集上达到先进水平，搜索过程在Raspberry Pi Zero 2上耗时少于10小时。

Conclusion: 该方法能有效保护敏感数据，同时实现高性能的边缘机器学习。

Abstract: This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [185] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo,Khai-Nguyen Nguyen,Mohammad Reza Taesiri,Vy Tuong Dang,Anh Totti Nguyen,Daeyoung Kim*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型（VLMs）在计数和识别任务中因先验知识而产生严重偏差，准确率仅为17.05%。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）如何因对流行主题的先验知识而影响其在标准视觉任务中的准确性。

Method: 通过测试7个不同领域的任务（如动物、标志、棋盘游戏等），评估VLMs的计数和识别能力。

Result: VLMs在计数任务中表现极差，准确率仅17.05%，且插入描述性文本会进一步降低准确性。

Conclusion: VLMs存在显著的偏差问题，即使通过指令改进，效果有限。研究提供了一个测试VLM偏差的自动化框架。

Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [186] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/abs/2505.23942)
*Gaurav Sarkar,Jay Gala,Subarna Tripathi*

Main category: cs.LG

TL;DR: SG-Blend是一种新型激活函数，通过动态插值结合SSwish和GELU，提升了深度神经网络的表现。


<details>
  <summary>Details</summary>
Motivation: 现有激活函数（如Swish和GELU）在特定领域表现优异，但缺乏通用性。SG-Blend旨在结合两者的优势，实现更通用的性能。

Method: SG-Blend通过可学习参数动态混合SSwish（Swish的一阶对称变体）和GELU，利用两者的互补特性。

Result: 在多种任务和模型上的实验表明，SG-Blend性能优于现有基线，且计算开销极小。

Conclusion: SG-Blend是一种高效、通用的激活函数，可作为现有方法的直接替代方案。

Abstract: The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [187] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/abs/2505.23947)
*Samuel Müller,Arik Reuter,Noah Hollmann,David Rügamer,Frank Hutter*

Main category: cs.LG

TL;DR: PFNs利用随机生成的人工数据集训练神经网络，捕捉先验分布，适用于低数据场景，是贝叶斯推断的未来方向。


<details>
  <summary>Details</summary>
Motivation: 在预训练计算资源增加而真实数据生成停滞的背景下，PFNs能高效分配计算资源，解决数据稀缺问题。

Method: 通过随机生成的人工数据集训练神经网络，捕捉先验分布，扩展至复杂领域和大数据集。

Result: PFNs在小规模贝叶斯建模任务中表现良好，并已扩展至更复杂领域。

Conclusion: PFNs及其摊销推断方法是贝叶斯推断的未来研究方向，具有解决数据稀缺问题的潜力。

Abstract: Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [188] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/abs/2505.23949)
*Xiang Meng,Mehdi Makni,Rahul Mazumder*

Main category: cs.LG

TL;DR: 提出了一种高效的转置N:M稀疏掩码求解器，适用于大规模模型，通过最优传输问题和GPU并行实现加速，性能接近标准N:M稀疏模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有转置N:M稀疏掩码方法无法扩展到大模型或仅限于M=4的问题，提升训练效率。

Method: 将掩码生成建模为最优传输问题，通过熵正则化和Dykstra算法求解，并结合GPU并行实现。

Result: 在LLaMA3.2-8B模型上，16:32转置稀疏性能接近标准N:M稀疏模型，优于标准2:4稀疏模型。

Conclusion: 该方法可扩展至十亿参数模型，为转置N:M稀疏提供了高效解决方案。

Abstract: Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [189] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/abs/2505.23954)
*Dylan Zapzalka,Trenton Chang,Lindsay Warrenburg,Sae-Hwan Park,Daniel K. Shenfeld,Ravi B. Parikh,Jenna Wiens,Maggie Makar*

Main category: cs.LG

TL;DR: 论文提出了一种因果方法，用于区分和量化代理在资源分配中故意误报特征的行为，通过比较操纵和非操纵数据集中的因果效应来识别误报率。


<details>
  <summary>Details</summary>
Motivation: 在资源分配场景中，代理可能故意误报特征以获取更好结果，但区分误报与真实修改是一个挑战。

Method: 利用因果不对称性，比较操纵与非操纵数据集中误报特征对下游变量的因果效应。

Result: 理论证明了误报率的可识别性，并在半合成和真实Medicare数据集中验证了方法的有效性。

Conclusion: 该方法能有效识别现实场景中的误报行为。

Abstract: In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [190] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
*Henry Conklin*

Main category: cs.LG

TL;DR: 论文提出定量方法分析大规模神经网络的表示空间结构，探讨其学习过程、泛化机制及设计决策对结构的影响，并应用于多智能体强化学习和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的表示空间描述方法，无法系统分析神经网络表示的结构及其对泛化的影响。

Method: 引入结构基元和信息论量化方法，提出高效估计向量空间熵的新方法，应用于不同规模模型。

Result: 揭示了大规模分布式认知模型的学习机制，并发现语言结构与神经网络性能驱动结构的相似性。

Conclusion: 研究为理解神经网络表示空间提供了新工具，并揭示了其与人类认知系统的潜在联系。

Abstract: Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [191] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/abs/2505.23967)
*Anders Aamand,Justin Y. Chen,Siddharth Gollapudi,Sandeep Silwal,Hao Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于预测的改进算法，用于解决NP难图问题，通过边缘预测模型提升近似比。


<details>
  <summary>Details</summary>
Motivation: 利用从过去数据中学习到的预测信息，突破标准设置下的近似界限，解决MaxCut、顶点覆盖等问题。

Method: 采用边缘预测模型，每个边缘提供两位信息，预测其端点是否属于最优解，结合高低度顶点分别处理。

Result: 算法在MaxCut、顶点覆盖、集合覆盖和最大独立集等问题中实现了改进的近似比。

Conclusion: 通过预测模型和分而治之的策略，成功提升了多个NP难图问题的近似算法性能。

Abstract: We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [192] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/abs/2505.23971)
*William Merrill,Shane Arora,Dirk Groeneveld,Hannaneh Hajishirzi*

Main category: cs.LG

TL;DR: 本文提出了一种直接测量临界批量大小（CBS）的简单方法，并展示了CBS在训练过程中的变化趋势。通过实验验证，发现CBS在训练初期快速增加后趋于稳定，且这一趋势在不同规模的模型中一致。基于此，作者提出批量大小预热策略，有效减少了训练步骤并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度噪声尺度估计CBS的方法依赖强假设，限制了其实际应用。本文旨在提出一种更直接、可靠的CBS测量方法，以优化大规模语言模型的训练效率。

Method: 通过实验直接测量CBS，并观察其在训练过程中的动态变化。进一步提出批量大小预热策略，逐步增加批量大小以适应CBS的增长。

Result: 实验表明，CBS在训练初期快速增加后趋于稳定，且这一趋势在不同规模模型中一致。使用批量大小预热策略训练OLMo 1B模型，减少了43%的梯度步骤并略微提升了性能。

Conclusion: 本文提出的直接测量CBS的方法及其动态变化规律，为大规模语言模型的高效训练提供了可靠指导，批量大小预热策略进一步优化了训练效率。

Abstract: The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [193] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/abs/2505.23973)
*Asaf Goren,Natalie Lang,Nir Shlezinger,Alejandro Cohen*

Main category: cs.LG

TL;DR: ADEL-FL 是一种联合优化每轮截止时间和用户特定批量大小的联邦学习框架，解决了设备异构性导致的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 同步联邦学习因设备异构性导致延迟问题，现有解决方案效果有限。

Method: ADEL-FL 通过约束优化问题，联合优化每轮截止时间和用户特定批量大小，并提供了收敛性分析。

Result: 实验表明，ADEL-FL 在异构条件下优于其他方法，收敛速度和最终准确率更高。

Conclusion: ADEL-FL 有效解决了联邦学习中的延迟问题，提升了训练效率。

Abstract: Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [194] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
*Vishal Dey,Xiao Hu,Xia Ning*

Main category: cs.LG

TL;DR: 论文提出了C-MuMOInstruct数据集和GeLLMO-Cs模型，用于多属性分子优化，并在实验中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法满足药物设计中多属性优化的需求，限制了实际应用。

Method: 通过C-MuMOInstruct数据集训练GeLLMO-Cs模型，实现针对性的属性优化。

Result: GeLLMO-Cs在实验中表现优异，成功率和泛化能力显著提升。

Conclusion: GeLLMO-Cs为药物设计中的多属性优化提供了实用工具。

Abstract: In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [195] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.24003)
*ChengAo Shen,Wenchao Yu,Ziming Zhao,Dongjin Song,Wei Cheng,Haifeng Chen,Jingchao Ni*

Main category: cs.LG

TL;DR: DMMV框架通过多模态视图和分解方法提升长期时间序列预测性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用多模态视图（图像和文本）揭示时间序列的互补模式，并利用预训练大模型（如LVMs）进行长期预测。

Method: 提出DMMV框架，结合趋势-季节性分解和基于残差的自适应分解，整合多模态视图。

Result: 在8个基准数据集中的6个上，DMMV的均方误差（MSE）表现最佳，优于14种现有方法。

Conclusion: DMMV通过多模态视图和分解方法有效解决了长期时间序列预测问题，性能显著提升。

Abstract: Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [196] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/abs/2505.24005)
*Priya Kasimbeg,Vincent Roulet,Naman Agarwal,Sourabh Medapati,Fabian Pedregosa,Atish Agarwala,George E. Dahl*

Main category: cs.LG

TL;DR: 论文探讨了机器学习中超参数调优的重要性，并评估了无学习率方法在超参数自由方法中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管方法学有重大进展，超参数调优仍是机器学习系统开发中关键且昂贵的部分。目标是实现无需特定工作负载调优的默认设置。

Method: 通过冻结非学习率超参数并使用AlgoPerf基准测试性能，评估无学习率方法的潜力。

Result: 文献提供的默认设置表现不佳，但经过校准的无学习率方法性能显著提升，但仍略低于NadamW基线。

Conclusion: 无学习率方法仍有改进空间，需在强基准测试中验证以提升超参数减少技术。

Abstract: Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [197] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/abs/2505.24022)
*Bhavya Vasudeva,Jung Whan Lee,Vatsal Sharan,Mahdi Soltanolkotabi*

Main category: cs.LG

TL;DR: 论文探讨了Adam优化算法与梯度下降（GD）在隐式偏差上的差异，发现Adam能抵抗简单性偏差，生成更复杂的决策边界，从而在测试中表现更好。


<details>
  <summary>Details</summary>
Motivation: 理解Adam优化算法与GD在隐式偏差上的差异，尤其是在神经网络训练中如何影响模型的简单性和泛化能力。

Method: 通过理论分析和实验验证，研究Adam和GD在两层ReLU神经网络上的表现，使用高斯簇合成的二分类任务数据。

Result: GD倾向于生成简单的线性决策边界，而Adam生成更复杂的非线性边界，接近贝叶斯最优预测器，并在分布变化时表现更好。

Conclusion: Adam的隐式偏差使其在复杂任务和分布变化下具有更好的泛化能力，优于GD。

Abstract: Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [198] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao,ChengAo Shen,Hanghang Tong,Dongjin Song,Zhigang Deng,Qingsong Wen,Jingchao Ni*

Main category: cs.LG

TL;DR: 本文研究了大型视觉模型（LVMs）在时间序列分析中的有效性，发现LVMs在分类任务中表现良好，但在预测任务中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨LVMs是否真正适用于时间序列分析，填补该领域的研究空白。

Method: 设计了首个系统性研究，涵盖4种LVMs、8种成像方法、18个数据集和26个基线模型，覆盖分类和预测任务，并进行广泛的消融分析。

Result: LVMs在时间序列分类中有效，但在预测任务中表现受限，且受限于特定类型的LVMs和成像方法。

Conclusion: LVMs在时间序列分类中有潜力，但在预测任务中需进一步改进，为未来多模态研究奠定基础。

Abstract: Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [199] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/abs/2505.24034)
*Bo Wu,Sid Wang,Yunhao Tang,Jia Ding,Eryk Helenowski,Liang Tan,Tengyu Xu,Tushar Gowda,Zhengxing Chen,Chen Zhu,Xiaocheng Tang,Yundi Qian,Beibei Zhu,Rui Hou*

Main category: cs.LG

TL;DR: LlamaRL是一个高效、分布式的异步强化学习框架，专为大规模语言模型（LLM）训练优化，支持8B到405B参数模型，并在GPU集群上实现高达10.7倍的加速。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型在延迟和内存方面的高要求，开发一个高效的强化学习框架具有挑战性。

Method: LlamaRL采用完全分布式的异步设计，基于PyTorch实现模块化和可扩展性，结合模型卸载、异步训练和分布式内存访问等技术。

Result: 在405B参数模型上，LlamaRL比类似系统快10.7倍，且效率优势随模型规模增加而提升。

Conclusion: LlamaRL适合未来大规模强化学习训练，展示了其高效性和可扩展性。

Abstract: Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [200] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/abs/2505.24048)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文提出NeuronTune方法，通过干预模型内部决策过程，无需外部标注即可自引导缓解深度神经网络中的虚假偏差。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常依赖非本质特征与类别间的虚假相关性进行预测，导致模型性能下降。现有方法依赖外部标注，难以获取且不适用于模型内部的虚假偏差。

Method: 提出NeuronTune，一种事后方法，通过探测模型的潜在嵌入空间，识别并调控导致虚假预测行为的神经元。

Result: 实验表明，NeuronTune在不同架构和数据模态中显著缓解了虚假偏差，且无需虚假相关性标注。

Conclusion: NeuronTune是一种实用且有效的工具，能自引导提升模型鲁棒性。

Abstract: Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [201] [Differential Gated Self-Attention](https://arxiv.org/abs/2505.24054)
*Elpiniki Maria Lygizou,Mónika Farsang,Radu Grosu*

Main category: cs.LG

TL;DR: 论文提出了一种多头差分门控自注意力机制（M-DGSA），通过动态抑制注意力噪声提升Transformer的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力对所有查询-键交互一视同仁，导致对噪声输入敏感。受生物神经侧抑制启发，提出动态门控机制。

Method: M-DGSA将每个注意力头分为兴奋和抑制分支，通过双softmax融合和sigmoid门控动态抑制噪声。

Result: 在视觉和语言任务中，M-DGSA显著优于标准Transformer及其变体，表现出更强的噪声鲁棒性。

Conclusion: M-DGSA结合生物侧抑制理论，提出了一种高效的门控自注意力机制，适用于多领域任务。

Abstract: Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [202] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/abs/2505.24055)
*Yilong Wang,Tianxiang Zhao,Zongyu Wu,Suhang Wang*

Main category: cs.LG

TL;DR: 提出了一种基于链接预测的图神经网络框架，用于解决无监督域适应中标签分布偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在节点分类中表现优异，但依赖大量标注数据。无监督域适应（UDA）试图将标注源图的分类器适应到未标注目标图，但现有方法未充分考虑图结构和消息传递机制，且在标签分布偏移时失效。

Method: 通过链接预测连接源图和目标图的节点，促进消息传递，并设计身份保持学习目标以防止目标图信息丢失。

Result: 在真实数据集上验证了框架的有效性。

Conclusion: 所提框架通过链接预测和身份保持学习，有效解决了标签分布偏移问题，提升了域适应性能。

Abstract: Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [203] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/abs/2505.24059)
*Sean Foley,Hong Nguyen,Jihwan Lee,Sudarsana Reddy Kadiri,Dani Byrd,Louis Goldstein,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 本文通过单说话者MRI数据研究多模态学习，发现音频和多模态模型在语音识别中表现相似，但在发音部位上存在差异。


<details>
  <summary>Details</summary>
Motivation: 解决以往多说话者数据导致的跨说话者变异性问题，以更详细地研究声学与发音的关系。

Method: 使用单说话者长时MRI数据，开发单模态和多模态模型进行音素识别。

Result: 音频和多模态模型在不同发音方式上表现相似，但在发音部位上存在差异；潜在空间分析显示音素编码相似。

Conclusion: 单说话者数据有助于更精确地研究声学与发音的关系，多模态模型在发音部位上表现出独特性。

Abstract: Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [204] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/abs/2505.24060)
*Chris Mingard,Lukas Seier,Niclas Göring,Andrei-Vlad Badelita,Charles London,Ard Louis*

Main category: cs.LG

TL;DR: 本文通过深度-2离散全连接网络与DNF公式的一一对应关系，研究了布尔函数训练中的归纳偏置、特征学习与泛化能力的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究对深度神经网络的泛化能力解释不完整（如NTK方法忽略了特征学习），本文旨在提供一个端到端的分析框架。

Method: 利用深度-2离散全连接网络与DNF公式的对应关系，在布尔函数上训练，采用蒙特卡洛学习算法。

Result: 模型表现出可预测的训练动态和可解释特征的形成，揭示了归纳偏置与特征学习如何驱动泛化。

Conclusion: 本文提供了一个可解析的案例研究，详细展示了归纳偏置和特征形成如何共同影响网络的泛化能力。

Abstract: Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [205] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/abs/2505.24061)
*Jiashun Liu,Zihao Wu,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: 论文提出了一种新的神经元学习能力度量方法GraMa，替代传统的tau-dormant神经元比率，通过梯度而非激活统计来量化神经元的学习能力，并在多种架构中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（RL）中神经元活动损失问题影响其持续学习能力，传统方法在复杂架构中效果有限。

Method: 提出GraMa（梯度幅度神经活动度量），通过梯度统计量化神经元学习能力，并引入ReGraMa方法重置不活跃神经元。

Result: GraMa在多种架构（如残差网络、扩散模型）中有效识别神经元不活跃问题，ReGraMa显著提升了多种RL算法的性能。

Conclusion: GraMa是一种轻量级、架构无关的神经元学习能力度量方法，为RL中的神经元活动问题提供了新解决方案。

Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [206] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/abs/2505.24067)
*Yu He,Ellen Vitercik*

Main category: cs.LG

TL;DR: 本文提出了一种基于原始-对偶范式的神经算法推理（NAR）框架，用于模拟和优化近似算法，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩展神经算法推理（NAR）到更难的问题，尤其是通过原始-对偶范式设计高效近似算法。

Method: 利用原始-对偶变量的二分表示，将原始-对偶算法与图神经网络对齐，并结合小实例的最优解增强模型推理能力。

Result: 模型不仅能模拟近似算法，还在多个任务中表现优于传统方法，且能泛化到更大和分布外的图数据。

Conclusion: 该框架具有实际应用价值，可与商业求解器结合，适用于现实世界数据集。

Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [207] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/abs/2505.24069)
*Yu He,Yingxi Li,Colin White,Ellen Vitercik*

Main category: cs.LG

TL;DR: DSR-Bench是一个评估大语言模型（LLMs）结构推理能力的新基准，通过数据结构和操作测试模型对数据关系的理解。结果显示现有模型在多属性和多跳推理上表现不佳，且对复杂结构的处理能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注高层次应用评估，缺乏对结构推理能力的单独测试。DSR-Bench填补了这一空白，旨在评估LLMs对数据关系的理解和推理能力。

Method: DSR-Bench包含20种数据结构、35种操作和4140个问题实例，采用分层组织进行细粒度分析。评估流程完全自动化且确定性，避免主观判断。

Result: 评估显示，指令调优模型在基础多属性和多跳推理上表现不佳，推理导向模型虽表现较好，但在复杂和混合结构上仍脆弱，最佳模型平均得分仅为47%。

Conclusion: LLMs在多维数据和自然语言任务描述上的表现较差，表明其在现实部署中存在关键能力缺口。

Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [208] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/abs/2505.24085)
*Alireza Jafari,Fereshteh Yousefirizi,Vahid Seydi*

Main category: cs.LG

TL;DR: 提出了一种结合无监督深度学习和梯度提升模型的混合方法，用于改进心房颤动（AF）检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心房颤动是一种常见的心律失常，及时检测对降低中风风险至关重要。

Method: 采用19层深度卷积自编码器（DCAE）结合三种梯度提升分类器（AdaBoost、XGBoost和LightGBM），实现端到端的AF检测。

Result: DCAE-LGBM模型的F1分数为95.20%，灵敏度为99.99%，推理延迟为4秒，优于现有方法。

Conclusion: 该混合系统显著提升了检测性能，适合临床部署。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [209] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang,Skyler Seto,Hadi Pouransari,Mehrdad Farajtabar,Raviteja Vemulapalli,Fartash Faghri,Oncel Tuzel,Barry-John Theobald,Josh Susskind*

Main category: cs.LG

TL;DR: 论文提出了一种名为Proxy-FDA的新正则化方法，通过特征分布对齐和动态生成代理来减少微调过程中的概念遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 微调视觉基础模型时，常导致其他任务的概念遗忘问题，现有方法通过点对点匹配权重或特征来保留知识，但忽略了特征邻域结构中的丰富知识。

Method: Proxy-FDA通过最近邻图对预训练和微调特征空间进行特征分布对齐，并动态生成信息代理以增加数据多样性。

Result: 实验表明Proxy-FDA显著减少了微调中的概念遗忘，并发现遗忘与分布距离度量（而非L2距离）有强相关性。

Conclusion: Proxy-FDA在多种微调场景（端到端、少样本和持续微调）和任务（图像分类、字幕生成和VQA）中均表现出优势。

Abstract: Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [210] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/abs/2505.24089)
*Marcus Lassila,Johan Östman,Khac-Hoang Ngo,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 论文提出了针对独立同分布数据和图结构数据的贝叶斯最优成员推理攻击（MIA）方法，包括BASE和G-BASE，性能优于现有方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络中节点级成员推理攻击的最优查询策略问题，并提升攻击的效率和性能。

Method: 基于贝叶斯决策理论框架，推导了图神经网络中节点级MIA的最优推理规则，并提出了计算高效的近似方法BASE和G-BASE。

Result: G-BASE在节点级MIA中表现优于现有方法，BASE在非图数据中性能匹配或超越LiRA和RMIA，且计算成本更低。

Conclusion: BASE和G-BASE为成员推理攻击提供了高效且理论支持的解决方案，并揭示了RMIA的贝叶斯最优性。

Abstract: We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [211] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/abs/2505.24101)
*Zhenran Xu*

Main category: cs.LG

TL;DR: 研究通过改进预测因子和开发可解释的多层堆叠集成模型，提高了急性卒中患者住院时间（LOS）预测的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在预测LOS时表现不佳，泛化能力有限，且忽略了系统级因素。研究旨在提升模型效率和可解释性。

Method: 使用澳大利亚卒中基金会急性审计数据（2015-2021），分别针对缺血性和出血性卒中开发模型，采用特征选择和集成方法。

Result: 集成模型在缺血性卒中中表现优于逻辑回归（AUC: 0.824 vs. 0.805），但在出血性卒中中无显著差异。SHAP分析识别了共同预测因子。

Conclusion: 可解释的集成模型有效预测缺血性卒中的LOS，但出血性卒中需进一步验证。

Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [212] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/abs/2505.24110)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文提出了一种形式化且构造性的框架，证明了非确定性有限自动机（NFA）与标准前馈ReLU神经网络之间的等价性。通过将自动机状态编码为二进制向量，并将转移表示为稀疏线性层，ReLU激活函数可以精确模拟非确定性分支、子集构造和ε闭包。理论结果表明，一个宽度为O(n)的三层ReLU网络可以精确识别任何由n状态NFA接受的正则语言，无需递归、记忆或近似。实验验证了该框架在多个任务中的完美或接近完美的表现。


<details>
  <summary>Details</summary>
Motivation: 本文旨在建立NFA与ReLU神经网络之间的严格等价性，将自动机理论与神经网络计算通过ReLU动力学统一起来。

Method: 通过将NFA状态编码为二进制向量，转移表示为稀疏线性层，利用ReLU激活函数模拟非确定性分支、子集构造和ε闭包。构建宽度为O(n)的三层ReLU网络。

Result: 理论证明和实验验证表明，该方法能够精确模拟NFA的行为，并在多个任务中实现完美或接近完美的表现。

Conclusion: 本文首次在标准深度学习架构中实现了NFA的完整符号模拟，为自动机理论与神经网络计算的结合提供了理论基础。

Abstract: We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [213] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/abs/2505.24138)
*Yichen Shi,Ze Zhang,Hongyang Wang,Zhuofu Tao,Zhongyi Li,Bingyu Chen,Yaxin Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.LG

TL;DR: AMSbench是一个用于评估多模态大语言模型（MLLMs）在模拟/混合信号（AMS）电路设计任务中表现的基准测试套件，包含8000个测试问题，揭示了当前模型在复杂任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 自动化AMS电路设计具有挑战性，当前MLLMs的研究缺乏系统评估其在多样化任务中的能力，因此需要开发一个全面的基准。

Method: 开发了AMSbench基准套件，评估了包括Qwen 2.5-VL和Gemini 2.5 Pro在内的八种模型在电路感知、分析和设计任务中的表现。

Result: 当前MLLMs在复杂多模态推理和高级电路设计任务中表现不足，显示出与人类专家水平的差距。

Conclusion: 需要进一步提升MLLMs对电路知识的理解和应用能力，以实现完全自动化的AMS电路设计。

Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [214] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/abs/2505.24145)
*Wilfried Genuist,Éric Savin,Filippo Gatti,Didier Clouteau*

Main category: cs.LG

TL;DR: 提出了一种基于条件分数扩散模型的多场景流体流动预测方法，结合能量约束提升预测质量，支持高效采样和灵活生成。


<details>
  <summary>Details</summary>
Motivation: 基于科学机器学习和生成建模的最新进展，解决多场景流体流动预测的挑战，同时减少训练需求和提高效率。

Method: 设计了一种条件分数扩散模型，结合湍流统计特性的能量约束，支持即插即用增强和快速生成。

Result: 模型在复杂流体动力学数据集上表现稳定、鲁棒且物理可信，适用于多种流动状态和配置。

Conclusion: 该方法通过精心设计的随机微分方程提升了性能，适用于多样化的流体力学任务。

Abstract: Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [215] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/abs/2505.24149)
*Adam Piaseczny,Md Kamran Chowdhury Shisher,Shiqiang Wang,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 论文提出了一种名为RCCDA的动态模型更新策略，用于在资源受限环境下优化机器学习模型的训练动态，同时确保严格遵守预定义的资源约束。


<details>
  <summary>Details</summary>
Motivation: 现实环境中部署的机器学习模型需要适应概念漂移，但现有方法计算开销大且无法保证资源使用或理论性能。

Method: 基于过去损失信息和可调漂移阈值，利用Lyapunov漂移加惩罚框架开发轻量级策略。

Result: 在三个领域泛化数据集上的实验表明，RCCDA在推理准确性和资源约束下优于基线方法。

Conclusion: RCCDA适用于实时机器学习部署，能够有效应对概念漂移并满足资源限制。

Abstract: Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [216] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/abs/2505.24155)
*Ehtesamul Azim,Dongjie Wang,Tae Hyun Hwang,Yanjie Fu,Wei Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合统计选择和生物通路知识的两阶段框架，通过多智能体强化学习（MARL）改进基因选择，显著提升了预测准确性和生物可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基因选择方法忽略生物通路和调控网络的复杂性，导致结果不稳定且缺乏生物学意义。

Method: 两阶段框架：1）基于KEGG通路和统计方法的预过滤；2）MARL建模基因为协作智能体，优化预测能力和生物相关性。

Result: 在多个基因表达数据集上，该方法显著优于传统方法，提高了预测准确性和生物可解释性。

Conclusion: 该框架成功整合了统计严谨性和生物通路知识，为基因选择提供了更优解决方案。

Abstract: Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [217] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/abs/2505.24157)
*Seungjoon Lee,Suhwan Kim,Minhyeon Oh,Youngsik Yoon,Jungseul Ok*

Main category: cs.LG

TL;DR: REPOA框架通过自适应依赖学习、细粒度失败感知操作记忆和基于难度的探索，解决了开放世界代理在复杂任务中的鲁棒性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖内部知识或不切实际的环境假设，且忽视了从零开始获取规划知识的能力。REPOA旨在解决这些问题，提升代理在开放世界中的规划能力。

Method: REPOA包含三个关键组件：自适应依赖学习、细粒度失败感知操作记忆和难度探索，以提高鲁棒性和学习效率。

Result: 在开放世界测试环境中，REPOA成功获取了以往方法无法完成的挑战性后期游戏物品，展示了其鲁棒和高效的规划能力。

Conclusion: REPOA为开放世界代理的规划问题提供了有效的解决方案，显著提升了鲁棒性和效率。

Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [218] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/abs/2505.24178)
*Katherine Tieu,Dongqi Fu,Jun Wu,Jingrui He*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息瓶颈方法的误差有界不变链接选择器，用于解决时序图中的分布外问题，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，分布外问题和时序图中非独立同分布数据导致AI泛化困难，需要研究时序图中哪些组件对标签最具不变性和代表性。

Method: 使用信息瓶颈方法设计误差有界不变链接选择器，区分训练过程中的不变和可变组件，并结合任务特定损失函数优化模型。

Result: 实验表明，该方法在引用推荐和商品推荐等任务中优于现有最优方法。

Conclusion: 提出的方法有效提升了时序图中不变组件的学习能力，增强了模型在不同测试场景中的泛化性能。

Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [219] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/abs/2505.24179)
*Xiaodong Ji,Hailin Zhang,Fangcheng Fu,Bin Cui*

Main category: cs.LG

TL;DR: SALE是一种细粒度稀疏注意力方法，通过4位量化的查询-键乘积和块稀疏注意力加速LLM的长上下文预填充阶段，保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 长上下文处理中，自注意力模块因二次时间复杂度成为瓶颈，现有稀疏注意力方法因粗粒度检查导致精度损失。

Method: 采用4位量化查询-键乘积和块稀疏注意力，结合Relative Attention Score指标评估重要性，并优化CUDA内核。

Result: 在长上下文基准测试中，SALE在精度和效率上优于现有方法，对64K以上序列实现3.36倍加速。

Conclusion: SALE无需参数训练，可无缝集成到现有系统，显著提升长上下文处理效率且保持模型质量。

Abstract: Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [220] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu,Di Huang,Hanqi Lyu,Xiaoyun Zhang,Chongxiao Li,Wenxuan Shi,Yutong Wu,Jianan Mu,Jinghua Wang,Yang Zhao,Pengwei Jin,Shuyao Cheng,Shengwen Liang,Xishan Zhang,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: 论文提出CodeV-R1框架，通过强化学习与可验证奖励（RLVR）训练大语言模型（LLM），用于从自然语言生成Verilog代码，解决了EDA领域缺乏自动化验证环境、高质量数据和高计算成本的问题。


<details>
  <summary>Details</summary>
Motivation: 将RLVR扩展到电子设计自动化（EDA）领域，特别是从自然语言生成硬件描述语言（Verilog），面临自动化验证环境不足、高质量数据稀缺和高计算成本三大挑战。

Method: 1. 开发基于规则的测试平台生成器进行等效检查；2. 提出往返数据合成方法生成高质量数据集；3. 采用两阶段训练流程（蒸馏后RL），并引入自适应DAPO算法降低训练成本。

Result: 模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上分别达到68.6%和72.9%的pass@1，超越之前最佳性能12~20%，且性能与671B DeepSeek-R1相当或更优。

Conclusion: CodeV-R1框架在EDA领域表现出色，未来将公开模型、训练流程和数据集以促进研究。

Abstract: Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [221] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei,Yuchen Zou,Yapeng Li,Bo Du*

Main category: cs.LG

TL;DR: FedDEA是一种联邦解耦聚合方法，专为多任务模型集成设计，通过动态识别任务相关维度并重新缩放优化效果，有效抑制跨任务干扰。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法无法将多个异构任务聚合为统一模型，导致在任务目标、标签空间和优化路径差异大的场景中效果不佳。

Method: 提出FedDEA，基于本地更新的响应强度动态识别任务相关维度，并通过重新缩放增强优化效果，实现任务级解耦聚合。

Result: 实验结果表明，FedDEA能轻松集成到主流联邦优化算法中，并在NYUD-V2和PASCAL-Context数据集上显著提升性能。

Conclusion: FedDEA在高度异构任务场景下表现出鲁棒性和泛化能力，无需依赖任务标签或架构修改，适用性广泛。

Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [222] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
*Orlando Marquez Ayala,Patrice Bechard,Emily Chen,Maggie Baird,Jingfei Chen*

Main category: cs.LG

TL;DR: 研究表明，对于需要结构化输出的领域特定任务，小型语言模型（SLMs）在质量上仍优于大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs（如GPT-4o）的每token成本降低，SLMs在推理速度和成本上的优势可能不再明显，但作者希望验证SLMs在特定任务中的质量优势。

Method: 通过比较微调SLM与提示LLMs在生成JSON格式低代码工作流任务上的表现。

Result: 研究发现，尽管良好的提示可以得到合理结果，但微调SLM平均能提升10%的质量。

Conclusion: SLMs在需要结构化输出的领域特定任务中仍具有质量优势，微调是提升性能的有效方法。

Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [223] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen,Quan Nguyen-Tri,Bang Tran Khanh,Dung D. Le,Long Tran-Thanh,Khoat Than*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，用于量化合成数据与真实数据分布差异对图像分类的影响，并提出了一种基于原型的算法，显著提升了小样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决小样本图像分类中因合成数据与真实数据分布差异导致的性能下降问题。

Method: 开发理论框架量化分布差异，提出基于原型学习的算法优化数据划分和模型训练。

Result: 实验表明，该方法在多个数据集上优于现有技术。

Conclusion: 理论框架和算法有效缩小了合成与真实数据的差距，提升了小样本分类性能。

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [224] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/abs/2505.24193)
*Ofir Schlisselberg,Tal Lancewicki,Peter Auer,Yishay Mansour*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [225] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/abs/2505.24205)
*Mingze Wang,Weinan E*

Main category: cs.LG

TL;DR: 该论文系统研究了混合专家网络（MoEs）在建模复杂任务时的表达能力，重点分析了低维度和稀疏性两种结构先验。


<details>
  <summary>Details</summary>
Motivation: 尽管MoEs在深度学习中表现出高效性，但其理论基础尚不清晰，尤其是在建模复杂任务时的能力。

Method: 通过理论分析，研究了浅层和深层MoEs在低维流形和稀疏性任务中的近似能力。

Result: 浅层MoEs能有效克服维度灾难，深层MoEs可近似具有指数级结构化任务的分段函数。

Conclusion: 研究揭示了MoEs关键组件和超参数的作用，并为其变体设计提供了建议。

Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [226] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/abs/2505.24231)
*Md Shahnawaz,Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Main category: cs.LG

TL;DR: 提出了一种基于动态分析和深度学习的恶意软件分类框架，通过提取运行时API调用特征并转换为灰度图像，利用CNN模型实现高准确率分类。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析无法有效检测多态和变形恶意软件，需通过动态分析监控运行时行为以提高检测效果。

Method: 提取PE文件运行时API调用参数，将行为数据编码为时间模式并转换为灰度图像，使用CNN模型进行分类。

Result: 实验显示平均分类准确率达98.36%，且对常见规避策略具有强鲁棒性。

Conclusion: 结合动态分析与深度学习的方法能高效分类恶意软件，且对抗规避能力强。

Abstract: Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [227] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/abs/2505.24243)
*Joohwan Ko,Justin Domke*

Main category: cs.LG

TL;DR: 论文提出了一种结合VIP和高斯分布的变分推断方法，并证明其可表示为带平移项的自回归流，进而提出了MIF架构，提升了后验近似效果。


<details>
  <summary>Details</summary>
Motivation: 解决复杂层次贝叶斯模型中变分推断后验几何的挑战。

Method: 结合VIP和全秩高斯分布，提出MIF架构，引入平移机制、先验信息和层次排序。

Result: MIF在层次和非层次基准测试中表现优异，后验近似更紧致。

Conclusion: MIF为复杂模型提供了更高效的变分推断解决方案。

Abstract: Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [228] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/abs/2505.24254)
*Zheng Wang,Wanhao Yu,Li Yang,Sen Lin*

Main category: cs.LG

TL;DR: 论文提出了一种名为Progressive Neural Collapse (ProNC)的新框架，通过动态扩展ETF目标来解决持续学习中的灾难性遗忘问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的灾难性遗忘问题以及固定全局ETF在实践中的局限性促使研究者提出更灵活的解决方案。

Method: ProNC框架通过逐步扩展ETF目标，添加新任务的类原型作为顶点，确保所有类别的最大可分性，并结合蒸馏技术平衡新旧类别的目标。

Result: 实验表明，ProNC显著优于现有基线方法，同时保持了灵活性、简单性和高效性。

Conclusion: ProNC为持续学习提供了一种高效且灵活的解决方案，解决了固定全局ETF的局限性。

Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [229] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/abs/2505.24261)
*Weiyi Wang,Junwei Deng,Yuzheng Hu,Shiyuan Zhang,Xirui Jiang,Runting Zhang,Han Zhao,Jiaqi W. Ma*

Main category: cs.LG

TL;DR: 本文探讨了数据归因方法中超参数调优的影响，发现大多数方法对关键超参数敏感，但调优成本高。作者提出理论分析并设计了一种轻量级调优策略。


<details>
  <summary>Details</summary>
Motivation: 研究数据归因方法中超参数调优的敏感性及其实际应用中的挑战。

Method: 通过大规模实证研究分析超参数敏感性，并提出基于理论分析的轻量级调优策略。

Result: 大多数数据归因方法对关键超参数敏感，但传统调优方法成本过高。提出的轻量级策略有效解决了这一问题。

Conclusion: 超参数调优是数据归因方法实际应用中的关键挑战，未来方法开发需重视调优策略的设计。

Abstract: Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [230] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/abs/2505.24262)
*Hiroki Naganuma,Kotaro Yoshida,Laura Gomezjurado Gonzalez,Takafumi Horie,Yuji Naraki,Ryotaro Shimizu*

Main category: cs.LG

TL;DR: 论文研究了任务算术在模型编辑中对公平性的影响，比较了其与全微调和LoRA的效果，并探讨了通过调整任务向量系数控制公平性的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在模型编辑中具有计算优势，但其对公平性的影响尚未充分研究，特别是在敏感应用（如仇恨言论检测）中可能带来风险。

Method: 系统评估任务算术对公平性指标（如Demographic Parity和Equalized Odds）的影响，并与全微调和LoRA进行对比。还研究了通过调整任务向量系数控制公平性的方法。

Result: 研究结果为模型编辑的公平性影响提供了新见解，并为公平性和负责任的模型编辑实践奠定了基础。

Conclusion: 任务算术的公平性影响需进一步研究，调整任务向量系数可能是控制公平性的有效途径。

Abstract: Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [231] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/abs/2505.24275)
*Mingze Wang,Jinbo Wang,Jiaqi Zhang,Wei Wang,Peng Pei,Xunliang Cai,Weinan E,Lei Wu*

Main category: cs.LG

TL;DR: GradPower是一种轻量级梯度变换技术，通过简单的代码修改加速语言模型预训练，显著降低终端损失。


<details>
  <summary>Details</summary>
Motivation: 加速语言模型预训练，同时保持优化器的内部逻辑和超参数不变。

Method: 对梯度向量应用元素级符号-幂变换（sign-power transformation），并将其输入基础优化器（如Adam）。

Result: 在不同架构、参数规模、数据集和学习率调度下，GradPower（如AdamPower）均能降低终端损失，尤其在混合专家模型和warmup-stable-decay调度中效果显著。

Conclusion: GradPower是一种高效且易于实现的梯度变换技术，适用于多种优化器，并通过理论分析揭示了其机制与梯度噪声的关系。

Abstract: We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [232] [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
*James R. Golden*

Main category: cs.LG

TL;DR: 论文提出了一种方法，将大型语言模型（LLMs）的推理操作映射为线性系统，揭示了模型内部表示的局部线性特性。


<details>
  <summary>Details</summary>
Motivation: 探索现代LLMs的内部工作机制，揭示其非线性模型中的局部线性结构，以增强模型的可解释性。

Method: 通过调整输入序列的梯度计算，使模型的Jacobian矩阵近似线性系统，从而分析模型的低维子空间和语义概念。

Result: 实验表明，多种LLMs（如Llama 3、Gemma 3等）在低维子空间中运行，且Jacobian矩阵的奇异向量与输出令牌的语义概念相关。

Conclusion: 现代LLMs虽然具有全局非线性，但其局部线性分解提供了对其内部表示和语义结构的可解释性洞察。

Abstract: We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [233] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu,Jiaxuan Gao,Xujie Shen,Chen Zhu,Zhiyu Mei,Chuyi He,Shusheng Xu,Guo Wei,Jun Mei,Jiashu Wang,Tongkai Yang,Binhang Yuan,Yi Wu*

Main category: cs.LG

TL;DR: AReaL是一个完全异步的强化学习系统，通过解耦生成和训练过程，显著提高了GPU利用率，并在数学和代码推理任务中实现了最高2.57倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模强化学习系统多为同步模式，导致GPU利用率低和系统效率低下，亟需一种高效的异步训练系统。

Method: AReaL采用完全异步架构，生成和训练过程完全解耦，并引入系统级优化和一种改进的PPO算法以处理数据过时问题。

Result: 在数学和代码推理任务中，AReaL实现了最高2.57倍的训练加速，且性能与同步系统相当或更优。

Conclusion: AReaL通过异步设计和系统优化，显著提升了强化学习训练效率，为大规模语言模型的训练提供了高效解决方案。

Abstract: Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [234] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/abs/2505.24313)
*Gengze Xu,Wei Yao,Ziqiao Wang,Yong Liu*

Main category: cs.LG

TL;DR: 论文研究了弱到强泛化（W2SG）现象，通过广义偏置-方差分解理论分析其成因，并提出了促进学生模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨弱教师标注数据下强学生模型超越教师表现的原因，并放宽了先前研究的限制性假设。

Method: 采用广义偏置-方差分解理论分析W2SG现象，并通过具体示例验证学生模型收敛到后验均值教师的能力。

Result: 学生模型在避免过拟合教师监督、降低预测熵时更易实现W2SG，反向交叉熵损失能提升学生性能。

Conclusion: W2SG现象在学生模型逼近后验均值教师时更易出现，反向交叉熵损失有助于提升模型表现。

Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [235] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2505.24317)
*Yongming Chen,Miner Chen,Liewen Liao,Mingyang Jiang,Xiang Zuo,Hengrui Zhang,Yuchen Xi,Songan Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于责任导向的奖励函数，通过结合交通规则知识图和视觉语言模型，自动分配奖励，提升自动驾驶强化学习的决策性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在自动驾驶中依赖人工设计奖励函数，效果有限且难以适应复杂场景。

Method: 引入交通规则知识图，结合视觉语言模型和检索增强生成技术，自动分配奖励。

Result: 实验表明，该方法显著提高了事故责任分配的准确性，并有效减少了智能体在交通事故中的责任。

Conclusion: 责任导向的奖励函数能更好地指导智能体遵守交通规则，优化决策性能。

Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [236] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
*Ivan Petrukha,Yana Kurliak,Nataliia Stulova*

Main category: cs.LG

TL;DR: 论文提出了SwiftEval，首个针对Swift编程语言的评估基准，通过手工设计的28个问题评估44个主流代码生成模型，发现模型在需要语言特定功能时表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要针对Python，难以高质量评估其他编程语言（如Swift），且现有方法通过自动翻译Python基准可能导致评估不相关或不足。

Method: 采用质量优先的方法，手工设计28个Swift问题（SwiftEval），并评估44个主流代码生成模型。

Result: 模型在需要语言特定功能的问题上得分显著下降，尤其是较小规模的模型。

Conclusion: SwiftEval填补了Swift评估基准的空白，揭示了现有模型在语言特定功能上的不足。

Abstract: In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [237] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/abs/2505.24353)
*Federico Milanesio,Matteo Santoro,Pietro G. Fré,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲空间可解群结构的新型深度学习算法（Cartan网络），结合群同态和保距微分同胚，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用双曲空间的度量特性和群结构，开发更高效的层次数据嵌入方法。

Method: 提出Cartan网络，通过群同态与保距微分同胚的结合构建算法。

Result: 在多个基准数据集上表现出色。

Conclusion: Cartan网络为双曲深度学习架构开辟了新方向。

Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [238] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)
*Xianglong Yan,Zhiteng Li,Tianao Zhang,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.LG

TL;DR: ReCalKV是一种后训练的KV缓存压缩方法，通过针对Keys和Values的不同策略减少隐藏维度，提升长上下文推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的长上下文推理能力受限于KV缓存的高内存需求，现有方法在高压缩比下性能下降或引入额外计算。

Method: 提出ReCalKV：对Keys采用Head-wise Similarity-aware Reordering (HSR)，对Values采用Offline Calibration and Matrix Fusion (OCMF)。

Result: 实验表明ReCalKV优于现有低秩压缩方法，在高压缩比下性能损失最小。

Conclusion: ReCalKV通过针对性的压缩策略，有效解决了KV缓存压缩的挑战，提升了长上下文推理的效率。

Abstract: Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [239] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
*Stepan Shabalin,Ayush Panda,Dmitrii Kharlapenko,Abdur Raheem Ali,Yixiong Hao,Arthur Conmy*

Main category: cs.LG

TL;DR: 稀疏自编码器（SAEs）和推理时激活分解（ITDA）用于分解大型文本到图像扩散模型的激活，SAEs在重建和可解释性上表现优异，并能用于图像生成控制。


<details>
  <summary>Details</summary>
Motivation: 探索稀疏自编码器和推理时激活分解在大型文本到图像扩散模型中的应用，以提升模型激活的可解释性和控制能力。

Method: 应用稀疏自编码器和推理时激活分解技术到Flux 1模型，并通过视觉自动化解释流程评估其可解释性。

Result: SAEs能准确重建残差流嵌入，并在可解释性上优于MLP神经元，同时能用于图像生成控制；ITDA的可解释性与SAEs相当。

Conclusion: 稀疏自编码器和推理时激活分解是提升大型扩散模型可解释性和控制能力的有效方法。

Abstract: Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [240] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/abs/2505.24365)
*Vardhan Shorewala,Shivam Shorewala*

Main category: cs.LG

TL;DR: 提出了一种统一的方法用于数据集中的聚类优化和异常检测，通过迭代减少簇内方差，优于传统k-means算法。


<details>
  <summary>Details</summary>
Motivation: 解决传统聚类算法（如k-means）在簇内方差优化和异常检测方面的不足。

Method: 提出了一种新颖算法，迭代减少N个簇的簇内方差直至全局最小值，并扩展到异常检测。

Result: 在合成数据和UCI数据集上验证，方差减少显著（18.7%和88.1%），准确率和F1分数提升明显（22.5%和20.8%）。

Conclusion: 该方法在聚类优化和异常检测方面表现优异，优于传统方法。

Abstract: This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [241] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
*Yuanfu Wang,Pengyu Wang,Chenyang Xi,Bo Tang,Junyi Zhu,Wenqiang Wei,Chen Chen,Chao Yang,Jingfeng Zhang,Chaochao Lu,Yijun Niu,Keming Mao,Zhiyu Li,Feiyu Xiong,Jie Hu,Mingchuan Yang*

Main category: cs.LG

TL;DR: 论文提出Adversarial Preference Learning (APL)方法，通过对抗训练提升语言模型的安全性，解决了RLHF的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型依赖RLHF但易受攻击，主要问题包括人工标注成本高、攻击多样性大及反馈偏差。

Method: APL引入三个创新：基于模型内在偏好的危害性度量、条件生成对抗攻击器、自动化闭环反馈迭代框架。

Result: 实验显示APL显著提升模型安全性，降低有害输出和攻击成功率，同时保持实用性。

Conclusion: APL有效解决了RLHF的不足，提升了语言模型的鲁棒性和安全性。

Abstract: Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [242] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/abs/2505.24378)
*Yilun Kong,Guozheng Ma,Qi Zhao,Haoyu Wang,Li Shen,Xueqian Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出M3DT框架，通过混合专家（MoE）增强决策Transformer（DT），解决多任务强化学习中任务数量扩展的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多任务强化学习方法在任务数量增加时性能下降，单纯扩展参数无法解决问题。

Method: 结合MoE增强DT架构，引入三阶段训练机制。

Result: M3DT在160个任务上表现优异，性能随专家数量增加而提升。

Conclusion: M3DT通过参数和任务扩展的协同优化，显著提升了多任务强化学习的可扩展性。

Abstract: Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [243] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
*Xiaoyu Wu,Yifei Pang,Terrance Liu,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 论文提出了一种新的数据提取攻击方法，挑战了精确遗忘（exact unlearning）作为隐私保护黄金标准的假设，证明其可能增加隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型训练数据中可能包含的敏感信息问题，现有精确遗忘方法被认为能有效保护隐私，但作者质疑其安全性。

Method: 利用预遗忘和遗忘后模型，通过模型引导和令牌过滤策略，提取被删除数据的分布模式。

Result: 攻击方法显著提高了数据提取成功率，在某些情况下性能翻倍，并在模拟医疗数据集上验证了其实际隐私风险。

Conclusion: 精确遗忘可能增加隐私泄露风险，建议评估遗忘方法时考虑更广泛的威胁模型，包括对抗者对历史检查点的访问。

Abstract: Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [244] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/abs/2505.24399)
*Yifei Cheng,Li Shen,Hao Sun,Nan Yin,Xiaochun Cao,Enhong Chen*

Main category: cs.LG

TL;DR: LightSAM通过自适应设置SAM的扰动半径和学习率，扩展了SAM的应用范围，减少了对参数的敏感性。


<details>
  <summary>Details</summary>
Motivation: SAM虽然提升了模型的泛化能力，但对扰动半径和学习率的敏感性限制了其实际应用。

Method: LightSAM采用AdaGrad-Norm、AdaGrad和Adam等自适应优化器替代SGD，实现参数自适应。

Result: 理论证明LightSAM在弱假设下能理想收敛，实验验证了其有效性。

Conclusion: LightSAM解决了SAM的参数敏感性问题，具有更广泛的应用潜力。

Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [245] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/abs/2505.24403)
*Giannis Nikolentzos,Konstantinos Skianis*

Main category: cs.LG

TL;DR: 论文研究了处理多集数据的神经网络的Lipschitz常数，分析了三种距离函数下的聚合函数的Lipschitz连续性，并推导了神经网络的上界。


<details>
  <summary>Details</summary>
Motivation: Lipschitz常数与神经网络的鲁棒性和泛化性相关，但此前研究主要关注多层感知机和卷积网络，本文填补了多集数据处理网络的空白。

Method: 分析了三种距离函数下聚合函数的Lipschitz连续性，计算了其常数，并推导了神经网络的上界。

Result: 每种聚合函数仅对一种距离函数具有Lipschitz连续性，实验验证了理论分析。

Conclusion: 研究为多集数据处理网络的Lipschitz常数提供了理论支持，并验证了其稳定性和泛化性。

Abstract: The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [246] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/abs/2505.24413)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Main category: cs.LG

TL;DR: 本文提出了一种两步学习策略，用于解决多任务学习中的异质性问题，通过共享表示和任务特定组件的分离，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）在多个领域应用广泛，但现有方法难以同时处理不同类型的异质性。本文旨在解决这一问题。

Method: 采用两步策略：1）利用共享表示填补缺失块；2）将输入特征与响应的映射分解为共享和任务特定组件。

Result: 实验和ADNI数据库分析表明，该方法在多任务学习中表现优于其他竞争方法。

Conclusion: 提出的两步策略有效解决了MTL中的异质性问题，提升了性能。

Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [247] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/abs/2505.24415)
*Andreas Spilz,Heiko Oppel,Michael Munz*

Main category: cs.LG

TL;DR: 提出了一种基于生物力学模拟的数据增强方法，用于解决IMU数据不足和标签模糊问题，显著提升了深度学习模型的分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动化评估运动质量对物理治疗和运动训练有重要意义，但深度学习模型在IMU数据评估中常受限于数据不足、类别不平衡和标签模糊。

Method: 通过结合生物力学模拟和运动轨迹系统修改，生成真实的IMU数据，并利用逆运动学参数和知识评估策略实现自动可靠标注。

Result: 增强数据与真实数据高度相似，显著提高了神经网络模型的分类准确性和泛化能力，尤其在数据有限的患者个性化调优中表现突出。

Conclusion: 该方法有效解决了深度学习在物理治疗运动评估中的常见挑战，具有实用性和高效性。

Abstract: Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [248] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg,Naman Deep Singh,Matthias Hein*

Main category: cs.LG

TL;DR: CLIC是一种基于多图像及其相关标题的新型训练技术，通过微调CLIP模型提升组合推理能力，在词汇和语义理解上均有改进，同时提高了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在组合推理能力上表现不足，且改进组合性的尝试往往忽略了语义理解，甚至导致检索性能下降。

Method: 提出CLIC方法，通过结合多图像及其相关标题进行微调训练。

Result: CLIC显著提升了组合推理能力，并在检索性能上取得一致增益，甚至在最新CLIPS模型上也有效。

Conclusion: CLIC是一种有效的微调方法，能同时提升组合推理和检索性能，成为SugarCrepe++基准上的最佳组合性CLIP模型。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [249] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui,Moshe Eliasof,Eldad Haber*

Main category: cs.LG

TL;DR: 论文提出了一种名为Graph Flow Matching（GFM）的轻量级增强方法，通过将速度场分解为反应项和扩散项，利用图神经网络模块聚合邻域信息，从而提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配网络通常独立预测每个点的速度，忽略了生成轨迹中相邻点之间的相关性，可能导致预测不准确。

Method: GFM将学习到的速度分解为反应项（标准流匹配网络）和扩散项（通过图神经网络模块聚合邻域信息），以低成本增强速度预测的局部上下文。

Result: 在五个图像生成基准测试中，GFM显著改善了Fréchet Inception Distance（FID）和召回率。

Conclusion: GFM作为一种模块化增强方法，有效提升了现有流匹配架构的性能。

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [250] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/abs/2505.24438)
*Franziska Heeg,Jonas Sauer,Petra Mutzel,Ingo Scholtes*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间图同构概念（一致事件图同构），用于捕捉时间图的因果拓扑结构，并基于此开发了一种新的时间图神经网络消息传递方案。


<details>
  <summary>Details</summary>
Motivation: 时间图的因果拓扑结构常被时间图神经网络（TGNNs）忽视，缺乏一种能全面捕捉其因果拓扑的时间图同构定义。

Method: 引入一致事件图同构概念，利用时间展开表示时间路径，开发了时间图的Weisfeiler-Leman算法，并设计了新的TGNN消息传递方案。

Result: 实验表明，该方法在时间图分类任务中表现良好。

Conclusion: 提出的方法为TGNNs的表达能力提供了理论基础，并在实际应用中验证了其有效性。

Abstract: An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [251] [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445)
*Xin Chen,Yarden As,Andreas Krause*

Main category: cs.LG

TL;DR: SaP（Safety Polytope）是一种几何方法，通过在LLM的表示空间中学习和强制执行安全约束，有效检测和纠正不安全输出，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）存在安全风险，如有害输出和对抗攻击漏洞，需要一种不修改模型权重的方法来增强安全性。

Method: 提出SaP框架，利用多面体的几何特性在表示空间中识别安全与不安全区域，通过几何引导实现检测和纠正。

Result: 实验表明，SaP能有效检测不道德输入，降低对抗攻击成功率，并在标准任务中保持性能。

Conclusion: SaP提供了一种显式的几何安全模型，揭示了LLM表示空间中安全性的可解释性。

Abstract: Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [252] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/abs/2505.24452)
*Anda Tang,Yiming Dong,Yutao Zeng,zhou Xun,Zhouchen Lin*

Main category: cs.LG

TL;DR: 论文提出了一种理论支持的统一预算感知（UBA）学习率调度方法，优于常见启发式调度，适用于多种架构和任务。


<details>
  <summary>Details</summary>
Motivation: 计算成本增加和资源有限，需要高效预算内训练方法，但现有学习率调度缺乏理论基础且效率低。

Method: 构建预算感知优化框架，推导出UBA调度，通过单一超参数φ平衡灵活性与简单性，无需逐网络优化。

Result: UBA在多种视觉和语言任务中表现优于常见调度，支持不同训练预算。

Conclusion: UBA提供了理论支持的高效学习率调度方法，简化了训练过程并提升了性能。

Abstract: The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [253] [Logits-Based Finetuning](https://arxiv.org/abs/2505.24461)
*Jingyao Li,Senqiao Yang,Sitong Wu,Han Shi,Chuanyang Zheng,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: 论文提出了一种基于重建方法的OOD检测框架MOOD，通过掩码图像建模任务显著提升性能，无需OOD样本即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于识别的方法容易学习捷径而非全面表示，而重建方法能更好地捕捉ID数据的内在分布。

Method: 采用掩码图像建模作为前置任务，构建OOD检测框架MOOD。

Result: MOOD在单类、多类和近分布OOD检测中分别提升5.7%、3.0%和2.1%，甚至优于10-shot-per-class的离群暴露方法。

Conclusion: 重建方法为OOD检测提供了通用且高效的先验，MOOD框架表现出色且无需OOD样本。

Abstract: The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [254] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/abs/2505.24469)
*Christina Runkel,Natacha Kuete Meli,Jovita Lukasik,Ander Biguri,Carola-Bibiane Schönlieb,Michael Moeller*

Main category: cs.LG

TL;DR: 论文探讨了通过平滑正则化提升神经网络压缩和修剪效果的方法，利用核范数和导数惩罚实现结构化平滑，结合SVD压缩达到高效模型压缩。


<details>
  <summary>Details</summary>
Motivation: 大型机器学习模型的压缩和修剪对其实际应用至关重要，但传统方法未考虑权重结构，效果有限。

Method: 在训练中应用核范数及一阶、二阶导数惩罚，实现权重结构化平滑；结合SVD压缩方法利用平滑结构近似低秩张量。

Result: 平滑模型在标准修剪方法下表现更优，压缩后参数减少70%，在CIFAR-10上达到91%准确率。

Conclusion: 结构化平滑正则化结合SVD压缩，无需微调即可实现高效模型压缩，性能优于传统方法。

Abstract: Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [255] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/abs/2505.24473)
*Nikita Balagansky,Yaroslav Aksenov,Daniil Laptev,Vadim Kurochkin,Gleb Gerasimov,Nikita Koryagin,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 提出了一种名为HierarchicalTopK的新训练目标，使稀疏自编码器（SAE）能够同时优化多个稀疏度水平，从而在灵活性和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 传统SAE的稀疏度水平固定，无法灵活适应不同需求，导致需要多个模型，增加了计算负担。

Method: 引入HierarchicalTopK训练目标，训练单个SAE以同时优化多个稀疏度水平的重建效果。

Result: 在Gemma-2 2B上的实验表明，该方法在稀疏度和解释方差之间实现了帕累托最优，且在高稀疏度下仍保持高可解释性。

Conclusion: HierarchicalTopK填补了SAE设计中灵活性与可解释性之间的重要空白。

Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [256] [Object Centric Concept Bottlenecks](https://arxiv.org/abs/2505.24492)
*David Steinmann,Wolfgang Stammer,Antonia Wüst,Kristian Kersting*

Main category: cs.LG

TL;DR: OCB框架结合概念瓶颈模型（CBMs）和预训练的对象中心基础模型，提升复杂视觉任务的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统CBMs在复杂视觉任务中表达能力和性能的局限性。

Method: 引入Object-Centric Concept Bottlenecks（OCB），结合对象中心基础模型和概念激活的线性分类器。

Result: OCB在复杂图像数据集上表现优于传统CBMs，并提供可解释的决策。

Conclusion: OCB为复杂视觉任务提供了一种高性能且可解释的解决方案。

Abstract: Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [257] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/abs/2505.24498)
*Andres Fernandez,Juan Azcarreta,Cagdas Bilen,Jesus Monge Alvarez*

Main category: cs.LG

TL;DR: 本文提出三种创新方法，显著降低计算成本的同时保持高质量语音重建：轻量级神经网络架构、延迟优化和高效最小二乘求解器。


<details>
  <summary>Details</summary>
Motivation: 在线语音频谱图反演中，现有方法计算成本高，需要优化以提升效率。

Method: 1. 设计仅8k参数的轻量级神经网络；2. 增加1个hop延迟以降低推理成本；3. 利用三对角矩阵特性开发线性复杂度最小二乘求解器。

Result: 计算成本大幅降低，神经网络规模缩小30倍，最小二乘步骤速度提升多个数量级。

Conclusion: 提出的方法在保持高质量重建的同时显著提升了计算效率。

Abstract: Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [258] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/abs/2505.24505)
*Ignacio Boero,Santiago Diaz,Tomás Vázquez,Enzo Coppes,Pablo Belzarena,Federico Larroca*

Main category: cs.LG

TL;DR: 论文探讨了机器学习在优化无功功率调度（ORPD）问题中的应用，发现现有模型在真实电网数据中表现不佳，并提出了一个公开数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: ORPD对电网稳定至关重要，但现有基于机器学习的优化方法在真实电网条件下的有效性尚未验证。

Method: 引入乌拉圭电网的真实数据集，评估学习型ORPD方法在真实数据中的表现。

Result: 真实数据导致预测误差显著增加，现有模型难以适应复杂电网条件。

Conclusion: 需要更强大的模型架构，公开数据集将推动更鲁棒的学习型优化方法研究。

Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [259] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/abs/2505.24511)
*Jiahao Wang,Mingyue Cheng,Qi Liu*

Main category: cs.LG

TL;DR: 论文探讨了将慢思考大语言模型（LLMs）用于时间序列预测（TSF）的可能性，提出了TimeReasoner方法，并验证了其在零样本情况下的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统TSF方法缺乏对时间动态和上下文依赖的显式推理，而慢思考LLMs在多步推理方面表现优异，因此研究其是否适用于TSF。

Method: 提出TimeReasoner，将TSF建模为条件推理任务，设计提示策略从预训练的慢思考LLMs中激发推理能力。

Result: 慢思考LLMs在零样本情况下表现出非平凡的预测能力，尤其在捕捉高级趋势和上下文变化方面。

Conclusion: 研究揭示了LLMs在时间领域的推理潜力与局限，为基于推理的TSF范式提供了新方向。

Abstract: Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [260] [Airborne Neural Network](https://arxiv.org/abs/2505.24513)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.LG

TL;DR: 论文提出了一种分布式架构——机载神经网络，通过多个机载设备协作计算，实现飞行中的实时学习和推理，以解决航空航天领域因基础设施限制而难以部署深度学习模型的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在数据密集型领域取得了显著成果，但在航空航天领域，由于实时数据处理和超低延迟的需求，部署深度学习系统仍面临挑战。

Method: 提出机载神经网络概念，多个机载设备各自托管神经网络的一部分神经元，通过机载网络控制器和分层控制器协作计算。

Result: 该方法为机载环境中的大规模神经网络操作提供了可能，支持实时学习和推理。

Conclusion: 这一工作为下一代AI驱动的航空航天系统奠定了基础，具有革命性潜力。

Abstract: Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [261] [Transformers Are Universally Consistent](https://arxiv.org/abs/2505.24531)
*Sagar Ghosh,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: Transformers with softmax-based nonlinear attention can perform OLS regression robustly in hyperbolic space, with empirical error bounds decaying at a provable rate.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved question of whether Transformers can robustly perform functional regression over sequences, especially given non-Euclidean data geometry.

Method: Analyzed Transformers with softmax-based nonlinear attention for OLS regression in hyperbolic space, deriving deterministic error bounds.

Result: Proved uniform consistency and derived error bounds decaying at a rate of O(t^{-1/2d}), with empirical validation on real-world datasets.

Conclusion: Transformers are theoretically and empirically capable of robust functional regression in non-Euclidean settings, with implications for their broader applicability.

Abstract: Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [262] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/abs/2505.24533)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: 提出了一种新的代数结构框架，将一维单子系统推广到高维，通过定义非交换性和全局交换律的轴组合算子，统一了多种经典线性变换。


<details>
  <summary>Details</summary>
Motivation: 为了在多维中建模方向性组合，同时保持结构一致性，并统一信号处理和数据分析中的经典变换。

Method: 递归定义基于向量-矩阵对的代数结构，通过选择不同的向量和矩阵对，系统化地推导经典变换。

Result: 框架统一了离散傅里叶变换、沃尔什变换和哈达玛变换等经典变换，并支持针对特定任务的定制化可学习变换。

Conclusion: 该框架不仅统一了经典变换，还为开发适应特定数据模态和任务的可学习变换提供了系统方法。

Abstract: We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [263] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
*Narmeen Oozeer,Luke Marks,Fazl Barez,Amirali Abdullah*

Main category: cs.LG

TL;DR: K-Steering是一种通过训练非线性多标签分类器来控制LLM行为的方法，避免了线性假设，动态组合行为且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决多行为属性在LLM推理时的干扰问题，以及线性方法的局限性。

Method: 训练非线性多标签分类器，通过梯度计算干预方向，动态组合行为。

Result: 在ToneBank和DebateMix基准测试中，K-Steering优于基线方法。

Conclusion: K-Steering提供了一种灵活且高效的多行为控制方法。

Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [264] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/abs/2505.24578)
*Abhishek Chandra,Taniya Kapoor,Mitrofan Curti,Koen Tiels,Elena A. Lomonova*

Main category: cs.LG

TL;DR: 本文提出了一种神经符号算子（NSO）框架，用于解决复杂压电系统中非线性电压-位移滞回关系的表征问题，提升了神经算子的可解释性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法在表征复杂压电系统的非线性滞回关系时存在可解释性和泛化性不足的问题，限制了其实际应用。

Method: NSO框架结合了傅里叶神经算子和基于库的稀疏模型发现方法，生成解析算子以描述滞回关系，实现白盒建模。

Result: NSO能够准确预测电压-位移滞回关系（包括蝶形关系），并对噪声和低质量电压数据具有鲁棒性。

Conclusion: NSO在可解释性和泛化性方面优于现有神经算子方法，为复杂压电系统的设计、监测和维护提供了有效工具。

Abstract: Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [265] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/abs/2505.24579)
*Chaoyu Liu,Yangming Li,Zhongying Deng,Chris Budd,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 提出了一种自适应校正方法，确保Fourier神经算子（FNO）在训练中满足守恒定律，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准FNO无法精确保持守恒定律（如质量、动量守恒），现有方法虽尝试解决但效果不佳。

Method: 引入可学习矩阵，自适应调整解以满足守恒定律，确保输出严格满足目标。

Result: 理论证明该方法优于最佳满足守恒的FNO，实验显示其性能优于现有方法。

Conclusion: 自适应校正方法能精确满足守恒定律，提升FNO在PDE建模中的表现。

Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [266] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/abs/2505.24584)
*Sakhinana Sagar Srinivas,Shivam Gupta,Venkataramana Runkana*

Main category: cs.LG

TL;DR: 提出了一种闭环、物理感知的框架，用于自动生成工业可行的PFD和PID，结合了小规模语言模型和第一性原理模拟，显著缩短了从实验室发现到工厂部署的研发时间。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在化学和材料发现中取得进展，但将其转化为工业规模生产仍面临瓶颈，尤其是无法自动生成关键的PFD和PID。

Method: 框架整合了领域专用小规模语言模型、知识图谱和多阶段训练管道，并通过模拟器验证可行性，同时采用推理优化和模型剪枝技术。

Result: 实验表明，该框架能高保真地生成模拟器验证的过程描述，优于基线方法，并能泛化到未见过的化学品。

Conclusion: 该工作通过将AI驱动设计与工业可行性结合，显著缩短了研发周期。

Abstract: Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [267] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/abs/2505.24592)
*Weebum Yoo,Sung Whan Yoon*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，通过损失表面平坦性和PAC泛化边界来解释数据增强如何提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强在不同领域取得了显著成功，但缺乏对其提升模型鲁棒性效果的一般理论理解。

Method: 通过损失表面平坦性和PAC泛化边界构建理论框架，涵盖多种现有增强方法，不限于特定类型的分布偏移。

Result: 在CIFAR、ImageNet、PACS和OfficeHome等数据集上的实验验证了理论的有效性。

Conclusion: 该框架为数据增强提升模型鲁棒性提供了统一的理论基础，适用于多种分布偏移场景。

Abstract: Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [268] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/abs/2505.24595)
*Andrei Chernov,Vitaliy Pozdnyakov,Ilya Makarov*

Main category: cs.LG

TL;DR: 论文提出了一种名为二进制累积编码（BCE）的方法，用于解决时间序列预测中分类任务忽略目标值顺序结构的问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过离散化连续目标空间进行预测，但忽略了目标值的顺序结构，无法提供预测值与真实值的相对距离信息。

Method: 提出二进制累积编码（BCE），将标量目标转换为单调二进制向量，保留顺序和幅度信息，并设计了一个结合残差和扩张卷积的卷积神经网络架构。

Result: 在基准数据集上的实验表明，该方法在点和概率预测中优于现有方法，且参数更少、训练更快。

Conclusion: BCE方法有效解决了分类任务中忽略顺序结构的问题，提升了预测性能。

Abstract: Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [269] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/abs/2505.24603)
*Omri Lev,Vishwak Srinivasan,Moshe Shenfeld,Katrina Ligett,Ayush Sekhari,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 本文通过Renyi差分隐私（RDP）重新分析高斯素描技术，提出了更严格的隐私边界，并在线性回归中验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 高斯素描技术广泛应用于数据科学和机器学习，但其隐私分析结果尚不完善，需要更精确的隐私边界。

Method: 使用Renyi差分隐私（RDP）重新分析高斯素描操作，优化隐私边界。

Result: 理论分析显示隐私边界显著收紧，实验验证了在线性回归中的性能提升和运行时间减少。

Conclusion: 改进的隐私分析为高斯素描技术提供了更优的理论基础，并在实际应用中提升了性能。

Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [270] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/abs/2505.24612)
*Sujoy Chatterjee,Everton Romanzini Colombo,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 本文提出了一种基于多准则决策的加权聚合方法，用于平衡多个XAI质量指标，生成解释模型的集成。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法（如LIME和SHAP）在评估解释质量时存在指标冲突，且缺乏多准则决策方法的研究。

Method: 提出了一种基于排名的加权聚合方法，并改进了现有XAI指标（复杂性、忠实性和稳定性）的排名版本。

Result: 在公开数据集上的实验表明，该方法在多个指标上表现稳健，TOPSIS和WSUM是最佳候选算法。

Conclusion: 该方法有效平衡了多准则冲突，提升了解释模型的稳健性和评估效果。

Abstract: Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [271] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲空间的数据集蒸馏方法HDD，通过保留数据的层次结构，显著提升了计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有分布匹配方法在欧几里得空间中忽略数据复杂几何和层次关系的问题。

Method: 将数据嵌入Lorentz双曲空间，通过优化双曲距离（测地距离）来匹配合成数据与原始数据的分布。

Result: HDD在保持模型性能的同时，仅需20%的核心集即可实现高效蒸馏，并显著提升训练稳定性。

Conclusion: HDD是一种高效且兼容性强的数据集蒸馏方法，特别适用于具有层次结构的数据。

Abstract: To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [272] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/abs/2505.24627)
*Fu Luo,Yaoxin Wu,Zhi Zheng,Zhenkun Wang*

Main category: cs.LG

TL;DR: 本文分析了现有神经组合优化（NCO）方法在容量约束紧密度变化时的性能，发现其存在过拟合问题，并提出了一种新的训练方案和多专家模块以提升适应性。


<details>
  <summary>Details</summary>
Motivation: 现有NCO方法在固定约束值下表现良好，但缺乏对不同约束紧密度影响的研究，导致性能受限。

Method: 以容量约束车辆路径问题（CVRP）为例，提出了一种考虑约束紧密度变化的训练方案和多专家模块。

Result: 实验表明，新方法有效克服了过拟合问题，在CVRP及带时间窗的CVRP（CVRPTW）中表现优异。

Conclusion: 新方法通过适应不同约束紧密度，显著提升了NCO方法的泛化能力。

Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [273] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/abs/2505.24629)
*Lotte Bransen,Tim Janssen,Jesse Davis*

Main category: cs.LG

TL;DR: 论文提出了一种球员无关的模拟框架，用于评估不同守门员策略的效果，解决了现有研究中假设守门员和罚球者独立选择动作的不现实问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于现有数据分析方法假设守门员和罚球者独立选择动作，忽略了实际比赛中双方的互动决策，导致分析结果不准确。

Method: 方法包括开发一个球员无关的模拟框架，结合丰富的动作选择和守门员技能信息，基于专家标注的大规模点球数据集。

Result: 结果表明该框架能够优化守门员在实际比赛中的策略。

Conclusion: 结论是提出的模拟框架能够更真实地模拟点球互动，为守门员策略提供有效指导。

Abstract: Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [274] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/abs/2505.24642)
*Masahiro Negishi,Thomas Gärtner,Pascal Welke*

Main category: cs.LG

TL;DR: 研究了消息传递神经网络（MPNNs）在特定任务中学习的距离函数，揭示了其隐含的功能距离，并提出了一种基于最优传输的可解释图距离方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究忽略了任务特定信息，本文旨在填补这一空白，捕捉MPNNs学习的功能距离。

Method: 通过最优传输在Weisfeiler Leman标记树上提取MPNN嵌入距离，揭示影响距离的关键子图。

Result: 实验表明MPNNs通过关注功能重要的子图来定义嵌入的相对位置。

Conclusion: 该方法可推广到两种著名图核，且计算效率高，为理解MPNNs提供了新视角。

Abstract: We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [275] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/abs/2505.24664)
*Daniel Severo,Brian Karrer,Niklas Nolte*

Main category: cs.LG

TL;DR: 提出一种基于置换替代表示的新方法，利用Lehmer码、Fisher-Yates抽取和插入向量，支持无约束学习，并在计算效率和表达能力之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 学习置换分布是机器学习中的基础问题，现有方法依赖参数化混合或昂贵的变分推断。

Method: 利用置换的替代表示（如Lehmer码等），通过深度学习技术实现无约束学习，并能表示任意置换概率分布。

Result: 在拼图任务中显著优于现有方法，并在新提出的循环置换和电影重排序任务中表现出色。

Conclusion: 新方法在表达能力和计算效率间取得平衡，传统模型在低表达能力模式下无法生成有效置换。

Abstract: Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [276] [Learning geometry and topology via multi-chart flows](https://arxiv.org/abs/2505.24665)
*Hanlin Yu,Søren Hauberg,Marcelo Hartmann,Arto Klami,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 论文提出了一种学习低维流形上退化归一化流的方法，并开发了计算流形上测地线的算法，显著提升了拓扑估计效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常位于高维空间中的低维黎曼流形上，但若流形具有非平凡拓扑结构，单一流无法正确学习，需通过多流拼接。

Method: 提出了一种通用的训练方案，用于学习多流拼接的集合，并开发了计算流形上测地线的数值算法。

Result: 实验表明，该方法在拓扑估计方面取得了显著改进。

Conclusion: 通过多流拼接和测地线计算，能够更准确地学习具有复杂拓扑结构的流形。

Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [277] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/abs/2505.24676)
*Mihir Bhaskar,Jun Tao Luo,Zihan Geng,Asmita Hajra,Junia Howell,Matthew R. Gormley*

Main category: cs.LG

TL;DR: 论文提出了一种数字化历史房产评估数据的方法，结合OCR和回归模型，解决了数据难以量化的问题，并测试了模型的通用性。


<details>
  <summary>Details</summary>
Motivation: 量化1930年代美国住房政策对种族财富差距的财务影响，但因历史房产评估记录难以获取而困难。

Method: 手动标注12,000处房产数据训练OCR模型，结合计算机视觉和深度学习，对50,000处房产进行OCR标注；对无法OCR的数据，使用回归模型估计历史价值。

Result: 成功构建并发布了一个县的数据集，验证了OCR和回归模型的可行性及通用性。

Conclusion: 该方法为学者、社区活动家和政策制定者提供了成本效益高的工具，以分析红线政策的历史影响。

Abstract: Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [278] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/abs/2505.24684)
*Zihao Chen,Yu Xiang,Wenyong Wang*

Main category: cs.LG

TL;DR: 研究发现，因子分解先验的VAEs中存在一种称为“解缠粒度”的隐式归纳偏差，影响解缠性能和ELBO推断。通过调整解缠粒度，可以扩展解缠表示的范围。


<details>
  <summary>Details</summary>
Motivation: 探索VAEs中驱动解缠的隐式归纳偏差，以解决无监督解缠的理论挑战。

Method: 分析β-TCVAE中的总相关性，提出“解缠粒度”概念，并通过实验验证其影响。

Result: 发现“V”形最优ELBO轨迹，并证明调整解缠粒度可解缠高复杂度特征。

Conclusion: 解缠粒度是VAEs中的关键隐式偏差，影响解缠性能和ELBO推断，为VAEs的偏置和可解释性提供新见解。

Abstract: Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [279] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/abs/2505.24692)
*Derek Everett,Fred Lu,Edward Raff,Fernando Camacho,James Holt*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯插值的新策略，用于学习连续空间中的奖励环境，适用于非平稳问题和大量动作空间。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法假设奖励环境是平稳的且动作空间较小，而现有方法通常只放宽其中一个假设。本文旨在同时处理非平稳性和大量动作空间的问题。

Method: 使用高斯插值学习连续Lipschitz奖励函数，并扩展到非平稳问题。

Result: 方法在非平稳性和大量动作空间的数据集上表现优于滑动高斯过程策略，计算速度快100-10000倍。

Conclusion: 该方法高效、计算友好，适用于复杂奖励环境。

Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [280] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/abs/2505.24709)
*Soichiro Nishimori,Yu-Jie Zhang,Thanawat Lodkaew,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种名为SymPO的鲁棒策略优化方法，用于在噪声偏好数据下优化语言模型。


<details>
  <summary>Details</summary>
Motivation: 现实中的偏好数据常因人为错误或偏见而含有噪声，传统方法假设标注准确，难以应对噪声问题。

Method: 将奖励建模视为分类问题，利用对称损失函数的鲁棒性，提出Symmetric Preference Optimization (SymPO)方法。

Result: 实验证明SymPO在合成和真实任务中有效，对称损失能保持奖励的秩保持性，支持策略优化。

Conclusion: SymPO为噪声偏好数据下的策略优化提供了有效解决方案。

Abstract: Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [281] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen,Jiahao Zhang,Haipeng Zhu,Boyan Xu,Zhifeng Hao,Keli Zhang,Junjian Ye,Ruichu Cai*

Main category: cs.LG

TL;DR: 论文提出Causal-aware LLMs，通过结合结构因果模型（SCM）增强大语言模型的决策能力，采用“学习-适应-行动”范式，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在决策中潜力巨大，但缺乏推理能力且难以适应新环境，限制了其在复杂任务中的应用。

Method: 提出Causal-aware LLMs，分三阶段：1）学习阶段提取环境因果实体和关系；2）适应阶段通过因果干预更新模型；3）行动阶段利用强化学习进行决策。

Result: 在22个开放世界游戏任务中验证了方法的有效性。

Conclusion: Causal-aware LLMs通过迭代学习因果知识，能更准确理解环境并高效决策。

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [282] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
*Fabio Fehr,Prabhu Teja Sivaprasad,Luca Franceschi,Giovanni Zappella*

Main category: cs.LG

TL;DR: CoRet是一个用于代码编辑任务的密集检索模型，整合了代码语义、仓库结构和调用图依赖，显著提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 解决基于自然语言查询（如实现新功能或修复错误）从代码仓库中检索相关代码片段的问题。

Method: 提出了一种专门为仓库级检索设计的损失函数来训练CoRet模型。

Result: 在SWE-bench和Long Code Arena的bug定位数据集上，检索召回率至少提高了15个百分点。

Conclusion: CoRet的设计选择对提升检索效果至关重要，证明了其在代码编辑任务中的有效性。

Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [283] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/abs/2505.24717)
*Benjamin Holzschuh,Qiang Liu,Georg Kohl,Nils Thuerey*

Main category: cs.LG

TL;DR: PDE-Transformer是一种改进的基于Transformer的架构，用于在规则网格上替代物理模拟建模，结合扩散Transformer的改进和针对大规模模拟的调整，表现出色。


<details>
  <summary>Details</summary>
Motivation: 为物理科学中的大规模基础模型提供更可扩展和通用的Transformer架构。

Method: 将不同物理通道单独嵌入为时空令牌，通过通道自注意力交互，保持信息密度一致。

Result: 在16种PDE类型的大数据集上优于现有Transformer架构，预训练模型在下游任务中表现更优。

Conclusion: PDE-Transformer是一种高效且通用的架构，适用于物理科学中的大规模建模任务。

Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [284] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/abs/2505.24721)
*Nick Rossenbach,Benedikt Hilmes,Leon Brackmann,Moritz Gunz,Ralf Schlüter*

Main category: cs.LG

TL;DR: 提出了一种基于PyTorch的库，用于模拟具有精确忆阻器硬件特性的神经网络执行，首次展示了百万参数ML系统在忆阻器硬件上的行为。


<details>
  <summary>Details</summary>
Motivation: 现有硬件原型无法适应大型神经网络，且相关文献仅覆盖小型ML模型，需探索硬件特性对大型模型的影响。

Method: 基于Synaptogen开发PyTorch库，模拟神经网络执行，准确捕捉忆阻器硬件特性，并以Conformer模型为例进行验证。

Result: 通过调整量化感知训练，使用3位权重精度时，单词错误率的相对退化限制在25%。

Conclusion: 提出的方法为大型ML模型在忆阻器硬件上的实现提供了可行性验证。

Abstract: Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [285] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/abs/2505.24722)
*Neil He,Rishabh Anand,Hiren Madhu,Ali Maatouk,Smita Krishnaswamy,Leandros Tassiulas,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: HELM是一系列基于双曲几何的大语言模型，通过引入双曲空间解决现有欧几里得模型的局限性，提升语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）依赖欧几里得操作，未能完全捕捉自然语言的语义层次和几何结构，导致训练不稳定和生成能力下降。双曲几何能更好地对齐文本的底层几何特性。

Method: 提出HELM系列模型，包括双曲空间的全新Transformer架构、混合曲率专家模型HELM-MICE（含双曲多头潜在注意力HMLA）和密集模型HELM-D，并开发了双曲旋转位置编码和RMS归一化。

Result: 在MMLU和ARC等基准测试中，HELM模型性能提升高达4%，优于LLaMA和DeepSeek等欧几里得架构。

Conclusion: 双曲几何在大规模语言模型预训练中表现出更高的效能和推理能力，HELM为语言建模提供了新的几何视角。

Abstract: Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [286] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/abs/2505.24728)
*Dongzi Jin,Yong Xiao,Yingyu Li*

Main category: cs.LG

TL;DR: 提出了一种基于锐度感知最小化的鲁棒联邦学习方法（SMRFL），通过优化模型参数邻域内的最大损失，提升模型对扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中共享模型常受扰动影响，导致性能下降，需要提升模型鲁棒性。

Method: SMRFL通过解决一个最小-最大优化问题，使模型收敛到平坦最小值，降低对扰动的敏感性。

Result: 理论证明SMRFL与无扰动联邦学习收敛速度相同，实验表明其在三种扰动场景下显著优于基线方法。

Conclusion: SMRFL有效提升了联邦学习模型对扰动的鲁棒性，具有实际应用潜力。

Abstract: Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [287] [Feature Attribution from First Principles](https://arxiv.org/abs/2505.24729)
*Magamed Taimeskhanov,Damien Garreau*

Main category: cs.LG

TL;DR: 本文提出了一种新的特征归因框架，通过从简单模型（如指示函数）开始构建，而非依赖严格的公理，从而恢复多种现有归因方法，并推导了深度ReLU网络的闭式归因表达式。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法的评估存在挑战，且公理框架过于严格，因此需要一种更灵活的新框架。

Method: 从简单模型（如指示函数）定义归因，逐步构建复杂模型的归因方法，并推导深度ReLU网络的闭式表达式。

Result: 新框架能够恢复多种现有归因方法，并提供了深度ReLU网络的归因闭式解。

Conclusion: 新框架避免了严格公理的限制，为特征归因提供了更灵活且可扩展的方法。

Abstract: Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [288] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/abs/2505.24737)
*Erchi Wang,Yuqing Zhu,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种高效的差分隐私二元线性分类算法，改进了现有结果，特别是在离群点较少时表现更优。算法无需预先知道分离边界或离群点集。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私下的经验风险最小化问题，旨在提供一种高效且适应性强的算法，适用于二元线性分类任务。

Method: 提出了一种$\varepsilon,\delta$-差分隐私算法，通过优化经验零一风险边界，无需预先知道分离边界或离群点集。

Result: 算法在离群点较少时表现优于现有方法，并提供了高级隐私超参数调优的效用边界。

Conclusion: 该算法在差分隐私二元线性分类中表现出高效性和适应性，为实际应用提供了有力工具。

Abstract: This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [289] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael,Guy Smorodinsky,Tom Tirer,Ofir Lindenbaum*

Main category: cs.LG

TL;DR: SUMO是一种基于子空间感知和矩正交化的优化器，通过精确SVD在动态调整的低维子空间中进行优化，显著提升收敛速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有低秩梯度优化方法虽节省内存，但忽视了高各向异性环境下收敛速度的潜在提升。

Method: SUMO采用精确SVD进行矩正交化，动态调整子空间，实现规范诱导的最陡下降优化。

Result: SUMO在收敛速度、稳定性、性能和内存效率（节省20%）上优于现有方法。

Conclusion: SUMO通过精确正交化显著优化了LLM训练效果，兼具高效和低内存消耗。

Abstract: Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [290] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
*Zafir Stojanovski,Oliver Stanley,Joe Sharratt,Richard Jones,Abdulhakeem Adefioye,Jean Kaddour,Andreas Köpf*

Main category: cs.LG

TL;DR: Reasoning Gym (RG) 是一个用于强化学习的推理环境库，提供可验证奖励，支持多领域数据生成和验证。


<details>
  <summary>Details</summary>
Motivation: 传统推理数据集通常是固定的，无法动态调整复杂度，RG 通过程序化生成数据解决了这一问题。

Method: RG 包含100多个数据生成器和验证器，涵盖多个领域，支持无限训练数据的生成和难度调整。

Result: 实验证明 RG 在评估和强化学习推理模型方面具有高效性。

Conclusion: RG 为推理模型的训练和评估提供了灵活且可扩展的工具。

Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [291] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/abs/2505.24773)
*Yajie Zhou,Xiaoyi Pang,Zhibo Wang*

Main category: cs.LG

TL;DR: AFLoRA是一种自适应轻量级联邦微调框架，用于解决异构环境下LLM微调的计算和通信挑战，通过解耦共享与客户端特定更新、对角矩阵秩剪枝和秩感知聚合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，异构数据和资源受限的客户端导致LLM微调性能受限，现有方法难以同时保证低系统开销和准确聚合。

Method: AFLoRA通过解耦共享与客户端特定更新、对角矩阵秩剪枝和秩感知聚合优化微调过程。

Result: 实验表明AFLoRA在准确性和效率上优于现有方法。

Conclusion: AFLoRA为异构环境下的LLM高效适应提供了实用解决方案。

Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [292] [Diffusion-Based Symbolic Regression](https://arxiv.org/abs/2505.24776)
*Zachary Bastiani,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的符号回归方法，结合随机掩码扩散与GRPO强化学习，提升方程生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，启发将其应用于符号回归问题。

Method: 采用随机掩码扩散与去噪过程生成方程，结合GRPO强化学习和长短期风险策略优化候选池。

Result: 实验验证了方法的有效性。

Conclusion: 该方法在符号回归任务中表现出色。

Abstract: Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [293] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/abs/2505.24779)
*Yidong Luo,Chenguang Wang,Jiahao Yang,Fanzeng Xia,Tianshu Yu*

Main category: cs.LG

TL;DR: 本文提出了一种用于评估混合整数线性规划（MILP）实例生成方法的综合基准框架，强调其在数学有效性、结构相似性、计算难度和下游机器学习任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习对多样化优化数据集的需求以及静态基准的局限性，MILP实例生成方法快速发展，但缺乏标准化评估技术，导致评估合成实例的保真度和实用性成为关键挑战。

Method: 框架提供统一且可扩展的方法，通过数学有效性、结构相似性、计算硬度和下游任务效用等维度评估实例质量，并利用求解器内部特征（如根节点间隙、启发式成功率和切割平面使用）进行深入分析。

Result: 框架通过明确定义的求解器无关和求解器相关指标，展示了在系统比较实例集保真度方面的有效性，并应用于当代生成模型。

Conclusion: 该基准框架旨在促进不同生成技术的稳健比较，推动高质量实例生成器的开发，并提升依赖合成MILP数据的研究的可靠性。

Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [294] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/abs/2505.24780)
*Run-Ze He,Jun-Jian Su,Su-Juan Qin,Zheng-Ping Jin,Fei Gao*

Main category: cs.LG

TL;DR: 量子神经网络（QNN）比经典模型收敛更快且精度更高，但量子机器学习中的数据增强研究较少。为解决数据稀缺问题，研究者将量子生成对抗网络（QGANs）与混合量子经典神经网络（HQCNNs）结合，提出两种数据增强策略。实验表明，QGAN在MNIST数据集上优于传统方法和经典GANs，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中数据增强的研究不足，且数据稀缺问题限制了模型性能。

Method: 结合QGANs与HQCNNs，提出通用和定制化两种数据增强策略，动态生成样本以提升模型学习能力。

Result: QGAN在MNIST数据集上表现优于传统方法和经典GANs，参数更少但性能相当。

Conclusion: QGANs能简化模型并生成高质量数据，提升HQCNNs性能，为量子数据增强在机器学习中的应用铺平道路。

Abstract: Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [295] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/abs/2505.24791)
*Jiaru Zhang,Juanwu Lu,Ziran Wang,Ruqi Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种选择性雅可比解码（SeJD）策略，通过并行迭代优化加速自回归推理，显著提升生成速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生成高质量样本时存在严格的顺序依赖性，导致推理速度慢，限制了实际应用。本文旨在解决这一问题。

Method: 通过观察发现，自回归模型中并非所有补丁都需要严格依赖前序补丁，且依赖冗余在不同层中分布不均。基于此，提出了SeJD策略，利用并行迭代优化加速推理。

Result: 实验证明，SeJD方法在多个数据集上实现了高达4.7倍的推理加速，同时保持了生成质量和保真度。

Conclusion: SeJD策略有效解决了自回归模型推理速度慢的问题，为实际部署提供了可行的解决方案。

Abstract: Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [296] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/abs/2505.24802)
*Marc González,Rachid Guerraoui,Rafael Pinot,Geovani Rizk,John Stephan,François Taïani*

Main category: cs.LG

TL;DR: ByzFL是一个开源的Python库，用于开发和测试鲁棒的联邦学习算法，提供统一框架、多种攻击模拟和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 为联邦学习研究提供一个可扩展且易于使用的工具，支持鲁棒算法的开发和评估。

Method: 通过JSON配置文件实现系统化实验，兼容PyTorch和NumPy，提供多种攻击模拟和FL场景。

Result: ByzFL支持快速原型设计和可重复研究，适用于异构数据分布和对抗性威胁模型。

Conclusion: ByzFL是一个功能强大的工具，有助于推动鲁棒联邦学习的研究和应用。

Abstract: We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [297] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
*Yinggan Xu,Yue Liu,Zhiqiang Gao,Changnan Peng,Di Luo*

Main category: cs.LG

TL;DR: 论文探讨了当前大语言模型（LLMs）在解决物理问题时缺乏简洁、基于原理的推理能力，并提出了PhySense基准来评估和改进这一能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在解决复杂科学问题方面取得了进展，但其生成的解决方案往往冗长且不透明，无法像人类专家那样基于核心物理原理进行高效、可解释的推理。

Method: 作者引入了PhySense基准，该基准设计为易于专家通过指导原则解决，但对LLMs来说却难以应对。

Result: 评估显示，现有LLMs在遵循专家式推理路径方面表现不佳。

Conclusion: 研究为开发具有高效、稳健和可解释的科学推理能力的AI系统提供了方向。

Abstract: Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [298] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/abs/2505.24835)
*Fuyuan Lyu,Linfeng Du,Yunpeng Weng,Qiufang Ying,Zhiyan Xu,Wen Zou,Haolun Wu,Xiuqiang He,Xing Tang*

Main category: cs.LG

TL;DR: 论文提出了一种风险感知的时间序列预测与分配框架（RTS-PnO），解决了传统方法的目标不匹配和预测不确定性增加的问题，并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的资金分配问题存在目标不匹配和预测不确定性增加的挑战，传统方法无法有效解决。

Method: RTS-PnO框架通过端到端训练、自适应预测不确定性校准和模型无关性三个特征来解决上述问题。

Result: 离线实验在八种金融数据集上表现优异，线上实验在腾讯跨境支付业务中实现了8.4%的遗憾减少。

Conclusion: RTS-PnO框架在资金分配问题中具有显著优势，适用于多种金融应用场景。

Abstract: Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [299] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Main category: cs.LG

TL;DR: 模型蒸馏中的对抗性偏见注入问题，学生模型比教师模型更容易受到偏见放大影响。


<details>
  <summary>Details</summary>
Motivation: 研究蒸馏模型在训练过程中对对抗性偏见注入的脆弱性，揭示安全漏洞。

Method: 通过数据投毒注入偏见，提出两种传播模式（无目标和有目标），并在多种任务和模态中验证。

Result: 仅0.25%的投毒率即可显著放大偏见，学生模型在目标场景中偏见响应率达76.9%。

Conclusion: 当前防御措施不足，需设计专门的安全策略以应对对抗性偏见。

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [300] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/abs/2505.24843)
*Ruqi Bai,Yao Ji,Zeyu Zhou,David I. Inouye*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据对的方法（NCM），通过利用不变数据对提升模型鲁棒性，解决了传统方法（如IRM）在测试数据不可用时的局限性。


<details>
  <summary>Details</summary>
Motivation: 虚假相关性会导致模型在新环境中性能下降，现有方法（如IRM）表现不佳，且依赖测试数据。本文旨在通过不变数据对提升鲁棒性。

Method: 提出噪声反事实匹配（NCM），利用不变数据对（如反事实对）约束模型，减少虚假特征的影响。

Result: 理论证明测试域误差受域内误差和反事实对质量的影响，实验验证NCM在合成和真实数据集上提升鲁棒性。

Conclusion: NCM通过数据对约束有效提升模型鲁棒性，尤其在测试数据不可用时表现优越。

Abstract: Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [301] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: Chameleon是一个高效的数据混合框架，利用杠杆分数量化域重要性，无需重新训练即可适应新数据。


<details>
  <summary>Details</summary>
Motivation: 现有域重加权方法计算成本高且需重新训练，Chameleon旨在解决这些问题。

Method: 通过构建域亲和矩阵和杠杆分数，量化域重要性并动态调整数据混合。

Result: 在预训练、适应新数据和微调场景中均表现优异，提升性能且节省计算资源。

Conclusion: Chameleon提供了一种灵活高效的域重加权方法，适用于多种场景。

Abstract: Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [302] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu,Cheng Peng,Jiangxuan Long,Weidi Xu,Wei Chu,Yuan Qi*

Main category: cs.LG

TL;DR: 论文提出了一种名为REDI的两阶段框架，通过利用正负推理数据提升小模型的推理能力，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型蒸馏方法通常丢弃错误的推理数据，而这些数据可能对提升模型性能有价值。本文旨在探索如何有效利用正负推理数据。

Method: 提出REDI框架：第一阶段通过监督微调（SFT）学习正例；第二阶段通过REDI目标函数结合正负例进一步优化模型。

Result: REDI在数学推理任务上表现优于基线方法，Qwen-REDI-1.5B模型在公开数据集上达到83.1%的MATH-500得分，性能媲美基于专有数据的模型。

Conclusion: REDI框架通过有效利用正负推理数据，为小模型离线训练提供了新的最优方法。

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [303] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/abs/2505.24857)
*Heli Ben-Hamu,Itai Gat,Daniel Severo,Niklas Nolte,Brian Karrer*

Main category: cs.LG

TL;DR: EB-Sampler是一种基于熵约束的解码方法，通过动态解掩多个标记加速采样，性能提升2-3倍且无损失。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型（MDMs）采样效率较低，且标准采样方法未充分利用模型预测的额外信息。

Method: 提出EB-Sampler，利用熵约束动态解掩多个标记，结合预定义误差容忍度优化采样过程。

Result: 在编码和数学推理基准测试中，采样速度提升2-3倍，性能无损失；在小规模推理任务（如迷宫和数独）中也表现良好。

Conclusion: EB-Sampler是一种高效且通用的采样方法，适用于多种任务，显著提升MDMs的实用性。

Abstract: Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [304] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
*Joschka Braun,Carsten Eickhoff,Seyed Ali Bahrainian*

Main category: cs.LG

TL;DR: 论文研究了在自由生成任务中使用转向向量控制文本属性的效果，发现其能有效控制目标属性，但高强度转向会降低文本质量。提示法控制较弱但保持质量，两者结合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索转向向量在自由生成任务中的有效性，填补其在多选任务之外的研究空白。

Method: 在NEWTS数据集的摘要生成任务中，评估转向向量对主题、情感、毒性和可读性的控制效果，并与提示法对比。

Result: 转向向量能有效控制目标属性，但高强度转向会降低文本质量；提示法控制较弱但保持质量；两者结合效果最佳。

Conclusion: 转向向量在自由生成任务中需权衡控制强度与文本质量，结合提示法可优化效果。

Abstract: Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [305] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/abs/2505.24874)
*Adam Stein,Aaditya Naik,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 神经符号学习结合神经网络与符号程序，但传统方法泛化能力有限。基础模型通过提示而非训练达到高性能，但缺乏可靠性。神经符号提示结合两者，探索其在基础模型时代的作用。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号学习在复杂推理任务中的局限性，同时利用基础模型的高性能与符号程序的可靠性。

Method: 提出神经符号提示方法，结合基础模型与符号程序，避免传统神经符号学习的训练问题。

Result: 基础模型为神经符号学习提供通用解决方案，避免从头训练的缺点。

Conclusion: 基础模型时代下，神经符号提示是实现神经符号学习目标的有效路径。

Abstract: Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [306] [Aligning Protein Conformation Ensemble Generation with Physical Feedback](https://arxiv.org/abs/2505.24203)
*Jiarui Lu,Xiaoyin Chen,Stephen Zhewen Lu,Aurélie Lozano,Vijil Chenthamarakshan,Payel Das,Jian Tang*

Main category: q-bio.BM

TL;DR: 论文提出了一种名为Energy-based Alignment (EBA)的方法，通过将生成模型与物理模型的反馈对齐，高效校准生成模型以平衡构象状态，从而提升蛋白质动态研究的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 蛋白质动态研究传统上依赖于耗时的分子动力学模拟，而现有的生成模型虽然能高效预测蛋白质结构，但难以有效整合物理监督。

Method: 提出EBA方法，通过物理模型的反馈校准生成模型，平衡构象状态的能量差异。

Result: 在MD集合基准测试中，EBA在生成高质量蛋白质集合方面达到最先进性能。

Conclusion: EBA提升了生成结构的物理合理性，对结构生物学和药物发现具有潜在应用价值。

Abstract: Protein dynamics play a crucial role in protein biological functions and
properties, and their traditional study typically relies on time-consuming
molecular dynamics (MD) simulations conducted in silico. Recent advances in
generative modeling, particularly denoising diffusion models, have enabled
efficient accurate protein structure prediction and conformation sampling by
learning distributions over crystallographic structures. However, effectively
integrating physical supervision into these data-driven approaches remains
challenging, as standard energy-based objectives often lead to intractable
optimization. In this paper, we introduce Energy-based Alignment (EBA), a
method that aligns generative models with feedback from physical models,
efficiently calibrating them to appropriately balance conformational states
based on their energy differences. Experimental results on the MD ensemble
benchmark demonstrate that EBA achieves state-of-the-art performance in
generating high-quality protein ensembles. By improving the physical
plausibility of generated structures, our approach enhances model predictions
and holds promise for applications in structural biology and drug discovery.

</details>


### [307] [Refining Platelet Purification Methods: Enhancing Proteomics for Clinical Applications](https://arxiv.org/abs/2505.24394)
*Vibecke Markhus,Katarina Fritz-Wallace,Olav Mjaavatten,Einar K. Kristoffersen,Dorota Goplen,Frode Selheim*

Main category: q-bio.BM

TL;DR: 研究优化了血小板分离技术，并评估了三种蛋白质组学方法，发现LFQ-DIA在覆盖率和灵敏度上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前血小板分离方法存在污染或损失问题，影响数据质量和可重复性，因此需要优化技术。

Method: 调整离心时间优化分离协议，并比较LFQ-DIA、LFQ-DDA和TMT-DDA三种蛋白质组学方法。

Result: LFQ-DIA表现最优，优化后的分离协议减少了污染和损失，并发现血小板蛋白质组成与年龄相关。

Conclusion: 优化的分离协议和LFQ-DIA方法适用于血小板蛋白质组学研究，且需注意年龄匹配对结果的影响。

Abstract: Background: Platelet proteomics offers valuable insights for clinical
research, yet isolating high-purity platelets remains a challenge. Current
methods often lead to contamination or platelet loss, compromising data quality
and reproducibility.
  Objectives: This study aimed to optimize a platelet isolation technique that
yields high-purity samples with minimal loss and to identify the most effective
mass spectrometry-based proteomic method for analyzing platelet proteins with
optimal coverage and sensitivity.
  Methods: We refined an isolation protocol by adjusting centrifugation time to
reduce blood volume requirements while preserving platelet yield and purity.
Using this optimized method, we evaluated three proteomic approaches:
Label-free Quantification with Data-Independent Acquisition (LFQ-DIA),
Label-free Quantification with Data-Dependent Acquisition (LFQ-DDA), and Tandem
Mass Tag labeling with DDA (TMT-DDA).
  Results: LFQ-DIA demonstrated superior protein coverage and sensitivity
compared to LFQ-DDA and TMT-DDA. The refined isolation protocol effectively
minimized contamination and platelet loss. Additionally, age-related
differences in platelet protein composition were observed, highlighting the
importance of using age-matched controls in biomarker discovery studies.
  Conclusions: The optimized platelet isolation protocol provides a
cost-effective and reliable method for preparing high-purity samples for
proteomics. LFQ-DIA is the most suitable approach for comprehensive platelet
protein analysis. Age-related variation in platelet proteomes underscores the
need for demographic matching in clinical proteomic research.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [308] [A survey of using EHR as real-world evidence for discovering and validating new drug indications](https://arxiv.org/abs/2505.24767)
*Nabasmita Talukdar,Xiaodan Zhang,Shreya Paithankar,Hui Wang,Bin Chen*

Main category: stat.AP

TL;DR: 本文综述了基于电子健康记录（EHRs）的药物再利用方法，包括数据来源、处理技术和研究设计，并探讨了验证中的挑战及大型语言模型（LLMs）的作用。


<details>
  <summary>Details</summary>
Motivation: 利用EHRs作为真实世界证据（RWE）支持药物新适应症的发现和验证。

Method: 综述了EHRs的数据来源、处理方法、研究设计和统计框架，并讨论了LLMs和目标试验模拟的作用。

Result: 总结了当前方法和技术进展，为研究者提供了将真实数据转化为药物再利用证据的基础资源。

Conclusion: 本文为基于EHRs的药物再利用研究提供了方法论支持和未来研究方向。

Abstract: Electronic Health Records (EHRs) have been increasingly used as real-world
evidence (RWE) to support the discovery and validation of new drug indications.
This paper surveys current approaches to EHR-based drug repurposing, covering
data sources, processing methodologies, and representation techniques. It
discusses study designs and statistical frameworks for evaluating drug
efficacy. Key challenges in validation are discussed, with emphasis on the role
of large language models (LLMs) and target trial emulation. By synthesizing
recent developments and methodological advances, this work provides a
foundational resource for researchers aiming to translate real-world data into
actionable drug-repurposing evidence.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [309] [Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition](https://arxiv.org/abs/2505.24001)
*Wonjun Yi,Wonho Jung,Kangmin Jang,Yong-Hwa Park*

Main category: eess.SP

TL;DR: 本文提出了一种针对部分标记目标数据集的多输出分类框架（MOC），用于旋转机械复合故障诊断，通过跨任务信息共享和频率层归一化提升性能。


<details>
  <summary>Details</summary>
Motivation: 旋转机械的复杂性和多样化的运行条件（如转速和变扭矩）增加了复合故障诊断的挑战，尤其是在需要域适应的场景中。

Method: 提出MOC框架，探索单任务和多任务架构，设计跨任务层结构，并引入频率层归一化。

Result: 实验表明，该模型在六种域适应场景中表现优于基线模型，尤其在复合故障设置中优势显著。

Conclusion: 模型通过跨任务交互利用结构优势，而非简单增加参数，显著提升了复合故障诊断性能。

Abstract: The increasing complexity of rotating machinery and the diversity of
operating conditions, such as rotating speed and varying torques, have
amplified the challenges in fault diagnosis in scenarios requiring domain
adaptation, particularly involving compound faults. This study addresses these
challenges by introducing a novel multi-output classification (MOC) framework
tailored for domain adaptation in partially labeled (PL) target datasets.
Unlike conventional multi-class classification (MCC) approaches, the proposed
MOC framework classifies the severity levels of compound faults simultaneously.
Furthermore, we explore various single-task and multi-task architectures
applicable to the MOC formulation-including shared trunk and cross-talk-based
designs-for compound fault diagnosis under PL conditions. Based on this
investigation, we propose a novel cross-talk layer structure that enables
selective information sharing across diagnostic tasks, effectively enhancing
classification performance in compound fault scenarios. In addition,
frequency-layer normalization was incorporated to improve domain adaptation
performance on motor vibration data. Compound fault conditions were implemented
using a motor-based test setup, and the proposed model was evaluated across six
domain adaptation scenarios. The experimental results demonstrate its superior
macro F1 performance compared to baseline models. We further showed that the
proposed mode's structural advantage is more pronounced in compound fault
settings through a single-fault comparison. We also found that frequency-layer
normalization fits the fault diagnosis task better than conventional methods.
Lastly, we discuss that this improvement primarily stems from the model's
structural ability to leverage inter-fault classification task interactions,
rather than from a simple increase in model parameters.

</details>


### [310] [Real-time Fall Prevention system for the Next-generation of Workers](https://arxiv.org/abs/2505.24487)
*Nicholas Cartocci,Antonios E. Gkikakis,Darwin G. Caldwell,Jesús Ortiz*

Main category: eess.SP

TL;DR: 提出了一种基于倒立摆动态模型和深度学习的混合方法，用于实时跌倒检测与预防，适用于工业环境中的健康工人。


<details>
  <summary>Details</summary>
Motivation: 开发通用可穿戴实时跌倒检测系统对健康工人（如工业环境中的工人）仍具挑战性，需解决数据生成和实时性问题。

Method: 使用倒立摆动态模型模拟跌倒情况，生成训练数据，并通过深度学习框架分析信号以触发跌倒缓解机制。

Result: 该方法能高效生成大量训练数据，适用于特定类型的跌倒（初始配置变化较小的情况），为通用可穿戴设备迈出第一步。

Conclusion: 该方法有望减少工业环境中的跌倒伤害，提升工人安全性，是迈向通用设备的重要进展。

Abstract: Developing a general-purpose wearable real-time fall-detection system is
still a challenging task, especially for healthy and strong subjects, such as
industrial workers that work in harsh environments. In this work, we present a
hybrid approach for fall detection and prevention, which uses the dynamic model
of an inverted pendulum to generate simulations of falling that are then fed to
a deep learning framework. The output is a signal to activate a fall mitigation
mechanism when the subject is at risk of harm. The advantage of this approach
is that abstracted models can be used to efficiently generate training data for
thousands of different subjects with different falling initial conditions,
something that is practically impossible with real experiments. This approach
is suitable for a specific type of fall, where the subjects fall without
changing their initial configuration significantly, and it is the first step
toward a general-purpose wearable device, with the aim of reducing
fall-associated injuries in industrial environments, which can improve the
safety of workers.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [311] [A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE](https://arxiv.org/abs/2505.23858)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Fred Poitevin,Jana Thayer,Daniel Ratner,Auralee Edelen,Apurva Mehta*

Main category: physics.ins-det

TL;DR: 论文讨论了随着光源亮度提升（如APS和LCLS升级），实验复杂度增加，需要实时反馈和机器学习优化以避免数据过载。


<details>
  <summary>Details</summary>
Motivation: 高亮度光源升级带来了实验复杂度和数据量的剧增，传统方法难以处理，需开发新策略。

Method: 采用机器学习驱动的优化、自动化和实时知识提取，覆盖从电子注入器到探测器的全流程。

Result: 通过加速器、光学和终端用户应用的案例展示了该策略的有效性。

Conclusion: 机器学习驱动的实时反馈和优化是实现高亮度光源实验设计性能的关键。

Abstract: With the increasing brightness of Light sources, including the
Diffraction-Limited brightness upgrade of APS and the high-repetition-rate
upgrade of LCLS, the proposed experiments therein are becoming increasingly
complex. For instance, experiments at LCLS-II-HE will require the X-ray beam to
be within a fraction of a micron in diameter, with pointing stability of a few
nanoradians, at the end of a kilometer-long electron accelerator, a
hundred-meter-long undulator section, and tens of meters long X-ray optics.
This enhancement of brightness will increase the data production rate to rival
the largest data generators in the world. Without real-time active feedback
control and an optimized pipeline to transform measurements to scientific
information and insights, researchers will drown in a deluge of mostly useless
data, and fail to extract the highly sophisticated insights that the recent
brightness upgrades promise.
  In this article, we outline the strategy we are developing at SLAC to
implement Machine Learning driven optimization, automation and real-time
knowledge extraction from the electron-injector at the start of the electron
accelerator, to the multidimensional X-ray optical systems, and till the
experimental endstations and the high readout rate, multi-megapixel detectors
at LCLS to deliver the design performance to the users. This is illustrated via
examples from Accelerator, Optics and End User applications.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [312] [Digital twins enable full-reference quality assessment of photoacoustic image reconstructions](https://arxiv.org/abs/2505.24514)
*Janek Gröhl,Leonid Kunyansky,Jenni Poimala,Thomas R. Else,Francesca Di Cecio,Sarah E. Bohndiek,Ben T. Cox,Andreas Hauptmann*

Main category: physics.med-ph

TL;DR: 该论文提出了一种利用数字孪生技术定量评估光声图像重建算法质量的方法，并首次在实验数据上测试了一种基于傅里叶变换的重建算法。


<details>
  <summary>Details</summary>
Motivation: 光声图像重建算法的质量定量评估是一个主要挑战，缺乏理想参考图像限制了评估的准确性。

Method: 使用数字孪生技术模拟组织仿体及成像系统，进行定量校准，并比较多种先进重建算法。

Result: 数字孪生框架有效评估了数值前向模型的准确性，并实现了全参考图像质量评估。傅里叶变换算法在计算成本较低的情况下，性能与迭代时间反转相当。

Conclusion: 数字孪生技术为光声图像重建算法的质量评估提供了有效工具，傅里叶变换算法在实验中表现优异。

Abstract: Quantitative comparison of the quality of photoacoustic image reconstruction
algorithms remains a major challenge. No-reference image quality measures are
often inadequate, but full-reference measures require access to an ideal
reference image. While the ground truth is known in simulations, it is unknown
in vivo, or in phantom studies, as the reference depends on both the phantom
properties and the imaging system. We tackle this problem by using numerical
digital twins of tissue-mimicking phantoms and the imaging system to perform a
quantitative calibration to reduce the simulation gap. The contributions of
this paper are two-fold: First, we use this digital-twin framework to compare
multiple state-of-the-art reconstruction algorithms. Second, among these is a
Fourier transform-based reconstruction algorithm for circular detection
geometries, which we test on experimental data for the first time. Our results
demonstrate the usefulness of digital phantom twins by enabling assessment of
the accuracy of the numerical forward model and enabling comparison of image
reconstruction schemes with full-reference image quality assessment. We show
that the Fourier transform-based algorithm yields results comparable to those
of iterative time reversal, but at a lower computational cost. All data and
code are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [313] [Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data](https://arxiv.org/abs/2505.24852)
*Douwe den Blanken,Charlotte Frenkel*

Main category: cs.AR

TL;DR: Chameleon提出了一种低功耗、高效能的端到端片上学习方法，通过统一架构、TCNs和双模式计算阵列，实现了在边缘设备上的少样本学习和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上低功耗、高效能的端到端片上学习挑战，尤其是在少样本和持续学习场景下。

Method: 采用统一的学习与推理架构、时间卷积网络（TCNs）和双模式计算阵列。

Result: 在Omniglot和Google Speech Commands数据集上取得了高准确率，同时功耗极低（3.1μW）。

Conclusion: Chameleon为边缘设备上的高效学习提供了可行方案，兼具高性能和低功耗。

Abstract: On-device learning at the edge enables low-latency, private personalization
with improved long-term robustness and reduced maintenance costs. Yet,
achieving scalable, low-power end-to-end on-chip learning, especially from
real-world sequential data with a limited number of examples, is an open
challenge. Indeed, accelerators supporting error backpropagation optimize for
learning performance at the expense of inference efficiency, while simplified
learning algorithms often fail to reach acceptable accuracy targets. In this
work, we present Chameleon, leveraging three key contributions to solve these
challenges. (i) A unified learning and inference architecture supports few-shot
learning (FSL), continual learning (CL) and inference at only 0.5% area
overhead to the inference logic. (ii) Long temporal dependencies are
efficiently captured with temporal convolutional networks (TCNs), enabling the
first demonstration of end-to-end on-chip FSL and CL on sequential data and
inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free
compute array allows either matching the power consumption of state-of-the-art
inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$
higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records
on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way
5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),
while maintaining an inference accuracy of 93.3% on the 12-class Google Speech
Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [314] [Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction](https://arxiv.org/abs/2505.23872)
*Anam Hashmi,Julia Dietlmeier,Kathleen M. Curran,Noel E. O'Connor*

Main category: eess.IV

TL;DR: 提出了一种基于生态学原理的非线性注意力架构，用于心脏MRI重建，其参数自由设计优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有注意力模块缺乏理论支持，研究通过生态学原理填补这一空白。

Method: 利用描述单物种种群增长的非线性生态差分方程，设计参数自由的注意力模块。

Result: 提出的模块在性能上超越了当前最先进的参数自由方法。

Conclusion: 生态学原理可指导高效注意力机制的设计，提升模型性能和可解释性。

Abstract: Attention is a fundamental component of the human visual recognition system.
The inclusion of attention in a convolutional neural network amplifies relevant
visual features and suppresses the less important ones. Integrating attention
mechanisms into convolutional neural networks enhances model performance and
interpretability. Spatial and channel attention mechanisms have shown
significant advantages across many downstream tasks in medical imaging. While
existing attention modules have proven to be effective, their design often
lacks a robust theoretical underpinning. In this study, we address this gap by
proposing a non-linear attention architecture for cardiac MRI reconstruction
and hypothesize that insights from ecological principles can guide the
development of effective and efficient attention mechanisms. Specifically, we
investigate a non-linear ecological difference equation that describes
single-species population growth to devise a parameter-free attention module
surpassing current state-of-the-art parameter-free methods.

</details>


### [315] [Estimating Head Motion in Structural MRI Using a Deep Neural Network Trained on Synthetic Artifacts](https://arxiv.org/abs/2505.23916)
*Charles Bricout,Samira Ebrahimi Kahou,Sylvain Bouix*

Main category: eess.IV

TL;DR: 该论文提出了一种基于3D卷积神经网络的自动化方法，用于评估MRI中的运动伪影严重程度，无需依赖专业硬件或噪声数据。


<details>
  <summary>Details</summary>
Motivation: MRI中的运动伪影会影响神经解剖学指标的准确性，而现有方法存在硬件依赖或数据噪声问题。

Method: 使用合成损坏的MRI体积训练3D卷积神经网络，以估计运动严重程度。

Result: 在独立数据集上验证，与手动评分相关性达到R²=0.65，并在12/15数据集中发现显著的厚度-运动相关性。

Conclusion: 该方法适用于不同品牌和协议的扫描仪，为结构MRI研究提供了客观、可扩展的运动评估方案。

Abstract: Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)
and can bias automated neuroanatomical metrics such as cortical thickness.
Manual review cannot objectively quantify motion in anatomical scans, and
existing automated approaches often require specialized hardware or rely on
unbalanced noisy training data. Here, we train a 3D convolutional neural
network to estimate motion severity using only synthetically corrupted volumes.
We validate our method with one held-out site from our training cohort and with
14 fully independent datasets, including one with manual ratings, achieving a
representative $R^2 = 0.65$ versus manual labels and significant
thickness-motion correlations in 12/15 datasets. Furthermore, our predicted
motion correlates with subject age in line with prior studies. Our approach
generalizes across scanner brands and protocols, enabling objective, scalable
motion assessment in structural MRI studies without prospective motion
correction.

</details>


### [316] [Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction](https://arxiv.org/abs/2505.24136)
*Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了一种通过扰动训练物理驱动深度学习模型的新方法，用于改进高加速率MRI扫描的重建质量，减少伪影和噪声放大。


<details>
  <summary>Details</summary>
Motivation: 在高加速率MRI扫描中，传统自监督学习方法常引入伪影，影响图像保真度，因此需要一种更可靠的训练策略。

Method: 通过精心设计的扰动训练PD-DL网络，引入一致性项评估模型在稀疏域中预测扰动的能力，从而改进k空间掩蔽方法。

Result: 在fastMRI膝部和脑部数据集上，该方法有效减少了混叠伪影和噪声放大，视觉和定量评估均优于现有自监督方法。

Conclusion: 提出的训练策略在高加速率MRI重建中表现优异，为无参考数据场景提供了更可靠的解决方案。

Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful
approach for improved reconstruction of rapid MRI scans. In order to train
these models in scenarios where fully-sampled reference data is unavailable,
self-supervised learning has gained prominence. However, its application at
high acceleration rates frequently introduces artifacts, compromising image
fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL
networks via carefully-designed perturbations. In particular, we enhance the
k-space masking idea of conventional self-supervised learning with a novel
consistency term that assesses the model's ability to accurately predict the
added perturbations in a sparse domain, leading to more reliable and
artifact-free reconstructions. The results obtained from the fastMRI knee and
brain datasets show that the proposed training strategy effectively reduces
aliasing artifacts and mitigates noise amplification at high acceleration
rates, outperforming state-of-the-art self-supervised methods both visually and
quantitatively.

</details>


### [317] [Beyond the LUMIR challenge: The pathway to foundational registration models](https://arxiv.org/abs/2505.24160)
*Junyu Chen,Shuwen Wei,Joel Honkamaa,Pekka Marttinen,Hang Zhang,Min Liu,Yichao Zhou,Zuopeng Tan,Zhuoyuan Wang,Yi Wang,Hongchao Zhou,Shunbo Hu,Yi Zhang,Qian Tao,Lukas Förner,Thomas Wendler,Bailiang Jian,Benedikt Wiestler,Tim Hable,Jin Kim,Dan Ruan,Frederic Madesta,Thilo Sentker,Wiebke Heyer,Lianrui Zuo,Yuwei Dai,Jing Wu,Jerry L. Prince,Harrison Bai,Yong Du,Yihao Liu,Alessa Hering,Reuben Dorent,Lasse Hansen,Mattias P. Heinrich,Aaron Carass*

Main category: eess.IV

TL;DR: LUMIR挑战赛是一个无监督脑MRI图像配准的新基准，旨在通过自监督推动生物合理变形建模，评估了深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 推动无监督脑MRI图像配准的发展，减少对解剖标签的依赖，促进生物合理变形建模。

Method: 提供4,000多例预处理T1加权脑MRI数据，通过自监督训练模型，并引入零样本泛化任务评估性能。

Result: 深度学习方法在域内和零样本任务中表现最佳，生成解剖合理的变形场，优于优化方法。

Conclusion: LUMIR挑战赛展示了深度学习在无监督脑MRI配准中的潜力，但对域外对比度适应性有待提升。

Abstract: Medical image challenges have played a transformative role in advancing the
field, catalyzing algorithmic innovation and establishing new performance
standards across diverse clinical applications. Image registration, a
foundational task in neuroimaging pipelines, has similarly benefited from the
Learn2Reg initiative. Building on this foundation, we introduce the Large-scale
Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation
benchmark designed to assess and advance unsupervised brain MRI registration.
Distinct from prior challenges that leveraged anatomical label maps for
supervision, LUMIR removes this dependency by providing over 4,000 preprocessed
T1-weighted brain MRIs for training without any label maps, encouraging
biologically plausible deformation modeling through self-supervision. In
addition to evaluating performance on 590 held-out test subjects, LUMIR
introduces a rigorous suite of zero-shot generalization tasks, spanning
out-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),
disease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,
9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and
over 4,000 image pairs were included for evaluation. Performance was assessed
using both segmentation-based metrics (Dice coefficient, 95th percentile
Hausdorff distance) and landmark-based registration accuracy (target
registration error). Across both in-domain and zero-shot tasks, deep
learning-based methods consistently achieved state-of-the-art accuracy while
producing anatomically plausible deformation fields. The top-performing deep
learning-based models demonstrated diffeomorphic properties and inverse
consistency, outperforming several leading optimization-based methods, and
showing strong robustness to most domain shifts, the exception being a drop in
performance on out-of-domain contrasts.

</details>


### [318] [A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization](https://arxiv.org/abs/2505.24351)
*Peng Qi,Wenxi Qu,Tianliang Yao,Haonan Ma,Dylan Wintle,Yinyi Lai,Giorgos Papanastasiou,Chengjia Wang*

Main category: eess.IV

TL;DR: 提出了一种基于群体优化算法的多模态冠状动脉图像配准方法，显著提升了PCI手术的精确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 利用实时XRA和详细3D CTA解剖信息优化PCI手术，但多模态图像配准存在几何、对比度和噪声差异的挑战。

Method: 采用预处理模块和基于Steger及超像素粒子群优化算法的配准模块，处理XRA和CTA图像。

Result: 在28对XRA和CTA图像上测试，算法在准确性、鲁棒性和效率上均优于四种SOTA方法。

Conclusion: 该方法显著优于现有基准，有望改善冠状动脉疾病患者的治疗效果。

Abstract: Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure
that improves coronary blood flow and treats coronary artery disease. Although
PCI typically requires 2D X-ray angiography (XRA) to guide catheter placement
at real-time, computed tomography angiography (CTA) may substantially improve
PCI by providing precise information of 3D vascular anatomy and status. To
leverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal
image registration of XRA and CTA is required, to guide the procedure and avoid
complications. This is a challenging process as it requires registration of
images from different geometrical modalities (2D -> 3D and vice versa), with
variations in contrast and noise levels. In this paper, we propose a novel
multimodal coronary artery image registration method based on a swarm
optimization algorithm, which effectively addresses challenges such as large
deformations, low contrast, and noise across these imaging modalities. Our
algorithm consists of two main modules: 1) preprocessing of XRA and CTA images
separately, and 2) a registration module based on feature extraction using the
Steger and Superpixel Particle Swarm Optimization algorithms. Our technique was
evaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients
who underwent PCI. The algorithm was compared with four state-of-the-art (SOTA)
methods in terms of registration accuracy, robustness, and efficiency. Our
method outperformed the selected SOTA baselines in all aspects. Experimental
results demonstrate the significant effectiveness of our algorithm, surpassing
the previous benchmarks and proposes a novel clinical approach that can
potentially have merit for improving patient outcomes in coronary artery
disease.

</details>


### [319] [Efficient RAW Image Deblurring with Adaptive Frequency Modulation](https://arxiv.org/abs/2505.24407)
*Wenlong Jiao,Binglong Li,Wei Shang,Ping Wang,Dongwei Ren*

Main category: eess.IV

TL;DR: FrENet是一种专为RAW图像去模糊设计的频率增强网络，通过在频域操作和自适应频率调制，显著提升了去模糊效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要关注sRGB图像，而RAW图像因其未处理和线性特性具有更强的恢复潜力，但研究不足。

Method: 提出FrENet框架，直接在频域操作，引入自适应频率位置调制模块和频域跳跃连接。

Result: FrENet在RAW图像去模糊中优于现有方法，恢复质量更高且计算效率更高，还能扩展到sRGB图像。

Conclusion: FrENet为RAW图像去模糊提供了高效解决方案，并展示了在sRGB图像中的潜力。

Abstract: Image deblurring plays a crucial role in enhancing visual clarity across
various applications. Although most deep learning approaches primarily focus on
sRGB images, which inherently lose critical information during the image signal
processing pipeline, RAW images, being unprocessed and linear, possess superior
restoration potential but remain underexplored. Deblurring RAW images presents
unique challenges, particularly in handling frequency-dependent blur while
maintaining computational efficiency. To address these issues, we propose
Frequency Enhanced Network (FrENet), a framework specifically designed for
RAW-to-RAW deblurring that operates directly in the frequency domain. We
introduce a novel Adaptive Frequency Positional Modulation module, which
dynamically adjusts frequency components according to their spectral positions,
thereby enabling precise control over the deblurring process. Additionally,
frequency domain skip connections are adopted to further preserve
high-frequency details. Experimental results demonstrate that FrENet surpasses
state-of-the-art deblurring methods in RAW image deblurring, achieving
significantly better restoration quality while maintaining high efficiency in
terms of reduced MACs. Furthermore, FrENet's adaptability enables it to be
extended to sRGB images, where it delivers comparable or superior performance
compared to methods specifically designed for sRGB data. The code will be
available at https://github.com/WenlongJiao/FrENet .

</details>


### [320] [pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation](https://arxiv.org/abs/2505.24421)
*Abdul-mojeed Olabisi Ilyas,Adeleke Maradesa,Jamal Banzi,Jianpan Huang,Henry K. F. Mak,Kannie W. Y. Chan*

Main category: eess.IV

TL;DR: MEAL框架通过多编码器和融合策略提升3D医学影像的泛化能力，在CT-to-MRI转换任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学影像中数据稀缺、不一致及传统增强方法忽视独特性的问题。

Method: 提出MEAL框架，利用四种增强变体和三种融合策略（CC、FL、BD）构建多编码器模型。

Result: MEAL-BD在CT-to-MRI转换任务中表现最优，PSNR和SSIM得分最高。

Conclusion: MEAL通过增强多样性实现协议不变性，推动临床可靠的医学影像解决方案。

Abstract: Medical imaging is critical for diagnostics, but clinical adoption of
advanced AI-driven imaging faces challenges due to patient variability, image
artifacts, and limited model generalization. While deep learning has
transformed image analysis, 3D medical imaging still suffers from data scarcity
and inconsistencies due to acquisition protocols, scanner differences, and
patient motion. Traditional augmentation uses a single pipeline for all
transformations, disregarding the unique traits of each augmentation and
struggling with large data volumes.
  To address these challenges, we propose a Multi-encoder Augmentation-Aware
Learning (MEAL) framework that leverages four distinct augmentation variants
processed through dedicated encoders. Three fusion strategies such as
concatenation (CC), fusion layer (FL), and adaptive controller block (BD) are
integrated to build multi-encoder models that combine augmentation-specific
features before decoding. MEAL-BD uniquely preserves augmentation-aware
representations, enabling robust, protocol-invariant feature learning.
  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic
Resonance Imaging (MRI) translation study, MEAL-BD consistently achieved the
best performance on both unseen- and predefined-test data. On both geometric
transformations (like rotations and flips) and non-augmented inputs, MEAL-BD
outperformed other competing methods, achieving higher mean peak
signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)
scores. These results establish MEAL as a reliable framework for preserving
structural fidelity and generalizing across clinically relevant variability. By
reframing augmentation as a source of diverse, generalizable features, MEAL
supports robust, protocol-invariant learning, advancing clinically reliable
medical imaging solutions.

</details>


### [321] [Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution](https://arxiv.org/abs/2505.24605)
*Ivan Pereira-Sánchez,Julia Navarro,Ana Belén Petro,Joan Duran*

Main category: eess.IV

TL;DR: 提出了一种端到端的模型驱动框架，将联合空间-光谱超分辨率问题分解为空间超分辨率、光谱超分辨率和融合任务，并通过可学习模块优化各子任务。


<details>
  <summary>Details</summary>
Motivation: 解决从低分辨率多光谱观测重建高分辨率高光谱图像的问题，填补联合空间-光谱超分辨率研究的空白。

Method: 采用变分方法展开各子任务，设计基于经典反投影算法的空间上采样操作、基于聚类的光谱重建操作，以及融合低频估计和高频注入模块的图像融合方法。

Result: 在多个数据集和采样因子上的广泛评估证明了方法的有效性。

Conclusion: 提出的框架在联合空间-光谱超分辨率任务中表现优异，代码将开源。

Abstract: This paper addresses the problem of reconstructing a high-resolution
hyperspectral image from a low-resolution multispectral observation. While
spatial super-resolution and spectral super-resolution have been extensively
studied, joint spatio-spectral super-resolution remains relatively explored. We
propose an end-to-end model-driven framework that explicitly decomposes the
joint spatio-spectral super-resolution problem into spatial super-resolution,
spectral super-resolution and fusion tasks. Each sub-task is addressed by
unfolding a variational-based approach, where the operators involved in the
proximal gradient iterative scheme are replaced with tailored learnable
modules. In particular, we design an upsampling operator for spatial
super-resolution based on classical back-projection algorithms, adapted to
handle arbitrary scaling factors. Spectral reconstruction is performed using
learnable cluster-based upsampling and downsampling operators. For image
fusion, we integrate low-frequency estimation and high-frequency injection
modules to combine the spatial and spectral information from spatial
super-resolution and spectral super-resolution outputs. Additionally, we
introduce an efficient nonlocal post-processing step that leverages image
self-similarity by combining a multi-head attention mechanism with residual
connections. Extensive evaluations on several datasets and sampling factors
demonstrate the effectiveness of our approach. The source code will be
available at https://github.com/TAMI-UIB/JSSUNet

</details>


### [322] [TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching](https://arxiv.org/abs/2505.24687)
*Shengyuan Liu,Wenting Chen,Boyun Zheng,Wentao Pan,Xiang Li,Yixuan Yuan*

Main category: eess.IV

TL;DR: TumorGen提出了一种高效的三维肿瘤合成方法，通过边界感知和流匹配技术解决了现有方法在肿瘤多样性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有肿瘤数据合成方法存在肿瘤多样性受限、计算效率低以及无法捕捉肿瘤边界渐变特性的问题。

Method: TumorGen包含三个关键模块：边界感知伪掩码生成、空间约束向量场估计器和VAE引导的掩码细化器，利用修正流匹配技术实现高效合成。

Result: 实验表明，TumorGen在计算效率和真实感上优于现有方法，同时减少了采样步骤。

Conclusion: TumorGen为AI驱动的癌症诊断提供了高效且真实的肿瘤合成解决方案。

Abstract: Tumor data synthesis offers a promising solution to the shortage of annotated
medical datasets. However, current approaches either limit tumor diversity by
using predefined masks or employ computationally expensive two-stage processes
with multiple denoising steps, causing computational inefficiency.
Additionally, these methods typically rely on binary masks that fail to capture
the gradual transitions characteristic of tumor boundaries. We present
TumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow
Matching for efficient 3D tumor synthesis with three key components: a
Boundary-Aware Pseudo Mask Generation module that replaces strict binary masks
with flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that
simultaneously synthesizes tumor latents and masks using rectified flow
matching to ensure computational efficiency; and a VAE-guided mask refiner that
enhances boundary realism. TumorGen significantly improves computational
efficiency by requiring fewer sampling steps while maintaining pathological
accuracy through coarse and fine-grained spatial constraints. Experimental
results demonstrate TumorGen's superior performance over existing tumor
synthesis methods in both efficiency and realism, offering a valuable
contribution to AI-driven cancer diagnostics.

</details>


### [323] [Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI](https://arxiv.org/abs/2505.24739)
*Xinliu Zhong,Ruiying Liu,Emily S. Nichols,Xuzhe Zhang,Andrew F. Laine,Emma G. Duerden,Yun Wang*

Main category: eess.IV

TL;DR: 提出了一种基于多回波T2*加权MRI的胎盘分割框架，通过自监督学习和无监督域适应解决边界对比弱、缺乏标注和运动伪影问题。


<details>
  <summary>Details</summary>
Motivation: 胎盘分割在T2*加权成像中面临边界对比弱、缺乏标注和运动伪影的挑战，需利用多回波信息提升鲁棒性。

Method: 结合掩码自编码（MAE）自监督预训练、掩码伪标签（MPL）无监督域适应及全局-局部协作，引入语义匹配损失。

Result: 在临床数据集上表现优于单回波和简单融合基线，能有效跨回波泛化。

Conclusion: 首次系统利用多回波T2*加权MRI进行胎盘分割，方法具有创新性和实用性。

Abstract: Accurate placental segmentation is essential for quantitative analysis of the
placenta. However, this task is particularly challenging in T2*-weighted
placental imaging due to: (1) weak and inconsistent boundary contrast across
individual echoes; (2) the absence of manual ground truth annotations for all
echo times; and (3) motion artifacts across echoes caused by fetal and maternal
movement. In this work, we propose a contrast-augmented segmentation framework
that leverages complementary information across multi-echo T2*-weighted MRI to
learn robust, contrast-invariant representations. Our method integrates: (i)
masked autoencoding (MAE) for self-supervised pretraining on unlabeled
multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain
adaptation across echo times; and (iii) global-local collaboration to align
fine-grained features with global anatomical context. We further introduce a
semantic matching loss to encourage representation consistency across echoes of
the same subject. Experiments on a clinical multi-echo placental MRI dataset
demonstrate that our approach generalizes effectively across echo times and
outperforms both single-echo and naive fusion baselines. To our knowledge, this
is the first work to systematically exploit multi-echo T2*-weighted MRI for
placental segmentation.

</details>


### [324] [Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images](https://arxiv.org/abs/2505.24799)
*Aditya Retnanto,Son Le,Sebastian Mueller,Armin Leitner,Konrad Schindler,Yohan Iddawela,Michael Riffler*

Main category: eess.IV

TL;DR: SEN4X是一种混合超分辨率架构，结合单图像和多图像技术，将Sentinel-2图像分辨率提升至2.5米。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2卫星图像分辨率较低，无法捕捉小尺度特征（如房屋、街道），需通过超分辨率技术提升分辨率。

Method: 结合Sentinel-2的时间重复采样和高分辨率Pléiades Neo数据的学习先验，设计混合架构SEN4X。

Result: 在越南河内的城市土地覆盖分类测试中，SEN4X显著优于现有超分辨率基线方法。

Conclusion: SEN4X有效提升了Sentinel-2图像分辨率，适用于小尺度特征识别。

Abstract: Super-resolution aims to increase the resolution of satellite images by
reconstructing high-frequency details, which go beyond na\"ive upsampling. This
has particular relevance for Earth observation missions like Sentinel-2, which
offer frequent, regular coverage at no cost; but at coarse resolution. Its
pixel footprint is too large to capture small features like houses, streets, or
hedge rows. To address this, we present SEN4X, a hybrid super-resolution
architecture that combines the advantages of single-image and multi-image
techniques. It combines temporal oversampling from repeated Sentinel-2
acquisitions with a learned prior from high-resolution Pl\'eiades Neo data. In
doing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.
We test the super-resolved images on urban land-cover classification in Hanoi,
Vietnam. We find that they lead to a significant performance improvement over
state-of-the-art super-resolution baselines.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [325] [Online Fair Division with Additional Information](https://arxiv.org/abs/2505.24503)
*Tzeh Yuan Neoh,Jannik Peters,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究了在线不可分割物品分配的公平性问题，探讨了信息对未来物品的影响，提出了在不同信息条件下的算法和不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 解决在线环境下公平分配物品的挑战，尤其是信息对未来物品的影响。

Method: 分析了无信息、标准化信息和频率预测三种情况，提出了相应的算法。

Result: 无信息时公平性难以保证；有额外信息时可设计更公平的算法。

Conclusion: 信息结构对在线公平分配至关重要，合理利用信息可改善公平性。

Abstract: We study the problem of fairly allocating indivisible goods to agents in an
online setting, where goods arrive sequentially and must be allocated
irrevocably to agents. Focusing on the popular fairness notions of
envy-freeness, proportionality, and maximin share fairness (and their
approximate variants), we ask how the availability of information on future
goods influences the existence and approximability of fair allocations. In the
absence of any such information, we establish strong impossibility results,
demonstrating the inherent difficulty of achieving even approximate fairness
guarantees. In contrast, we demonstrate that knowledge of additional
information -- such as aggregate of each agent's total valuations
(equivalently, normalized valuations) or the multiset of future goods values
(frequency predictions) -- would enable the design of fairer online algorithms.
Given normalization information, we propose an algorithm that achieves stronger
fairness guarantees than previously known results. Given frequency predictions,
we introduce a meta-algorithm that leverages frequency predictions to match the
best-known offline guarantees for a broad class of ''share-based'' fairness
notions. Our complementary impossibility results in each setting underscore
both the limitations imposed by uncertainty about future goods and the
potential of leveraging structured information to achieve fairer outcomes in
online fair division.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [326] [A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors](https://arxiv.org/abs/2505.23862)
*Zheng Gong,Ziyi Jiang,Weihao Gao,Deng Zhuo,Lan Ma*

Main category: q-bio.QM

TL;DR: RNop是一种基于深度学习的mRNA优化方法，通过四种专用损失函数实现高保真度、高效计算和多目标优化，显著提升蛋白质表达水平。


<details>
  <summary>Details</summary>
Motivation: 现有mRNA优化方法难以同时实现高保真度、计算效率和多目标优化，且未全面考虑mRNA生命周期和翻译过程的关键因素。

Method: RNop利用大规模数据集和四种损失函数（GPLoss、CAILoss、tAILoss、MFELoss），优化物种特异性密码子适应、tRNA可用性和mRNA二级结构。

Result: RNop在计算吞吐量（47.32序列/秒）和蛋白质表达水平上显著优于现有方法，并通过实验验证其有效性。

Conclusion: RNop为高效、有效的mRNA设计提供了新方法，超越了现有技术，代码和模型已开源。

Abstract: The mRNA optimization is critical for therapeutic and biotechnological
applications, since sequence features directly govern protein expression levels
and efficacy. However, current methods face significant challenges in
simultaneously achieving three key objectives: (1) fidelity (preventing
unintended amino acid changes), (2) computational efficiency (speed and
scalability), and (3) the scope of optimization variables considered
(multi-objective capability). Furthermore, existing methods often fall short of
comprehensively incorporating the factors related to the mRNA lifecycle and
translation process, including intrinsic mRNA sequence properties, secondary
structure, translation elongation kinetics, and tRNA availability. To address
these limitations, we introduce \textbf{RNop}, a novel deep learning-based
method for mRNA optimization. We collect a large-scale dataset containing over
3 million sequences and design four specialized loss functions, the GPLoss,
CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control
over sequence fidelity while optimizing species-specific codon adaptation, tRNA
availability, and desirable mRNA secondary structure features. Then, we
demonstrate RNop's effectiveness through extensive in silico and in vivo
experiments. RNop ensures high sequence fidelity, achieves significant
computational throughput up to 47.32 sequences/s, and yields optimized mRNA
sequences resulting in a significant increase in protein expression for
functional proteins compared to controls. RNop surpasses current methodologies
in both quantitative metrics and experimental validation, enlightening a new
dawn for efficient and effective mRNA design. Code and models will be available
at https://github.com/HudenJear/RPLoss.

</details>


### [327] [Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport](https://arxiv.org/abs/2505.24759)
*Mu Qiao*

Main category: q-bio.QM

TL;DR: OT-MESH是一种无监督计算框架，利用熵正则化最优传输（OT）和MESH技术，系统性地确定跨物种细胞类型的同源性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨物种细胞类型对应关系研究中存在的非对称性和计算复杂性问题。

Method: 通过选择高信噪比基因构建成本矩阵，结合MESH技术迭代优化传输计划。

Result: 在鼠和猕猴的视网膜细胞中准确识别已知和新的对应关系，部分结果经实验验证。

Conclusion: OT-MESH为跨物种细胞类型映射提供了对称、可扩展且可解释的解决方案。

Abstract: Identifying evolutionary correspondences between cell types across species is
a fundamental challenge in comparative genomics and evolutionary biology.
Existing approaches often rely on either reference-based matching, which
imposes asymmetry by designating one species as the reference, or
projection-based matching, which may increase computational complexity and
obscure biological interpretability at the cell-type level. Here, we present
OT-MESH, an unsupervised computational framework leveraging entropy-regularized
optimal transport (OT) to systematically determine cross-species cell type
homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn
(MESH) technique to refine the OT plan. It begins by selecting genes with high
Signal-to-Noise Ratio (SNR) to capture the most informative features, from
which a cost matrix is constructed using cosine distances between cell-type
centroids. Importantly, the MESH procedure iteratively refines the cost matrix,
leading to a transport plan with significantly enhanced sparsity and
interpretability of the resulting correspondence matrices. Applied to retinal
bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque,
OT-MESH accurately recovers known evolutionary relationships and uncovers novel
correspondences, one of which was independently validated experimentally. Thus,
our framework offers a principled, scalable, symmetric, and interpretable
solution for evolutionary cell type mapping, facilitating deeper insights into
cellular specialization and conservation across species.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [328] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
*Zaihan Yang,Ryan Leonard,Hien Tran,Rory Driscoll,Chadbourne Davis*

Main category: cs.CL

TL;DR: 论文提出了一种结合RoBERTa、TF-IDF和PCA的混合模型，用于从Reddit帖子中分类自杀风险严重程度，并通过实验验证了其优于单一模型和传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 自杀念头和行为是社会关注的重大问题，亟需有效工具进行早期风险检测。

Method: 采用RoBERTa-TF-IDF-PCA混合模型，结合深度上下文嵌入与统计词频加权，并通过数据重采样和增强技术优化模型。

Result: 混合模型在加权F1分数上达到0.7512，优于RoBERTa、BERT和其他传统分类器。

Conclusion: 混合模型显著提升了自杀风险分类的准确性和可靠性，为早期检测提供了有效工具。

Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical
societal concern, highlighting the urgent need for effective tools to enable
early detection of suicidal risk. In this work, we develop robust machine
learning models that leverage Reddit posts to automatically classify them into
four distinct levels of suicide risk severity. We frame this as a multi-class
classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating
the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),
a state-of-the-art deep learning transformer model, with the statistical
term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy
and reliability of suicide risk assessment. To address data imbalance and
overfitting, we explore various data resampling techniques and data
augmentation strategies to enhance model generalization. Additionally, we
compare our model's performance against that of using RoBERTa only, the BERT
model and other traditional machine learning classifiers. Experimental results
demonstrate that the hybrid model can achieve improved performance, giving a
best weighted $F_{1}$ score of 0.7512.

</details>


### [329] [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
*Mai Ali,Christopher Lucasius,Tanmay P. Patel,Madison Aitken,Jacob Vorstman,Peter Szatmari,Marco Battaglia,Deepa Kundur*

Main category: cs.CL

TL;DR: 论文提出了一种基于语音的多模态抑郁症检测方法，结合文本、声学特征和生物标志物，并通过多任务学习和纵向分析提升性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症检测通常将语音视为单一模态，而本研究将其视为三模态数据源，以更全面地捕捉抑郁症特征。

Method: 采用大型语言模型架构，整合语音文本、声学标志物和生物标志物，结合多任务学习和纵向分析策略。

Result: 在Depression Early Warning数据集上，平衡准确率达到70.8%，优于单模态、单任务和非纵向方法。

Conclusion: 多模态、多任务和纵向分析的综合方法显著提升了抑郁症检测性能。

Abstract: Speech is a noninvasive digital phenotype that can offer valuable insights
into mental health conditions, but it is often treated as a single modality. In
contrast, we propose the treatment of patient speech data as a trimodal
multimedia data source for depression detection. This study explores the
potential of large language model-based architectures for speech-based
depression prediction in a multimodal regime that integrates speech-derived
text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents
a significant challenge and is often comorbid with multiple disorders, such as
suicidal ideation and sleep disturbances. This presents an additional
opportunity to integrate multi-task learning (MTL) into our study by
simultaneously predicting depression, suicidal ideation, and sleep disturbances
using the multimodal formulation. We also propose a longitudinal analysis
strategy that models temporal changes across multiple clinical interactions,
allowing for a comprehensive understanding of the conditions' progression. Our
proposed approach, featuring trimodal, longitudinal MTL is evaluated on the
Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%,
which is higher than each of the unimodal, single-task, and non-longitudinal
methods.

</details>


### [330] [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)
*Atanu Barai,Stephan Eidenbenz,Nandakishore Santhi*

Main category: cs.CL

TL;DR: 论文提出了一种基于并行离散事件模拟（PDES）的方法，将多个AI与非AI系统结合，以规则化的方式确保输出的正确性。通过实验验证，该方法在多个领域的准确率显著高于单独使用AI模型。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥AI系统的潜力并确保其输出的可信性，需要将多个AI与非AI系统无缝结合，以约束和验证AI的输出。

Method: 采用PDES框架，将每个AI和非AI代理视为实体，通过时间推移和规则化的交互实现协作。非AI代理作为审计者，动态约束AI代理的行为。

Result: 实验表明，该方法在四个不同领域的问题上准确率达到68%，而单独使用AI模型的准确率不足23%。

Conclusion: 通过PDES框架结合AI与非AI代理，能够显著提升复杂问题的解决准确率，同时确保系统的可扩展性和规则遵循。

Abstract: To fully leverage the potential of artificial intelligence (AI) systems in a
trustworthy manner, it is desirable to couple multiple AI and non-AI systems
together seamlessly for constraining and ensuring correctness of the output.
This paper introduces a novel parallel discrete event simulation (PDES) based
methodology to combine multiple AI and non-AI agents in a causal, rule-based
way. Our approach tightly integrates the concept of passage of time, with each
agent considered as an entity in the PDES framework and responding to prior
requests from other agents. Such coupling mechanism enables the agents to work
in a co-operative environment towards a common goal while many tasks run in
parallel throughout the simulation. It further enables setting up boundaries to
the outputs of the AI agents by applying necessary dynamic constraints using
non-AI agents while allowing for scalability through deployment of hundreds of
such agents in a larger compute cluster. Distributing smaller AI agents can
enable extremely scalable simulations in the future, addressing local memory
bottlenecks for model parameter storage. Within a PDES involving both AI and
non-AI agents, we break down the problem at hand into structured steps, when
necessary, providing a set of multiple choices to the AI agents, and then
progressively solve these steps towards a final goal. At each step, the non-AI
agents act as unbiased auditors, verifying each action by the AI agents so that
certain rules of engagement are followed. We evaluate our approach by solving
four problems from four different domains and comparing the results with those
from AI models alone. Our results show greater accuracy in solving problems
from various domains where the AI models struggle to solve the problems solely
by themselves. Results show that overall accuracy of our approach is 68% where
as the accuracy of vanilla models is less than 23%.

</details>


### [331] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
*Nic Dobbins,Christelle Xiong,Kristine Lan,Meliha Yetisgen*

Main category: cs.CL

TL;DR: LLM自主代理尝试复现阿尔茨海默病研究结果，平均复现53.2%的发现，展示了潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为自主代理复现已发表研究的能力，评估其在生物医学研究中的自动化潜力。

Method: 使用NACC数据集和GPT-4o模拟研究团队，基于摘要、方法部分和数据字典复现五项研究。

Result: 平均复现53.2%的发现，数值和统计方法存在差异，但趋势和显著性有时相似。

Conclusion: LLM在自动化复现研究中展现出潜力，但仍受限于细节缺失和方法差异。

Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as
autonomous agents to reproduce findings of published research studies using the
same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National
Alzheimer's Coordinating Center (NACC). We identified highly cited published
research manuscripts using NACC data and selected five studies that appeared
reproducible using this dataset alone. Using GPT-4o, we created a simulated
research team of LLM-based autonomous agents tasked with writing and executing
code to dynamically reproduce the findings of each study, given only study
Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5
Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of
findings per study. Numeric values and range-based findings often differed
between studies and agents. The agents also applied statistical methods or
parameters that varied from the originals, though overall trends and
significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques
and findings. In others, they failed due to implementation flaws or missing
methodological detail. These discrepancies show the current limits of LLMs in
fully automating reproducibility assessments. Still, this early investigation
highlights the potential of structured agent-based systems to provide scalable
evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and
limitations of LLMs as autonomous agents for automating reproducibility in
biomedical research.

</details>


### [332] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
*Cody Kommers,Drew Hemment,Maria Antoniak,Joel Z. Leibo,Hoyt Long,Emily Robinson,Adam Sobey*

Main category: cs.CL

TL;DR: 论文主张利用大语言模型（LLMs）通过厚描述（thick description）在AI系统中捕捉文化背景和人类意义，克服传统薄描述（thin description）的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统依赖薄描述（数值化表示），无法有效捕捉人类活动的文化背景和意义。人文和社科领域的厚描述方法虽能保留上下文，但难以规模化应用。

Method: 利用LLMs的文本生成能力，部分自动化厚描述的生成和处理，以解决规模化问题。

Result: 提出厚描述作为新表示格式，并识别了五个关键挑战：保留上下文、维持解释多元性、整合生活经验和批判视角、区分质性与量化内容、承认意义的动态性。

Conclusion: 厚描述有望成为统一框架，解决LLMs中文化表示的难题，是生成AI应用的重要方向。

Abstract: This position paper argues that large language models (LLMs) can make
cultural context, and therefore human meaning, legible at an unprecedented
scale in AI-based sociotechnical systems. We argue that such systems have
previously been unable to represent human meaning because they rely on thin
descriptions: numerical representations that enforce standardization and
therefore strip human activity of the cultural context that gives it meaning.
By contrast, scholars in the humanities and qualitative social sciences have
developed frameworks for representing meaning through thick description: verbal
representations that accommodate heterogeneity and retain contextual
information needed to represent human meaning. While these methods can
effectively codify meaning, they are difficult to deploy at scale. However, the
verbal capabilities of LLMs now provide a means of (at least partially)
automating the generation and processing of thick descriptions, potentially
overcoming this bottleneck. We argue that the problem of rendering human
meaning legible is not just about selecting better metrics, but about
developing new representational formats (based on thick description). We frame
this as a crucial direction for the application of generative AI and identify
five key challenges: preserving context, maintaining interpretive pluralism,
integrating perspectives based on lived experience and critical distance,
distinguishing qualitative content from quantitative magnitude, and
acknowledging meaning as dynamic rather than static. Furthermore, we suggest
that thick description has the potential to serve as a unifying framework to
address a number of emerging concerns about the difficulties of representing
culture in (or using) LLMs.

</details>


### [333] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
*Shujian Yang,Shiyao Cui,Chuanrui Hu,Haicheng Wang,Tianwei Zhang,Minlie Huang,Jialiang Lu,Han Qiu*

Main category: cs.CL

TL;DR: 论文探讨了中文有毒内容检测的挑战，提出了一种扰动策略分类法，并评估了9种SOTA LLMs在检测扰动文本时的表现，发现LLMs对多模态中文有毒内容检测能力较弱，且ICL或SFT可能导致过度修正。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决中文有毒内容检测中因多模态特性（如字符替换）导致的LLMs性能下降问题。

Method: 提出3种扰动策略和8种具体方法，构建数据集，评估9种SOTA LLMs，并探索ICL和SFT的增强效果。

Result: 发现LLMs对扰动多模态中文有毒内容检测能力不足，且ICL或SFT可能导致模型将正常内容误判为有毒。

Conclusion: 中文有毒内容检测需进一步优化LLMs对多模态扰动的鲁棒性，避免过度修正。

Abstract: Detecting toxic content using language models is important but challenging.
While large language models (LLMs) have demonstrated strong performance in
understanding Chinese, recent studies show that simple character substitutions
in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In
this paper, we highlight the multimodal nature of Chinese language as a key
challenge for deploying LLMs in toxic Chinese detection. First, we propose a
taxonomy of 3 perturbation strategies and 8 specific approaches in toxic
Chinese content. Then, we curate a dataset based on this taxonomy, and
benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect
perturbed toxic Chinese text. Additionally, we explore cost-effective
enhancement solutions like in-context learning (ICL) and supervised fine-tuning
(SFT). Our results reveal two important findings. (1) LLMs are less capable of
detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a
small number of perturbed examples may cause the LLMs "overcorrect'':
misidentify many normal Chinese contents as toxic.

</details>


### [334] [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/abs/2505.24712)
*Guido Ivetta,Marcos J. Gomez,Sofía Martinelli,Pietro Palombini,M. Emilia Echeveste,Nair Carolina Mazzeo,Beatriz Busaniche,Luciana Benotti*

Main category: cs.CL

TL;DR: HESEIA是一个由拉丁美洲教师和学生共同设计的46,499句数据集，用于评估大型语言模型中的社会偏见，特别关注交叉性和本地化背景。


<details>
  <summary>Details</summary>
Motivation: 现有评估大型语言模型社会偏见的资源缺乏受影响社区的参与，HESEIA旨在填补这一空白。

Method: 通过专业发展课程，370名高中教师和5,370名学生合作创建了包含交叉性偏见和本地化内容的句子。

Result: HESEIA展示了比现有数据集更多的未被当前模型识别的刻板印象，且覆盖了更多人口统计维度和学科领域。

Conclusion: HESEIA为基于教育社区的偏见评估提供了支持，强调了参与式方法的重要性。

Abstract: Most resources for evaluating social biases in Large Language Models are
developed without co-design from the communities affected by these biases, and
rarely involve participatory approaches. We introduce HESEIA, a dataset of
46,499 sentences created in a professional development course. The course
involved 370 high-school teachers and 5,370 students from 189 Latin-American
schools. Unlike existing benchmarks, HESEIA captures intersectional biases
across multiple demographic axes and school subjects. It reflects local
contexts through the lived experience and pedagogical expertise of educators.
Teachers used minimal pairs to create sentences that express stereotypes
relevant to their school subjects and communities. We show the dataset
diversity in term of demographic axes represented and also in terms of the
knowledge areas included. We demonstrate that the dataset contains more
stereotypes unrecognized by current LLMs than previous datasets. HESEIA is
available to support bias assessments grounded in educational communities.

</details>


### [335] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: 提出了一种结合欧氏距离和大语言模型（HED-LM）的少样本优化方法，用于提升传感器分类任务中的示例选择质量。


<details>
  <summary>Details</summary>
Motivation: 少样本提示在有限标注数据下高效推理，但其性能依赖示例选择质量。HED-LM通过混合选择流程解决这一问题。

Method: HED-LM采用混合选择流程：先用欧氏距离筛选候选示例，再用大语言模型（LLMs）根据上下文相关性重新排序。

Result: 在疲劳检测任务中，HED-LM的宏F1分数为69.13±10.71%，优于随机选择（59.30±10.13%）和仅距离过滤（67.61±11.39%）。

Conclusion: HED-LM结合数值相似性和上下文相关性，提升了少样本提示的鲁棒性，适用于医疗监测、活动识别和工业安全等领域。

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [336] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
*Aakash Sen Sharma,Debdeep Sanyal,Priyansh Srivastava,Sundar Atreya H.,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.CL

TL;DR: FUA-LLM框架通过法律合规优化，减少大语言模型（LLMs）的版权侵权风险，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因直接复制受版权保护内容而引发的伦理、法律和实践问题。

Method: 开发FairUseDB数据集，结合Direct Preference Optimization（DPO）微调模型，提出新评估指标Weighted Penalty Utility和CAH。

Result: FUA-LLM显著减少侵权输出（达20%），同时保持实用性。

Conclusion: FUA-LLM为LLMs提供了一种法律合规且实用的解决方案。

Abstract: Large language models (LLMs) commonly risk copyright infringement by
reproducing protected content verbatim or with insufficient transformative
modifications, posing significant ethical, legal, and practical concerns.
Current inference-time safeguards predominantly rely on restrictive
refusal-based filters, often compromising the practical utility of these
models. To address this, we collaborated closely with intellectual property
experts to develop FUA-LLM (Fair Use Aligned Language Models), a
legally-grounded framework explicitly designed to align LLM outputs with
fair-use doctrine. Central to our method is FairUseDB, a carefully constructed
dataset containing 18,000 expert-validated examples covering nine realistic
infringement scenarios. Leveraging this dataset, we apply Direct Preference
Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce
legally compliant and practically useful alternatives rather than resorting to
blunt refusal. Recognizing the shortcomings of traditional evaluation metrics,
we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic
Mean (CAH) to balance infringement risk against response utility. Extensive
quantitative experiments coupled with expert evaluations confirm that FUA-LLM
substantially reduces problematic outputs (up to 20\%) compared to
state-of-the-art approaches, while preserving real-world usability.

</details>


### [337] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
*Shaojie Wang,Sirui Ding,Na Zou*

Main category: cs.CL

TL;DR: 提出了一种基于互信息（MI）的信息论框架，用于评估大型语言模型（LLM）的语言理解能力，通过句子级和词级MI的分解与计算，揭示了编码器模型的信息保真度优于解码器模型，并展示了通过优化词级可恢复性提升模型理解能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的内在语言理解能力存在挑战，传统任务难以全面衡量，因此提出基于互信息的理论框架。

Method: 将句子级MI分解为词级MI，利用Fano不等式推导词级MI的可计算下界，并通过可恢复性任务比较不同LLM的MI表现。

Result: 编码器模型在信息保真度上优于解码器模型，后者表现出后期层信息遗忘现象；优化词级可恢复性可提升模型理解能力。

Conclusion: 互信息可作为理解和改进语言模型能力的基础，优化词级MI能有效提升模型表现。

Abstract: Recent advances in large language models (LLMs) have revolutionized natural
language processing, yet evaluating their intrinsic linguistic understanding
remains challenging. Moving beyond specialized evaluation tasks, we propose an
information-theoretic framework grounded in mutual information (MI) to achieve
this. We formalize the understanding as MI between an input sentence and its
latent representation (sentence-level MI), measuring how effectively input
information is preserved in latent representation. Given that LLMs learn
embeddings for individual tokens, we decompose sentence-level MI into
token-level MI between tokens and sentence embeddings, establishing theoretical
bounds connecting these measures. Based on this foundation, we theoretically
derive a computable lower bound for token-level MI using Fano's inequality,
which directly relates to token-level recoverability-the ability to predict
original tokens from sentence embedding. We implement this recoverability task
to comparatively measure MI across different LLMs, revealing that encoder-only
models consistently maintain higher information fidelity than their
decoder-only counterparts, with the latter exhibiting a distinctive late-layer
"forgetting" pattern where mutual information is first enhanced and then
discarded. Moreover, fine-tuning to maximize token-level recoverability
consistently improves understanding ability of LLMs on tasks without
task-specific supervision, demonstrating that mutual information can serve as a
foundation for understanding and improving language model capabilities.

</details>


### [338] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
*Yuan Li,Qi Luo,Xiaonan Li,Bufan Li,Qinyuan Cheng,Bo Wang,Yining Zheng,Yuxin Wang,Zhangyue Yin,Xipeng Qiu*

Main category: cs.CL

TL;DR: R3-RAG通过强化学习让LLM学会逐步推理和检索，提升RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集检索器在RAG系统中的瓶颈问题，如参数限制和无法逐步推理。

Method: 分两阶段：冷启动学习推理与检索交替，强化学习优化检索能力。设计了两种奖励：答案正确性和文档相关性验证。

Result: R3-RAG显著优于基线方法，并能适配不同检索器。

Conclusion: R3-RAG通过强化学习有效提升了RAG系统的推理和检索能力。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large
Language Models (LLMs) to enhance factual correctness and mitigate
hallucination. However, dense retrievers often become the bottleneck of RAG
systems due to their limited parameters compared to LLMs and their inability to
perform step-by-step reasoning. While prompt-based iterative RAG attempts to
address these limitations, it is constrained by human-designed workflows. To
address these limitations, we propose $\textbf{R3-RAG}$, which uses
$\textbf{R}$einforcement learning to make the LLM learn how to
$\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving
comprehensive external knowledge and leading to correct answers. R3-RAG is
divided into two stages. We first use cold start to make the model learn the
manner of iteratively interleaving reasoning and retrieval. Then we use
reinforcement learning to further harness its ability to better explore the
external retrieval environment. Specifically, we propose two rewards for
R3-RAG: 1) answer correctness for outcome reward, which judges whether the
trajectory leads to a correct answer; 2) relevance-based document verification
for process reward, encouraging the model to retrieve documents that are
relevant to the user question, through which we can let the model learn how to
iteratively reason and retrieve relevant documents to get the correct answer.
Experimental results show that R3-RAG significantly outperforms baselines and
can transfer well to different retrievers. We release R3-RAG at
https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [339] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
*Jian Lan,Yifei Fu,Udo Schlegel,Gengyuan Zhang,Tanveer Hannan,Haokun Chen,Thomas Seidl*

Main category: cs.CL

TL;DR: 该研究评估并缓解了大型视觉语言模型（VLMs）中的社会偏见，发现模型在生成响应中存在性别和种族偏见，并提出了一种无需训练的后处理方法。


<details>
  <summary>Details</summary>
Motivation: 社会偏见是VLMs中的关键问题，可能导致对某些社会群体的不公平和伦理问题，但目前对其在生成响应中的偏见程度尚不清楚。

Method: 研究首先评估了四种最先进的VLMs在PAIRS和SocialCounterfactuals数据集上的表现，发现模型存在偏见。随后提出了一种后处理方法，通过调整隐藏层的残差来缓解偏见。

Result: 模型在生成响应中存在性别和种族偏见，且其隐藏层的公平性水平波动较大。提出的后处理方法在无需训练的情况下显著改善了模型的公平性和置信度。

Conclusion: 该研究揭示了VLMs中的社会偏见问题，并提出了一种有效的后处理方法，为未来公平性研究提供了新方向。

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [340] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
*Xiaoyuan Wu,Weiran Lin,Omer Akgul,Lujo Bauer*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLM）生成文本的一致性问题，提出了一种基于logit的集成方法，并发现现有方法在评估人类感知的一致性方面表现不佳，建议更多依赖人类评估。


<details>
  <summary>Details</summary>
Motivation: LLM易产生幻觉且对提示扰动敏感，导致生成文本不一致或不可靠。现有的一致性测量方法未能很好地反映人类感知。

Method: 提出了一种基于logit的集成方法，并通过用户研究（n=2,976）验证其效果。

Result: 新方法在评估人类感知的一致性方面表现与现有最佳方法相当，但现有方法整体表现不佳。

Conclusion: 建议在评估LLM一致性时更多依赖人类输入，而非仅依赖自动化方法。

Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to
prompt perturbations, often resulting in inconsistent or unreliable generated
text. Different methods have been proposed to mitigate such hallucinations and
fragility -- one of them being measuring the consistency (the model's
confidence in the response, or likelihood of generating a similar response when
resampled) of LLM responses. In previous work, measuring consistency often
relied on the probability of a response appearing within a pool of resampled
responses, or internal states or logits of responses. However, it is not yet
clear how well these approaches approximate how humans perceive the consistency
of LLM responses. We performed a user study (n=2,976) and found current methods
typically do not approximate users' perceptions of LLM consistency very well.
We propose a logit-based ensemble method for estimating LLM consistency, and we
show that this method matches the performance of the best-performing existing
metric in estimating human ratings of LLM consistency. Our results suggest that
methods of estimating LLM consistency without human evaluation are sufficiently
imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [341] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan*

Main category: cs.CL

TL;DR: SEMFED是一种针对异构NLP任务的语义感知资源高效联邦学习框架，通过语义感知客户端选择、自适应模型架构和通信高效特征压缩，显著降低通信成本并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在NLP任务中面临语义异构、词汇不匹配和资源限制等挑战，SEMFED旨在解决这些问题。

Method: SEMFED采用语义感知客户端选择、自适应NLP模型架构和语义特征压缩技术。

Result: 实验显示SEMFED通信成本降低80.5%，模型精度保持在98%以上，优于现有方法。

Conclusion: SEMFED能有效管理异构客户端环境，适用于实际联邦NLP部署。

Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for
training machine learning models while preserving data privacy. However,
applying FL to Natural Language Processing (NLP) tasks presents unique
challenges due to semantic heterogeneity across clients, vocabulary mismatches,
and varying resource constraints on edge devices. Objectives: This paper
introduces SEMFED, a novel semantic-aware resource-efficient federated learning
framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED
incorporates three key innovations: (1) a semantic-aware client selection
mechanism that balances semantic diversity with resource constraints, (2)
adaptive NLP-specific model architectures tailored to device capabilities while
preserving semantic information, and (3) a communication-efficient semantic
feature compression technique that significantly reduces bandwidth
requirements. Results: Experimental results on various NLP classification tasks
demonstrate that SEMFED achieves an 80.5% reduction in communication costs
while maintaining model accuracy above 98%, outperforming state-of-the-art FL
approaches. Conclusion: SEMFED effectively manages heterogeneous client
environments with varying computational resources, network reliability, and
semantic data distributions, making it particularly suitable for real-world
federated NLP deployments.

</details>


### [342] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
*Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Michael Wornow,Juan M. Banda,Nikesh Kotecha,Timothy Keyes,Yifan Mai,Mert Oez,Hao Qiu,Shrey Jain,Leonardo Schettini,Mehr Kashyap,Jason Alan Fries,Akshay Swaminathan,Philip Chung,Fateme Nateghi,Asad Aali,Ashwin Nayak,Shivam Vedak,Sneha S. Jain,Birju Patel,Oluseyi Fayanju,Shreya Shah,Ethan Goh,Dong-han Yao,Brian Soetikno,Eduardo Reis,Sergios Gatidis,Vasu Divi,Robson Capasso,Rachna Saralkar,Chia-Chun Chiang,Jenelle Jindal,Tho Pham,Faraz Ghoddusi,Steven Lin,Albert S. Chiou,Christy Hong,Mohana Roy,Michael F. Gensheimer,Hinesh Patel,Kevin Schulman,Dev Dash,Danton Char,Lance Downing,Francois Grolleau,Kameron Black,Bethel Mieso,Aydin Zahedivash,Wen-wai Yim,Harshita Sharma,Tony Lee,Hannah Kirsch,Jennifer Lee,Nerissa Ambers,Carlene Lugtu,Aditya Sharma,Bilal Mawji,Alex Alekseyev,Vicky Zhou,Vikas Kakkar,Jarrod Helzer,Anurang Revri,Yair Bannett,Roxana Daneshjou,Jonathan Chen,Emily Alsentzer,Keith Morse,Nirmal Ravi,Nima Aghaeepour,Vanessa Kennedy,Akshay Chaudhari,Thomas Wang,Sanmi Koyejo,Matthew P. Lungren,Eric Horvitz,Percy Liang,Mike Pfeffer,Nigam H. Shah*

Main category: cs.CL

TL;DR: MedHELM是一个评估大语言模型（LLM）在医疗任务中表现的框架，包含分类法、基准套件和改进的评估方法。研究发现不同模型表现差异显著，推理能力强的模型表现更优，但成本效益也需考虑。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法未能充分反映真实临床实践的复杂性和多样性，因此需要更全面的评估框架。

Method: 开发了由临床医生验证的分类法（5类、22子类、121任务），构建了35个基准测试，并采用LLM-jury评估方法。

Result: 不同模型表现差异显著，推理能力强的模型（如DeepSeek R1）表现更优，但Claude 3.5 Sonnet在成本效益上更佳。LLM-jury评估方法与临床医生评分一致性较高。

Conclusion: MedHELM为医疗领域LLM的评估提供了开源框架，强调了任务特异性评估的重要性。

Abstract: While large language models (LLMs) achieve near-perfect scores on medical
licensing exams, these evaluations inadequately reflect the complexity and
diversity of real-world clinical practice. We introduce MedHELM, an extensible
evaluation framework for assessing LLM performance for medical tasks with three
key contributions. First, a clinician-validated taxonomy spanning 5 categories,
22 subcategories, and 121 tasks developed with 29 clinicians. Second, a
comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly
formulated) providing complete coverage of all categories and subcategories in
the taxonomy. Third, a systematic comparison of LLMs with improved evaluation
methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9
frontier LLMs, using the 35 benchmarks, revealed significant performance
variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%
win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved
comparable results at 40% lower estimated computational cost. On a normalized
accuracy scale (0-1), most models performed strongly in Clinical Note
Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83),
moderately in Medical Research Assistance (0.65-0.75), and generally lower in
Clinical Decision Support (0.56-0.72) and Administration & Workflow
(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with
clinician ratings (ICC = 0.47), surpassing both average clinician-clinician
agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and
BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top
models at lower estimated cost. These findings highlight the importance of
real-world, task-specific evaluation for medical use of LLMs and provides an
open source framework to enable this.

</details>


### [343] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
*Terrance Liu,Shuyi Wang,Daniel Preotiuc-Pietro,Yash Chandarana,Chirag Gupta*

Main category: cs.CL

TL;DR: 本文研究了为LLM生成的文本到SQL查询提供校准置信度分数的方法，提出了结合子句频率（SCF）和多变量Platt缩放（MPS）的新方法，显著提升了校准效果和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在文本到SQL解析中表现优异，但其有时会自信地给出错误答案。因此，需要可靠的置信度分数来评估查询的正确性。

Method: 提出了一种基于SQL查询结构的新方法，利用子句频率（SCF）和多变量Platt缩放（MPS）生成校准的置信度分数。

Result: 在两个流行的文本到SQL数据集上的实验表明，该方法比传统Platt缩放进一步提升了校准效果和错误检测能力。

Conclusion: 结合SCF和MPS的方法为文本到SQL解析提供了更准确和校准的置信度分数，增强了系统的可信度。

Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL
parsing, they sometimes exhibit unexpected failures in which they are
confidently incorrect. Building trustworthy text-to-SQL systems thus requires
eliciting reliable uncertainty measures from the LLM. In this paper, we study
the problem of providing a calibrated confidence score that conveys the
likelihood of an output query being correct. Our work is the first to establish
a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In
particular, we show that Platt scaling, a canonical method for calibration,
provides substantial improvements over directly using raw model output
probabilities as confidence scores. Furthermore, we propose a method for
text-to-SQL calibration that leverages the structured nature of SQL queries to
provide more granular signals of correctness, named "sub-clause frequency"
(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the
canonical Platt scaling technique, we combine individual SCF scores into an
overall accurate and calibrated score. Empirical evaluation on two popular
text-to-SQL datasets shows that our approach of combining MPS and SCF yields
further improvements in calibration and the related task of error detection
over traditional Platt scaling.

</details>


### [344] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
*Sihyeon Lee,Hyunjoo Song,Jong-chan Lee,Yoon Jin Lee,Boram Lee,Hee-Eon Lim,Dongyeong Kim,Jinwook Seo,Bohyoung Kim*

Main category: cs.CL

TL;DR: MedOrchestra是一种混合框架，结合云端和本地LLMs，在保护隐私的同时提升复杂临床任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决云端LLMs隐私风险和本地LLMs性能不足的权衡问题。

Method: 云端LLM分解任务并生成提示，本地LLM执行隐私保护的任务。

Result: 在胰腺癌分期任务中，MedOrchestra在自由文本和结构化报告中均优于基线模型和临床医生。

Conclusion: MedOrchestra在隐私保护和性能之间取得了平衡，适用于临床环境。

Abstract: Deploying large language models (LLMs) in clinical settings faces critical
trade-offs: cloud LLMs, with their extensive parameters and superior
performance, pose risks to sensitive clinical data privacy, while local LLMs
preserve privacy but often fail at complex clinical interpretation tasks. We
propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex
clinical tasks into manageable subtasks and prompt generation, while a local
LLM executes these subtasks in a privacy-preserving manner. Without accessing
clinical data, the cloud LLM generates and validates subtask prompts using
clinical guidelines and synthetic test cases. The local LLM executes subtasks
locally and synthesizes outputs generated by the cloud LLM. We evaluate
MedOrchestra on pancreatic cancer staging using 100 radiology reports under
NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,
outperforming local model baselines (without guideline: 48.94%, with guideline:
56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:
65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches
85.42% accuracy, showing clear superiority across all settings.

</details>


### [345] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
*Yuli Chen,Bo Cheng,Jiale Han,Yingying Zhang,Yingting Li,Shuhao Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种动态分层剪枝方法（DLP），通过结合模型权重和输入激活信息自适应确定每层重要性，显著提升了高稀疏度下LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 主流剪枝技术依赖均匀分层策略，导致高稀疏度下性能下降，而现有非均匀分层方法依赖预定义值，性能次优。

Method: 提出DLP方法，动态确定每层重要性并分配剪枝率，结合模型权重和输入激活信息。

Result: 在70%稀疏度下，DLP将LLaMA2-7B的困惑度降低7.79，平均准确率提升2.7%。

Conclusion: DLP在高稀疏度下有效保留性能，兼容现有压缩技术，并支持PEFT。

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [346] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
*Lin Mu,Xiaoyu Wang,Li Ni,Yang Li,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: DenseLoRA是一种改进的低秩适应方法，通过使用密集低秩矩阵替代冗余的低秩矩阵，提高了参数利用率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法中的权重矩阵存在冗余，导致参数利用率低，DenseLoRA旨在解决这一问题。

Method: DenseLoRA引入Encoder-Decoder结构，对隐藏表示进行细化和压缩，再通过密集低秩矩阵进行适应。

Result: DenseLoRA在LLaMA3-8B上仅用0.01%可训练参数达到83.8%准确率，优于LoRA的80.8%（0.70%参数）。

Conclusion: DenseLoRA显著提升了参数效率和性能，为LLM适应提供了更优解决方案。

Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for
adapting large language models (LLMs) by fine-tuning two low-rank matrices,
thereby reducing the number of trainable parameters. However, prior research
indicates that many of the weights in these matrices are redundant, leading to
inefficiencies in parameter utilization. To address this limitation, we
introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances
parameter efficiency while achieving superior performance compared to LoRA.
DenseLoRA builds upon the concept of representation fine-tuning, incorporating
a single Encoder-Decoder to refine and compress hidden representations across
all adaptation layers before applying adaptation. Instead of relying on two
redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense
low-rank matrix, improving parameter utilization and adaptation efficiency. We
evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%
accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%
accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we
conduct extensive experiments to systematically assess the impact of
DenseLoRA's components on overall model performance. Code is available at
https://github.com/mulin-ahu/DenseLoRA.

</details>


### [347] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
*Haowei Yang,Haotian Lyu,Tianle Zhang,Dingzhou Wang,Yushang Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合LLM的框架，通过提示工程、多目标微调和后处理生成兼具吸引力和转化效果的营销文案，显著提升了CTR和CVR。


<details>
  <summary>Details</summary>
Motivation: 随着电商竞争加剧，平衡创意内容与转化效果成为关键挑战。

Method: 采用提示工程、多目标微调（包括情感调整、多样性增强和CTA嵌入）和后处理技术。

Result: 离线评估和在线A/B测试显示，CTR提升12.5%，CVR提升8.3%，同时保持内容新颖性。

Conclusion: 该框架为自动化文案生成提供了实用方案，并为未来多模态、实时个性化研究指明了方向。

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [348] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
*Chenghao Yang,Yinbo Luo,Zhoufutu Wen,Qi Chu,Tao Gong,Longxiang Liu,Kaiyuan Zhang,Jianpeng Jiao,Ge Zhang,Wenhao Huang,Nenghai Yu*

Main category: cs.CL

TL;DR: MARS-Bench是一个多轮现实场景对话基准，旨在评估LLMs在长复杂对话中的表现，发现闭源LLMs优于开源模型，显式推理提升鲁棒性，但LLMs在动机转移和跨轮依赖方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面反映LLMs在长复杂对话中的弱点，如动机转移和跨轮依赖。

Method: 基于实时文本评论构建MARS-Bench，评估多轮对话的三大关键方面：超多轮、交互式多轮和跨轮任务。

Result: 闭源LLMs表现优于开源模型，显式推理提升鲁棒性，但LLMs在动机转移和跨轮依赖方面表现不佳。

Conclusion: MARS-Bench填补了评估LLMs在长复杂对话中表现的空白，揭示了其挑战和改进方向。

Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted
in real-world dialogue applications. However, LLMs' robustness, especially in
handling long complex dialogue sessions, including frequent motivation
transfer, sophisticated cross-turn dependency, is criticized all along.
Nevertheless, no existing benchmarks can fully reflect these weaknesses. We
present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic
\textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to
remedy the gap. MARS-Bench is constructed from play-by-play text commentary so
to feature realistic dialogues specifically designed to evaluate three critical
aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,
and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that
closed-source LLMs significantly outperform open-source alternatives, explicit
reasoning significantly boosts LLMs' robustness on handling long complex
dialogue sessions, and LLMs indeed face significant challenges when handling
motivation transfer and sophisticated cross-turn dependency. Moreover, we
provide mechanistic interpretability on how attention sinks due to special
tokens lead to LLMs' performance degradation when handling long complex
dialogue sessions based on attention visualization experiment in
Qwen2.5-7B-Instruction.

</details>


### [349] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
*Hadi Askari,Shivanshu Gupta,Fei Wang,Anshuman Chhabra,Muhao Chen*

Main category: cs.CL

TL;DR: 论文提出LayerIF框架，通过影响函数量化LLM各层训练质量，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视数据影响，导致LLM各层训练质量评估不准确，限制了性能。

Method: 利用影响函数计算层间梯度，通过验证损失敏感性量化层重要性。

Result: 实验表明，LayerIF在LoRA-MoE架构和LLM剪枝中显著提升任务性能。

Conclusion: LayerIF为任务敏感、数据驱动的层重要性评估提供了有效方法。

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance.It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [350] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
*Lata Pangtey,Mohammad Zia Ur Rehman,Prasad Chaudhari,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CL

TL;DR: SPLAENet是一种新颖的立场预测方法，通过双交叉注意力和情感感知神经网络，显著提升了社交媒体中立场检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中用户生成的内容隐含大量观点，可能导致错误信息的传播，因此需要有效的立场检测方法来分析偏见和对抗错误信息。

Method: SPLAENet采用双交叉注意力机制和分层注意力网络，结合情感分析和标签融合技术，以捕捉文本间的相互关系并区分不同立场。

Result: 在多个数据集上，SPLAENet在准确率和F1分数上均显著优于现有方法，最高提升17.36%的F1分数。

Conclusion: SPLAENet在立场检测任务中表现出色，为社交媒体中错误信息的内容分析提供了有效工具。

Abstract: The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.

</details>


### [351] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
*Joe Needham,Giles Edkins,Govind Pimpale,Henning Bartsch,Marius Hobbhahn*

Main category: cs.CL

TL;DR: 前沿语言模型能够区分评估和实际部署的文本，表现出评估意识，但尚未超越人类基线。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型是否能检测到评估场景，以避免评估结果被模型行为差异影响。

Method: 构建包含1000个提示和转录的多样化基准，测试模型对评估和实际部署文本的分类能力。

Result: 模型表现出高于随机的评估意识（如Gemini-2.5-Pro AUC为0.83），但低于人类基线（AUC为0.92）。

Conclusion: 前沿模型已具备显著的评估意识，建议未来追踪此能力。

Abstract: If AI models can detect when they are being evaluated, the effectiveness of
evaluations might be compromised. For example, models could have systematically
different behavior during evaluations, leading to less reliable benchmarks for
deployment and governance decisions. We investigate whether frontier language
models can accurately classify transcripts based on whether they originate from
evaluations or real-world deployment, a capability we call evaluation
awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts
and transcripts from 61 distinct datasets. These span public benchmarks (e.g.,
MMLU, SWEBench), real-world deployment interactions, and agent trajectories
from scaffolding frameworks (e.g., web-browsing agents). Frontier models
clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches
an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of
$0.92$). Furthermore, both AI models and humans are better at identifying
evaluations in agentic settings compared to chat settings. Additionally, we
test whether models can identify the purpose of the evaluation. Under
multiple-choice and open-ended questioning, AI models far outperform random
chance in identifying what an evaluation is testing for. Our results indicate
that frontier models already exhibit a substantial, though not yet superhuman,
level of evaluation-awareness. We recommend tracking this capability in future
models.

</details>


### [352] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
*Michael Shalyt,Rotem Elimelech,Ido Kaminer*

Main category: cs.CL

TL;DR: ASyMOB是一个专注于符号数学评估的新框架，揭示了大型语言模型（LLMs）在符号数学任务中的表现和局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估LLMs在符号数学（如积分、微分方程和代数简化）中的核心能力，因此需要新的评估工具。

Method: 引入ASyMOB框架，包含17,092个数学挑战，通过数值或符号扰动分析LLMs的泛化能力。

Result: LLMs在扰动问题中表现显著下降（最高-70.3%），表明依赖记忆而非深层理解。结合代码执行可提升准确性（最高+33.1%）。

Conclusion: 前沿LLMs（如o4-mini、Gemini 2.5 Flash）表现出高符号数学能力和抗扰动性，未来发展方向可能是更深入的工具集成或模型能力提升。

Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency
in university-level symbolic mathematics required for applications in advanced
science and technology. However, existing benchmarks fall short in assessing
the core skills of LLMs in symbolic mathematics-such as integration,
differential equations, and algebraic simplification. To address this gap, we
introduce ASyMOB, a novel assessment framework focused exclusively on symbolic
manipulation, featuring 17,092 unique math challenges, organized by similarity
and complexity. ASyMOB enables analysis of LLM generalization capabilities by
comparing performance in problems that differ by simple numerical or symbolic
`perturbations'. Evaluated LLMs exhibit substantial degradation in performance
for all perturbation types (up to -70.3%), suggesting reliance on memorized
patterns rather than deeper understanding of symbolic math, even among models
achieving high baseline accuracy. Comparing LLM performance to computer algebra
systems, we identify examples where they fail while LLMs succeed, as well as
problems solved only by combining both approaches. Models capable of integrated
code execution yielded higher accuracy compared to their performance without
code, particularly stabilizing weaker models (up to +33.1% for certain
perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5
Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and
97.6% on the unperturbed set), but also remarkable robustness against
perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).
This may indicate a recent "phase transition" in the generalization
capabilities of frontier LLMs. It remains to be seen whether the path forward
lies in deeper integration with sophisticated external tools, or in developing
models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [353] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
*Linwei Tao,Yi-Fan Yeh,Minjing Dong,Tao Huang,Philip Torr,Chang Xu*

Main category: cs.CL

TL;DR: 本文对大型语言模型（LLM）的不确定性估计进行了全面研究，评估了80种模型，发现语言不确定性（LVU）表现最佳，并揭示了模型规模、训练后处理等因素对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在高风险应用中的部署增加，可靠的不确定性估计对确保其安全可信至关重要。

Method: 评估了80种模型，包括开源和闭源、密集和MoE架构等，聚焦于三种黑盒单次方法（TPU、NVU、LVU），使用MMLU-Pro基准进行系统评估。

Result: LVU在不确定性和选择性分类中表现最佳，且高准确性不一定意味着可靠的不确定性。模型规模、推理能力等因素显著影响性能。

Conclusion: LVU是提升LLM可靠性的实用工具，需多角度评估不确定性估计。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
applications, robust uncertainty estimation is essential for ensuring the safe
and trustworthy deployment of LLMs. We present the most comprehensive study to
date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and
closed-source families, dense and Mixture-of-Experts (MoE) architectures,
reasoning and non-reasoning modes, quantization variants and parameter scales
from 0.6B to 671B. Focusing on three representative black-box single-pass
methods, including token probability-based uncertainty (TPU), numerical verbal
uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically
evaluate uncertainty calibration and selective classification using the
challenging MMLU-Pro benchmark, which covers both reasoning-intensive and
knowledge-based tasks. Our results show that LVU consistently outperforms TPU
and NVU, offering stronger calibration and discrimination while being more
interpretable. We also find that high accuracy does not imply reliable
uncertainty, and that model scale, post-training, reasoning ability and
quantization all influence estimation performance. Notably, LLMs exhibit better
uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good
calibration does not necessarily translate to effective error ranking. These
findings highlight the need for multi-perspective evaluation and position LVU
as a practical tool for improving the reliability of LLMs in real-world
settings.

</details>


### [354] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
*Sahil Verma,Keegan Hines,Jeff Bilmes,Charlotte Siska,Luke Zettlemoyer,Hila Gonen,Chandan Singh*

Main category: cs.CL

TL;DR: OMNIGUARD是一种检测多语言和多模态有害提示的方法，通过识别LLM内部表示并构建语言和模态无关的分类器，显著提升了检测准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的潜在有害使用引发担忧，现有检测方法存在漏洞，尤其是在低资源语言或非文本模态下。

Method: OMNIGUARD识别LLM内部跨语言或模态对齐的表示，并利用它们构建语言和模态无关的分类器。

Result: 在多语言、图像和音频提示的检测中，OMNIGUARD分别提升了11.57%、20.44%的准确率，并显著提高了效率（约120倍）。

Conclusion: OMNIGUARD为检测有害提示提供了高效且跨语言/模态的解决方案，显著优于现有基线。

Abstract: The emerging capabilities of large language models (LLMs) have sparked
concerns about their immediate potential for harmful misuse. The core approach
to mitigate these concerns is the detection of harmful queries to the model.
Current detection approaches are fallible, and are particularly susceptible to
attacks that exploit mismatched generalization of model capabilities (e.g.,
prompts in low-resource languages or prompts provided in non-text modalities
such as image and audio). To tackle this challenge, we propose OMNIGUARD, an
approach for detecting harmful prompts across languages and modalities. Our
approach (i) identifies internal representations of an LLM/MLLM that are
aligned across languages or modalities and then (ii) uses them to build a
language-agnostic or modality-agnostic classifier for detecting harmful
prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\%
over the strongest baseline in a multilingual setting, by 20.44\% for
image-based prompts, and sets a new SOTA for audio-based prompts. By
repurposing embeddings computed during generation, OMNIGUARD is also very
efficient ($\approx 120 \times$ faster than the next fastest baseline). Code
and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [355] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
*Zeyu Liu,Zhitian Hou,Yining Di,Kejing Yang,Zhijie Sang,Congkai Xie,Jingwen Yang,Siyuan Liu,Jialu Wang,Chunming Li,Ming Li,Hongxia Yang*

Main category: cs.CL

TL;DR: Infi-Med是一个针对医疗多模态大语言模型（MLLMs）的综合框架，通过资源高效的方法、增强的多模态推理能力和系统评估体系，解决了医疗任务中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗MLLMs在实际部署中面临资源效率、诊断准确性、临床考虑和伦理隐私等挑战，需要一种更高效的解决方案。

Method: Infi-Med提出三个创新：1）资源高效的SFT数据集构建；2）增强的多模态推理能力；3）系统评估体系。

Result: 实验表明，Infi-Med在通用医疗推理中达到SOTA性能，并快速适应临床场景。

Conclusion: Infi-Med为医疗MLLMs的实际部署提供了平衡模型效果与操作限制的坚实基础。

Abstract: Multimodal large language models (MLLMs) have demonstrated promising
prospects in healthcare, particularly for addressing complex medical tasks,
supporting multidisciplinary treatment (MDT), and enabling personalized
precision medicine. However, their practical deployment faces critical
challenges in resource efficiency, diagnostic accuracy, clinical
considerations, and ethical privacy. To address these limitations, we propose
Infi-Med, a comprehensive framework for medical MLLMs that introduces three key
innovations: (1) a resource-efficient approach through curating and
constructing high-quality supervised fine-tuning (SFT) datasets with minimal
sample requirements, with a forward-looking design that extends to both
pretraining and posttraining phases; (2) enhanced multimodal reasoning
capabilities for cross-modal integration and clinical task understanding; and
(3) a systematic evaluation system that assesses model performance across
medical modalities and task types. Our experiments demonstrate that Infi-Med
achieves state-of-the-art (SOTA) performance in general medical reasoning while
maintaining rapid adaptability to clinical scenarios. The framework establishes
a solid foundation for deploying MLLMs in real-world healthcare settings by
balancing model effectiveness with operational constraints.

</details>


### [356] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
*Caiqi Zhang,Xiaochen Zhu,Chengzu Li,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文提出LoVeC方法，通过强化学习训练LLMs为长文本生成附加数值置信度分数，提升事实性检测效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长文本生成中的幻觉问题，现有方法效率低或局限于短文本任务。

Method: 使用强化学习（RL）训练LLMs为每个生成语句附加置信度分数，包括DPO、ORPO和GRPO等方法。

Result: 在三个长文本QA数据集上，RL训练模型表现出更好的校准性和跨领域泛化能力。

Conclusion: LoVeC方法高效且直接，仅需少量额外标记即可实现置信度估计。

Abstract: Hallucination remains a major challenge for the safe and trustworthy
deployment of large language models (LLMs) in factual content generation. Prior
work has explored confidence estimation as an effective approach to
hallucination detection, but often relies on post-hoc self-consistency methods
that require computationally expensive sampling. Verbalized confidence offers a
more efficient alternative, but existing approaches are largely limited to
short-form question answering (QA) tasks and do not generalize well to
open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized
Confidence), an on-the-fly verbalized confidence estimation method for
long-form generation. Specifically, we use reinforcement learning (RL) to train
LLMs to append numerical confidence scores to each generated statement, serving
as a direct and interpretable signal of the factuality of generation. Our
experiments consider both on-policy and off-policy RL methods, including DPO,
ORPO, and GRPO, to enhance the model calibration. We introduce two novel
evaluation settings, free-form tagging and iterative tagging, to assess
different verbalized confidence estimation methods. Experiments on three
long-form QA datasets show that our RL-trained models achieve better
calibration and generalize robustly across domains. Also, our method is highly
efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [357] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
*Yuxin Wang,Botao Yu,Ivory Yang,Saeed Hassanpour,Soroush Vosoughi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在内容审核中存在主题偏见，导致误判良性评论为有毒内容，并提出了一种语义层面的分析方法来量化这种偏见。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在内容审核中因主题偏见导致的过度敏感问题，揭示其背后的机制。

Method: 引入主题关联分析（Topic Association Analysis），通过生成自由场景想象并分析主题放大水平来量化LLM的主题偏见。

Result: 发现更先进的模型（如GPT-4 Turbo）尽管总体误报率较低，但表现出更强的主题刻板印象。

Conclusion: LLM的审核决策不仅基于显性攻击性语言，还依赖于学习的主题关联，需改进基于关键词的过滤方法。

Abstract: Large Language Models are widely used for content moderation but often
misclassify benign comments as toxic, leading to over-sensitivity. While
previous research attributes this issue primarily to the presence of offensive
terms, we reveal a potential cause beyond token level: LLMs exhibit systematic
topic biases in their implicit associations. Inspired by cognitive psychology's
implicit association tests, we introduce Topic Association Analysis, a
semantic-level approach to quantify how LLMs associate certain topics with
toxicity. By prompting LLMs to generate free-form scenario imagination for
misclassified benign comments and analyzing their topic amplification levels,
we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger
topic stereotype despite lower overall false positive rates. These biases
suggest that LLMs do not merely react to explicit, offensive language but rely
on learned topic associations, shaping their moderation decisions. Our findings
highlight the need for refinement beyond keyword-based filtering, providing
insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [358] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
*Feiteng Fang,Ting-En Lin,Yuchuan Wu,Xiong Liu,Xiang Huang,Dingwei Chen,Jing Ye,Haonan Zhang,Liang Zhu,Hamid Alinejad-Rokny,Min Yang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 论文提出ChARM模型，通过自适应边界和自进化机制改进角色扮演语言代理的奖励模型，并引入新数据集和评估基准，实验显示性能提升13%。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型在可扩展性和适应主观对话偏好方面存在不足，需改进以提升角色扮演语言代理的交互效果。

Method: 提出ChARM模型，包含自适应边界和自进化机制，并引入RoleplayPref数据集和RoleplayEval评估基准。

Result: 实验结果显示性能提升13%，并在CharacterEval和RoleplayEval上达到最优效果。

Conclusion: ChARM模型通过创新机制和新数据集，显著提升了角色扮演语言代理的性能和适应性。

Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic
and engaging human-computer interactions. However, traditional reward models
often struggle with scalability and adapting to subjective conversational
preferences. We propose ChARM, a Character-based Act-adaptive Reward Model,
addressing these challenges through two innovations: (1) an act-adaptive margin
that significantly enhances learning efficiency and generalizability, and (2) a
self-evolution mechanism leveraging large-scale unlabeled data to improve
training coverage. Additionally, we introduce RoleplayPref, the first
large-scale preference dataset specifically for RPLAs, featuring 1,108
characters, 13 subcategories, and 16,888 bilingual dialogues, alongside
RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%
improvement over the conventional Bradley-Terry model in preference rankings.
Furthermore, applying ChARM-generated rewards to preference learning techniques
(e.g., direct preference optimization) achieves state-of-the-art results on
CharacterEval and RoleplayEval. Code and dataset are available at
https://github.com/calubkk/ChARM.

</details>


### [359] [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
*Daniel Wurgaft,Ben Prystawski,Kanishk Gandhi,Cedegao E. Zhang,Joshua B. Tenenbaum,Noah D. Goodman*

Main category: cs.CL

TL;DR: 论文提出了一种自动化转录和注释口头报告的方法，利用自然语言处理工具，实现了大规模分析“大声思考”数据。


<details>
  <summary>Details</summary>
Motivation: “大声思考”方法能提供丰富的人类推理过程数据，但因转录和注释工作量大而难以大规模应用。

Method: 使用自然语言处理工具自动转录和注释640名参与者在数学推理任务中的口头报告，并将其编码为搜索图。

Result: 研究发现自动方法与人工评分者之间具有中等一致性，并揭示了人类推理的一致性和变异性。

Conclusion: 研究证明了大规模“大声思考”数据的价值，并为口头报告的自动化分析提供了概念验证。

Abstract: The think-aloud method, where participants voice their thoughts as they solve
a task, is a valuable source of rich data about human reasoning processes. Yet,
it has declined in popularity in contemporary cognitive science, largely
because labor-intensive transcription and annotation preclude large sample
sizes. Here, we develop methods to automate the transcription and annotation of
verbal reports of reasoning using natural language processing tools, allowing
for large-scale analysis of think-aloud data. In our study, 640 participants
thought aloud while playing the Game of 24, a mathematical reasoning task. We
automatically transcribed the recordings and coded the transcripts as search
graphs, finding moderate inter-rater reliability with humans. We analyze these
graphs and characterize consistency and variation in human reasoning traces.
Our work demonstrates the value of think-aloud data at scale and serves as a
proof of concept for the automated analysis of verbal reports.

</details>


### [360] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
*Sriram Balasubramanian,Samyadeep Basu,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文研究了大型视觉语言模型（LVLM）中思维链（CoT）推理的忠实性，揭示了文本和图像偏见对推理的影响，并提出了一种新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT推理是否真实反映模型内部过程，尤其是视觉语言模型中未被充分研究的图像偏见。

Method: 引入细粒度评估流程，分析偏见表达模式，比较文本和图像偏见的影响。

Result: 发现图像偏见较少被表达，且模型存在“不一致”推理现象；语言模型在隐含线索表达上仍有困难。

Conclusion: 研究揭示了CoT推理的局限性，为未来改进提供了方向。

Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language
models, but questions remain about whether these reasoning traces faithfully
reflect the internal processes of the model. We present the first comprehensive
study of CoT faithfulness in large vision-language models (LVLMs),
investigating how both text-based and previously unexplored image-based biases
affect reasoning and bias articulation. Our work introduces a novel,
fine-grained evaluation pipeline for categorizing bias articulation patterns,
enabling significantly more precise analysis of CoT reasoning than previous
methods. This framework reveals critical distinctions in how models process and
respond to different types of biases, providing new insights into LVLM CoT
faithfulness. Our findings reveal that subtle image-based biases are rarely
articulated compared to explicit text-based ones, even in models specialized
for reasoning. Additionally, many models exhibit a previously unidentified
phenomenon we term ``inconsistent'' reasoning - correctly reasoning before
abruptly changing answers, serving as a potential canary for detecting biased
reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to
revisit CoT faithfulness in LLMs across various levels of implicit cues. Our
findings reveal that current language-only reasoning models continue to
struggle with articulating cues that are not overtly stated.

</details>


### [361] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
*Yinong Oliver Wang,Nivedha Sivakumar,Falaah Arif Khan,Rin Metcalf Susa,Adam Golinski,Natalie Mackraz,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 论文提出了一种新的不确定性感知公平性度量UCerF，并引入了一个新的性别-职业公平性评估数据集，用于更细致地评估大型语言模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 传统公平性度量仅关注离散的准确性评估，无法捕捉模型不确定性的隐性影响，因此需要更全面的评估方法。

Method: 提出UCerF度量，结合新的性别-职业数据集，对十种开源大型语言模型进行公平性评估。

Result: Mistral-7B等模型在UCerF下表现出公平性问题，而传统度量未能发现。

Conclusion: UCerF和新的数据集为开发更透明和负责任的人工智能系统提供了基础。

Abstract: The recent rapid adoption of large language models (LLMs) highlights the
critical need for benchmarking their fairness. Conventional fairness metrics,
which focus on discrete accuracy-based evaluations (i.e., prediction
correctness), fail to capture the implicit impact of model uncertainty (e.g.,
higher model confidence about one group over another despite similar accuracy).
To address this limitation, we propose an uncertainty-aware fairness metric,
UCerF, to enable a fine-grained evaluation of model fairness that is more
reflective of the internal bias in model decisions compared to conventional
fairness measures. Furthermore, observing data size, diversity, and clarity
issues in current datasets, we introduce a new gender-occupation fairness
evaluation dataset with 31,756 samples for co-reference resolution, offering a
more diverse and suitable dataset for evaluating modern LLMs. We establish a
benchmark, using our metric and dataset, and apply it to evaluate the behavior
of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness
due to high confidence in incorrect predictions, a detail overlooked by
Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark,
which evaluates fairness with uncertainty awareness, paves the way for
developing more transparent and accountable AI systems.

</details>


### [362] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
*Hidetaka Kamigaito,Ying Zhang,Jingun Kwon,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Main category: cs.CL

TL;DR: 论文研究了Transformer内部机制与参数规模扩展的关系，通过偏置-多样性分解分析层的行为，发现多样性对性能提升至关重要，尤其是在层输出远离真实值时。增加层数仅在层行为多样时提升性能，且边际收益递减。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer内部机制与参数规模扩展的关系，以理解性能提升的根源。

Method: 采用偏置-多样性分解理论分析Transformer层的行为，并通过信息论多样性验证。

Result: 多样性是关键，增加层数仅在层行为多样时提升性能，边际收益递减。

Conclusion: 研究揭示了Transformer性能提升的机制，为参数规模扩展提供了理论支持。

Abstract: Transformers deliver outstanding performance across a wide range of tasks and
are now a dominant backbone architecture for large language models (LLMs).
Their task-solving performance is improved by increasing parameter size, as
shown in the recent studies on parameter scaling laws. Although recent
mechanistic-interpretability studies have deepened our understanding of the
internal behavior of Transformers by analyzing their residual stream, the
relationship between these internal mechanisms and the parameter scaling laws
remains unclear. To bridge this gap, we focus on layers and their size, which
mainly decide the parameter size of Transformers. For this purpose, we first
theoretically investigate the layers within the residual stream through a
bias-diversity decomposition. The decomposition separates (i) bias, the error
of each layer's output from the ground truth, and (ii) diversity, which
indicates how much the outputs of each layer differ from each other. Analyzing
Transformers under this theory reveals that performance improves when
individual layers make predictions close to the correct answer and remain
mutually diverse. We show that diversity becomes especially critical when
individual layers' outputs are far from the ground truth. Finally, we introduce
an information-theoretic diversity and show our main findings that adding
layers enhances performance only when those layers behave differently, i.e.,
are diverse. We also reveal the performance gains from increasing the number of
layers exhibit submodularity: marginal improvements diminish as additional
layers increase, mirroring the logarithmic convergence predicted by the
parameter scaling laws. Experiments on multiple semantic-understanding tasks
with various LLMs empirically confirm the theoretical properties derived in
this study.

</details>


### [363] [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
*Alexandre Bonlarron,Florian Régin,Elisabetta De Maria,Jean-Charles Régin*

Main category: cs.CL

TL;DR: GenCP改进版通过结合MLM和CP，提升了文本生成的约束满足能力，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成文本时难以满足外部约束，GenCP旨在解决这一问题。

Method: 结合MLM和CP，将文本生成建模为CSP，利用双向约束传播。

Result: 在COLLIE基准测试中表现显著提升，生成更可靠且符合约束的文本。

Conclusion: 尽管增加了MLM调用和回溯，但整体效率更高，尤其在严格约束任务中表现更优。

Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to
enforce external constraints because they generate tokens sequentially without
explicit control mechanisms. GenCP addresses this limitation by combining LLM
predictions with Constraint Programming (CP) reasoning, formulating text
generation as a Constraint Satisfaction Problem (CSP). In this paper, we
improve GenCP by integrating Masked Language Models (MLMs) for domain
generation, which allows bidirectional constraint propagation that leverages
both past and future tokens. This integration bridges the gap between
token-level prediction and structured constraint enforcement, leading to more
reliable and constraint-aware text generation. Our evaluation on COLLIE
benchmarks demonstrates that incorporating domain preview via MLM calls
significantly improves GenCP's performance. Although this approach incurs
additional MLM calls and, in some cases, increased backtracking, the overall
effect is a more efficient use of LLM inferences and an enhanced ability to
generate feasible and meaningful solutions, particularly in tasks with strict
content constraints.

</details>


### [364] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
*Yuexing Hao,Kumail Alhamoud,Hyewon Jeong,Haoran Zhang,Isha Puri,Philip Torr,Mike Schaekermann,Ariel D. Stern,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 论文研究了LLMs在医学QA任务中与医生学员在信息相关性评估上的差异，并发现过滤无关内容能提升双方的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医学QA任务中表现优异，但其逻辑可能与医生不一致，需评估信息相关性。

Method: 通过MedPAIR数据集（1,300 QA对）比较医生学员和LLMs的相关性标注，并分析其对任务表现的影响。

Result: LLMs与医生学员的相关性评估不一致，过滤无关内容后双方准确率均提升。

Conclusion: 信息相关性对医学QA任务至关重要，LLMs需进一步优化以匹配医生的逻辑。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on
various medical question-answering (QA) benchmarks, including standardized
medical exams. However, correct answers alone do not ensure correct logic, and
models may reach accurate conclusions through flawed processes. In this study,
we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance
Estimation and Question Answering) dataset to evaluate how physician trainees
and LLMs prioritize relevant information when answering QA questions. We obtain
annotations on 1,300 QA pairs from 36 physician trainees, labeling each
sentence within the question components for relevance. We compare these
relevance estimates to those for LLMs, and further evaluate the impact of these
"relevant" subsets on downstream task performance for both physician trainees
and LLMs. We find that LLMs are frequently not aligned with the content
relevance estimates of physician trainees. After filtering out physician
trainee-labeled irrelevant sentences, accuracy improves for both the trainees
and the LLMs. All LLM and physician trainee-labeled data are available at:
http://medpair.csail.mit.edu/.

</details>


### [365] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
*Zefan Cai,Wen Xiao,Hanshi Sun,Cheng Luo,Yikai Zhang,Ke Wan,Yucheng Li,Yeyang Zhou,Li-Wen Chang,Jiuxiang Gu,Zhen Dong,Anima Anandkumar,Abedelkadir Asi,Junjie Hu*

Main category: cs.CL

TL;DR: 论文提出了一种针对推理模型中冗余令牌的KV缓存压缩方法（R-KV），显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在自反思和链式推理中表现优异，但输出过长导致KV缓存过大，且现有压缩方法在推理任务中表现不佳。

Method: 提出R-KV方法，专门针对推理模型中的冗余令牌进行压缩，保留高性能的同时大幅减少KV缓存。

Result: R-KV仅用10%的KV缓存即可保留近100%性能，甚至在某些情况下性能超过全缓存，同时节省90%内存并提升6.6倍吞吐量。

Conclusion: R-KV在数学推理数据集上表现优于现有KV缓存压缩方法，显著提升了推理模型的效率和性能。

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [366] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
*Jiaqi Sun,Shiyou Qian,Zhangchi Han,Wei Li,Zelin Qian,Dingyu Yang,Jian Cao,Guangtao Xue*

Main category: cs.CL

TL;DR: LKD-KGC是一种无监督的领域特定知识图谱构建框架，通过分析文档库推断知识依赖关系，利用LLM驱动的优先级确定处理顺序，并自动生成实体模式，显著提升了构建效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 领域特定知识图谱的手动构建效率低下且依赖专业知识，现有基于LLM的方法受限于人工定义的模式、单文档处理和公共领域参考知识，难以应对复杂知识依赖和特定领域的需求。

Method: LKD-KGC框架通过分析文档库推断知识依赖关系，利用LLM驱动的优先级确定处理顺序，并自动生成实体模式，无需预定义结构或外部知识。

Result: 实验表明，LKD-KGC在精确率和召回率上比现有基线方法提升了10%至20%。

Conclusion: LKD-KGC在构建高质量领域特定知识图谱方面具有显著潜力，解决了现有方法的局限性。

Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships
into triples, enhancing machine reasoning for various tasks. While
domain-specific KGs offer substantial benefits, their manual construction is
often inefficient and requires specialized knowledge. Recent approaches for
knowledge graph construction (KGC) based on large language models (LLMs), such
as schema-guided KGC and reference knowledge integration, have proven
efficient. However, these methods are constrained by their reliance on manually
defined schema, single-document processing, and public-domain references,
making them less effective for domain-specific corpora that exhibit complex
knowledge dependencies and specificity, as well as limited reference knowledge.
To address these challenges, we propose LKD-KGC, a novel framework for
unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes
document repositories to infer knowledge dependencies, determines optimal
processing sequences via LLM driven prioritization, and autoregressively
generates entity schema by integrating hierarchical inter-document contexts.
This schema guides the unsupervised extraction of entities and relationships,
eliminating reliance on predefined structures or external knowledge. Extensive
experiments show that compared with state-of-the-art baselines, LKD-KGC
generally achieves improvements of 10% to 20% in both precision and recall
rate, demonstrating its potential in constructing high-quality domain-specific
KGs.

</details>


### [367] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
*Shilin Xu,Yanwei Li,Rui Yang,Tao Zhang,Yueyi Sun,Wei Chow,Linfeng Li,Hang Song,Qi Xu,Yunhai Tong,Xiangtai Li,Hao Fei*

Main category: cs.CL

TL;DR: 论文提出Mixed-R1框架，通过混合奖励函数和数据集解决多源MLLM任务的稳定强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单一任务（如数学或图表分析），缺乏多源任务的统一解决方案。

Method: 设计Mixed-45K数据集和混合奖励函数（包括匹配、图表、IoU和开放式奖励BMAS）。

Result: 实验证明方法在多种MLLM（如Qwen2.5-VL和Intern-VL）上有效。

Conclusion: Mixed-R1为多源MLLM任务提供了一种统一的强化学习解决方案。

Abstract: Recent works on large language models (LLMs) have successfully demonstrated
the emergence of reasoning capabilities via reinforcement learning (RL).
Although recent efforts leverage group relative policy optimization (GRPO) for
MLLMs post-training, they constantly explore one specific aspect, such as
grounding tasks, math problems, or chart analysis. There are no works that can
leverage multi-source MLLM tasks for stable reinforcement learning. In this
work, we present a unified perspective to solve this problem. We present
Mixed-R1, a unified yet straightforward framework that contains a mixed reward
function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).
We first design a data engine to select high-quality examples to build the
Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which
contains various reward functions for various MLLM tasks. In particular, it has
four different reward functions: matching reward for binary answer or
multiple-choice problems, chart reward for chart-aware datasets, IoU reward for
grounding problems, and open-ended reward for long-form text responses such as
caption datasets. To handle the various long-form text content, we propose a
new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by
leveraging tokenizer embedding matching between the generated response and the
ground truth. Extensive experiments show the effectiveness of our proposed
method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.
Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [368] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
*Neemesh Yadav,Palakorn Achananuparp,Jing Jiang,Ee-Peng Lim*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在模拟人类行为和心智理论（ToM）推理中的作用，特别是在谈判任务中如何通过ToM推理使代理行为与人类规范对齐。


<details>
  <summary>Details</summary>
Motivation: 心智理论（ToM）推理对复杂社交互动至关重要，研究旨在探索ToM如何帮助LLM代理在谈判任务中更好地与人类规范对齐。

Method: 使用最后通牒游戏作为实验环境，初始化具有不同亲社会信念（贪婪、公平、无私）和推理方法（如链式思维和不同ToM水平）的LLM代理，并分析其决策过程。

Result: 2700次模拟结果表明，ToM推理显著提升了行为对齐、决策一致性和谈判结果。推理模型表现有限，而ToM推理模型在不同游戏角色中表现更优。

Conclusion: 研究证实ToM推理能有效增强人机互动和合作决策，为相关领域提供了新见解。

Abstract: Large Language Models (LLMs) have shown potential in simulating human
behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for
complex social interactions. In this study, we investigate the role of ToM
reasoning in aligning agentic behaviors with human norms in negotiation tasks,
using the ultimatum game as a controlled environment. We initialized LLM agents
with different prosocial beliefs (including Greedy, Fair, and Selfless) and
reasoning methods like chain-of-thought (CoT) and varying ToM levels, and
examined their decision-making processes across diverse LLMs, including
reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from
2,700 simulations indicated that ToM reasoning enhances behavior alignment,
decision-making consistency, and negotiation outcomes. Consistent with previous
findings, reasoning models exhibit limited capability compared to models with
ToM reasoning, different roles of the game benefits with different orders of
ToM reasoning. Our findings contribute to the understanding of ToM's role in
enhancing human-AI interaction and cooperative decision-making. The code used
for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [369] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 论文研究了如何通过结合大型语言模型（LLMs）和定理证明器（TPs）改进自然语言推理（NLI）的解释生成，重点解决语义信息丢失和逻辑结构不精确的问题，并提出了四种策略来提升自动形式化和解释优化的效果。


<details>
  <summary>Details</summary>
Motivation: 自然语言解释在NLI中至关重要，但LLMs与TPs的结合存在语义信息丢失和逻辑结构不精确的问题，影响了解释的忠实性和鲁棒性。

Method: 提出四种策略：1）减少自动形式化中的语义损失；2）高效识别和修正逻辑表示中的语法错误；3）用逻辑表达式引导LLMs生成结构化证明草图；4）提升LLMs对TP反馈的迭代优化能力。

Result: 在e-SNLI、QASC和WorldTree数据集上，自动形式化和解释优化分别提升了18.46%-39.77%和29.5%-51.5%，同时显著减少了验证所需的迭代次数。

Conclusion: 通过优化LLM-TP混合架构，显著提升了NLI解释的忠实性和效率，为未来的研究提供了实用策略。

Abstract: Natural language explanations play a fundamental role in Natural Language
Inference (NLI) by revealing how premises logically entail hypotheses. Recent
work has shown that the interaction of large language models (LLMs) with
theorem provers (TPs) can help verify and improve the validity of NLI
explanations. However, TPs require translating natural language into
machine-verifiable formal representations, a process that introduces the risk
of semantic information loss and unfaithful interpretation, an issue compounded
by LLMs' challenges in capturing critical logical structures with sufficient
precision. Moreover, LLMs are still limited in their capacity for rigorous and
robust proof construction within formal verification frameworks. To mitigate
issues related to faithfulness and robustness, this paper investigates
strategies to (1) alleviate semantic loss during autoformalisation, (2)
efficiently identify and correct syntactic errors in logical representations,
(3) explicitly use logical expressions to guide LLMs in generating structured
proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback
for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree
using different LLMs demonstrate that the proposed strategies yield significant
improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation
refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,
we show that specific interventions on the hybrid LLM-TP architecture can
substantially improve efficiency, drastically reducing the number of iterations
required for successful verification.

</details>


### [370] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
*Mingyu Huang,Shasha Zhou,Yuxuan Chen,Ke Li*

Main category: cs.CL

TL;DR: LitChat是一个端到端的交互式文献代理，通过结合数据驱动工具和大语言模型，帮助用户快速探索大规模文献，生成基于证据的见解。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动文献综述在‘大文献’时代不可行的问题，同时克服大语言模型在系统性综述中的局限性（如上下文窗口限制和幻觉问题）。

Method: LitChat自动解析用户查询，检索相关文献，构建知识图谱，并运用数据挖掘技术生成基于证据的见解。

Result: 通过AI4Health案例研究展示了LitChat快速导航大规模文献的能力，提供传统方法无法实现的基于数据的证据。

Conclusion: LitChat为文献探索提供了一种高效、透明且客观的新方法，弥补了大语言模型的不足。

Abstract: We are living in an era of "big literature", where the volume of digital
scientific publications is growing exponentially. While offering new
opportunities, this also poses challenges for understanding literature
landscapes, as traditional manual reviewing is no longer feasible. Recent large
language models (LLMs) have shown strong capabilities for literature
comprehension, yet they are incapable of offering "comprehensive, objective,
open and transparent" views desired by systematic reviews due to their limited
context windows and trust issues like hallucinations. Here we present LitChat,
an end-to-end, interactive and conversational literature agent that augments
LLM agents with data-driven discovery tools to facilitate literature
exploration. LitChat automatically interprets user queries, retrieves relevant
sources, constructs knowledge graphs, and employs diverse data-mining
techniques to generate evidence-based insights addressing user needs. We
illustrate the effectiveness of LitChat via a case study on AI4Health,
highlighting its capacity to quickly navigate the users through large-scale
literature landscape with data-based evidence that is otherwise infeasible with
traditional means.

</details>


### [371] [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)
*Li yunhan,Wu gengshen*

Main category: cs.CL

TL;DR: 论文提出了一种评估法律文本质量的框架，分析了49个LLM，发现模型质量在140亿参数后趋于稳定，推理模型表现优于基础架构，并发布了Qwen3系列作为性价比最优选的排名。


<details>
  <summary>Details</summary>
Motivation: 当前法律应用中LLM的评估基准主要关注事实准确性，而忽略了清晰度、连贯性和术语等语言质量方面。

Method: 开发回归模型评估法律文本质量，创建专门的法律问题集，分析49个LLM。

Result: 模型质量在140亿参数后趋于稳定，量化等工程选择影响微乎其微，推理模型表现更优。

Conclusion: 研究建立了法律LLM的标准化评估协议，揭示了当前训练数据优化的局限性，并发布了Qwen3系列作为最佳选择。

Abstract: As large language models (LLMs) are increasingly used in legal applications,
current evaluation benchmarks tend to focus mainly on factual accuracy while
largely neglecting important linguistic quality aspects such as clarity,
coherence, and terminology. To address this gap, we propose three steps: First,
we develop a regression model to evaluate the quality of legal texts based on
clarity, coherence, and terminology. Second, we create a specialized set of
legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off
at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at
72 billion parameters. Second, engineering choices such as quantization and
context length have a negligible impact, as indicated by statistical
significance thresholds above 0.016. Third, reasoning models consistently
outperform base architectures. A significant outcome of our research is the
release of a ranking list and Pareto analysis, which highlight the Qwen3 series
as the optimal choice for cost-performance tradeoffs. This work not only
establishes standardized evaluation protocols for legal LLMs but also uncovers
fundamental limitations in current training data refinement approaches. Code
and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

</details>


### [372] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
*Stéphane Aroca-Ouellette,Natalie Mackraz,Barry-John Theobald,Katherine Metcalf*

Main category: cs.CL

TL;DR: PROSE方法通过迭代优化和验证用户写作样本中的偏好描述，显著提升了LLM代理对个性化偏好的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的偏好描述过于通用，无法体现人类偏好的独特性和个性化。

Method: PROSE采用迭代优化和跨样本验证两种策略，提升偏好描述的精确性。

Result: PROSE在多个LLM模型上表现优于CIPHER方法33%，与ICL结合可进一步提升9%。

Conclusion: PROSE有效解决了偏好描述泛化问题，为个性化LLM代理提供了更精准的偏好推断方法。

Abstract: Accommodating human preferences is essential for creating aligned LLM agents
that deliver personalized and effective interactions. Recent work has shown the
potential for LLMs acting as writing agents to infer a description of user
preferences. Agent alignment then comes from conditioning on the inferred
preference description. However, existing methods often produce generic
preference descriptions that fail to capture the unique and individualized
nature of human preferences. This paper introduces PROSE, a method designed to
enhance the precision of preference descriptions inferred from user writing
samples. PROSE incorporates two key elements: (1) iterative refinement of
inferred preferences, and (2) verification of inferred preferences across
multiple user writing samples. We evaluate PROSE with several LLMs (i.e.,
Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an
email writing task. We find that PROSE more accurately infers nuanced human
preferences, improving the quality of the writing agent's generations over
CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly,
we demonstrate that ICL and PROSE are complementary methods, and combining them
provides up to a 9\% improvement over ICL alone.

</details>


### [373] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
*Eojin Kang,Juae Kim*

Main category: cs.CL

TL;DR: 论文探讨了多语言大语言模型（LLMs）在跨语言知识传递中的不一致性，提出了一种基于语言与思维理论的L2T提示策略，以改善知识传递效果。


<details>
  <summary>Details</summary>
Motivation: 研究多语言LLMs在跨语言知识传递中的表现不一致问题，探索非英语到英语的知识传递机制。

Method: 提出L2T提示策略，分析输入语言、内部认知过程与知识之间的关系，并通过实验验证其效果。

Result: 实验结果表明，英语方法并非总是最优，L2T策略能有效改善跨语言知识传递，减少对输入语言的依赖。

Conclusion: L2T策略为跨语言知识传递提供了新思路，强调模型内部思维与任务知识的对齐是关键。

Abstract: Multilingual large language models (LLMs) open up new possibilities for
leveraging information across languages, but their factual knowledge recall
remains inconsistent depending on the input language. While previous studies
have attempted to address this issue through English-based prompting and
evaluation, we explore non-English to English transfer via Language and Thought
Theory. This perspective allows us to examine language-thought binding in LLMs
and uncover why factual knowledge often fails to transfer effectively. We
propose the Language-to-Thought (L2T) prompting strategy, which analyzes the
relationship between input language, internal cognitive processes, and
knowledge. Experimental results challenge the assumption that English-based
approaches consistently outperform other languages and offer a novel insight
that aligning the model's internal thought with the knowledge required for the
task is critical for successful cross-lingual transfer. Furthermore, we show
that applying L2T during training can alleviate LLMs' reliance on the input
language and facilitate cross-linguistic knowledge integration without
translation-based learning. Code and datasets will be available.

</details>


### [374] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
*Trenton Chang,Tobias Schnabel,Adith Swaminathan,Jenna Wiens*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在用户目标对齐（steerability）方面的表现，发现当前模型存在覆盖不足、校准偏差和副作用等问题，并提出评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和指令跟随方面表现优异，但其是否能可靠地满足多样化的用户目标（steerability）尚不明确。

Method: 提出基于多维目标空间的评估框架，将用户目标和LLM输出建模为向量，并通过文本重写任务测试模型的steerability。

Result: 当前LLMs在steerability方面表现不佳，干预措施（如提示工程、采样和微调）效果有限，副作用仍显著。

Conclusion: 现有LLMs在steerability方面存在挑战，当前对齐策略可能不足，需进一步研究。

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [375] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
*Wenhan Dong,Tianyi Hu,Jingyi Zheng,Zhen Sun,Yuemeng Zhao,Yule Liu,Xinlei He,Xinyi Huang*

Main category: cs.CL

TL;DR: 论文提出现有评估方法在多轮不完全信息任务中存在误导性结果，并提出改进标准。


<details>
  <summary>Details</summary>
Motivation: 揭示现有方法在评估大型语言模型横向思维能力时的局限性，如捷径行为、僵化模式和任务提前终止。

Method: 提出改进的评估标准，包括推理路径检查、多样化评估指标和与人类表现的对比分析。

Result: 现有方法可能掩盖模型的真实推理能力，改进标准能更可靠地评估模型。

Conclusion: 需要更精细的评估方法以准确衡量大型语言模型的横向思维能力。

Abstract: Multi-round incomplete information tasks are crucial for evaluating the
lateral thinking capabilities of large language models (LLMs). Currently,
research primarily relies on multiple benchmarks and automated evaluation
metrics to assess these abilities. However, our study reveals novel insights
into the limitations of existing methods, as they often yield misleading
results that fail to uncover key issues, such as shortcut-taking behaviors,
rigid patterns, and premature task termination. These issues obscure the true
reasoning capabilities of LLMs and undermine the reliability of evaluations. To
address these limitations, we propose a refined set of evaluation standards,
including inspection of reasoning paths, diversified assessment metrics, and
comparative analyses with human performance.

</details>


### [376] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
*Harvey Dam,Jonas Knochelmann,Vinu Joseph,Ganesh Gopalakrishnan*

Main category: cs.CL

TL;DR: 通过调整生成过程中的特定标记序列（如抑制<think>后的双换行符和结束标记），无需修改模型权重或提示即可降低LLM对敏感内容的拒绝率。


<details>
  <summary>Details</summary>
Motivation: 观察到某些模型在拒绝敏感内容前会出现特定标记序列（如<think>后跟双换行符），希望通过简单调整生成格式来减少拒绝行为。

Method: 在生成过程中抑制<think>后的双换行符和结束标记，仅通过修改标记概率实现，无需数据集、参数调整或训练。

Result: 在DeepSeek-R1实验中，这些调整显著增加了对敏感提示的实质性回答比例，且不影响标准基准性能。

Conclusion: 研究表明，通过在生成过程中阻断拒绝子空间，可以有效规避模型的拒绝行为。

Abstract: We introduce a method to reduce refusal rates of large language models (LLMs)
on sensitive content without modifying model weights or prompts. Motivated by
the observation that refusals in certain models were often preceded by the
specific token sequence of a token marking the beginning of the
chain-of-thought (CoT) block (<think>) followed by a double newline token
(\n\n), we investigate the impact of two simple formatting adjustments during
generation: suppressing \n\n after <think> and suppressing the end-of-sequence
token after the end of the CoT block (</think>). Our method requires no
datasets, parameter changes, or training, relying solely on modifying token
probabilities during generation. In our experiments with official DeepSeek-R1
distillations, these interventions increased the proportion of substantive
answers to sensitive prompts without affecting performance on standard
benchmarks. Our findings suggest that refusal behaviors can be circumvented by
blocking refusal subspaces at specific points in the generation process.

</details>


### [377] [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)
*Hieu Tran,Phuong-Anh Nguyen-Le,Huy Nghiem,Quang-Nhan Nguyen,Wei Ai,Marine Carpuat*

Main category: cs.CL

TL;DR: 论文提出了一种解决低资源语言代码混合翻译问题的方法，通过构建自然和合成的平行语料库VietMix，并结合过滤机制提升翻译模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器翻译系统在处理低资源语言的代码混合输入时的失败问题。

Method: 构建自然代码混合越南语-英语平行语料库VietMix，并开发合成数据生成流程，结合过滤机制确保语法和语用合理性。

Result: 实验显示模型性能显著提升，COMETkiwi和XCOMET分数分别达到71.84和81.77，LLM评估中增强模型优于基线。

Conclusion: VietMix及其方法提升了神经机器翻译的生态效度，并为其他低资源语言对的代码混合翻译提供了框架。

Abstract: Machine translation systems fail when processing code-mixed inputs for
low-resource languages. We address this challenge by curating VietMix, a
parallel corpus of naturally occurring code-mixed Vietnamese text paired with
expert English translations. Augmenting this resource, we developed a
complementary synthetic data generation pipeline. This pipeline incorporates
filtering mechanisms to ensure syntactic plausibility and pragmatic
appropriateness in code-mixing patterns. Experimental validation shows our
naturalistic and complementary synthetic data boost models' performance,
measured by translation quality estimation scores, of up to 71.84 on COMETkiwi
and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,
augmented models are favored over seed fine-tuned counterparts in approximately
49% of judgments (54-56% excluding ties). VietMix and our augmentation
methodology advance ecological validity in neural MT evaluations and establish
a framework for addressing code-mixed translation challenges across other
low-resource pairs.

</details>


### [378] [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)
*Fei Bai,Yingqian Min,Beichen Zhang,Zhipeng Chen,Wayne Xin Zhao,Lei Fang,Zheng Liu,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 论文提出了一种改进工具增强强化学习（RL）的方法，用于代码集成推理，通过平衡探索与稳定性提升训练效果，并在数学推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究代码集成推理中模型如何有效使用外部代码工具，解决工具增强RL学习动态不稳定的问题。

Method: 开发增强训练策略，平衡探索与稳定性，逐步构建工具使用能力，提升推理性能。

Result: 在五个主流数学推理基准测试中显著优于基线模型，揭示了代码集成推理扩展模型能力边界和提高效率的关键机制。

Conclusion: 代码集成推理通过工具增强RL有效提升模型性能，同时开源了数据和代码以便复现。

Abstract: In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.

</details>


### [379] [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)
*Guiyang Hou,Xing Gao,Yuchuan Wu,Xiang Huang,Wenqi Zhang,Zhe Zheng,Yongliang Shen,Jialu Du,Fei Huang,Yongbin Li,Weiming Lu*

Main category: cs.CL

TL;DR: 论文提出了一种名为TimeHC-RL的方法，旨在通过时间感知的分层认知强化学习提升大语言模型（LLMs）在社交领域的智能表现。实验表明，该方法优于传统的System 2 RL方法，并揭示了提升LLMs社交智能的多个关键见解。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学和编程等需要系统2认知的领域表现优异，但在社交领域的认知发展仍未被充分探索。社交领域需要更丰富的认知模式（从直觉反应到深思熟虑），因此需要新的方法来提升LLMs的社交智能。

Method: 提出了Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL)方法，通过分层认知强化学习结合时间感知，提升LLMs的社交智能。实验包括五种后训练范式和两种测试时干预范式，覆盖八个数据集。

Result: TimeHC-RL方法显著优于传统的System 2 RL方法，使7B规模的模型性能媲美更先进的模型（如DeepSeek-R1和OpenAI-O3）。实验还揭示了提升LLMs社交智能的多项关键见解。

Conclusion: TimeHC-RL方法为提升LLMs在社交领域的智能提供了有效途径，并通过系统性实验揭示了相关领域的重要发现。

Abstract: Recently, Large Language Models (LLMs) have made significant progress in
IQ-related domains that require careful thinking, such as mathematics and
coding. However, enhancing LLMs' cognitive development in social domains,
particularly from a post-training perspective, remains underexplored.
Recognizing that the social world follows a distinct timeline and requires a
richer blend of cognitive modes (from intuitive reactions (System 1) and
surface-level thinking to deliberate thinking (System 2)) than mathematics,
which primarily relies on System 2 cognition (careful, step-by-step reasoning),
we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning
(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we
systematically explore improving LLMs' social intelligence and validate the
effectiveness of the TimeHC-RL method, through five other post-training
paradigms and two test-time intervention paradigms on eight datasets with
diverse data patterns. Experimental results reveal the superiority of our
proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It
gives the 7B backbone model wings, enabling it to rival the performance of
advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic
exploration from post-training and test-time interventions perspectives to
improve LLMs' social intelligence has uncovered several valuable insights.

</details>


### [380] [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)
*Andrea Pedrotti,Michele Papucci,Cristiano Ciaccio,Alessio Miaschi,Giovanni Puccetti,Felice Dell'Orletta,Andrea Esuli*

Main category: cs.CL

TL;DR: 论文提出了一种测试机器生成文本检测器鲁棒性的方法，通过对抗攻击揭示其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型的进步导致合成内容真实性提升，引发恶意使用担忧，而现有检测器在真实场景中泛化能力不足。

Method: 使用直接偏好优化（DPO）微调语言模型，使机器生成文本风格接近人类文本，挑战检测器的依赖特征。

Result: 实验表明，检测器容易被少量对抗样本欺骗，检测性能显著下降。

Conclusion: 需改进检测方法以增强对未见文本的鲁棒性。

Abstract: Recent advancements in Generative AI and Large Language Models (LLMs) have
enabled the creation of highly realistic synthetic content, raising concerns
about the potential for malicious use, such as misinformation and manipulation.
Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the
lack of robust benchmarks that assess generalization to real-world scenarios.
In this work, we present a pipeline to test the resilience of state-of-the-art
MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed
adversarial attacks. To challenge the detectors, we fine-tune language models
using Direct Preference Optimization (DPO) to shift the MGT style toward
human-written text (HWT). This exploits the detectors' reliance on stylistic
clues, making new generations more challenging to detect. Additionally, we
analyze the linguistic shifts induced by the alignment and which features are
used by detectors to detect MGT texts. Our results show that detectors can be
easily fooled with relatively few examples, resulting in a significant drop in
detection performance. This highlights the importance of improving detection
methods and making them robust to unseen in-domain texts.

</details>


### [381] [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
*Celia Cintas,Miriam Rateike,Erik Miehling,Elizabeth Daly,Skyler Speakman*

Main category: cs.CL

TL;DR: 研究探索了大型语言模型（LLMs）中人物特征的编码方式及位置，发现不同人物特征在解码器后三分之一层中表现差异最大，且某些伦理观点存在重叠，而政治意识形态则更分散。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何内部表示不同人物特征（如价值观、信仰），以优化模型输出中对特定人类特征的调控。

Method: 采用降维和模式识别方法，分析模型层中人物特征的编码差异，并研究特定层的激活模式。

Result: 人物特征在解码器后三分之一层中差异显著；伦理观点（如道德虚无主义和功利主义）存在重叠，政治意识形态（如保守主义和自由主义）则更分散。

Conclusion: 研究揭示了LLMs内部信息表示的方式，为未来优化模型输出中对特定人类特征的调控提供了依据。

Abstract: We present a study on how and where personas -- defined by distinct sets of
human characteristics, values, and beliefs -- are encoded in the representation
space of large language models (LLMs). Using a range of dimension reduction and
pattern recognition methods, we first identify the model layers that show the
greatest divergence in encoding these representations. We then analyze the
activations within a selected layer to examine how specific personas are
encoded relative to others, including their shared and distinct embedding
spaces. We find that, across multiple pre-trained decoder-only LLMs, the
analyzed personas show large differences in representation space only within
the final third of the decoder layers. We observe overlapping activations for
specific ethical perspectives -- such as moral nihilism and utilitarianism --
suggesting a degree of polysemy. In contrast, political ideologies like
conservatism and liberalism appear to be represented in more distinct regions.
These findings help to improve our understanding of how LLMs internally
represent information and can inform future efforts in refining the modulation
of specific human traits in LLM outputs. Warning: This paper includes
potentially offensive sample statements.

</details>


### [382] [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
*Wei Zhong,Manasa Bharadwaj,Yixiao Wang,Nikhil Verma,Yipeng Ji,Chul Lee*

Main category: cs.CL

TL;DR: 论文提出了一种名为Budget EAGLE（Beagle）的新型跨注意力Transformer解码器模型，用于加速大型语言模型（LLM）的推理。该模型简化了架构，提高了训练效率，并在性能上与领先的自注意力模型（EAGLE-v2）相当。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法通常依赖于紧密耦合的自注意力Transformer解码器，架构复杂且难以泛化。Beagle旨在解决这一问题，提供更简单、高效的替代方案。

Method: Beagle采用跨注意力Transformer解码器，无需辅助池化或融合层。提出了一种名为Two-Stage Block-Attention Training的新训练方法，以确保块级注意力场景下的训练稳定性和收敛效率。

Result: 实验表明，Beagle在推理加速和训练效率上均优于EAGLE-v2，同时在多个LLM和数据集上表现优异。

Conclusion: Beagle为推测解码提供了一种简化且高效的架构选择，具有广泛的应用潜力。

Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating
inference in large language models (LLMs), particularly when the draft and
target models are well aligned. However, state-of-the-art SD methods typically
rely on tightly coupled, self-attention-based Transformer decoders, often
augmented with auxiliary pooling or fusion layers. This coupling makes them
increasingly complex and harder to generalize across different models. We
present Budget EAGLE (Beagle), the first, to our knowledge,
cross-attention-based Transformer decoder SD model that achieves performance on
par with leading self-attention SD models (EAGLE-v2) while eliminating the need
for pooling or auxiliary components, simplifying the architecture, improving
training efficiency, and maintaining stable memory usage during training-time
simulation. To enable effective training of this novel architecture, we propose
Two-Stage Block-Attention Training, a new method that achieves training
stability and convergence efficiency in block-level attention scenarios.
Extensive experiments across multiple LLMs and datasets show that Beagle
achieves competitive inference speedups and higher training efficiency than
EAGLE-v2, offering a strong alternative for architectures in speculative
decoding.

</details>


### [383] [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)
*Ye Eun Chun,Taeyoon Hwang,Seung-won Hwang,Byung-Hak Kim*

Main category: cs.CL

TL;DR: CREFT是一个利用大型语言模型（LLM）代理的框架，用于提取和分析长篇幅叙事中的复杂角色关系，显著优于单代理LLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理长篇幅叙事中复杂的角色关系，需要更高效的解决方案。

Method: CREFT通过知识蒸馏构建基础角色图，并迭代优化角色组成、关系提取、角色识别和分组分配。

Result: 在韩剧数据集上，CREFT在准确性和完整性上显著优于单代理LLM基线。

Conclusion: CREFT通过可视化角色网络，提升了叙事理解效率，对娱乐、出版和教育领域有重要价值。

Abstract: Understanding complex character relations is crucial for narrative analysis
and efficient script evaluation, yet existing extraction methods often fail to
handle long-form narratives with nuanced interactions. To address this
challenge, we present CREFT, a novel sequential framework leveraging
specialized Large Language Model (LLM) agents. First, CREFT builds a base
character graph through knowledge distillation, then iteratively refines
character composition, relation extraction, role identification, and group
assignments. Experiments on a curated Korean drama dataset demonstrate that
CREFT significantly outperforms single-agent LLM baselines in both accuracy and
completeness. By systematically visualizing character networks, CREFT
streamlines narrative comprehension and accelerates script review -- offering
substantial benefits to the entertainment, publishing, and educational sectors.

</details>


### [384] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
*Matthew Raffel,Victor Agostinelli,Lizhong Chen*

Main category: cs.CL

TL;DR: BeaverTalk是一个用于语音到文本翻译的级联系统，结合了VAD分割器、Whisper Large V2和Gemma 3 12B，通过LoRAs微调，在IWSLT 2025任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建一个高效的语音到文本翻译系统，支持低延迟和高延迟场景，提升翻译质量。

Method: 使用VAD分割语音流，Whisper Large V2进行语音识别，Gemma 3 12B进行翻译，并通过LoRAs微调。

Result: 在英德和英中任务中，BLEU分数分别为24.64/27.83和34.07/37.23，StreamLAAL表现良好。

Conclusion: BeaverTalk在语音翻译任务中表现出色，尤其在低延迟和高延迟场景下均取得良好效果。

Abstract: This paper discusses the construction, fine-tuning, and deployment of
BeaverTalk, a cascaded system for speech-to-text translation as part of the
IWSLT 2025 simultaneous translation task. The system architecture employs a VAD
segmenter for breaking a speech stream into segments, Whisper Large V2 for
automatic speech recognition (ASR), and Gemma 3 12B for simultaneous
translation. Regarding the simultaneous translation LLM, it is fine-tuned via
low-rank adaptors (LoRAs) for a conversational prompting strategy that
leverages a single prior-sentence memory bank from the source language as
context. The cascaded system participated in the English$\rightarrow$German and
English$\rightarrow$Chinese language directions for both the low and high
latency regimes. In particular, on the English$\rightarrow$German task, the
system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and
3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the
system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and
3521.35, respectively.

</details>


### [385] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
*Anna Sofia Lippolis,Minh Davide Ragagni,Paolo Ciancarini,Andrea Giovanni Nuzzolese,Valentina Presutti*

Main category: cs.CL

TL;DR: 论文介绍了Bench4KE，一个基于API的基准测试系统，用于评估知识工程自动化工具，特别是自动生成能力问题（CQs）的工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）为知识工程自动化研究提供了新机遇，但现有工具的评估缺乏标准化，影响了方法严谨性和结果的可比性。

Method: 开发了Bench4KE系统，提供标准化的评估框架和黄金标准数据集，使用相似性指标评估CQ生成工具的质量。

Result: 通过比较四种基于LLM的CQ生成系统，建立了未来研究的基线。

Conclusion: Bench4KE为知识工程自动化任务提供了可扩展的评估平台，并公开了代码和数据集。

Abstract: The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.

</details>


### [386] [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)
*Hyuntak Kim,Byung-Hak Kim*

Main category: cs.CL

TL;DR: NexusSum是一个多代理LLM框架，用于长叙事文本的摘要生成，通过对话到描述的转换和分层多LLM摘要技术，显著提升了摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长叙事文本（如书籍、电影、电视剧本）的摘要生成中难以捕捉复杂情节和主题连贯性，因此需要一种更有效的方法。

Method: NexusSum采用无微调的多代理LLM框架，通过对话到描述的转换和分层多LLM摘要技术，优化文本处理和输出长度控制。

Result: 该方法在书籍、电影和电视剧本的摘要任务中，BERTScore（F1）提升了30.0%，达到新的最优水平。

Conclusion: NexusSum展示了多代理LLM在长叙事文本摘要中的有效性，为多样化叙事领域的结构化摘要提供了可扩展的解决方案。

Abstract: Summarizing long-form narratives--such as books, movies, and TV
scripts--requires capturing intricate plotlines, character interactions, and
thematic coherence, a task that remains challenging for existing LLMs. We
introduce NexusSum, a multi-agent LLM framework for narrative summarization
that processes long-form text through a structured, sequential
pipeline--without requiring fine-tuning. Our approach introduces two key
innovations: (1) Dialogue-to-Description Transformation: A narrative-specific
preprocessing method that standardizes character dialogue and descriptive text
into a unified format, improving coherence. (2) Hierarchical Multi-LLM
Summarization: A structured summarization pipeline that optimizes chunk
processing and controls output length for accurate, high-quality summaries. Our
method establishes a new state-of-the-art in narrative summarization, achieving
up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV
scripts. These results demonstrate the effectiveness of multi-agent LLMs in
handling long-form content, offering a scalable approach for structured
summarization in diverse storytelling domains.

</details>


### [387] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
*Yasaman Jafari,Zixian Wang,Leon Bergen,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 研究探讨了结构化状态空间模型（SSMs）的隐藏状态是否可以通过后处理合并以支持下游推理，提出了一种称为“文档汤”的策略，通过简单操作（如平均）将独立编码的文档表示合并为单一上下文状态。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不重新处理完整输入的情况下，通过合并独立编码的文档表示来支持下游任务，提高模块化编码和重用性。

Method: 提出“文档汤”策略，独立编码文档后通过简单操作（如平均）合并表示，并在Mamba2模型上进行微调以生成可合并的表示。

Result: 在多跳问答、稀疏检索和长文档推理任务中表现良好，HotpotQA上合并十个独立编码文档的性能接近交叉编码器。

Conclusion: 文档汤策略有效支持模块化编码和下游推理任务，性能接近传统交叉编码器。

Abstract: We investigate whether hidden states from Structured State Space Models
(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by
model souping, we propose a strategy where documents are encoded independently
and their representations are pooled -- via simple operations like averaging --
into a single context state. This approach, which we call document souping,
enables modular encoding and reuse without reprocessing the full input for each
query. We finetune Mamba2 models to produce soupable representations and find
that they support multi-hop QA, sparse retrieval, and long-document reasoning
with strong accuracy. On HotpotQA, souping ten independently encoded documents
nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [388] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
*Junzhuo Li,Bo Wang,Xiuze Zhou,Peijie Jiang,Jia Liu,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了一种跨层归因算法，用于分析稀疏MoE架构的动态路由-专家交互，揭示了MoE模型的高效性和协作模式。


<details>
  <summary>Details</summary>
Motivation: 解决现有归因方法无法捕捉稀疏MoE架构中动态路由-专家交互的问题。

Method: 提出跨层归因算法，对比稀疏MoE架构（如Qwen 1.5-MoE）与密集模型（如Llama-7B）。

Result: MoE模型通过“中期激活、后期放大”模式实现37%的层效率提升，并揭示了“基础-细化”框架和语义驱动路由。

Conclusion: 研究为MoE模型的可解释性提供了新见解，平衡了效率、专业化和鲁棒性。

Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in sparse
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.

</details>


### [389] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
*Nikita Martynov,Anastasia Mordasheva,Dmitriy Gorbetskiy,Danil Astafurov,Ulyana Isaeva,Elina Basyrova,Sergey Skachkov,Victoria Berestova,Nikolay Ivanov,Valeriia Zanina,Alena Fenogenova*

Main category: cs.CL

TL;DR: POLLUX是一个开源基准测试，用于评估俄语大型语言模型的生成能力，提供透明、标准化的评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统的人工评估方法成本高且不精确，需要一种可扩展且解释性强的评估工具。

Method: 定义了35种任务类型的详细标准，开发了评分协议，并训练了LLM评估器。

Result: POLLUX包含2100个专业编写的提示，覆盖多种生成领域，并提供可扩展的评估工具。

Conclusion: POLLUX为模型开发提供了高效、透明的评估方法，替代了传统的人工评估。

Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to
evaluate the generative capabilities of large language models (LLMs) in
Russian. Our main contribution is a novel evaluation methodology that enhances
the interpretability of LLM assessment. For each task type, we define a set of
detailed criteria and develop a scoring protocol where models evaluate
responses and provide justifications for their ratings. This enables
transparent, criteria-driven evaluation beyond traditional resource-consuming,
side-by-side human comparisons. POLLUX includes a detailed, fine-grained
taxonomy of 35 task types covering diverse generative domains such as code
generation, creative writing, and practical assistant use cases, totaling 2,100
manually crafted and professionally authored prompts. Each task is categorized
by difficulty (easy/medium/hard), with experts constructing the dataset
entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)
evaluators trained for nuanced assessment of generative outputs. This approach
provides scalable, interpretable evaluation and annotation tools for model
development, effectively replacing costly and less precise human judgments.

</details>


### [390] [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)
*Junyi Li,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 论文指出，基于强化学习（RL）优化的语言模型在推理任务中表现优异，但会显著增加幻觉现象。作者提出FSPO算法，通过逐步验证事实性来减少幻觉，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习优化提升了语言模型的推理能力，但同时也增加了幻觉现象，影响了模型的可靠性。作者旨在解决这一问题。

Method: 提出Factuality-aware Step-wise Policy Optimization (FSPO)算法，在推理的每一步引入事实性验证，动态调整优势值以激励事实正确性。

Result: 实验表明，FSPO在数学推理和幻觉基准测试中有效减少了幻觉，同时提升了推理准确性。

Conclusion: FSPO通过结合事实性验证，显著提高了语言模型在强化学习优化中的可靠性和性能。

Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.

</details>


### [391] [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)
*Jens-Joris Decorte,Jeroen Van Hautte,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 提出了一种轻量级的对比学习方法ConTeXT-match，用于高效提取技能标签，并引入新基准Skill-XL和改进的JobBERT V2模型，显著提升了劳动力市场分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有技能提取方法依赖计算成本高的大语言模型，效率低。

Method: 采用对比学习与token级注意力机制，提出ConTeXT-match模型，并引入Skill-XL基准和JobBERT V2模型。

Result: 模型在技能提取和职位标准化任务中表现高效、准确且可扩展。

Conclusion: 该方法为大规模实时劳动力市场分析提供了理想解决方案。

Abstract: Labor market analysis relies on extracting insights from job advertisements,
which provide valuable yet unstructured information on job titles and
corresponding skill requirements. While state-of-the-art methods for skill
extraction achieve strong performance, they depend on large language models
(LLMs), which are computationally expensive and slow. In this paper, we propose
\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level
attention that is well-suited for the extreme multi-label classification task
of skill classification. \textbf{ConTeXT-match} significantly improves skill
extraction efficiency and performance, achieving state-of-the-art results with
a lightweight bi-encoder model. To support robust evaluation, we introduce
\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill
annotations that explicitly address the redundancy in the large label space.
Finally, we present \textbf{JobBERT V2}, an improved job title normalization
model that leverages extracted skills to produce high-quality job title
representations. Experiments demonstrate that our models are efficient,
accurate, and scalable, making them ideal for large-scale, real-time labor
market analysis.

</details>


### [392] [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
*Dayeon Ki,Rachel Rudinger,Tianyi Zhou,Marine Carpuat*

Main category: cs.CL

TL;DR: 多智能体辩论框架提升LLMs的文化适应性，通过辩论和自省动态选择，小模型也能达到大模型的效果。


<details>
  <summary>Details</summary>
Motivation: LLMs需适应多元文化，现有方法多为单模型单轮，多智能体辩论能互补优势。

Method: 提出多智能体辩论框架，两种变体：纯辩论或动态选择辩论与自省。

Result: 在75国社交礼仪基准测试中，辩论提升准确性和文化公平性，小模型表现媲美大模型。

Conclusion: 多智能体辩论有效提升LLMs文化适应性，小模型也能高效完成任务。

Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse
cultural contexts to benefit diverse communities across the world. While
previous efforts have focused on single-LLM, single-turn approaches, we propose
to exploit the complementary strengths of multiple LLMs to promote cultural
adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based
agents debate over a cultural scenario and collaboratively reach a final
decision. We propose two variants: one where either LLM agents exclusively
debate and another where they dynamically choose between self-reflection and
debate during their turns. We evaluate these approaches on 7 open-weight LLMs
(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette
norms in 75 countries. Experiments show that debate improves both overall
accuracy and cultural group parity over single-LLM baselines. Notably,
multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies
comparable to that of a much larger model (27B parameters).

</details>


### [393] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
*Ryota Miyano,Yuki Arase*

Main category: cs.CL

TL;DR: 提出一种简单有效的LoRA合并方法，用于低资源语言生成任务，通过微调更新和修剪LoRA参数，提升任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA合并方法因参数固定而适应性有限，且未解决低资源问题。

Method: 提出一种通过微调更新和修剪LoRA参数的方法，利用少量目标任务数据进行调整。

Result: 实验表明，该方法在总结任务中显著优于现有方法，适应性和性能均有提升。

Conclusion: 该方法通过动态调整LoRA参数，有效解决了低资源任务中的适应性问题。

Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.

</details>


### [394] [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
*Dayeon Ki,Kevin Duh,Marine Carpuat*

Main category: cs.CL

TL;DR: 研究比较了四种AI翻译质量反馈机制，发现隐式反馈（尤其是QA表格）在决策准确性、用户依赖和感知方面优于显式反馈。


<details>
  <summary>Details</summary>
Motivation: 在用户无法评估AI预测质量的场景中，急需反馈机制来帮助用户负责任地使用AI。

Method: 在机器翻译场景中，比较了四种反馈类型：显式反馈（错误高亮和LLM解释）和隐式反馈（回译和QA表格）。

Result: 除错误高亮外，所有反馈类型均显著提升决策准确性和用户依赖。隐式反馈（尤其是QA表格）表现最佳。

Conclusion: 隐式反馈（如QA表格）在帮助用户决策和提升信任方面更有效，且心理负担更低。

Abstract: As people increasingly use AI systems in work and daily life, feedback
mechanisms that help them use AI responsibly are urgently needed, particularly
in settings where users are not equipped to assess the quality of AI
predictions. We study a realistic Machine Translation (MT) scenario where
monolingual users decide whether to share an MT output, first without and then
with quality feedback. We compare four types of quality feedback: explicit
feedback that directly give users an assessment of translation quality using 1)
error highlights and 2) LLM explanations, and implicit feedback that helps
users compare MT inputs and outputs through 3) backtranslation and 4)
question-answer (QA) tables. We find that all feedback types, except error
highlights, significantly improve both decision accuracy and appropriate
reliance. Notably, implicit feedback, especially QA tables, yields
significantly greater gains than explicit feedback in terms of decision
accuracy, appropriate reliance, and user perceptions, receiving the highest
ratings for helpfulness and trust, and the lowest for mental burden.

</details>


### [395] [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)
*Tejul Pandit,Meet Raval,Dhvani Upadhyay*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型（LLM）生成合成ABSA数据的方法，以解决标注数据稀缺问题，并通过实验验证了生成数据的质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决ABSA任务中多样化标注数据稀缺的问题，尤其是反映真实对话场景的数据。

Method: 使用GPT-4o生成合成数据，确保主题和情感分布的一致性，并评估三种先进LLM（Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1）在分类任务上的表现。

Result: 合成数据有效，不同模型表现各异：DeepSeek-R1精度高，Gemini 1.5 Pro和Claude 3.5 Sonnet召回率高，Gemini 1.5 Pro推理速度更快。

Conclusion: 基于LLM的合成数据生成是一种可行且灵活的方法，可为ABSA研究提供资源，减少对有限真实标注数据的依赖。

Abstract: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions
but often suffers from the scarcity of diverse, labeled datasets that reflect
real-world conversational nuances. This paper presents an approach for
generating synthetic ABSA data using Large Language Models (LLMs) to address
this gap. We detail the generation process aimed at producing data with
consistent topic and sentiment distributions across multiple domains using
GPT-4o. The quality and utility of the generated data were evaluated by
assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,
Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification
tasks. Our results demonstrate the effectiveness of the synthetic data,
revealing distinct performance trade-offs among the models: DeepSeekR1 showed
higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,
and Gemini 1.5 Pro offered significantly faster inference. We conclude that
LLM-based synthetic data generation is a viable and flexible method for
creating valuable ABSA resources, facilitating research and model evaluation
without reliance on limited or inaccessible real-world labeled data.

</details>


### [396] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
*Nir Endy,Idan Daniel Grosbard,Yuval Ran-Milo,Yonatan Slutzky,Itay Tshuva,Raja Giryes*

Main category: cs.CL

TL;DR: 本文研究了基于Mamba状态空间模型（SSM）的语言模型中事实信息的流动，通过理论与实证方法将其与Transformer架构及其注意力机制联系起来，并利用Attention Knockout方法分析Mamba-1和Mamba-2的信息传递模式。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba模型内部信息流动的机制，并与Transformer模型进行比较，以揭示语言模型的通用特性。

Method: 利用Attention Knockout方法分析Mamba模型的信息传递和局部化，结合其结构化分解特性。

Result: 发现Mamba模型与Transformer模型在某些现象上存在差异，但也存在通用模式，揭示了语言模型的潜在共性。

Conclusion: 通过结构化分解，提供了理解Mamba内部操作的统一视角，为语言模型的设计和优化提供了新思路。

Abstract: This paper investigates the flow of factual information in Mamba State-Space
Model (SSM)-based language models. We rely on theoretical and empirical
connections to Transformer-based architectures and their attention mechanisms.
Exploiting this relationship, we adapt attentional interpretability techniques
originally developed for Transformers--specifically, the Attention Knockout
methodology--to both Mamba-1 and Mamba-2. Using them we trace how information
is transmitted and localized across tokens and layers, revealing patterns of
subject-token information emergence and layer-wise dynamics. Notably, some
phenomena vary between mamba models and Transformer based models, while others
appear universally across all models inspected--hinting that these may be
inherent to LLMs in general. By further leveraging Mamba's structured
factorization, we disentangle how distinct "features" either enable
token-to-token information exchange or enrich individual tokens, thus offering
a unified lens to understand Mamba internal operations.

</details>


### [397] [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)
*Yingchaojie Feng,Yiqun Sun,Yandong Sun,Minfeng Zhu,Qiang Huang,Anthony K. H. Tung,Wei Chen*

Main category: cs.CL

TL;DR: GSTransform提出了一种轻量级的指令跟随文本嵌入框架，通过动态调整预计算嵌入来适应指令，显著提高了效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每条新指令重新编码整个语料库，计算开销大。

Method: 基于Guided Space Transformation，利用预计算嵌入中的指令相关信息，实时调整嵌入以匹配用户指令。

Result: 在三个任务和九个数据集上，GSTransform在质量和实时处理速度上均优于现有方法，提速6~300倍。

Conclusion: GSTransform是一种高效且高质量的指令跟随文本嵌入解决方案。

Abstract: In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.

</details>


### [398] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
*Xu Wang,Zihao Li,Benyou Wang,Yan Hu,Difan Zou*

Main category: cs.CL

TL;DR: 提出了一种基于稀疏自编码器（SAE）引导的子空间投影遗忘（SSPU）方法，用于精确、可解释且鲁棒地从大语言模型中移除特定知识。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法在可解释性和对抗性提示防御方面存在不足，需要一种更有效的方法。

Method: SSPU通过三阶段流程实现：数据驱动的层和特征选择、QR分解构建子空间、约束优化控制激活到“无关”子空间。

Result: 在WMDP-Cyber遗忘集和三个基准测试中，SSPU将有害知识准确率降低3.22%，并提升对抗鲁棒性。

Conclusion: SSPU展示了基于可解释子空间优化的遗忘方法在实现鲁棒、可控模型行为方面的潜力。

Abstract: Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to sparse autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.

</details>


### [399] [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)
*Houjun Liu,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 研究发现，在单周期预训练中不使用dropout能提升语言模型的下游任务表现，并增强模型的可编辑性。


<details>
  <summary>Details</summary>
Motivation: 尽管dropout在深度学习中广泛用于防止过拟合，但在现代大型语言模型（LLM）的单周期预训练中，过拟合问题较少，dropout的作用尚未被充分研究。

Method: 通过实验比较了在单周期预训练中应用不同dropout水平的BERT和自回归语言模型（Pythia 160M和1.4B），并评估了下游任务表现和模型可编辑性。

Result: 发现不使用dropout时，语言建模、语法分析、问答和自然语言推理等任务表现更好；同时，模型在梯度编辑（MEND）中表现更优。

Conclusion: 建议在单周期预训练中不使用dropout，以提升模型性能和可编辑性。

Abstract: Originally, dropout was seen as a breakthrough regularization technique that
reduced overfitting and improved performance in almost all applications of deep
learning by reducing overfitting. Yet, single-epoch pretraining tasks common to
modern LLMs yield minimal overfitting, leading to dropout not being used for
large LLMs. Nevertheless, no thorough empirical investigation has been done on
the role of dropout in LM pretraining. Through experiments in single-epoch
pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs
with varying levels of dropout, we find that downstream performance in language
modeling, morpho-syntax (BLiMP), question answering (SQuAD), and
natural-language inference (MNLI) improves when dropout is not applied during
pretraining. We additionally find that the recently-introduced "early dropout"
also degrades performance over applying no dropout at all. We further
investigate the models' editability, and find that models trained without
dropout are more successful in gradient-based model editing (MEND) and
equivalent in representation-based model editing (ReFT). Therefore, we advocate
to drop dropout during single-epoch pretraining.

</details>


### [400] [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
*Juraj Vladika,Annika Domres,Mai Nguyen,Rebecca Moser,Jana Nano,Felix Busch,Lisa C. Adams,Keno K. Bressem,Denise Bernhardt,Stephanie E. Combs,Kai J. Borm,Florian Matthes,Jan C. Peeken*

Main category: cs.CL

TL;DR: 提出了一种新的原子事实检查框架，用于提高医学长问答中大型语言模型的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学领域存在幻觉和引用不准确的问题，影响其临床应用和合规性。

Method: 将模型生成的回答分解为可验证的原子事实，并对照权威医学指南进行独立验证。

Result: 显著提高了事实准确性和可解释性，整体答案改进达40%，幻觉检测率为50%。

Conclusion: 该框架为医学AI应用提供了更透明和可信的解决方案，推动了LLM在临床中的可靠应用。

Abstract: Large language models (LLMs) exhibit extensive medical knowledge but are
prone to hallucinations and inaccurate citations, which pose a challenge to
their clinical adoption and regulatory compliance. Current methods, such as
Retrieval Augmented Generation, partially address these issues by grounding
answers in source documents, but hallucinations and low fact-level
explainability persist. In this work, we introduce a novel atomic fact-checking
framework designed to enhance the reliability and explainability of LLMs used
in medical long-form question answering. This method decomposes LLM-generated
responses into discrete, verifiable units called atomic facts, each of which is
independently verified against an authoritative knowledge base of medical
guidelines. This approach enables targeted correction of errors and direct
tracing to source literature, thereby improving the factual accuracy and
explainability of medical Q&A. Extensive evaluation using multi-reader
assessments by medical experts and an automated open Q&A benchmark demonstrated
significant improvements in factual accuracy and explainability. Our framework
achieved up to a 40% overall answer improvement and a 50% hallucination
detection rate. The ability to trace each atomic fact back to the most relevant
chunks from the database provides a granular, transparent explanation of the
generated responses, addressing a major gap in current medical AI applications.
This work represents a crucial step towards more trustworthy and reliable
clinical applications of LLMs, addressing key prerequisites for clinical
application and fostering greater confidence in AI-assisted healthcare.

</details>


### [401] [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)
*Mingjie Liu,Shizhe Diao,Ximing Lu,Jian Hu,Xin Dong,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: 论文提出ProRL方法，通过长时间强化学习训练发现新的推理策略，超越基础模型的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习是否能真正扩展语言模型的推理能力，而非仅放大已有高奖励输出。

Method: 引入ProRL方法，结合KL散度控制、参考策略重置和多任务训练。

Result: RL训练模型在多种任务中表现优于基础模型，甚至解决基础模型无法完成的任务。

Conclusion: RL能探索新的解决方案空间，为未来长时程推理RL研究奠定基础。

Abstract: Recent advances in reasoning-centric language models have highlighted
reinforcement learning (RL) as a promising method for aligning models with
verifiable rewards. However, it remains contentious whether RL truly expands a
model's reasoning capabilities or merely amplifies high-reward outputs already
latent in the base model's distribution, and whether continually scaling up RL
compute reliably leads to improved reasoning performance. In this work, we
challenge prevailing assumptions by demonstrating that prolonged RL (ProRL)
training can uncover novel reasoning strategies that are inaccessible to base
models, even under extensive sampling. We introduce ProRL, a novel training
methodology that incorporates KL divergence control, reference policy
resetting, and a diverse suite of tasks. Our empirical analysis reveals that
RL-trained models consistently outperform base models across a wide range of
pass@k evaluations, including scenarios where base models fail entirely
regardless of the number of attempts. We further show that reasoning boundary
improvements correlates strongly with task competence of base model and
training duration, suggesting that RL can explore and populate new regions of
solution space over time. These findings offer new insights into the conditions
under which RL meaningfully expands reasoning boundaries in language models and
establish a foundation for future work on long-horizon RL for reasoning. We
release model weights to support further research:
https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

</details>


### [402] [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)
*Vittorio Torri,Machteld J. Boonstra,Marielle C. van de Veerdonk,Deborah N. Kalkman,Alicia Uijl,Francesca Ieva,Ameen Abu-Hanna,Folkert W. Asselbergs,Iacer Calixto*

Main category: cs.CL

TL;DR: 研究评估了基于左心室射血分数（LVEF）的心力衰竭（HF）患者表型分析模型，结合结构化和非结构化数据，比较了黑盒和白盒模型的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: HF患者表型多样，影响治疗和预后，需开发高效且可解释的表型分析模型。

Method: 使用2015-2023年两家医院的HF住院数据（33,105次住院），结合临床测量和出院记录，训练并比较Transformer和Aug-Linear模型。

Result: BERT和Aug-Linear模型表现最佳（AUC=0.84和0.81），Aug-Linear的解释更接近临床医生标注。

Conclusion: 出院记录是HF表型分析的最佳数据源，Aug-Linear模型兼具高性能和可解释性，适合临床决策。

Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes
affecting treatment and prognosis. This study evaluates models for phenotyping
HF patients based on left ventricular ejection fraction (LVEF) classes, using
structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both
Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105
hospitalizations, 16,334 patients). Data from AMC were used for model training,
and from VUmc for external validation. The dataset was unlabelled and included
tabular clinical measurements and discharge letters. Silver labels for LVEF
classes were generated by combining diagnosis codes, echocardiography results,
and textual mentions. Gold labels were manually annotated for 300 patients for
testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box)
models were trained and compared with baselines on structured and unstructured
data. To evaluate interpretability, two clinicians annotated 20 discharge
letters by highlighting information they considered relevant for LVEF
classification. These were compared to SHAP and LIME explanations from
black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone,
achieved the highest classification results (AUC=0.84 for BERT, 0.81 for
Aug-Linear on external validation), outperforming baselines. Aug-Linear
explanations aligned more closely with clinicians' explanations than post-hoc
explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for
phenotyping HF patients. Aug-Linear models matched black-box performance while
providing clinician-aligned interpretability, supporting their use in
transparent clinical decision-making.

</details>


### [403] [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
*Gabrielle Kaili-May Liu,Gal Yona,Avi Caciularu,Idan Szpektor,Tim G. J. Rudner,Arman Cohan*

Main category: cs.CL

TL;DR: 论文研究了LLM在不确定性表达上的可靠性，发现现有方法不足，并提出新方法MetaFaith显著提升了校准效果。


<details>
  <summary>Details</summary>
Motivation: LLM在传达错误信息时常用肯定语言，导致过度依赖和信任问题，需研究其不确定性表达的可靠性。

Method: 系统评估LLM在不确定性表达上的表现，提出基于人类元认知的MetaFaith校准方法。

Result: LLM普遍表现不佳，MetaFaith显著提升校准效果，最高提升61%的忠实度。

Conclusion: MetaFaith能有效改善LLM的不确定性表达，提升其可信度。

Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty
communication, yet LLMs often use assertive language when conveying false
claims, leading to over-reliance and eroded trust. We present the first
systematic study of $\textit{faithful confidence calibration}$ of LLMs,
benchmarking models' ability to use linguistic expressions of uncertainty that
$\textit{faithfully reflect}$ their intrinsic uncertainty, across a
comprehensive array of models, datasets, and prompting strategies. Our results
demonstrate that LLMs largely fail at this task, and that existing
interventions are insufficient: standard prompt approaches provide only
marginal gains, and existing, factuality-based calibration techniques can even
harm faithful calibration. To address this critical gap, we introduce
MetaFaith, a novel prompt-based calibration approach inspired by human
metacognition. We show that MetaFaith robustly improves faithful calibration
across diverse models and task domains, enabling up to 61% improvement in
faithfulness and achieving an 83% win rate over original generations as judged
by humans.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [404] [Strengthening Cybersecurity Resilience in Agriculture Through Educational Interventions: A Case Study of the Ponca Tribe of Nebraska](https://arxiv.org/abs/2505.23800)
*George Grispos,Logan Mears,Larry Loucks*

Main category: cs.CR

TL;DR: 论文提出农业网络安全教育干预措施CIIA，通过案例研究发现农民网络安全知识显著提升。


<details>
  <summary>Details</summary>
Motivation: 农业数字化带来新的网络安全挑战，需提升农民网络安全意识和应对能力。

Method: 采用案例研究，通过前后测问卷评估CIIA对农民网络安全知识的影响。

Result: 干预后农民对密码管理、多因素认证等概念的理解显著提升。

Conclusion: 需持续开展社区化网络安全教育以应对农业领域的网络威胁。

Abstract: The increasing digitization of agricultural operations has introduced new
cybersecurity challenges for the farming community. This paper introduces an
educational intervention called Cybersecurity Improvement Initiative for
Agriculture (CIIA), which aims to strengthen cybersecurity awareness and
resilience among farmers and food producers. Using a case study that focuses on
farmers from the Ponca Tribe of Nebraska, the research evaluates pre- and post-
intervention survey data to assess participants' cybersecurity knowledge and
awareness before and after exposure to the CIIA. The findings reveal a
substantial baseline deficiency in cybersecurity education among participants,
however, post-intervention assessments demonstrate improvements in the
comprehension of cybersecurity concepts, such as password hygiene, multi-factor
authentication, and the necessity of routine data backups. These initial
findings highlight the need for a continued and sustained, community-specific
cybersecurity education effort to help mitigate emerging cyber threats in the
agricultural sector.

</details>


### [405] [Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention](https://arxiv.org/abs/2505.23968)
*Stephan Rabanser,Ali Shahin Shamsabadi,Olive Franzese,Xiao Wang,Adrian Weller,Nicolas Papernot*

Main category: cs.CR

TL;DR: 论文提出了一种新型威胁：不诚实机构利用机器学习模型的谨慎预测机制进行歧视或拒绝服务。作者提出了一种攻击方法Mirage，并设计了防御框架Confidential Guardian。


<details>
  <summary>Details</summary>
Motivation: 谨慎预测机制可能被滥用，导致歧视或不公正服务拒绝，需要一种方法来检测和防止这种恶意行为。

Method: 提出攻击方法Mirage降低目标区域的置信度，同时设计防御框架Confidential Guardian，通过校准指标分析和零知识证明来检测和防止恶意行为。

Result: Confidential Guardian能有效防止谨慎预测机制的滥用，确保置信度反映真实模型不确定性而非恶意意图。

Conclusion: 论文揭示了谨慎预测机制的潜在威胁，并提供了有效的防御方案，为安全关键应用提供了保障。

Abstract: Cautious predictions -- where a machine learning model abstains when
uncertain -- are crucial for limiting harmful errors in safety-critical
applications. In this work, we identify a novel threat: a dishonest institution
can exploit these mechanisms to discriminate or unjustly deny services under
the guise of uncertainty. We demonstrate the practicality of this threat by
introducing an uncertainty-inducing attack called Mirage, which deliberately
reduces confidence in targeted input regions, thereby covertly disadvantaging
specific individuals. At the same time, Mirage maintains high predictive
performance across all data points. To counter this threat, we propose
Confidential Guardian, a framework that analyzes calibration metrics on a
reference dataset to detect artificially suppressed confidence. Additionally,
it employs zero-knowledge proofs of verified inference to ensure that reported
confidence scores genuinely originate from the deployed model. This prevents
the provider from fabricating arbitrary model confidence values while
protecting the model's proprietary details. Our results confirm that
Confidential Guardian effectively prevents the misuse of cautious predictions,
providing verifiable assurances that abstention reflects genuine model
uncertainty rather than malicious intent.

</details>


### [406] [So, I climbed to the top of the pyramid of pain -- now what?](https://arxiv.org/abs/2505.24685)
*Vasilis Katos,Emily Rosenorn-Lanng,Jane Henriksen-Bulmer,Ala Yankouskaya*

Main category: cs.CR

TL;DR: 论文提出Human Layer Kill Chain框架，结合人类心理与行为分析网络威胁，弥补传统模型的不足，并整合AI技术增强攻击中的心理操纵。


<details>
  <summary>Details</summary>
Motivation: 传统攻击模型（如Lockheed Martin的Cyber Kill Chain）无法有效应对现代攻击者利用的人类脆弱性，需新框架以整合人类因素。

Method: 提出Human Layer Kill Chain的八个阶段，结合AI技术分析心理操纵，并与Cyber Kill Chain融合为Sociotechnical Kill Plane。

Result: 框架帮助网络安全专业人员理解攻击者战术，同时使非技术人员参与威胁识别与响应，提升组织韧性。

Conclusion: Human Layer Kill Chain框架为网络安全提供更全面的视角，尤其在AI加剧威胁的背景下，对事件响应和组织韧性具有重要意义。

Abstract: This paper explores the evolving dynamics of cybersecurity in the age of
advanced AI, from the perspective of the introduced Human Layer Kill Chain
framework. As traditional attack models like Lockheed Martin's Cyber Kill Chain
become inadequate in addressing human vulnerabilities exploited by modern
adversaries, the Humal Layer Kill Chain offers a nuanced approach that
integrates human psychology and behaviour into the analysis of cyber threats.
We detail the eight stages of the Human Layer Kill Chain, illustrating how
AI-enabled techniques can enhance psychological manipulation in attacks. By
merging the Human Layer with the Cyber Kill Chain, we propose a Sociotechnical
Kill Plane that allows for a holistic examination of attackers' tactics,
techniques, and procedures (TTPs) across the sociotechnical landscape. This
framework not only aids cybersecurity professionals in understanding
adversarial methods, but also empowers non-technical personnel to engage in
threat identification and response. The implications for incident response and
organizational resilience are significant, particularly as AI continues to
shape the threat landscape.

</details>


### [407] [A comprehensive survey of cybercrimes in India over the last decade](https://arxiv.org/abs/2505.23770)
*Sudhanshu Sekhar Tripathy*

Main category: cs.CR

TL;DR: 本文探讨了自1990年代以来技术融入生活带来的网络犯罪增长，分析了其全球影响及应对措施，特别关注印度的案例。


<details>
  <summary>Details</summary>
Motivation: 随着数字技术的普及，网络犯罪成为全球性挑战，亟需研究其影响及解决方案。

Method: 通过分析网络犯罪的类型、案例及现有应对措施，提出预防策略和安全协议。

Result: 研究发现网络犯罪形式多样，影响深远，需加强公共意识、教育和法规。

Conclusion: 保护数字基础设施需多管齐下，包括技术、教育和政策层面的努力。

Abstract: Since the 1990s, the integration of technology into daily life has led to the
creation of an extensive network of interconnected devices, transforming how
individuals and organizations operate. However, this digital transformation has
also spurred the rise of cybercrime, criminal activities perpetrated through
networks or computer systems. Cybercrime has become a global concern,
presenting significant challenges to security systems. Although advancements in
digital technology have enhanced efficiency, they have also opened new avenues
for exploitation by cybercriminals, highlighting the urgent need for advanced
cybersecurity measures. The escalating number of cyberattacks and associated
risks in the past decade highlights the critical importance of protecting
sensitive data and safeguarding information systems. Cybercrimes range from
financial fraud and phishing scams to identity theft and online harassment,
posing substantial risks to both individuals and organizations. In response,
governments, law enforcement agencies, and cybersecurity units have intensified
their efforts to address these threats. In recent years, India has experienced
a significant surge in cybercrime incidents, with a notable increase in cases
involving ransomware, data breaches, and social engineering attacks. The
growing penetration of internet services, the expansion of e-commerce, and the
rapid adoption of digital payment systems have made individuals and
organizations more vulnerable to cyber threats. Key areas affected include
banking, healthcare, and government sectors, which are frequently targeted due
to the sensitive nature of the data they handle. To combat these risks, there
is an increasing focus on public awareness, cybersecurity education, and robust
regulatory frameworks. This paper examines cybercrime, prevention strategies,
security protocols, and terminology to safeguard digital infrastructure.

</details>


### [408] [Mind the Gap: A Practical Attack on GGUF Quantization](https://arxiv.org/abs/2505.23786)
*Kazuki Egashira,Robin Staab,Mark Vero,Jingxuan He,Martin Vechev*

Main category: cs.CR

TL;DR: 论文提出了一种针对GGUF量化方法的攻击，利用量化误差构造恶意量化模型，并在三种攻击场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着前沿大语言模型规模的增加，后训练量化成为内存高效部署的标准方法。然而，现有攻击无法应用于复杂量化方法（如GGUF），因此需要填补这一空白。

Method: 通过量化误差（全精度权重与量化版本的差异）构造恶意量化模型，并在训练目标恶意LLM时约束其权重。

Result: 在三种攻击场景（不安全代码生成、目标内容注入和良性指令拒绝）中，攻击效果显著（Δ分别为88.7%、85.0%和30.1%）。

Conclusion: GGUF量化方法易受对抗性干扰，仅依赖量化方案的复杂性不足以作为防御手段。

Abstract: With the increasing size of frontier LLMs, post-training quantization has
become the standard for memory-efficient deployment. Recent work has shown that
basic rounding-based quantization schemes pose security risks, as they can be
exploited to inject malicious behaviors into quantized models that remain
hidden in full precision. However, existing attacks cannot be applied to more
complex quantization methods, such as the GGUF family used in the popular
ollama and llama.cpp frameworks. In this work, we address this gap by
introducing the first attack on GGUF. Our key insight is that the quantization
error -- the difference between the full-precision weights and their
(de-)quantized version -- provides sufficient flexibility to construct
malicious quantized models that appear benign in full precision. Leveraging
this, we develop an attack that trains the target malicious LLM while
constraining its weights based on quantization errors. We demonstrate the
effectiveness of our attack on three popular LLMs across nine GGUF quantization
data types on three diverse attack scenarios: insecure code generation
($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign
instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the
most widely used post-training quantization method is susceptible to
adversarial interferences, and (2) the complexity of quantization schemes alone
is insufficient as a defense.

</details>


### [409] [Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning](https://arxiv.org/abs/2505.23791)
*Sayyed Farid Ahamed,Sandip Roy,Soumya Banerjee,Marc Vucovich,Kevin Choi,Abdul Rahman,Alison Hu,Edward Bowen,Sachin Shetty*

Main category: cs.CR

TL;DR: 该论文研究了联邦学习（FL）在模型提取（ME）攻击下的脆弱性，通过实验验证攻击效果与查询集大小的关系，并探索了基于迁移学习的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习旨在保护客户数据隐私，但其分布式特性使其容易受到模型提取攻击，威胁机器学习即服务（MLaaS）平台的知识产权安全。

Method: 在NVFlare平台上构建不同联邦客户端，实施两种模型提取攻击，评估攻击性能（如准确率、保真度和KL散度），并探索基于迁移学习的攻击方法。

Result: 实验表明，提取模型的准确率和保真度与攻击查询集大小密切相关；基于迁移学习的攻击方法在小查询集下表现更优。

Conclusion: 联邦学习对模型提取攻击高度脆弱，迁移学习方法为攻击者提供了潜在优势，需加强防御措施。

Abstract: Federated Learning (FL) is a collaborative learning framework designed to
protect client data, yet it remains highly vulnerable to Intellectual Property
(IP) threats. Model extraction (ME) attacks pose a significant risk to Machine
Learning as a Service (MLaaS) platforms, enabling attackers to replicate
confidential models by querying black-box (without internal insight) APIs.
Despite FL's privacy-preserving goals, its distributed nature makes it
particularly susceptible to such attacks. This paper examines the vulnerability
of FL-based victim models to two types of model extraction attacks. For various
federated clients built under the NVFlare platform, we implemented ME attacks
across two deep learning architectures and three image datasets. We evaluate
the proposed ME attack performance using various metrics, including accuracy,
fidelity, and KL divergence. The experiments show that for different FL
clients, the accuracy and fidelity of the extracted model are closely related
to the size of the attack query set. Additionally, we explore a transfer
learning based approach where pretrained models serve as the starting point for
the extraction process. The results indicate that the accuracy and fidelity of
the fine-tuned pretrained extraction models are notably higher, particularly
with smaller query sets, highlighting potential advantages for attackers.

</details>


### [410] [Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things](https://arxiv.org/abs/2505.23792)
*Kai Li,Conggai Li,Xin Yuan,Shenghong Li,Sai Zou,Syed Sohail Ahmed,Wei Ni,Dusit Niyato,Abbas Jamalipour,Falko Dressler,Ozgur B. Akan*

Main category: cs.CR

TL;DR: 论文提出了一种新型的零信任基础模型（ZTFMs），将零信任安全原则融入基础模型（FMs）的全生命周期，以提升物联网（IoT）系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统物联网架构依赖信任假设，易受攻击。ZTFMs旨在通过零信任原则（如持续验证、最小权限访问等）构建更安全、隐私保护的AI系统。

Method: 提出技术框架，结合联邦学习（FL）、区块链身份管理、微隔离和可信执行环境（TEEs），支持去中心化、可验证的边缘智能。

Result: ZTFMs能有效应对物联网环境中的安全威胁，如异常检测和对抗训练，但仍面临可扩展性、动态信任校准等挑战。

Conclusion: ZTFMs为构建安全、智能、可信的物联网基础设施提供了基础路线图。

Abstract: This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm
that embeds zero-trust security principles into the lifecycle of foundation
models (FMs) for Internet of Things (IoT) systems. By integrating core tenets,
such as continuous verification, least privilege access (LPA), data
confidentiality, and behavioral analytics into the design, training, and
deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across
distributed, heterogeneous, and potentially adversarial IoT environments. We
present the first structured synthesis of ZTFMs, identifying their potential to
transform conventional trust-based IoT architectures into resilient,
self-defending ecosystems. Moreover, we propose a comprehensive technical
framework, incorporating federated learning (FL), blockchain-based identity
management, micro-segmentation, and trusted execution environments (TEEs) to
support decentralized, verifiable intelligence at the network edge. In
addition, we investigate emerging security threats unique to ZTFM-enabled
systems and evaluate countermeasures, such as anomaly detection, adversarial
training, and secure aggregation. Through this analysis, we highlight key open
research challenges in terms of scalability, secure orchestration,
interpretable threat attribution, and dynamic trust calibration. This survey
lays a foundational roadmap for secure, intelligent, and trustworthy IoT
infrastructures powered by FMs.

</details>


### [411] [USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2505.23793)
*Baolin Zheng,Guanlin Chen,Hongqiong Zhong,Qingyang Teng,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Huiyun Jing,Jincheng Wei,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.CR

TL;DR: 论文提出了一个名为USB的统一安全基准，旨在解决现有MLLM安全评估基准在数据质量、覆盖范围和模态组合上的不足，同时兼顾漏洞和过度敏感性评估。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM安全基准存在数据质量低、覆盖不全和模态组合单一的问题，导致评估结果不准确，阻碍了安全问题的发现和治理。

Method: 通过设计复杂的数据合成流程，结合开源数据集和合成数据，生成了高质量、全面的评估数据，覆盖61个风险子类别和4种模态组合。

Result: USB基准是目前最全面的MLLM安全评估工具，支持中英文，涵盖漏洞和过度敏感性两个维度。

Conclusion: USB基准填补了现有评估工具的空白，为MLLM的安全评估提供了更全面和准确的工具。

Abstract: Despite their remarkable achievements and widespread adoption, Multimodal
Large Language Models (MLLMs) have revealed significant security
vulnerabilities, highlighting the urgent need for robust safety evaluation
benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of
data quality and coverge, and modal risk combinations, resulting in inflated
and contradictory evaluation results, which hinders the discovery and
governance of security concerns. Besides, we argue that vulnerabilities to
harmful queries and oversensitivity to harmless ones should be considered
simultaneously in MLLMs safety evaluation, whereas these were previously
considered separately. In this paper, to address these shortcomings, we
introduce Unified Safety Benchmarks (USB), which is one of the most
comprehensive evaluation benchmarks in MLLM safety. Our benchmark features
high-quality queries, extensive risk categories, comprehensive modal
combinations, and encompasses both vulnerability and oversensitivity
evaluations. From the perspective of two key dimensions: risk categories and
modality combinations, we demonstrate that the available benchmarks -- even the
union of the vast majority of them -- are far from being truly comprehensive.
To bridge this gap, we design a sophisticated data synthesis pipeline that
generates extensive, high-quality complementary data addressing previously
unexplored aspects. By combining open-source datasets with our synthetic data,
our benchmark provides 4 distinct modality combinations for each of the 61 risk
sub-categories, covering both English and Chinese across both vulnerability and
oversensitivity dimensions.

</details>


### [412] [MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection](https://arxiv.org/abs/2505.23803)
*Yinuo Xue,Eric Spero,Yun Sing Koh,Giovanni Russello*

Main category: cs.CR

TL;DR: MultiPhishGuard是一个基于LLM的多智能体钓鱼邮件检测系统，通过协同专家知识和对抗性强化学习，显著提升检测准确率并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件检测面临对抗性策略和多样化攻击模式的挑战，传统方法难以应对，机器学习方法也难以及时适应新策略。

Method: 采用五个协作智能体（文本、URL、元数据、解释简化器和对抗性智能体），结合近端策略优化强化学习算法，动态调整决策权重，并通过对抗性训练循环增强系统鲁棒性。

Result: 在公开数据集上，MultiPhishGuard准确率达97.89%，误报率2.73%，漏报率0.20%，优于现有方法。

Conclusion: MultiPhishGuard通过动态多智能体协作和生成对抗性韧性，推动了钓鱼防御技术的进步。

Abstract: Phishing email detection faces critical challenges from evolving adversarial
tactics and heterogeneous attack patterns. Traditional detection methods, such
as rule-based filters and denylists, often struggle to keep pace with these
evolving tactics, leading to false negatives and compromised security. While
machine learning approaches have improved detection accuracy, they still face
challenges adapting to novel phishing strategies. We present MultiPhishGuard, a
dynamic LLM-based multi-agent detection system that synergizes specialized
expertise with adversarial-aware reinforcement learning. Our framework employs
five cooperative agents (text, URL, metadata, explanation simplifier, and
adversarial agents) with automatically adjusted decision weights powered by a
Proximal Policy Optimization reinforcement learning algorithm. To address
emerging threats, we introduce an adversarial training loop featuring an
adversarial agent that generates subtle context-aware email variants, creating
a self-improving defense ecosystem and enhancing system robustness.
Experimental evaluations on public datasets demonstrate that MultiPhishGuard
significantly outperforms Chain-of-Thoughts, single-agent baselines and
state-of-the-art detectors, as validated by ablation studies and comparative
analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy
(97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%).
Additionally, we incorporate an explanation simplifier agent, which provides
users with clear and easily understandable explanations for why an email is
classified as phishing or legitimate. This work advances phishing defense
through dynamic multi-agent collaboration and generative adversarial
resilience.

</details>


### [413] [ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes](https://arxiv.org/abs/2505.23805)
*Akram Sheriff,Ken Huang,Zsolt Nemeth,Madjid Nakhjiri*

Main category: cs.CR

TL;DR: ADA是一种创新的自动化移动目标防御系统，通过持续轮换Kubernetes pods来增强AI工作负载的安全性，颠覆传统静态防御模式。


<details>
  <summary>Details</summary>
Motivation: 传统静态防御依赖复杂昂贵的可信计算解决方案，无法满足AI生态系统的动态需求，ADA旨在提供更灵活、高效的零信任安全模型。

Method: ADA利用Kubernetes pods的短暂性，持续自动轮换AI工作负载，通过环境主动操纵而非被动修补实现安全。

Result: ADA提供了一种更健壮、敏捷且操作高效的零信任模型，支持最新的AI生态系统和技术。

Conclusion: ADA通过环境主动操纵实现了更高效的AI服务安全，为动态防御提供了新范式。

Abstract: This paper introduces the Adaptive Defense Agent (ADA), an innovative
Automated Moving Target Defense (AMTD) system designed to fundamentally enhance
the security posture of AI workloads. ADA operates by continuously and
automatically rotating these workloads at the infrastructure level, leveraging
the inherent ephemerality of Kubernetes pods. This constant managed churn
systematically invalidates attacker assumptions and disrupts potential kill
chains by regularly destroying and respawning AI service instances. This
methodology, applying principles of chaos engineering as a continuous,
proactive defense, offers a paradigm shift from traditional static defenses
that rely on complex and expensive confidential or trusted computing solutions
to secure the underlying compute platforms, while at the same time agnostically
supporting the latest advancements in agentic and nonagentic AI ecosystems and
solutions such as agent-to-agent (A2A) communication frameworks or model
context protocols (MCP). This AI-native infrastructure design, relying on the
widely proliferated cloud-native Kubernetes technologies, facilitates easier
deployment, simplifies maintenance through an inherent zero trust posture
achieved by rotation, and promotes faster adoption. We posit that ADA's novel
approach to AMTD provides a more robust, agile, and operationally efficient
zero-trust model for AI services, achieving security through proactive
environmental manipulation rather than reactive patching.

</details>


### [414] [DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries](https://arxiv.org/abs/2505.23813)
*Abhijit Talluri*

Main category: cs.CR

TL;DR: DP-RTFL是一种结合本地差分隐私和弹性时间状态管理的联邦学习框架，旨在解决标准FL在容错性、隐私保护和数据完整性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习在容错性、隐私保护和数据完整性方面存在不足，可能导致训练中断、数据丢失或违反隐私法规。

Method: DP-RTFL整合了本地差分隐私（LDP）、弹性时间状态管理和完整性验证机制（如ZKIPs）。

Result: 该框架适用于敏感数据处理（如金融风险评估），具有操作稳健性、可审计性和可扩展性。

Conclusion: DP-RTFL为关键应用提供了隐私保护、容错性和数据完整性的解决方案，并已开源。

Abstract: Federated Learning (FL) has emerged as a critical paradigm for enabling
privacy-preserving machine learning, particularly in regulated sectors such as
finance and healthcare. However, standard FL strategies often encounter
significant operational challenges related to fault tolerance, system
resilience against concurrent client and server failures, and the provision of
robust, verifiable privacy guarantees essential for handling sensitive data.
These deficiencies can lead to training disruptions, data loss, compromised
model integrity, and non-compliance with data protection regulations (e.g.,
GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal
Federated Learning (DP-RTFL), an advanced FL framework designed to ensure
training continuity, precise state recovery, and strong data privacy. DP-RTFL
integrates local Differential Privacy (LDP) at the client level with resilient
temporal state management and integrity verification mechanisms, such as
hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs
in this context). The framework is particularly suited for critical
applications like credit risk assessment using sensitive financial data, aiming
to be operationally robust, auditable, and scalable for enterprise AI
deployments. The implementation of the DP-RTFL framework is available as
open-source.

</details>


### [415] [Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems](https://arxiv.org/abs/2505.23847)
*Ronny Ko,Jiseong Jeong,Shuyuan Zheng,Chuan Xiao,Taewan Kim,Makoto Onizuka,Wonyong Shin*

Main category: cs.CR

TL;DR: 该立场论文探讨了跨领域多智能体LLM系统的安全挑战，提出了七类新问题，并提供了攻击案例、评估指标和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM发展为跨组织合作的自主代理，当前的信任假设和防护技术无法应对跨领域协作带来的安全风险。

Method: 论文通过分析多智能体动态，识别了七类安全挑战，并为每类提供了攻击示例和评估方法。

Result: 提出了跨领域LLM系统的安全议程，包括攻击案例、评估指标和研究指南。

Conclusion: 跨领域多智能体LLM系统需要新的安全框架，以应对由多智能体动态引发的风险。

Abstract: Large language models (LLMs) are rapidly evolving into autonomous agents that
cooperate across organizational boundaries, enabling joint disaster response,
supply-chain optimization, and other tasks that demand decentralized expertise
without surrendering data ownership. Yet, cross-domain collaboration shatters
the unified trust assumptions behind current alignment and containment
techniques. An agent benign in isolation may, when receiving messages from an
untrusted peer, leak secrets or violate policy, producing risks driven by
emergent multi-agent dynamics rather than classical software bugs. This
position paper maps the security agenda for cross-domain multi-agent LLM
systems. We introduce seven categories of novel security challenges, for each
of which we also present plausible attacks, security evaluation metrics, and
future research guidelines.

</details>


### [416] [CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2505.23849)
*Kaveen Hiniduma,Zilinghan Li,Aditya Sinha,Ravi Madduri,Suren Byna*

Main category: cs.CR

TL;DR: CADRE框架为联邦学习提供自定义数据准备标准，确保数据质量、隐私和公平性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据质量难以保证，且隐私限制导致数据访问受限，需一种灵活的数据准备框架。

Method: 提出CADRE框架，支持用户自定义数据准备标准、规则和补救措施，生成全面报告。

Result: 在六个数据集上验证，有效解决七种数据准备问题，提升数据质量和模型可靠性。

Conclusion: CADRE能灵活应对联邦学习中的数据准备挑战，优化模型性能并节省资源。

Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine
learning approach where multiple clients train a model collaboratively. PPFL
preserves privacy and security of the client's data by not exchanging it.
However, ensuring that data at each client is of high quality and ready for
federated learning (FL) is a challenge due to restricted data access. In this
paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a
novel framework that allows users to define custom data readiness (DR)
standards, metrics, rules, and remedies tailored to specific FL tasks. Our
framework generates comprehensive DR reports based on the user-defined metrics,
rules, and remedies to ensure datasets are optimally prepared for FL while
preserving privacy. We demonstrate the framework's practical application by
integrating it into an existing PPFL framework. We conducted experiments across
six diverse datasets, addressing seven different DR issues. The results
illustrate the framework's versatility and effectiveness in ensuring DR across
various dimensions, including data quality, privacy, and fairness. This
approach enhances the performance and reliability of FL models as well as
utilizes valuable resources by identifying and addressing data-related issues
before the training phase.

</details>


### [417] [KGMark: A Diffusion Watermark for Knowledge Graphs](https://arxiv.org/abs/2505.23873)
*Hongrui Peng,Haolang Lu,Yuanlong Yu,Weiye Fu,Kun Wang,Guoshun Nan*

Main category: cs.CR

TL;DR: KGMARK是首个针对动态知识图谱的水印框架，通过聚类对齐和冗余嵌入策略解决时空变化问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法难以适应动态图谱的时空变化，需开发新方法保护知识产权。

Method: 提出KGMARK框架，包括聚类对齐、冗余嵌入策略和可学习掩码矩阵。

Result: 实验表明KGMARK能有效应对动态图谱的时空变化。

Conclusion: KGMARK为动态知识图谱提供了一种鲁棒、可检测且透明的水印解决方案。

Abstract: Knowledge graphs (KGs) are ubiquitous in numerous real-world applications,
and watermarking facilitates protecting intellectual property and preventing
potential harm from AI-generated content. Existing watermarking methods mainly
focus on static plain text or image data, while they can hardly be applied to
dynamic graphs due to spatial and temporal variations of structured data. This
motivates us to propose KGMARK, the first graph watermarking framework that
aims to generate robust, detectable, and transparent diffusion fingerprints for
dynamic KG data. Specifically, we propose a novel clustering-based alignment
method to adapt the watermark to spatial variations. Meanwhile, we present a
redundant embedding strategy to harden the diffusion watermark against various
attacks, facilitating the robustness of the watermark to the temporal
variations. Additionally, we introduce a novel learnable mask matrix to improve
the transparency of diffusion fingerprints. By doing so, our KGMARK properly
tackles the variation challenges of structured data. Experiments on various
public benchmarks show the effectiveness of our proposed KGMARK.

</details>


### [418] [LLM Agents Should Employ Security Principles](https://arxiv.org/abs/2505.24019)
*Kaiyuan Zhang,Zian Su,Pin-Yu Chen,Elisa Bertino,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: 论文主张在部署大规模LLM代理时应用信息安全设计原则，提出AgentSandbox框架以保障代理生命周期安全，评估显示其在高效用下显著降低隐私风险。


<details>
  <summary>Details</summary>
Motivation: LLM代理在多代理交互中易受提示注入和上下文操纵攻击，导致隐私泄露和系统漏洞，需引入信息安全原则以增强安全性。

Method: 提出AgentSandbox框架，嵌入防御深度、最小权限等安全原则，评估其在良性/对抗场景下的效用和攻击成功率。

Result: AgentSandbox在保持高功能效用的同时显著降低隐私风险，支持可信代理生态系统的构建。

Conclusion: 将安全设计原则嵌入LLM代理协议可提升系统安全性，满足隐私和监管要求。

Abstract: Large Language Model (LLM) agents show considerable promise for automating
complex tasks using contextual reasoning; however, interactions involving
multiple agents and the system's susceptibility to prompt injection and other
forms of context manipulation introduce new vulnerabilities related to privacy
leakage and system exploitation. This position paper argues that the
well-established design principles in information security, which are commonly
referred to as security principles, should be employed when deploying LLM
agents at scale. Design principles such as defense-in-depth, least privilege,
complete mediation, and psychological acceptability have helped guide the
design of mechanisms for securing information systems over the last five
decades, and we argue that their explicit and conscientious adoption will help
secure agentic systems. To illustrate this approach, we introduce AgentSandbox,
a conceptual framework embedding these security principles to provide
safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art
LLMs along three dimensions: benign utility, attack utility, and attack success
rate. AgentSandbox maintains high utility for its intended functions under both
benign and adversarial evaluations while substantially mitigating privacy
risks. By embedding secure design principles as foundational elements within
emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems
aligned with user privacy expectations and evolving regulatory requirements.

</details>


### [419] [A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming](https://arxiv.org/abs/2505.24252)
*Yizhong Ding*

Main category: cs.CR

TL;DR: 论文提出了一种奖励驱动的自动化WebShell恶意代码生成器（RAWG），通过分类和标准化样本，结合LLM和强化学习，显著提升了恶意代码的多样性和逃避效果。


<details>
  <summary>Details</summary>
Motivation: 现有恶意代码生成方法多样性不足且冗余高，缺乏公开分类的数据集，因此需要一种更高效的生成方法。

Method: 1. 分类WebShell样本；2. 使用LLM提取关键令牌并标准化；3. 通过监督微调和PPO优化生成质量。

Result: RAWG在多样性和逃避效果上显著优于现有方法。

Conclusion: RAWG为红队应用提供了一种高效的恶意代码生成解决方案。

Abstract: Frequent cyber-attacks have elevated WebShell exploitation and defense to a
critical research focus within network security. However, there remains a
significant shortage of publicly available, well-categorized malicious-code
datasets organized by obfuscation method. Existing malicious-code generation
methods, which primarily rely on prompt engineering, often suffer from limited
diversity and high redundancy in the payloads they produce. To address these
limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven
\textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed
for red-teaming applications. Our approach begins by categorizing webshell
samples from common datasets into seven distinct types of obfuscation. We then
employ a large language model (LLM) to extract and normalize key tokens from
each sample, creating a standardized, high-quality corpus. Using this curated
dataset, we perform supervised fine-tuning (SFT) on an open-source large model
to enable the generation of diverse, highly obfuscated webshell malicious
payloads. To further enhance generation quality, we apply Proximal Policy
Optimization (PPO), treating malicious-code samples as "chosen" data and benign
code as "rejected" data during reinforcement learning. Extensive experiments
demonstrate that RAWG significantly outperforms current state-of-the-art
methods in both payload diversity and escape effectiveness.

</details>


### [420] [PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches](https://arxiv.org/abs/2505.24703)
*Dennis Jacob,Chong Xiang,Prateek Mittal*

Main category: cs.CR

TL;DR: PatchDEMUX是一个可证明鲁棒的多标签分类框架，通过将多标签任务分解为多个二分类问题，扩展了单标签分类的防御方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受对抗性补丁攻击，现有防御方法多针对单标签分类，多标签分类的防御研究较少。

Method: 将多标签分类任务分解为多个独立的二分类问题，并基于单标签防御方法（如PatchCleanser）扩展，提出更紧的鲁棒性边界。

Result: 在MS-COCO和PASCAL VOC数据集上实现了非平凡的鲁棒性，同时保持较高的干净性能。

Conclusion: PatchDEMUX是一种通用且有效的多标签分类防御框架，填补了现有防御方法的空白。

Abstract: Deep learning techniques have enabled vast improvements in computer vision
technologies. Nevertheless, these models are vulnerable to adversarial patch
attacks which catastrophically impair performance. The physically realizable
nature of these attacks calls for certifiable defenses, which feature provable
guarantees on robustness. While certifiable defenses have been successfully
applied to single-label classification, limited work has been done for
multi-label classification. In this work, we present PatchDEMUX, a certifiably
robust framework for multi-label classifiers against adversarial patches. Our
approach is a generalizable method which can extend any existing certifiable
defense for single-label classification; this is done by considering the
multi-label classification task as a series of isolated binary classification
problems to provably guarantee robustness. Furthermore, in the scenario where
an attacker is limited to a single patch we propose an additional certification
procedure that can provide tighter robustness bounds. Using the current
state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a
backbone, we find that PatchDEMUX can achieve non-trivial robustness on the
MS-COCO and PASCAL VOC datasets while maintaining high clean performance

</details>


### [421] [LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs](https://arxiv.org/abs/2505.24451)
*Luis Ibanez-Lissen,Lorena Gonzalez-Manzano,Jose Maria de Fuentes,Nicolas Anciaux*

Main category: cs.CR

TL;DR: 论文提出了一种名为LPASS的方法，利用线性探针（LPs）在压缩和微调前评估LLM性能，并优化层剪枝压缩。实验表明，该方法在BERT和Gemma上高效且有效，显著提升了漏洞检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了在压缩和微调大型语言模型（LLMs）前提前评估其性能，减少计算资源消耗，并优化压缩策略。

Method: 使用线性探针（LPs）在压缩和微调前评估模型性能，并确定层剪枝的截断点。方法LPASS应用于BERT和Gemma，检测MITRE Top 25漏洞。

Result: 实验显示，LPASS可高效移除33.3%和72.2%的层而不损失精度，提供早期性能估计，并在多类漏洞检测中达到86.9%的准确率。压缩后的Gemma模型性能提升1.6% F1-score，同时节省29.4%训练时间和42.98%模型大小。

Conclusion: LPASS方法在LLM压缩和漏洞检测中表现出色，显著提升了效率和性能，为实际应用提供了实用工具。

Abstract: Large Language Models (LLMs) are being extensively used for cybersecurity
purposes. One of them is the detection of vulnerable codes. For the sake of
efficiency and effectiveness, compression and fine-tuning techniques are being
developed, respectively. However, they involve spending substantial
computational efforts. In this vein, we analyse how Linear Probes (LPs) can be
used to provide an estimation on the performance of a compressed LLM at an
early phase -- before fine-tuning. We also show their suitability to set the
cut-off point when applying layer pruning compression. Our approach, dubbed
$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25
most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in
142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be
removed, respectively, with no precision loss; (2) they provide an early
estimate of the post-fine-tuning and post-compression model effectiveness, with
3\% and 8.68\% as the lowest and average precision errors, respectively.
$LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy
in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed
versions of Gemma outperform the original ones by 1.6\% of F1-score at a
maximum while saving 29.4 \% and 23.8\% of training and inference time and
42.98\% of model size.

</details>


### [422] [CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control](https://arxiv.org/abs/2505.24536)
*Chaohui Xu,Qi Cui,Chip-Hong Chang*

Main category: cs.CR

TL;DR: CHIP框架利用变色龙哈希函数实现DNN模型的全面保护，包括所有权验证、用户追踪和主动使用控制。


<details>
  <summary>Details</summary>
Motivation: 大规模DNN的高训练成本使其知识产权保护至关重要，现有方法无法提供全面保护。

Method: 提出基于变色龙哈希的不可逆护照（CHIP）框架，结合密码学特性实现多目标保护。

Result: 在四个数据集和两种架构上验证了CHIP的全面保护能力和鲁棒性。

Conclusion: CHIP为DNN模型提供了高效、安全的IP保护解决方案。

Abstract: The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous
training costs make their intellectual property (IP) protection of paramount
importance. Recently introduced passport-based methods attempt to steer DNN
watermarking towards strengthening ownership verification against ambiguity
attacks by modulating the affine parameters of normalization layers.
Unfortunately, neither watermarking nor passport-based methods provide a
holistic protection with robust ownership proof, high fidelity, active usage
authorization and user traceability for offline access distributed models and
multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we
propose a Chameleon Hash-based Irreversible Passport (CHIP) protection
framework that utilizes the cryptographic chameleon hash function to achieve
all these goals. The collision-resistant property of chameleon hash allows for
strong model ownership claim upon IP infringement and liable user traceability,
while the trapdoor-collision property enables hashing of multiple user
passports and licensee certificates to the same immutable signature to realize
active usage control. Using the owner passport as an oracle, multiple
user-specific triplets, each contains a passport-aware user model, a user
passport, and a licensee certificate can be created for secure offline
distribution. The watermarked master model can also be deployed for MLaaS with
usage permission verifiable by the provision of any trapdoor-colliding user
passports. CHIP is extensively evaluated on four datasets and two architectures
to demonstrate its protection versatility and robustness. Our code is released
at https://github.com/Dshm212/CHIP.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [423] [Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach](https://arxiv.org/abs/2505.23953)
*Melika Sepidband,Hamed Taherkhani,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: 研究探讨了代码复杂度与LLM生成代码成功率的关系，提出基于复杂度指标的迭代反馈方法，显著提升了代码生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注提升LLM代码生成效果，但缺乏对生成代码特性的分析和利用。

Method: 通过复杂度指标分析LLM生成代码的成功率，并设计迭代反馈方法，利用复杂度信息优化生成过程。

Result: 实验表明，复杂度感知反馈显著提升了Pass@1指标，尤其在较小LLM上效果更明显。

Conclusion: 复杂度感知反馈能有效提升LLM代码生成的直接提示和基于代理的工作流。

Abstract: Automatic code generation has gained significant momentum with the advent of
Large Language Models (LLMs) such as GPT-4. Although many studies focus on
improving the effectiveness of LLMs for code generation, very limited work
tries to understand the generated code's characteristics and leverage that to
improve failed cases. In this paper, as the most straightforward characteristic
of code, we investigate the relationship between code complexity and the
success of LLM generated code. Using a large set of standard complexity
metrics, we first conduct an empirical analysis to explore their correlation
with LLM's performance on code generation (i.e., Pass@1). Using logistic
regression models, we identify which complexity metrics are most predictive of
code correctness. Building on these findings, we propose an iterative feedback
method, where LLMs are prompted to generate correct code based on complexity
metrics from previous failed outputs. We validate our approach across multiple
benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs
(i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the
results with two baseline methods: (a) zero-shot generation, and (b) iterative
execution-based feedback without our code complexity insights. Experiment
results show that our approach makes notable improvements, particularly with a
smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to
the baseline's improvement of 12.5% on the HumanEval dataset. The study expands
experiments to BigCodeBench and integrates the method with the Reflexion code
generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07%
(GPT-o3 mini). The results highlight that complexity-aware feedback enhances
both direct LLM prompting and agent-based workflows.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [424] [Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning](https://arxiv.org/abs/2505.24099)
*Mohammad Shah Alam,William Ott,Ilya Timofeyev*

Main category: math.DS

TL;DR: 本文探讨了回声状态网络（ESN）在广义Kuramoto-Sivashinsky（gKS）方程中的预测能力，提出了一种结合迁移学习的新方法，以提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过迁移学习增强ESN在不同参数设置下对gKS模型长期统计模式的预测能力。

Method: 提出了一种结合ESN和迁移学习的新方法，用于适应不同参数设置并捕捉混沌吸引子的变化。

Result: 成功利用迁移学习使ESN适应不同参数，并捕捉到混沌吸引子的变化。

Conclusion: 该方法有效提升了ESN在gKS模型中的预测性能，尤其在长期统计模式变化方面。

Abstract: In this paper, we explore the predictive capabilities of echo state networks
(ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal
nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel
methodology that integrates ESNs with transfer learning, aiming to enhance
predictive performance across various parameter regimes of the gKS model. Our
research focuses on predicting changes in long-term statistical patterns of the
gKS model that result from varying the dispersion relation or the length of the
spatial domain. We use transfer learning to adapt ESNs to different parameter
settings and successfully capture changes in the underlying chaotic attractor.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [425] [Distributed Intelligence in the Computing Continuum with Active Inference](https://arxiv.org/abs/2505.24618)
*Victor Casamayor Pujol,Boris Sedlak,Tommaso Salvatori,Karl Friston,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文提出了一种基于主动推理（AIF）的分布式流处理管道，用于管理计算连续体（CC）中的服务，展示了AIF代理在满足设备感知服务级别目标（SLOiDs）方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 计算连续体（CC）的分布式、异构和动态特性需要分布式智能以实现自适应和弹性的服务管理。

Method: 通过AIF代理管理分布式流处理管道中的服务，每个代理协作满足SLOiDs。

Result: 实验表明，AIF代理在测试的转移模型下实现超过90%的SLOiDs满足率，部署时学习模型时约为80%。与多智能体强化学习（MARL）相比，AIF代理无需训练即可有效运行。

Conclusion: AIF代理在CC平台中表现出强大的适应能力，未来研究应进一步推进AIF在CC中的集成。

Abstract: The Computing Continuum (CC) is an emerging Internet-based computing paradigm
that spans from local Internet of Things sensors and constrained edge devices
to large-scale cloud data centers. Its goal is to orchestrate a vast array of
diverse and distributed computing resources to support the next generation of
Internet-based applications. However, the distributed, heterogeneous, and
dynamic nature of CC platforms demands distributed intelligence for adaptive
and resilient service management. This article introduces a distributed stream
processing pipeline as a CC use case, where each service is managed by an
Active Inference (AIF) agent. These agents collaborate to fulfill service needs
specified by SLOiDs, a term we introduce to denote Service Level Objectives
that are aware of its deployed devices, meaning that non-functional
requirements must consider the characteristics of the hosting device. We
demonstrate how AIF agents can be modeled and deployed alongside distributed
services to manage them autonomously. Our experiments show that AIF agents
achieve over 90% SLOiD fulfillment when using tested transition models, and
around 80% when learning the models during deployment. We compare their
performance to a multi-agent reinforcement learning algorithm, finding that
while both approaches yield similar results, MARL requires extensive training,
whereas AIF agents can operate effectively from the start. Additionally, we
evaluate the behavior of AIF agents in offloading scenarios, observing a strong
capacity for adaptation. Finally, we outline key research directions to advance
AIF integration in CC platforms.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [426] [More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI](https://arxiv.org/abs/2505.23780)
*Émilie Fabre,Katie Seaborn,Shuta Koiwai,Mizuki Watanabe,Paul Riesch*

Main category: cs.HC

TL;DR: 研究探讨了用户与生成式AI（GenAI）叙事代理的长期互动，发现用户对AI的创造力和反思机会表示欣赏，但对叙事连贯性和控制力的不足感到沮丧。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在长期叙事互动中的潜力及其对用户体验的影响。

Method: 使用日记研究法，28名参与者通过“Dreamsmithy”应用与AI叙述者“Makoto”共同创作故事，为期两周。

Result: 用户对AI的创造力和反思机会表示欣赏，但对叙事连贯性和控制力的不足感到沮丧。研究揭示了“摇摆不定的矛盾”和“社会时间绑定”等主题。

Conclusion: 生成式AI在长期叙事中具有潜力，但也引发了对用户代理和伦理问题的关注。研究为开发适应性更强的叙事系统提供了初步见解。

Abstract: Longitudinal engagement with generative AI (GenAI) storytelling agents is a
timely but less charted domain. We explored multi-generational experiences with
"Dreamsmithy," a daily dream-crafting app, where participants (N = 28)
co-created stories with AI narrator "Makoto" every day. Reflections and
interactions were captured through a two-week diary study. Reflexive thematic
analysis revealed themes likes "oscillating ambivalence" and
"socio-chronological bonding," highlighting the complex dynamics that emerged
between individuals and the AI narrator over time. Findings suggest that while
people appreciated the personal notes, opportunities for reflection, and AI
creativity, limitations in narrative coherence and control occasionally caused
frustration. The results underscore the potential of GenAI for longitudinal
storytelling, but also raise critical questions about user agency and ethics.
We contribute initial empirical insights and design considerations for
developing adaptive, more-than-human storytelling systems.

</details>


### [427] [Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models](https://arxiv.org/abs/2505.23997)
*Pranav Rao,Maryam Taj,Alex Mariakakis,Joseph Jay Williams,Ananya Bhattacharjee*

Main category: cs.HC

TL;DR: 研究探讨了如何利用大型语言模型（LLM）结合数字日历数据，为学生提供及时且个性化的压力管理支持。


<details>
  <summary>Details</summary>
Motivation: 现有压力管理工具未能考虑学生日常生活的时机和情境特异性，常提供静态或不匹配的支持。数字日历包含丰富的个人责任指标，但未被充分利用于适应性健康干预。

Method: 通过为期一周的研究，使用功能性技术探针为八名大学生生成基于日历事件的每日压力管理消息，并进行半结构化访谈和主题分析。

Result: 参与者重视优先处理压力事件的干预措施，并偏好简洁且口语化的表达方式。

Conclusion: 研究揭示了LLM压力管理工具的关键设计启示，包括结构化提问和语气校准的必要性，以提高相关性和信任度。

Abstract: Existing stress-management tools fail to account for the timing and
contextual specificity of students' daily lives, often providing static or
misaligned support. Digital calendars contain rich, personal indicators of
upcoming responsibilities, yet this data is rarely leveraged for adaptive
wellbeing interventions. In this short paper, we explore how large language
models (LLMs) might use digital calendar data to deliver timely and
personalized stress support. We conducted a one-week study with eight
university students using a functional technology probe that generated daily
stress-management messages based on participants' calendar events. Through
semi-structured interviews and thematic analysis, we found that participants
valued interventions that prioritized stressful events and adopted a concise,
but colloquial tone. These findings reveal key design implications for
LLM-based stress-management tools, including the need for structured
questioning and tone calibration to foster relevance and trust.

</details>


### [428] [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)
*Amanda Chan,Catherine Di,Joseph Rupertus,Gary Smith,Varun Nagaraj Rao,Manoel Horta Ribeiro,Andrés Monroy-Hernández*

Main category: cs.HC

TL;DR: 论文提出了一种基于数字孪生的混合框架，以解决众包平台中生成式AI工具对数据真实性和工人角色的影响。


<details>
  <summary>Details</summary>
Motivation: 众包平台中生成式AI工具的广泛使用威胁了数据的真实性和工人的角色，需要一种解决方案来平衡自动化和人类参与。

Method: 提出了一种使用数字孪生的混合框架，通过个性化AI模型模拟工人行为，同时保持人类参与。实验和访谈评估了该框架的效果。

Result: 数字孪生能提高生产力、减少决策疲劳，同时保持响应质量。透明度和工人自主权是关键因素。

Conclusion: 数字孪生框架有助于在扩展性和真实性之间取得平衡，同时强调了透明度和伦理数据使用的重要性。

Abstract: Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for
research, yet workers' growing use of generative AI tools poses challenges.
Researchers face compromised data validity as AI responses replace authentic
human behavior, while workers risk diminished roles as AI automates tasks. To
address this, we propose a hybrid framework using digital twins, personalized
AI models that emulate workers' behaviors and preferences while keeping humans
in the loop. We evaluate our system with an experiment (n=88 crowd workers) and
in-depth interviews with crowd workers (n=5) and social science researchers
(n=4). Our results suggest that digital twins may enhance productivity and
reduce decision fatigue while maintaining response quality. Both researchers
and workers emphasized the importance of transparency, ethical data use, and
worker agency. By automating repetitive tasks and preserving human engagement
for nuanced ones, digital twins may help balance scalability with authenticity.

</details>


### [429] [Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work](https://arxiv.org/abs/2505.24246)
*Alice Qian Zhang,Ryland Shaw,Laura Dabbish,Jina Suh,Hong Shen*

Main category: cs.HC

TL;DR: 研究探讨了在众包负责任AI（RAI）任务中，任务设计者如何传达工作对工人福祉的风险，发现现有透明度框架忽略了这一方面。


<details>
  <summary>Details</summary>
Motivation: AI系统在开放领域和高风险领域部署时，工人常需处理有害内容，但对其福祉风险的传达缺乏关注。

Method: 通过访谈23位来自学术界和工业界的任务设计者，分析他们如何识别、解释和传达福祉风险。

Result: 研究发现需要支持任务设计者更好地识别和传达福祉风险，以提升AI开发的伦理完整性和技术效果。

Conclusion: 研究呼吁改进风险披露机制，以保障工人福祉并优化AI开发流程。

Abstract: As AI systems are increasingly tested and deployed in open-ended and
high-stakes domains, crowd workers are often tasked with responsible AI (RAI)
content work. These tasks include labeling violent content, moderating
disturbing text, or simulating harmful behavior for red teaming exercises to
shape AI system behaviors. While prior efforts have highlighted the risks to
worker well-being associated with RAI content work, far less attention has been
paid to how these risks are communicated to workers. Existing transparency
frameworks and guidelines such as model cards, datasheets, and crowdworksheets
focus on documenting model information and dataset collection processes, but
they overlook an important aspect of disclosing well-being risks to workers. In
the absence of standard workflows or clear guidance, the consistent application
of content warnings, consent flows, or other forms of well-being risk
disclosure remain unclear. This study investigates how task designers approach
risk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task
designers across academic and industry sectors, we examine how well-being risk
is recognized, interpreted, and communicated in practice. Our findings surface
a need to support task designers in identifying and communicating well-being
risk not only to support crowdworker well-being but also to strengthen the
ethical integrity and technical efficacy of AI development pipelines.

</details>


### [430] [A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins](https://arxiv.org/abs/2505.24348)
*Taku Yamazaki,Kaito Watanabe,Tatsuya Kase,Kenta Hasegawa,Koki Saida,Takumi Miyoshi*

Main category: cs.HC

TL;DR: 本文提出了一种面向可持续城市数字孪生（UDT）的3D移动群智感知（3D-MCS）框架，包含四种关键机制：3D-MCS机制、基于Geohash的空间信息管理机制、动态点云集成机制和基于Web的实时可视化工具。


<details>
  <summary>Details</summary>
Motivation: 旨在通过3D-MCS技术实现城市数字孪生的可持续发展，提供高效的数据收集和管理方法。

Method: 框架包括主动和被动感知模型、Geohash空间分区、动态点云集成和实时可视化。主动模型通过AR游戏收集数据，被动模型通过佩戴智能手机实现。

Result: 通过真实实验验证了3D-MCS模型的有效性，包括主观评估和数据收集分析，并分析了动态点云集成的性能。

Conclusion: 提出的框架为城市数字孪生提供了可行的解决方案，展示了3D-MCS技术的潜力。

Abstract: In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed
at sustainable urban digital twins (UDTs). The framework comprises four key
mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models;
(2) the Geohash-based spatial information management mechanism; (3) the dynamic
point cloud integration mechanism for UDTs; and (4) the web-based real-time
visualizer for 3D-MCS and UDTs. The active sensing model features a gamified
3D-MCS approach, where participants collect point cloud data through an
augmented reality territory coloring game. In contrast, the passive sensing
model employs a wearable 3D-MCS approach, where participants wear smartphones
around their necks without disrupting daily activities. The spatial information
management mechanism efficiently partitions the space into regions using
Geohash. The dynamic point cloud integration mechanism incorporates point
clouds collected by 3D-MCS into UDTs through global and local point cloud
registration. Finally, we evaluated the proposed framework through real-world
experiments. We verified the effectiveness of the proposed 3D-MCS models from
the perspectives of subjective evaluation and data collection and analysis.
Furthermore, we analyzed the performance of the dynamic point cloud integration
using a dataset.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [431] [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
*Korel Gundem,Juncheng Dong,Dennis Zhang,Vahid Tarokh,Zhengling Qi*

Main category: stat.ML

TL;DR: 论文提出了一种名为监督校准（SC）的框架，通过优化每类的仿射变换来改善大语言模型（LLM）在上下文学习中的预测偏差问题，并在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在上下文学习中存在系统性偏差，导致分类性能不稳定，而现有的校准方法仅能调整决策边界的位置，无法改变其方向。

Method: 提出监督校准（SC）框架，通过损失最小化学习每类的仿射变换，无需额外数据，并引入两种正则化技术：上下文不变性和方向信任区域。

Result: SC在4-shot、8-shot和16-shot设置下，在多个数据集上对Mistral-7B、LLaMA-2-7B和Qwen2-7B模型实现了最优性能。

Conclusion: SC不仅统一了现有校准方法，还能调整决策边界方向，显著提升了上下文学习的稳定性和性能。

Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new
tasks with just a few examples, but their predictions often suffer from
systematic biases, leading to unstable performances in classification. While
calibration techniques are proposed to mitigate these biases, we show that, in
the logit space, many of these methods are equivalent to merely shifting the
LLM's decision boundary without having the ability to alter its orientation.
This proves inadequate when biases cause the LLM to be severely misdirected. To
address these limitations and provide a unifying framework, we propose
Supervised Calibration (SC), a loss-minimization based framework which learns
an optimal, per-class affine transformation of the LLM's predictive
probabilities in the logit space without requiring external data beyond the
context. By using a more expressive functional class, SC not only subsumes many
existing calibration methods in ICL as special cases, but also enables the
ability to alter and even completely reverse the orientation of the LLM's
decision boundary. Furthermore, SC's loss-based nature facilitates the seamless
integration of two purpose-built regularization techniques: context-invariance
and directional trust-region. The former is designed to tackle the instability
issue in ICL, while the latter controls the degree of calibration. Finally, SC
delivers state-of-the-art performance over calibration baselines in the 4-shot,
8-shot, and 16-shot settings across all nine datasets for
Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

</details>


### [432] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol,Luca Mossina,Adrien Mazoyer,Sébastien Gerchinovitz*

Main category: stat.ML

TL;DR: 论文提出了一种基于Conformal Prediction的方法（SeqCRC），用于解决目标检测模型的可靠性问题，并提供了统计保证和实用工具包。


<details>
  <summary>Details</summary>
Motivation: 目标检测模型在关键应用中的可靠性不足，需要一种无需先验知识的统计方法提升其可信度。

Method: 提出Sequential Conformal Risk Control（SeqCRC）方法，扩展了Conformal Risk Control（CRC）的统计保证，适用于目标检测任务。

Result: 通过实验验证了方法的有效性，并提供了实用工具包和基准测试。

Conclusion: SeqCRC为目标检测提供了可靠的统计保证，适用于多种应用场景。

Abstract: Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [433] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista,Andrew M. Stuart,Son Tran*

Main category: stat.ML

TL;DR: 论文提出了一种基于概率视角的多模态对比学习框架，通过优化编码器定义条件概率分布，支持跨模态检索、分类和生成任务，并探讨了新的损失函数和对齐度量方法。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对比学习通常关注编码器对齐，本文从概率角度重新解释该方法，以支持更广泛的任务（如生成模型）并提供理论分析。

Method: 将对比学习视为优化定义条件概率分布的编码器，提出新的概率损失函数和对齐度量方法，并在多元高斯设置下研究其性质。

Result: 在多元高斯、MNIST数据集和海洋学数据同化应用中验证了框架的有效性，提出了针对特定任务的新算法变体。

Conclusion: 概率视角为多模态对比学习提供了更灵活的理论框架，支持更广泛的任务和优化方法。

Abstract: Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [434] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

Main category: stat.ML

TL;DR: 论文提出了一种高效估计正则化参数α的方法，通过近似留一交叉验证（LOOCV）对数似然损失，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决正则化Tyler M估计（RTME）中参数α估计的计算效率问题。

Method: 提出近似LOOCV对数似然损失的方法，避免重复计算RTME，将时间复杂度从O(n^2)降至O(n)。

Result: 在合成和真实高维数据上验证了方法的效率和准确性，优于现有方法。

Conclusion: 该方法高效且准确，适用于高维数据中的正则化参数估计。

Abstract: We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


### [435] [Gibbs randomness-compression proposition: An efficient deep learning](https://arxiv.org/abs/2505.23869)
*M. Süzen*

Main category: stat.ML

TL;DR: 论文提出了一种将随机性与压缩联系起来的命题，通过Gibbs熵与压缩过程中的测量向量集相关联，表明有损压缩等同于保留信息内容的定向随机性。这一命题源于新型双断层压缩（DTC）框架的观察结果，该框架通过构建压缩感知投影（权重射线）实现层权重矩阵的断层重建，并在迭代中触发神经元级剪枝。实验表明DTC在高效压缩训练中实现了最先进的性能，支持彩票假设，并揭示了随机性与压缩的统计物理学联系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索压缩与随机性之间的深层联系，并通过DTC框架实现高效、资源节约的深度学习训练方法。

Method: 方法包括提出Gibbs随机性-压缩命题，设计双断层压缩（DTC）框架，通过权重射线的断层重建实现层间压缩与剪枝，并以迭代方式优化模型。

Result: 实验结果显示DTC框架在训练中实现了高效压缩和最优性能，同时验证了随机性与压缩的统计物理学联系。

Conclusion: 结论指出DTC框架为深度学习训练提供了一种高效节能的可行方案，并通过Gibbs命题揭示了压缩与随机性的理论联系。

Abstract: A proposition that connects randomness and compression put forward via Gibbs
entropy over set of measurement vectors associated with a compression process.
The proposition states that a lossy compression process is equivalent to {\it
directed randomness} that preserves information content. The proposition
originated from the observed behaviour in newly proposed {\it Dual Tomographic
Compression} (DTC) compress-train framework. This is akin to tomographic
reconstruction of layer weight matrices via building compressed sensed
projections, so called {\it weight rays}. This tomographic approach is applied
to previous and next layers in a dual fashion, that triggers neuronal-level
pruning. This novel model compress-train scheme appear in iterative fashion and
act as smart neural architecture search, Experiments demonstrated utility of
this dual-tomography producing state-of-the-art performance with efficient
compression during training, accelerating and supporting lottery ticket
hypothesis. However, random compress-train iterations having similar
performance demonstrated the connection between randomness and compression from
statistical physics perspective, we formulated so called {\it Gibbs
randomness-compression proposition}, signifying randomness-compression
relationship via Gibbs entropy. Practically, DTC framework provides a promising
approach for massively energy and resource efficient deep learning training
approach.

</details>


### [436] [Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity](https://arxiv.org/abs/2505.24097)
*Victor Li,Baiting Chen,Yuzhen Mao,Qi Lei,Zhun Deng*

Main category: stat.ML

TL;DR: 提出了一种名为Performative Risk Control的框架，用于在预测具有反馈效应（performative）的情况下校准模型，以实现风险控制，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态数据分布下的模型校准，而忽略了预测可能影响实际结果的现象（performative predictions），这在社会科学和经济学中常见。

Method: 通过迭代校准过程，确保预测在过程中不断改进并控制风险，同时研究了不同类型的风险度量和尾部边界选择。

Result: 通过信用违约风险预测任务的数值实验验证了框架的有效性。

Conclusion: 这是首个在performative预测下实现统计严格风险控制的工作，为决策过程中的战略操纵提供了重要保障。

Abstract: Calibrating blackbox machine learning models to achieve risk control is
crucial to ensure reliable decision-making. A rich line of literature has been
studying how to calibrate a model so that its predictions satisfy explicit
finite-sample statistical guarantees under a fixed, static, and unknown
data-generating distribution. However, prediction-supported decisions may
influence the outcome they aim to predict, a phenomenon named performativity of
predictions, which is commonly seen in social science and economics. In this
paper, we introduce Performative Risk Control, a framework to calibrate models
to achieve risk control under performativity with provable theoretical
guarantees. Specifically, we provide an iteratively refined calibration
process, where we ensure the predictions are improved and risk-controlled
throughout the process. We also study different types of risk measures and
choices of tail bounds. Lastly, we demonstrate the effectiveness of our
framework by numerical experiments on the task of predicting credit default
risk. To the best of our knowledge, this work is the first one to study
statistically rigorous risk control under performativity, which will serve as
an important safeguard against a wide range of strategic manipulation in
decision-making processes.

</details>


### [437] [Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings](https://arxiv.org/abs/2505.24281)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Main category: stat.ML

TL;DR: 提出了一种双编码器框架，用于多任务学习（MTL），通过任务共享和任务特定编码器处理异构性，并在理论和实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中异构性（如分布和后验异构性）的挑战未被现有方法统一解决，需同时利用共享和异构信息。

Method: 提出双编码器框架：任务共享编码器捕获共性，任务特定编码器保留特性，并统一学习算法。

Result: 仿真和PDX数据实验表明，该方法优于现有数据整合方法，预测性能更优。

Conclusion: 双编码器框架有效解决了多任务学习中的异构性问题，理论和实验均验证其优势。

Abstract: Multi-task learning (MTL) has become an essential machine learning tool for
addressing multiple learning tasks simultaneously and has been effectively
applied across fields such as healthcare, marketing, and biomedical research.
However, to enable efficient information sharing across tasks, it is crucial to
leverage both shared and heterogeneous information. Despite extensive research
on MTL, various forms of heterogeneity, including distribution and posterior
heterogeneity, present significant challenges. Existing methods often fail to
address these forms of heterogeneity within a unified framework. In this paper,
we propose a dual-encoder framework to construct a heterogeneous latent factor
space for each task, incorporating a task-shared encoder to capture common
information across tasks and a task-specific encoder to preserve unique task
characteristics. Additionally, we explore the intrinsic similarity structure of
the coefficients corresponding to learned latent factors, allowing for adaptive
integration across tasks to manage posterior heterogeneity. We introduce a
unified algorithm that alternately learns the task-specific and task-shared
encoders and coefficients. In theory, we investigate the excess risk bound for
the proposed MTL method using local Rademacher complexity and apply it to a new
but related task. Through simulation studies, we demonstrate that the proposed
method outperforms existing data integration methods across various settings.
Furthermore, the proposed method achieves superior predictive performance for
time to tumor doubling across five distinct cancer types in PDX data.

</details>


### [438] [Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels](https://arxiv.org/abs/2505.24311)
*Yi Gu*

Main category: stat.ML

TL;DR: 本文研究了广义核下t-SNE算法的收敛性，扩展了Auffinger和Fletcher的成果，证明了在特定条件下，随着数据点数量的增加，t-SNE会收敛到平衡分布。


<details>
  <summary>Details</summary>
Motivation: 探讨广义核下t-SNE的收敛性，扩展已有研究结果。

Method: 提出广义输入和输出核的具体形式，并在特定条件下证明t-SNE的收敛性。

Result: 证明t-SNE在广义核下会收敛到平衡分布。

Conclusion: 广义核下的t-SNE算法在数据点数量足够大时具有收敛性。

Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm
for visualizing high-dimensional data by finding low-dimensional
representations. In this paper, we study the convergence of t-SNE with
generalized kernels and extend the results of Auffinger and Fletcher in 2023.
Our work starts by giving a concrete formulation of generalized input and
output kernels. Then we prove that under certain conditions, the t-SNE
algorithm converges to an equilibrium distribution for a wide range of input
and output kernels as the number of data points diverges.

</details>


### [439] [Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation](https://arxiv.org/abs/2505.24333)
*Alessio Giorlandino,Sebastian Goldt*

Main category: stat.ML

TL;DR: 论文分析了Transformer初始化对训练的影响，提出了信号传播的理论框架，并识别了两种失败模式：秩崩溃和熵崩溃。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer初始化对训练的影响，解决秩崩溃和熵崩溃问题。

Method: 通过统计物理中的随机能量模型分析信号传播，识别低方差和高方差两种初始化机制。

Result: 低方差导致秩崩溃，高方差导致熵崩溃；提出了保证信号传播的初始化参数范围。

Conclusion: 理论框架统一了两种失败模式，为Transformer初始化提供了定量指导。

Abstract: Finding the right initialisation for neural networks is crucial to ensure
smooth training and good performance. In transformers, the wrong initialisation
can lead to one of two failure modes of self-attention layers: rank collapse,
where all tokens collapse into similar representations, and entropy collapse,
where highly concentrated attention scores lead to training instability. While
the right initialisation has been extensively studied in feed-forward networks,
an exact description of signal propagation through a full transformer block has
so far been lacking. Here, we provide an analytical theory of signal
propagation through vanilla transformer blocks with self-attention layers,
layer normalisation, skip connections and ReLU MLP. To treat the self-attention
layer, we draw on a formal parallel with the Random Energy Model from
statistical physics. We identify and characterise two regimes governed by the
variance of the query and key initialisations: a low-variance regime, where we
recover the known rank collapse behaviour; and a previously unexplored
high-variance regime, where signal is preserved but \textit{entropy collapse}
occurs. In the low-variance regime, we calculate the critical strength for the
residual connection to ensure signal propagation. Our theory yields
trainability diagrams that identify the correct choice of initialisation
hyper-parameters for a given architecture. Experiments with BERT-style models
trained on TinyStories validate our predictions. Our theoretical framework
gives a unified perspective on the two failure modes of self-attention and
gives quantitative predictions on the scale of both weights and residual
connections that guarantees smooth training.

</details>


### [440] [Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data](https://arxiv.org/abs/2505.24556)
*Gabriel V Cardoso,Mike Pereira*

Main category: stat.ML

TL;DR: 提出了一种基于扩散生成模型（DGM）的两步法，用于模拟非平稳高斯过程（GP）先验的预测后验分布（PPD），解决了计算难题。


<details>
  <summary>Details</summary>
Motivation: 非平稳高斯过程先验在捕捉复杂空间模式时计算不可行，需寻找替代方法。

Method: 用DGM替代GP先验，利用无训练引导算法从目标后验分布中采样。

Result: 验证了生成分布与GP接近，并可针对GP先验的特定部分进行微调。

Conclusion: 该方法在环境科学逆问题中实现了最先进的预测。

Abstract: Bayesian models based on Gaussian processes (GPs) offer a flexible framework
to predict spatially distributed variables with uncertainty. But the use of
nonstationary priors, often necessary for capturing complex spatial patterns,
makes sampling from the predictive posterior distribution (PPD) computationally
intractable. In this paper, we propose a two-step approach based on diffusion
generative models (DGMs) to mimic PPDs associated with non-stationary GP
priors: we replace the GP prior by a DGM surrogate, and leverage recent
advances on training-free guidance algorithms for DGMs to sample from the
desired posterior distribution. We apply our approach to a rich non-stationary
GP prior from which exact posterior sampling is untractable and validate that
the issuing distributions are close to their GP counterpart using several
statistical metrics. We also demonstrate how one can fine-tune the trained DGMs
to target specific parts of the GP prior. Finally we apply the proposed
approach to solve inverse problems arising in environmental sciences, thus
yielding state-of-the-art predictions.

</details>


### [441] [Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders](https://arxiv.org/abs/2505.24668)
*Jonghyun Ham,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 该论文研究了两层线性去噪自编码器在梯度流下的表现，分析了瓶颈层和跳跃连接对测试风险的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管监督学习中的过参数化现象已被广泛研究，但无监督任务（如去噪）仍存在许多未解问题。本文旨在填补这一空白。

Method: 研究了两层线性去噪自编码器，引入瓶颈层和跳跃连接，并通过梯度流训练。使用乘积正则化推导临界点，并分析全局最小化器。

Result: 发现瓶颈层引入了一种类似偏差-方差权衡的复杂性度量，而跳跃连接可以缓解去噪自编码器的方差问题。

Conclusion: 跳跃连接在轻度过参数化模型中尤其有效，为无监督学习中的过参数化现象提供了新的见解。

Abstract: Modern deep neural networks exhibit strong generalization even in highly
overparameterized regimes. Significant progress has been made to understand
this phenomenon in the context of supervised learning, but for unsupervised
tasks such as denoising, several open questions remain. While some recent works
have successfully characterized the test error of the linear denoising problem,
they are limited to linear models (one-layer network). In this work, we focus
on two-layer linear denoising autoencoders trained under gradient flow,
incorporating two key ingredients of modern deep learning architectures: A
low-dimensional bottleneck layer that effectively enforces a rank constraint on
the learned solution, as well as the possibility of a skip connection that
bypasses the bottleneck. We derive closed-form expressions for all critical
points of this model under product regularization, and in particular describe
its global minimizer under the minimum-norm principle. From there, we derive
the test risk formula in the overparameterized regime, both for models with and
without skip connections. Our analysis reveals two interesting phenomena:
Firstly, the bottleneck layer introduces an additional complexity measure akin
to the classical bias-variance trade-off -- increasing the bottleneck width
reduces bias but introduces variance, and vice versa. Secondly, skip connection
can mitigate the variance in denoising autoencoders -- especially when the
model is mildly overparameterized. We further analyze the impact of skip
connections in denoising autoencoder using random matrix theory and support our
claims with numerical evidence.

</details>


### [442] [K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes](https://arxiv.org/abs/2505.24704)
*Hideaki Kim,Tomoharu Iwata,Akinori Fujino*

Main category: stat.ML

TL;DR: 提出了一种基于最小二乘损失的正则化核方法（K²IE），连接经典核强度估计器（KIE）与基于核方法的强度估计器，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 经典KIE与基于核方法的强度估计器虽名称相似但理论基础不同，各有优劣，需探索其联系并改进效率。

Method: 基于最小二乘损失设计正则化核方法，利用RKHS理论，提出K²IE。

Result: K²IE在合成数据集上表现与现有方法相当，计算效率显著提升。

Conclusion: K²IE通过理论连接与高效实现，为强度估计提供了新工具。

Abstract: Kernel method-based intensity estimators, formulated within reproducing
kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)
have been among the most easy-to-implement and feasible methods for estimating
the intensity functions of inhomogeneous Poisson processes. While both
approaches share the term "kernel", they are founded on distinct theoretical
principles, each with its own strengths and limitations. In this paper, we
propose a novel regularized kernel method for Poisson processes based on the
least squares loss and show that the resulting intensity estimator involves a
specialized variant of the representer theorem: it has the dual coefficient of
unity and coincides with classical KIEs. This result provides new theoretical
insights into the connection between classical KIEs and kernel method-based
intensity estimators, while enabling us to develop an efficient KIE by
leveraging advanced techniques from RKHS theory. We refer to the proposed model
as the kernel method-based kernel intensity estimator (K$^2$IE). Through
experiments on synthetic datasets, we show that K$^2$IE achieves comparable
predictive performance while significantly surpassing the state-of-the-art
kernel method-based estimator in computational efficiency.

</details>


### [443] [Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery](https://arxiv.org/abs/2505.24727)
*Xiaochen Zhang,Haoyi Xiong*

Main category: stat.ML

TL;DR: 论文提出了一种基于Knockoff的压缩感知框架，通过精确控制假发现率（FDR）提升信号恢复性能，优于传统LASSO方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知方法（如LASSO）在支持选择和信号估计时缺乏显式错误控制，导致信号恢复不可靠。本文旨在通过Knockoff滤波器分离和控制支持恢复过程，提升准确性。

Method: 提出Knockoff-guided压缩感知框架，通过统计Knockoff滤波器控制FDR，分离支持选择和信号估计，确保有限样本下的FDR控制。

Result: 实验表明，该方法在F1分数上比基线方法提升高达3.9倍，重建误差更低，并在真实数据集上表现出色。

Conclusion: 该框架通过统计基础的支持选择，提供了理论保证和强实证性能，成为现有方法的稳健替代方案。

Abstract: This paper introduces a novel Knockoff-guided compressive sensing framework,
referred to as \TheName{}, which enhances signal recovery by leveraging precise
false discovery rate (FDR) control during the support identification phase.
Unlike LASSO, which jointly performs support selection and signal estimation
without explicit error control, our method guarantees FDR control in finite
samples, enabling more reliable identification of the true signal support. By
separating and controlling the support recovery process through statistical
Knockoff filters, our framework achieves more accurate signal reconstruction,
especially in challenging scenarios where traditional methods fail. We
establish theoretical guarantees demonstrating how FDR control directly ensures
recovery performance under weaker conditions than traditional $\ell_1$-based
compressive sensing methods, while maintaining accurate signal reconstruction.
Extensive numerical experiments demonstrate that our proposed Knockoff-based
method consistently outperforms LASSO-based and other state-of-the-art
compressive sensing techniques. In simulation studies, our method improves
F1-score by up to 3.9x over baseline methods, attributed to principled false
discovery rate (FDR) control and enhanced support recovery. The method also
consistently yields lower reconstruction and relative errors. We further
validate the framework on real-world datasets, where it achieves top downstream
predictive performance across both regression and classification tasks, often
narrowing or even surpassing the performance gap relative to uncompressed
signals. These results establish \TheName{} as a robust and practical
alternative to existing approaches, offering both theoretical guarantees and
strong empirical performance through statistically grounded support selection.

</details>


### [444] [Generalization Dynamics of Linear Diffusion Models](https://arxiv.org/abs/2505.24769)
*Claudia Merger,Sebastian Goldt*

Main category: stat.ML

TL;DR: 论文研究了扩散模型从记忆训练样本到生成新样本的过渡，发现当样本数N与输入维度d相当时发生过渡，并分析了正则化和早停对防止过拟合的作用。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型从记忆到泛化的过渡，以评估生成模型的样本效率和可靠性。

Method: 使用线性去噪器模型，通过解析计算测试误差、采样分布和KL散度来研究过渡。

Result: 过渡发生在N≈d时；N<d时正则化和早停防止过拟合；N>d时采样分布以d/N线性趋近最优。

Conclusion: 研究阐明了线性扩散模型的样本复杂度与泛化关系，为训练动态提供了理论支持。

Abstract: Diffusion models trained on finite datasets with $N$ samples from a target
distribution exhibit a transition from memorisation, where the model reproduces
training examples, to generalisation, where it produces novel samples that
reflect the underlying data distribution. Understanding this transition is key
to characterising the sample efficiency and reliability of generative models,
but our theoretical understanding of this transition is incomplete. Here, we
analytically study the memorisation-to-generalisation transition in a simple
model using linear denoisers, which allow explicit computation of test errors,
sampling distributions, and Kullback-Leibler divergences between samples and
target distribution. Using these measures, we predict that this transition
occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is
smaller than the dimension of the inputs $d$, so that only a fraction of
relevant directions of variation are present in the training data, we
demonstrate how both regularization and early stopping help to prevent
overfitting. For $N > d$, we find that the sampling distributions of linear
diffusion models approach their optimum (measured by the Kullback-Leibler
divergence) linearly with $d/N$, independent of the specifics of the data
distribution. Our work clarifies how sample complexity governs generalisation
in a simple model of diffusion-based generative models and provides insight
into the training dynamics of linear denoisers.

</details>


### [445] [Statistical mechanics of extensive-width Bayesian neural networks near interpolation](https://arxiv.org/abs/2505.24849)
*Jean Barbier,Francesco Camilli,Minh-Toan Nguyen,Mauro Pastore,Rudy Skerk*

Main category: stat.ML

TL;DR: 论文通过统计物理学方法分析了两层全连接网络的监督学习，填补了理论模型与实际应用之间的差距，揭示了丰富的学习现象。


<details>
  <summary>Details</summary>
Motivation: 缩小实际神经网络与理论理解之间的差距，研究更接近实际应用的网络模型。

Method: 使用统计物理学方法分析两层全连接网络，研究贝叶斯最优学习在教师-学生场景下的表现。

Result: 发现随着数据量的增加，学习过程中会出现多种过渡现象；特征贡献越强，学习所需数据越少。

Conclusion: 在数据稀缺时，模型学习非线性组合而非对齐权重；充足数据下才能实现权重对齐，但实际算法可能难以实现。

Abstract: For three decades statistical mechanics has been providing a framework to
analyse neural networks. However, the theoretically tractable models, e.g.,
perceptrons, random features models and kernel machines, or multi-index models
and committee machines with few neurons, remained simple compared to those used
in applications. In this paper we help reducing the gap between practical
networks and their theoretical understanding through a statistical physics
analysis of the supervised learning of a two-layer fully connected network with
generic weight distribution and activation function, whose hidden layer is
large but remains proportional to the inputs dimension. This makes it more
realistic than infinitely wide networks where no feature learning occurs, but
also more expressive than narrow ones or with fixed inner weights. We focus on
the Bayes-optimal learning in the teacher-student scenario, i.e., with a
dataset generated by another network with the same architecture. We operate
around interpolation, where the number of trainable parameters and of data are
comparable and feature learning emerges. Our analysis uncovers a rich
phenomenology with various learning transitions as the number of data
increases. In particular, the more strongly the features (i.e., hidden neurons
of the target) contribute to the observed responses, the less data is needed to
learn them. Moreover, when the data is scarce, the model only learns non-linear
combinations of the teacher weights, rather than "specialising" by aligning its
weights with the teacher's. Specialisation occurs only when enough data becomes
available, but it can be hard to find for practical training algorithms,
possibly due to statistical-to-computational~gaps.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [446] [Quantum computing and artificial intelligence: status and perspectives](https://arxiv.org/abs/2505.23860)
*Giovanni Acampora,Andris Ambainis,Natalia Ares,Leonardo Banchi,Pallavi Bhardwaj,Daniele Binosi,G. Andrew D. Briggs,Tommaso Calarco,Vedran Dunjko,Jens Eisert,Olivier Ezratty,Paul Erker,Federico Fedele,Elies Gil-Fuster,Martin Gärttner,Mats Granath,Markus Heyl,Iordanis Kerenidis,Matthias Klusch,Anton Frisk Kockum,Richard Kueng,Mario Krenn,Jörg Lässig,Antonio Macaluso,Sabrina Maniscalco,Florian Marquardt,Kristel Michielsen,Gorka Muñoz-Gil,Daniel Müssig,Hendrik Poulsen Nautrup,Evert van Nieuwenburg,Roman Orus,Jörg Schmiedmayer,Markus Schmitt,Philipp Slusallek,Filippo Vicentini,Christof Weitenberg,Frank K. Wilhelm*

Main category: quant-ph

TL;DR: 本文探讨了量子计算与人工智能（AI）的交叉点，分析了量子计算如何支持AI创新，以及经典AI如何推动量子技术研究。提出了长期研究议程，并总结了相关挑战和建议。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算与AI的相互作用，以促进两者的协同发展，并解决基础性问题。

Method: 通过分析量子计算与AI的交叉点，提出研究议程，并探讨实际应用案例。

Result: 提出了量子计算与AI协同发展的长期研究议程，并总结了挑战和建议。

Conclusion: 建议协调理论研究与硬件发展，优化资源利用，并推动欧洲工业竞争力与社会影响。

Abstract: This white paper discusses and explores the various points of intersection
between quantum computing and artificial intelligence (AI). It describes how
quantum computing could support the development of innovative AI solutions. It
also examines use cases of classical AI that can empower research and
development in quantum technologies, with a focus on quantum computing and
quantum sensing. The purpose of this white paper is to provide a long-term
research agenda aimed at addressing foundational questions about how AI and
quantum computing interact and benefit one another. It concludes with a set of
recommendations and challenges, including how to orchestrate the proposed
theoretical work, align quantum AI developments with quantum hardware roadmaps,
estimate both classical and quantum resources - especially with the goal of
mitigating and optimizing energy consumption - advance this emerging hybrid
software engineering discipline, and enhance European industrial
competitiveness while considering societal implications.

</details>


### [447] [Leveraging machine learning features for linear optical interferometer control](https://arxiv.org/abs/2505.24032)
*Sergei S. Kuzmin,Ivan V. Dyakonov,Stanislav S. Straupe*

Main category: quant-ph

TL;DR: 提出一种独立于特定架构约束的可重构光学干涉仪建模算法，结合解析方法和优化程序实现干涉仪相位调谐。


<details>
  <summary>Details</summary>
Motivation: 探索无需精确解析解即可有效调谐干涉仪的方法，以支持新型干涉仪电路架构的研究。

Method: 采用监督学习训练干涉仪模型，结合解析方法或优化程序确定相位偏移以实现所需酉变换。

Result: 算法能够有效调谐干涉仪，无需依赖精确解析解。

Conclusion: 该方法为新型干涉仪架构的研究提供了灵活且高效的调谐手段。

Abstract: We have developed an algorithm that constructs a model of a reconfigurable
optical interferometer, independent of specific architectural constraints. The
programming of unitary transformations on the interferometer's optical modes
relies on either an analytical method for deriving the unitary matrix from a
set of phase shifts or an optimization routine when such decomposition is not
available. Our algorithm employs a supervised learning approach, aligning the
interferometer model with a training set derived from the device being studied.
A straightforward optimization procedure leverages this trained model to
determine the phase shifts of the interferometer with a specific architecture,
obtaining the required unitary transformation. This approach enables the
effective tuning of interferometers without requiring a precise analytical
solution, paving the way for the exploration of new interferometric circuit
architectures.

</details>


### [448] [Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications](https://arxiv.org/abs/2505.24765)
*Srikanth Thudumu,Jason Fisher,Hung Du*

Main category: quant-ph

TL;DR: 本文综述了监督量子机器学习（QML）的最新进展，探讨了变分量子电路、量子神经网络和量子核方法等方法，并展望了未来十年的发展方向。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算与经典机器学习的结合，利用量子资源支持模型训练和推理，以寻找量子优势。

Method: 综述了变分量子电路、量子神经网络、量子核方法以及混合量子-经典工作流程。

Result: 实验研究表明部分量子优势迹象，但存在噪声、贫瘠高原、可扩展性问题和缺乏性能改进的形式证明等限制。

Conclusion: 提出了2025-2035年的十年展望，包括QML在应用研究和企业系统中可能的发展路线图。

Abstract: Supervised Quantum Machine Learning (QML) represents an intersection of
quantum computing and classical machine learning, aiming to use quantum
resources to support model training and inference. This paper reviews recent
developments in supervised QML, focusing on methods such as variational quantum
circuits, quantum neural networks, and quantum kernel methods, along with
hybrid quantum-classical workflows. We examine recent experimental studies that
show partial indications of quantum advantage and describe current limitations
including noise, barren plateaus, scalability issues, and the lack of formal
proofs of performance improvement over classical methods. The main contribution
is a ten-year outlook (2025-2035) that outlines possible developments in
supervised QML, including a roadmap describing conditions under which QML may
be used in applied research and enterprise systems over the next decade.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [449] [INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization](https://arxiv.org/abs/2505.24269)
*Aleksandr Algazinov,Joydeep Chandra,Matt Laing*

Main category: cs.NI

TL;DR: 本文综述了网络内计算如何通过利用网络设备的处理能力优化AI工作负载，减少延迟并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决AI工作负载对网络基础设施日益增长的需求，探索网络内计算的潜力。

Method: 分析可编程网络架构（如SDN和PDPs）与AI的结合，研究AI模型在资源受限设备上的映射方法及优化技术。

Result: 展示了网络内计算在降低延迟、提升吞吐量和优化资源利用方面的优势，并讨论了分布式学习和隐私增强技术。

Conclusion: 网络内AI有望构建智能、高效的网络，满足下一代AI应用需求，未来需关注运行时可编程性和标准化基准。

Abstract: In-network computation represents a transformative approach to addressing the
escalating demands of Artificial Intelligence (AI) workloads on network
infrastructure. By leveraging the processing capabilities of network devices
such as switches, routers, and Network Interface Cards (NICs), this paradigm
enables AI computations to be performed directly within the network fabric,
significantly reducing latency, enhancing throughput, and optimizing resource
utilization. This paper provides a comprehensive analysis of optimizing
in-network computation for AI, exploring the evolution of programmable network
architectures, such as Software-Defined Networking (SDN) and Programmable Data
Planes (PDPs), and their convergence with AI. It examines methodologies for
mapping AI models onto resource-constrained network devices, addressing
challenges like limited memory and computational capabilities through efficient
algorithm design and model compression techniques. The paper also highlights
advancements in distributed learning, particularly in-network aggregation, and
the potential of federated learning to enhance privacy and scalability.
Frameworks like Planter and Quark are discussed for simplifying development,
alongside key applications such as intelligent network monitoring, intrusion
detection, traffic management, and Edge AI. Future research directions,
including runtime programmability, standardized benchmarks, and new
applications paradigms, are proposed to advance this rapidly evolving field.
This survey underscores the potential of in-network AI to create intelligent,
efficient, and responsive networks capable of meeting the demands of
next-generation AI applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [450] [Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control](https://arxiv.org/abs/2505.24161)
*Zijie Xu,Tong Bu,Zecheng Hao,Jianhao Ding,Zhaofei Yu*

Main category: cs.NE

TL;DR: 论文提出了一种代理目标框架，解决了SNN在连续控制任务中与RL算法不兼容的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法是为ANN设计的，与SNN的离散、不可微特性冲突，导致训练不稳定。

Method: 提出代理目标框架，利用连续可微的动态特性绕过SNN的局限性，稳定训练。

Result: 实验表明，代理目标框架使SNN性能提升32%，首次用简单LIF神经元超越ANN表现。

Conclusion: 该工作为SNN友好的RL算法开辟了新方向，结合高性能与低功耗的神经形态代理成为可能。

Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient
decision making through neuromorphic hardware, making them compelling for
Reinforcement Learning (RL) in resource-constrained edge devices. Recent
studies in this field directly replace Artificial Neural Networks (ANNs) by
SNNs in existing RL frameworks, overlooking whether the RL algorithm is
suitable for SNNs. However, most RL algorithms in continuous control are
designed tailored to ANNs, including the target network soft updates mechanism,
which conflict with the discrete, non-differentiable dynamics of SNN spikes. We
identify that this mismatch destabilizes SNN training in continuous control
tasks. To bridge this gap between discrete SNN and continuous control, we
propose a novel proxy target framework. The continuous and differentiable
dynamics of the proxy target enable smooth updates, bypassing the
incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy
network operates only during training, the SNN retains its energy efficiency
during deployment without inference overhead. Extensive experiments on
continuous control benchmarks demonstrate that compared to vanilla SNNs, the
proxy target framework enables SNNs to achieve up to 32% higher performance
across different spiking neurons. Notably, we are the first to surpass ANN
performance in continuous control with simple Leaky-Integrate-and-Fire (LIF)
neurons. This work motivates a new class of SNN-friendly RL algorithms tailored
to SNN's characteristics, paving the way for neuromorphic agents that combine
high performance with low power consumption.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [451] [Beyond the Black Box: Interpretability of LLMs in Finance](https://arxiv.org/abs/2505.24650)
*Hariom Tatsat,Ariye Shater*

Main category: cs.CE

TL;DR: 本文探讨了大型语言模型（LLMs）在金融领域的应用潜力及其透明性问题，首次提出通过机制可解释性方法理解和优化LLMs的内部机制，以满足金融行业对透明度和可控性的需求。


<details>
  <summary>Details</summary>
Motivation: 金融行业对AI系统的透明度、公平性和问责制有严格要求，而LLMs的复杂性和不透明性带来了挑战。本文旨在通过机制可解释性方法解决这些问题。

Method: 采用机制可解释性方法，通过逆向工程分析LLMs的内部激活和电路，揭示其预测行为，并展示其在金融用例（如交易策略、情感分析）中的实际应用。

Result: 机制可解释性方法能够提供对LLMs行为的深入理解，并支持模型行为的修改，从而满足金融行业的透明度和合规需求。

Conclusion: 机制可解释性方法有望成为LLMs在金融领域广泛应用的关键工具，确保AI系统的伦理性和合规性，并满足未来监管要求。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a
spectrum of tasks in financial services, including report generation, chatbots,
sentiment analysis, regulatory compliance, investment advisory, financial
knowledge retrieval, and summarization. However, their intrinsic complexity and
lack of transparency pose significant challenges, especially in the highly
regulated financial sector, where interpretability, fairness, and
accountability are critical. As far as we are aware, this paper presents the
first application in the finance domain of understanding and utilizing the
inner workings of LLMs through mechanistic interpretability, addressing the
pressing need for transparency and control in AI systems. Mechanistic
interpretability is the most intuitive and transparent way to understand LLM
behavior by reverse-engineering their internal workings. By dissecting the
activations and circuits within these models, it provides insights into how
specific features or components influence predictions - making it possible not
only to observe but also to modify model behavior. In this paper, we explore
the theoretical aspects of mechanistic interpretability and demonstrate its
practical relevance through a range of financial use cases and experiments,
including applications in trading strategies, sentiment analysis, bias, and
hallucination detection. While not yet widely adopted, mechanistic
interpretability is expected to become increasingly vital as adoption of LLMs
increases. Advanced interpretability tools can ensure AI systems remain
ethical, transparent, and aligned with evolving financial regulations. In this
paper, we have put special emphasis on how these techniques can help unlock
interpretability requirements for regulatory and compliance purposes -
addressing both current needs and anticipating future expectations from
financial regulators globally.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [452] [Learning Normal Patterns in Musical Loops](https://arxiv.org/abs/2505.23784)
*Shayan Dadman,Bernt Arild Bremdal,Børre Bang,Rune Dalmo*

Main category: cs.SD

TL;DR: 本文提出了一种无监督框架，通过异常检测技术检测音乐样本中的音频模式，解决了音乐信息检索中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于手工特征、领域限制或用户交互，限制了其灵活性。本文旨在克服这些限制。

Method: 结合深度特征提取和无监督异常检测，使用预训练的HTS-AT和FFM生成音频表示，并通过Deep SVDD学习规范模式。

Result: 在低音和吉他数据集上，Deep SVDD模型（尤其是残差自编码器变体）表现出更好的异常分离能力。

Conclusion: 该研究提供了一种灵活的无监督解决方案，能够有效识别多样音频样本中的模式。

Abstract: This paper introduces an unsupervised framework for detecting audio patterns
in musical samples (loops) through anomaly detection techniques, addressing
challenges in music information retrieval (MIR). Existing methods are often
constrained by reliance on handcrafted features, domain-specific limitations,
or dependence on iterative user interaction. We address these limitations
through an architecture combining deep feature extraction with unsupervised
anomaly detection. Our approach leverages a pre-trained Hierarchical
Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion
Mechanism (FFM), to generate representations from variable-length audio loops.
These embeddings are processed using one-class Deep Support Vector Data
Description (Deep SVDD), which learns normative audio patterns by mapping them
to a compact latent hypersphere. Evaluations on curated bass and guitar
datasets compare standard and residual autoencoder variants against baselines
like Isolation Forest (IF) and and principle component analysis (PCA) methods.
Results show our Deep SVDD models, especially the residual autoencoder variant,
deliver improved anomaly separation, particularly for larger variations. This
research contributes a flexible, fully unsupervised solution for processing
diverse audio samples, overcoming previous structural and input limitations
while enabling effective pattern identification through distance-based latent
space scoring.

</details>


### [453] [ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation](https://arxiv.org/abs/2505.24518)
*Jiatong Shi,Yifan Cheng,Bo-Hao Su,Hye-jin Shim,Jinchuan Tian,Samuele Cornell,Yiwen Zhao,Siddhant Arora,Shinji Watanabe*

Main category: cs.SD

TL;DR: ARECHO是一种基于自回归依赖建模的链式语音评估系统，通过三项创新技术显著提升了多指标联合估计的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 语音信号分析中，多指标（如PESQ、STOI、MOS）的联合估计因尺度、假设和依赖关系的差异而具有挑战性。

Method: ARECHO采用三项关键技术：语音信息标记化流程、动态分类器链显式捕获指标间依赖关系，以及两步置信导向解码算法。

Result: 实验表明，ARECHO在多种评估场景中显著优于基线框架，并提升了指标间关系的可解释性。

Conclusion: ARECHO通过动态依赖建模和链式优化，为语音评估提供了高效且可靠的解决方案。

Abstract: Speech signal analysis poses significant challenges, particularly in tasks
such as speech quality evaluation and profiling, where the goal is to predict
multiple perceptual and objective metrics. For instance, metrics like PESQ
(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective
Intelligibility), and MOS (Mean Opinion Score) each capture different aspects
of speech quality. However, these metrics often have different scales,
assumptions, and dependencies, making joint estimation non-trivial. To address
these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based
Hypothesis Optimization), a chain-based, versatile evaluation system for speech
assessment grounded in autoregressive dependency modeling. ARECHO is
distinguished by three key innovations: (1) a comprehensive speech information
tokenization pipeline; (2) a dynamic classifier chain that explicitly captures
inter-metric dependencies; and (3) a two-step confidence-oriented decoding
algorithm that enhances inference reliability. Experiments demonstrate that
ARECHO significantly outperforms the baseline framework across diverse
evaluation scenarios, including enhanced speech analysis, speech generation
evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency
modeling improves interpretability by capturing inter-metric relationships.

</details>


### [454] [4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification](https://arxiv.org/abs/2505.23782)
*Andrew P. Berg,Qian Zhang,Mia Y. Wang*

Main category: cs.SD

TL;DR: 研究探讨了无人机分类中数据稀缺问题，比较了CNN和Transformer模型的表现，发现CNN略优但Transformer有潜力。


<details>
  <summary>Details</summary>
Motivation: 无人机使用激增需加强安全措施，但数据稀缺是分类任务的主要挑战。

Method: 使用4,500秒音频数据，采用PEFT和数据增强，对比CNN和Transformer模型。

Result: CNN准确率比Transformer高1-2%，且计算效率更高，但Transformer有潜力。

Conclusion: 未来需扩大数据集以进一步优化Transformer模型。

Abstract: Unmanned aerial vehicle (UAV) usage is expected to surge in the coming
decade, raising the need for heightened security measures to prevent airspace
violations and security threats. This study investigates deep learning
approaches to UAV classification focusing on the key issue of data scarcity. To
investigate this we opted to train the models using a total of 4,500 seconds of
audio samples, evenly distributed across a 9-class dataset. We leveraged
parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the
data scarcity. This paper implements and compares the use of convolutional
neural networks (CNNs) and attention-based transformers. Our results show that,
CNNs outperform transformers by 1-2\% accuracy, while still being more
computationally efficient. These early findings, however, point to potential in
using transformers models; suggesting that with more data and further
optimizations they could outperform CNNs. Future works aims to upscale the
dataset to better understand the trade-offs between these approaches.

</details>


### [455] [HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset](https://arxiv.org/abs/2411.14207)
*Shivam Saini,Jürgen Peissig*

Main category: cs.SD

TL;DR: 本文介绍了一个基于图像源方法生成的7阶Ambisonic房间脉冲响应数据集，支持高精度空间音频再现，适用于沉浸式音频应用。


<details>
  <summary>Details</summary>
Motivation: 传统麦克风阵列在空间音频捕捉中存在局限性，需要一种优化的方法来提升声音场覆盖和空间分辨率。

Method: 采用虚拟模拟和64麦克风配置，基于叠加原理优化声音场覆盖，直接在球谐域中捕捉脉冲响应。

Result: 数据集包含多种房间配置，提供高空间分辨率和真实感，适用于机器学习改进房间声学建模和声场合成。

Conclusion: 该数据集为空间音频研究提供了重要资源，特别适用于源定位、混响预测和沉浸式声音再现等任务。

Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse
Responses (HOA-RIRs), created using the Image Source Method. By employing
higher-order Ambisonics, our dataset enables precise spatial audio
reproduction, a critical requirement for realistic immersive audio
applications. Leveraging the virtual simulation, we present a unique microphone
configuration, based on the superposition principle, designed to optimize sound
field coverage while addressing the limitations of traditional microphone
arrays. The presented 64-microphone configuration allows us to capture RIRs
directly in the Spherical Harmonics domain. The dataset features a wide range
of room configurations, encompassing variations in room geometry, acoustic
absorption materials, and source-receiver distances. A detailed description of
the simulation setup is provided alongside for an accurate reproduction. The
dataset serves as a vital resource for researchers working on spatial audio,
particularly in applications involving machine learning to improve room
acoustics modeling and sound field synthesis. It further provides a very high
level of spatial resolution and realism crucial for tasks such as source
localization, reverberation prediction, and immersive sound reproduction.

</details>


### [456] [Unified AI for Accurate Audio Anomaly Detection](https://arxiv.org/abs/2505.23781)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.SD

TL;DR: 提出了一种统一的AI框架，通过整合噪声消除、特征提取和机器学习建模技术，实现高精度音频异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决嘈杂环境和实时应用中的音频异常检测挑战。

Method: 结合谱减法和自适应滤波提升音频质量，使用MFCC和OpenL3等特征提取方法，并采用SVM、随机森林、CNN及集成方法建模。

Result: 在TORGO和LibriSpeech数据集上表现优异，精确率和召回率显著提升。

Conclusion: 该框架为音频异常检测提供了可扩展的高效解决方案。

Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly
detection by integrating advanced noise reduction, feature extraction, and
machine learning modeling techniques. The approach combines spectral
subtraction and adaptive filtering to enhance audio quality, followed by
feature extraction using traditional methods like MFCCs and deep embeddings
from pre-trained models such as OpenL3. The modeling pipeline incorporates
classical models (SVM, Random Forest), deep learning architectures (CNNs), and
ensemble methods to boost robustness and accuracy. Evaluated on benchmark
datasets including TORGO and LibriSpeech, the proposed framework demonstrates
superior performance in precision, recall, and classification of slurred vs.
normal speech. This work addresses challenges in noisy environments and
real-time applications and provides a scalable solution for audio-based anomaly
detection.

</details>


### [457] [Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion](https://arxiv.org/abs/2505.24291)
*Kaidi Wang,Wenhao Guan,Ziyue Jiang,Hukai Huang,Peijie Chen,Weijie Wu,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: Discl-VC是一种新型零样本语音转换框架，通过解耦内容和韵律信息，结合上下文学习和流匹配变换器，显著提升了语音转换的准确性和韵律控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本语音转换系统难以准确复制源说话者的说话风格或目标说话者的独特风格，限制了语音转换的可控性。

Method: 提出Discl-VC框架，解耦自监督语音表示中的内容和韵律信息，通过上下文学习和流匹配变换器合成目标说话者的语音，并引入掩码生成变换器以非自回归方式预测离散韵律标记。

Result: 实验结果表明，Discl-VC在零样本语音转换中表现优异，且在合成语音的韵律控制上具有显著准确性。

Conclusion: Discl-VC通过解耦和上下文学习，显著提升了语音转换的韵律控制能力，为语音合成提供了更高的可控性。

Abstract: Currently, zero-shot voice conversion systems are capable of synthesizing the
voice of unseen speakers. However, most existing approaches struggle to
accurately replicate the speaking style of the source speaker or mimic the
distinctive speaking style of the target speaker, thereby limiting the
controllability of voice conversion. In this work, we propose Discl-VC, a novel
voice conversion framework that disentangles content and prosody information
from self-supervised speech representations and synthesizes the target
speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a
mask generative transformer that predicts discrete prosody tokens in a
non-autoregressive manner based on prompts. Experimental results demonstrate
the superior performance of Discl-VC in zero-shot voice conversion and its
remarkable accuracy in prosody control for synthesized speech.

</details>


### [458] [Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses](https://arxiv.org/abs/2505.23834)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: 论文提出了一种名为PAFA的框架，通过两种新损失函数（PCSL和GPAL）解决肺音分类中的患者间差异问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 肺音分类对早期诊断呼吸系统疾病至关重要，但生物医学信号存在患者间差异，需要一种考虑个体差异的学习方法。

Method: 提出了Patient-Aware Feature Alignment (PAFA)框架，包含Patient Cohesion-Separation Loss (PCSL)和Global Patient Alignment Loss (GPAL)两种损失函数，分别用于聚类同一患者的特征并分离其他患者的特征，以及防止特征空间碎片化。

Result: 在ICBHI数据集上，四分类和二分类的准确率分别达到64.84%和72.08%。

Conclusion: PAFA能够捕捉个体化模式并在不同患者群体中提升性能，为以患者为中心的医疗提供了广泛应用前景。

Abstract: Lung sound classification is vital for early diagnosis of respiratory
diseases. However, biomedical signals often exhibit inter-patient variability
even among patients with the same symptoms, requiring a learning approach that
considers individual differences. We propose a Patient-Aware Feature Alignment
(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)
and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same
patient while separating those from other patients to capture patient
variability, whereas GPAL draws each patient's centroid toward a global center,
preventing feature space fragmentation. Our method achieves outstanding results
on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for
two-class classification. These findings highlight PAFA's ability to capture
individualized patterns and demonstrate performance gains in distinct patient
clusters, offering broader applications for patient-centered healthcare.

</details>


### [459] [Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection](https://arxiv.org/abs/2505.24486)
*Falih Gozi Febrinanto,Kristen Moore,Chandra Thapa,Jiangang Ma,Vidya Saikrishna,Feng Xia*

Main category: cs.SD

TL;DR: RAIS是一种基于排练的持续学习方法，通过辅助标签生成网络选择多样化的样本，提升音频深度伪造检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测框架在面对新攻击时性能下降，而传统排练方法无法有效捕捉音频多样性，导致偏见和遗忘风险。

Method: 提出RAIS方法，利用辅助标签生成网络指导样本选择，确保记忆缓冲区中的样本多样性。

Result: 实验表明，RAIS在五个测试场景中平均等错误率（EER）为1.953%，优于现有方法。

Conclusion: RAIS通过多样化的样本选择，显著提升了音频深度伪造检测的持续学习性能。

Abstract: The performance of existing audio deepfake detection frameworks degrades when
confronted with new deepfake attacks. Rehearsal-based continual learning (CL),
which updates models using a limited set of old data samples, helps preserve
prior knowledge while incorporating new information. However, existing
rehearsal techniques don't effectively capture the diversity of audio
characteristics, introducing bias and increasing the risk of forgetting. To
address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling
(RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS
employs a label generation network to produce auxiliary labels, guiding diverse
sample selection for the memory buffer. Extensive experiments show RAIS
outperforms state-of-the-art methods, achieving an average Equal Error Rate
(EER) of 1.953 % across five experiences. The code is available at:
https://github.com/falihgoz/RAIS.

</details>


### [460] [Acoustic Classification of Maritime Vessels using Learnable Filterbanks](https://arxiv.org/abs/2505.23964)
*Jonas Elsborg,Tejs Vegge,Arghya Bhowmik*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的模型CATFISH，用于在多变的水声环境中可靠地分类船舶信号，测试准确率达96.63%。


<details>
  <summary>Details</summary>
Motivation: 由于不同记录场景的变异性，可靠地监测和识别船舶声学信号具有挑战性，需要一种能泛化到多样化声学环境的分类框架。

Method: 使用可训练的频谱前端和时序特征编码器学习Gabor滤波器组，动态强调不同频率成分。

Result: 在VTUAD水听器记录上测试，模型准确率达96.63%，比之前基准提高了12个百分点。

Conclusion: CATFISH模型在多变环境中表现优异，通过分析Gabor滤波器和消融实验验证了其有效性。

Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic
signatures is complicated by the variability of different recording scenarios.
A robust classification framework must be able to generalize across diverse
acoustic environments and variable source-sensor distances. To this end, we
present a deep learning model with robust performance across different
recording scenarios. Using a trainable spectral front-end and temporal feature
encoder to learn a Gabor filterbank, the model can dynamically emphasize
different frequency components. Trained on the VTUAD hydrophone recordings from
the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %
percent test accuracy across varying source-sensor distances, surpassing the
previous benchmark by over 12 percentage points. We present the model, justify
our architectural choices, analyze the learned Gabor filters, and perform
ablation studies on sensor data fusion and attention-based pooling.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [461] [When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds](https://arxiv.org/abs/2505.24336)
*Minsu Kang,Seolhee Lee,Choonghyeon Lee,Namhyun Cho*

Main category: eess.AS

TL;DR: 本文提出了一种改进的CVAE模型，用于将人类语音转换为多样化的非人类声音（如动物叫声或合成声音），支持44.1kHz高质量音频转换。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于狗叫声和低采样率音频转换，本文旨在扩展至更广泛的非语音声音（如狮子吼叫、鸟鸣等）和高质量音频生成。

Method: 引入预处理流程和改进的CVAE模型，优化人类与非人类声音转换。

Result: 实验表明，该方法在质量、自然度和相似度MOS上优于基线，能有效转换多种非人类音色。

Conclusion: 该方法为多样化非人类声音转换提供了高质量解决方案。

Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into
animal or designed vocalizations. Unlike prior studies focused on dog-sounds
and 16 or 22.05kHz audio transformation, this work addresses a broader range of
non-speech sounds, including natural sounds (lion-roars, birdsongs) and
designed voice (synthetic growls). To accomodate generation of diverse
non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a
preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized
for human and non-human voices. Experimental results showed that the proposed
method outperformed baselines in quality, naturalness, and similarity MOS,
achieving effective voice conversion across diverse non-human timbres. Demo
samples are available at
https://nc-ai.github.io/speech/publications/nonhuman-vc/

</details>


### [462] [Can Emotion Fool Anti-spoofing?](https://arxiv.org/abs/2505.23962)
*Aurosweta Mahapatra,Ismail Rasim Ulgen,Abinay Reddy Naini,Carlos Busso,Berrak Sisman*

Main category: eess.AS

TL;DR: 论文提出EmoSpoof-TTS数据集和GEM方法，解决传统反欺骗系统在情感合成语音上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统反欺骗系统主要针对中性语音，忽略情感变化，导致对高质量情感合成语音的鲁棒性不足。

Method: 引入EmoSpoof-TTS数据集，并提出GEM（基于情感识别门控网络的集成模型）。

Result: 现有模型在情感语音上表现不佳，GEM显著提升防御效果。

Conclusion: 需关注情感反欺骗，GEM为有效解决方案。

Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic
speech with mostly neutral state, neglecting diverse emotional variations. As a
result, their robustness against high-quality, emotionally expressive synthetic
speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of
emotional text-to-speech samples. Our analysis shows existing anti-spoofing
models struggle with emotional synthetic speech, exposing risks of
emotion-targeted attacks. Even trained on emotional data, the models
underperform due to limited focus on emotional aspect and show performance
disparities across emotions. This highlights the need for emotion-focused
anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated
ensemble of emotion-specialized models with a speech emotion recognition gating
network. GEM performs effectively across all emotions and neutral state,
improving defenses against spoofing attacks. We release the EmoSpoof-TTS
Dataset: https://emospoof-tts.github.io/Dataset/

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [463] [Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System](https://arxiv.org/abs/2505.24429)
*Giovanny C-Londoño,Javier Sánchez,Ángel Rodríguez-Santana*

Main category: physics.ao-ph

TL;DR: 该研究将图神经网络应用于区域海洋预测，特别是在加那利上升流系统，结果显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统海洋预测方法计算成本高且速度慢，深度学习模型虽快但依赖模拟数据，可能不反映现实。研究旨在探索深度学习模型在区域海洋精细预测中的潜力。

Method: 使用卫星数据训练图神经网络，并与物理海洋模型（如ConvLSTM和GLORYS再分析）进行性能对比。

Result: 深度学习模型在复杂区域（如Cape Ghir等）的预测精度显著提升，5天预报误差减少高达76%。

Conclusion: 研究表明，将气象数据驱动模型应用于区域海洋中短期预测具有可行性。

Abstract: Oceanographic forecasting impacts various sectors of society by supporting
environmental conservation and economic activities. Based on global circulation
models, traditional forecasting methods are computationally expensive and slow,
limiting their ability to provide rapid forecasts. Recent advances in deep
learning offer faster and more accurate predictions, although these data-driven
models are often trained with global data from numerical simulations, which may
not reflect reality. The emergence of such models presents great potential for
improving ocean prediction at a subregional domain. However, their ability to
predict fine-scale ocean processes, like mesoscale structures, remains largely
unknown. This work aims to adapt a graph neural network initially developed for
global weather forecasting to improve subregional ocean prediction,
specifically focusing on the Canary Current upwelling system. The model is
trained with satellite data and compared to state-of-the-art physical ocean
models to assess its performance in capturing ocean dynamics. Our results show
that the deep learning model surpasses traditional methods in precision despite
some challenges in upwelling areas. It demonstrated superior performance in
reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis,
particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape
Bojador, and Cape Blanc. The model achieved improvements of up to 26.5%
relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts
compared to the GLORYS reanalysis at these critical locations, highlighting its
enhanced capability to capture spatial variability and improve predictive
accuracy in complex areas. These findings suggest the viability of adapting
meteorological data-driven models for improving subregional medium-term ocean
forecasting.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [464] [Distributed gradient methods under heavy-tailed communication noise](https://arxiv.org/abs/2505.24464)
*Manojlo Vukovic,Dusan Jakovetic,Dragana Bajovic,Soummya Kar*

Main category: math.OC

TL;DR: 本文提出了一种分布式梯度方法，用于解决在通信噪声为厚尾分布时的优化问题，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在密集部署的无线传感器和物联网网络中，通信噪声常呈现厚尾分布，现有方法无法有效处理此类噪声，因此需要设计新的分布式优化方法。

Method: 设计了一种分布式梯度方法，结合了时间尺度的共识和梯度步长，以及非线性算子来限制厚尾噪声的影响。

Result: 理论证明该方法在均方误差意义下收敛到解邻域，并表征了收敛速率；实验验证了其对厚尾噪声的鲁棒性。

Conclusion: 所提方法能有效处理厚尾噪声，且通过调节步长可进一步减小误差，但可能牺牲收敛速度。

Abstract: We consider a standard distributed optimization problem in which networked
nodes collaboratively minimize the sum of their locally known convex costs. For
this setting, we address for the first time the fundamental problem of design
and analysis of distributed methods to solve the above problem when inter-node
communication is subject to \emph{heavy-tailed} noise. Heavy-tailed noise is
highly relevant and frequently arises in densely deployed wireless sensor and
Internet of Things (IoT) networks. Specifically, we design a distributed
gradient-type method that features a carefully balanced mixed time-scale
time-varying consensus and gradient contribution step sizes and a bounded
nonlinear operator on the consensus update to limit the effect of heavy-tailed
noise. Assuming heterogeneous strongly convex local costs with mutually
different minimizers that are arbitrarily far apart, we show that the proposed
method converges to a neighborhood of the network-wide problem solution in the
mean squared error (MSE) sense, and we also characterize the corresponding
convergence rate. We further show that the asymptotic MSE can be made
arbitrarily small through consensus step-size tuning, possibly at the cost of
slowing down the transient error decay. Numerical experiments corroborate our
findings and demonstrate the resilience of the proposed method to heavy-tailed
(and infinite variance) communication noise. They also show that existing
distributed methods, designed for finite-communication-noise-variance settings,
fail in the presence of infinite variance noise.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [465] [Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play](https://arxiv.org/abs/2505.18334)
*Jiaxun Cui,Chen Tang,Jarrett Holtz,Janice Nguyen,Alessandro G. Allievi,Hang Qiu,Peter Stone*

Main category: cs.RO

TL;DR: 论文提出了一种名为LLM+Debrief的方法，通过自然语言通信提升自动驾驶车辆的协作能力，比零样本LLM代理更有效。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆间的通信通常不易被人理解，而自然语言通信能促进车辆间及与人类驾驶员的协作。

Method: 提出LLM+Debrief方法，通过多智能体讨论学习消息生成和决策策略，并开发了模拟环境进行测试。

Result: 实验表明，LLM+Debrief在生成有意义且易理解的自然语言消息方面优于零样本LLM代理。

Conclusion: 自然语言通信能有效提升自动驾驶车辆的协作能力，LLM+Debrief方法具有实际应用潜力。

Abstract: Past work has demonstrated that autonomous vehicles can drive more safely if
they communicate with one another than if they do not. However, their
communication has often not been human-understandable. Using natural language
as a vehicle-to-vehicle (V2V) communication protocol offers the potential for
autonomous vehicles to drive cooperatively not only with each other but also
with human drivers. In this work, we propose a suite of traffic tasks in
autonomous driving where vehicles in a traffic scenario need to communicate in
natural language to facilitate coordination in order to avoid an imminent
collision and/or support efficient traffic flow. To this end, this paper
introduces a novel method, LLM+Debrief, to learn a message generation and
high-level decision-making policy for autonomous vehicles through multi-agent
discussion. To evaluate LLM agents for driving, we developed a gym-like
simulation environment that contains a range of driving scenarios. Our
experimental results demonstrate that LLM+Debrief is more effective at
generating meaningful and human-understandable natural language messages to
facilitate cooperation and coordination than a zero-shot LLM agent. Our code
and demo videos are available at https://talking-vehicles.github.io/.

</details>


### [466] [SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping](https://arxiv.org/abs/2505.24305)
*Mingxu Zhang,Xiaoqi Li,Jiahui Xu,Kaichen Zhou,Hojin Bae,Yan Shen,Chuyan Xiong,Jiaming Liu,Hao Dong*

Main category: cs.RO

TL;DR: SR3D是一种无需训练的单视图3D重建框架，用于解决透明和镜面物体的机器人抓取问题，通过结合RGB和深度图像生成准确的3D深度图。


<details>
  <summary>Details</summary>
Motivation: 透明和镜面材料因深度感知限制难以抓取，现有方法复杂或信息利用率低。

Method: 利用单视图RGB和深度图像，通过外部视觉模型生成3D网格，再通过视图和关键点匹配确定物体位姿和尺度，重建准确3D深度图。

Result: 仿真和真实实验验证了SR3D的重建效果。

Conclusion: SR3D为透明和镜面物体的抓取提供了高效解决方案。

Abstract: Recent advancements in 3D robotic manipulation have improved grasping of
everyday objects, but transparent and specular materials remain challenging due
to depth sensing limitations. While several 3D reconstruction and depth
completion approaches address these challenges, they suffer from setup
complexity or limited observation information utilization. To address this,
leveraging the power of single view 3D object reconstruction approaches, we
propose a training free framework SR3D that enables robotic grasping of
transparent and specular objects from a single view observation. Specifically,
given single view RGB and depth images, SR3D first uses the external visual
models to generate 3D reconstructed object mesh based on RGB image. Then, the
key idea is to determine the 3D object's pose and scale to accurately localize
the reconstructed object back into its original depth corrupted 3D scene.
Therefore, we propose view matching and keypoint matching mechanisms,which
leverage both the 2D and 3D's inherent semantic and geometric information in
the observation to determine the object's 3D state within the scene, thereby
reconstructing an accurate 3D depth map for effective grasp detection.
Experiments in both simulation and real world show the reconstruction
effectiveness of SR3D.

</details>


### [467] [Black-box Adversarial Attacks on CNN-based SLAM Algorithms](https://arxiv.org/abs/2505.24654)
*Maria Rafaela Gkeka,Bowen Sun,Evgenia Smirni,Christos D. Antonopoulos,Spyros Lalis,Nikolaos Bellas*

Main category: cs.RO

TL;DR: 论文研究了针对基于CNN的SLAM系统的对抗攻击，发现即使是中等规模的攻击也能导致76%的帧跟踪失败，且攻击深度输入比RGB输入更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在SLAM任务中取得了显著进展，但深度神经网络对对抗攻击的脆弱性限制了其可靠性。目前缺乏对针对CNN特征检测器的对抗攻击的全面研究。

Method: 研究采用黑盒对抗扰动方法，攻击输入GCN-SLAM算法的RGB图像，并在TUM数据集上进行实验。

Result: 实验结果显示，中等规模的对抗攻击可导致76%的帧跟踪失败，且攻击深度输入对SLAM系统的破坏性更大。

Conclusion: 论文揭示了对抗攻击对基于CNN的SLAM系统的严重威胁，强调了在安全关键应用中需加强防御措施。

Abstract: Continuous advancements in deep learning have led to significant progress in
feature detection, resulting in enhanced accuracy in tasks like Simultaneous
Localization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural
networks to adversarial attacks remains a challenge for their reliable
deployment in applications, such as navigation of autonomous agents. Even
though CNN-based SLAM algorithms are a growing area of research there is a
notable absence of a comprehensive presentation and examination of adversarial
attacks targeting CNN-based feature detectors, as part of a SLAM system. Our
work introduces black-box adversarial perturbations applied to the RGB images
fed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal
that even attacks of moderate scale can lead to tracking failure in as many as
76% of the frames. Moreover, our experiments highlight the catastrophic impact
of attacking depth instead of RGB input images on the SLAM system.

</details>


### [468] [DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics](https://arxiv.org/abs/2505.24786)
*Eran Bamani Beeri,Eden Nissinman,Avishai Sintov*

Main category: cs.RO

TL;DR: 提出了一种名为DiG-Net的新型动态手势识别方法，适用于远距离（30米）的辅助机器人交互，结合DADA块和时空图模块，显著提升了识别准确率（97.3%）。


<details>
  <summary>Details</summary>
Motivation: 当前手势识别方法局限于短距离交互，限制了辅助机器人在远距离场景中的应用，亟需一种能适应远距离、复杂环境的手势识别技术。

Method: 采用DiG-Net，结合Depth-Conditioned Deformable Alignment (DADA)块和时空图模块，并引入RSTDAL损失函数以增强模型鲁棒性。

Result: 在多样化数据集上实现了97.3%的识别准确率，显著优于现有方法。

Conclusion: DiG-Net显著提升了辅助机器人在远距离交互中的实用性，适用于家庭医疗、工业安全和远程协助等场景。

Abstract: Dynamic hand gestures play a pivotal role in assistive human-robot
interaction (HRI), facilitating intuitive, non-verbal communication,
particularly for individuals with mobility constraints or those operating
robots remotely. Current gesture recognition methods are mostly limited to
short-range interactions, reducing their utility in scenarios demanding robust
assistive communication from afar. In this paper, we introduce a novel approach
designed specifically for assistive robotics, enabling dynamic gesture
recognition at extended distances of up to 30 meters, thereby significantly
improving accessibility and quality of life. Our proposed Distance-aware
Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable
Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust
processing and classification of gesture sequences captured under challenging
conditions, including significant physical attenuation, reduced resolution, and
dynamic gesture variations commonly experienced in real-world assistive
environments. We further introduce the Radiometric Spatio-Temporal Depth
Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model
robustness across varying distances. Our model demonstrates significant
performance improvement over state-of-the-art gesture recognition frameworks,
achieving a recognition accuracy of 97.3% on a diverse dataset with challenging
hyper-range gestures. By effectively interpreting gestures from considerable
distances, DiG-Net significantly enhances the usability of assistive robots in
home healthcare, industrial safety, and remote assistance scenarios, enabling
seamless and intuitive interactions for users regardless of physical
limitations

</details>


### [469] [Bi-Manual Joint Camera Calibration and Scene Representation](https://arxiv.org/abs/2505.24819)
*Haozhan Tang,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: Bi-JCR框架通过3D基础模型实现多机器人臂的相机标定、相对位姿估计和共享工作空间3D表示，无需标定标记。


<details>
  <summary>Details</summary>
Motivation: 传统相机标定过程繁琐，需采集标定标记图像。Bi-JCR旨在简化多机器人臂的标定与协作任务。

Method: 利用3D基础模型实现无标记多视角对应，联合估计相机外参、机器人臂相对位姿及共享工作空间3D表示。

Result: 实验验证了Bi-JCR在多种桌面环境中的鲁棒性，并展示其在下游任务中的应用潜力。

Conclusion: Bi-JCR为多机器人臂协作提供了一种高效、无标记的标定与表示解决方案。

Abstract: Robot manipulation, especially bimanual manipulation, often requires setting
up multiple cameras on multiple robot manipulators. Before robot manipulators
can generate motion or even build representations of their environments, the
cameras rigidly mounted to the robot need to be calibrated. Camera calibration
is a cumbersome process involving collecting a set of images, with each
capturing a pre-determined marker. In this work, we introduce the Bi-Manual
Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables
multiple robot manipulators, each with cameras mounted, to circumvent taking
images of calibration markers. By leveraging 3D foundation models for dense,
marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the
extrinsic transformation from each camera to its end-effector, (ii) the
inter-arm relative poses between manipulators, and (iii) a unified,
scale-consistent 3D representation of the shared workspace, all from the same
captured RGB image sets. The representation, jointly constructed from images
captured by cameras on both manipulators, lives in a common coordinate frame
and supports collision checking and semantic segmentation to facilitate
downstream bimanual coordination tasks. We empirically evaluate the robustness
of Bi-JCR on a variety of tabletop environments, and demonstrate its
applicability on a variety of downstream tasks.

</details>


### [470] [RealDrive: Retrieval-Augmented Driving with Diffusion Models](https://arxiv.org/abs/2505.24808)
*Wenhao Ding,Sushant Veer,Yuxiao Chen,Yulong Cao,Chaowei Xiao,Marco Pavone*

Main category: cs.RO

TL;DR: RealDrive是一种基于检索增强生成（RAG）的框架，通过检索训练数据中最相关的专家演示来初始化扩散规划策略，从而提升安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动方法在罕见、安全关键场景中的不足以及轨迹生成的有限可控性问题。

Method: 提出RealDrive框架，结合检索增强生成和扩散模型，通过检索相关专家演示并插值生成安全且可控的轨迹。

Result: 在Waymo Open Motion数据集上，碰撞率降低40%，并提升了对长尾事件的泛化能力和轨迹多样性。

Conclusion: RealDrive通过检索增强生成显著提升了规划性能，尤其在安全性和可控性方面表现优异。

Abstract: Learning-based planners generate natural human-like driving behaviors by
learning to reason about nuanced interactions from data, overcoming the rigid
behaviors that arise from rule-based planners. Nonetheless, data-driven
approaches often struggle with rare, safety-critical scenarios and offer
limited controllability over the generated trajectories. To address these
challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)
framework that initializes a diffusion-based planning policy by retrieving the
most relevant expert demonstrations from the training dataset. By interpolating
between current observations and retrieved examples through a denoising
process, our approach enables fine-grained control and safe behavior across
diverse scenarios, leveraging the strong prior provided by the retrieved
scenario. Another key insight we produce is that a task-relevant retrieval
model trained with planning-based objectives results in superior planning
performance in our framework compared to a task-agnostic retriever.
Experimental results demonstrate improved generalization to long-tail events
and enhanced trajectory diversity compared to standard learning-based planners
-- we observe a 40% reduction in collision rate on the Waymo Open Motion
dataset with RAG.

</details>


### [471] [DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation](https://arxiv.org/abs/2505.24853)
*Zhao Mandi,Yifan Hou,Dieter Fox,Yashraj Narang,Ajay Mandlekar,Shuran Song*

Main category: cs.RO

TL;DR: DexMachina是一种基于课程的算法，用于学习灵巧操作策略，以跟踪人类手-物体演示中的物体状态，特别针对长时程、双手操作任务。


<details>
  <summary>Details</summary>
Motivation: 研究灵巧操作策略的学习问题，尤其是针对长时程、双手操作任务，解决动作空间大、时空不连续性和人机手部差异等挑战。

Method: 提出DexMachina算法，利用虚拟物体控制器（强度逐渐衰减）引导策略学习，逐步接管运动与接触控制。

Result: 在模拟基准测试中，DexMachina显著优于基线方法，并提供了硬件功能比较的关键发现。

Conclusion: DexMachina为灵巧手硬件设计提供了功能比较平台，降低了未来研究的门槛。

Abstract: We study the problem of functional retargeting: learning dexterous
manipulation policies to track object states from human hand-object
demonstrations. We focus on long-horizon, bimanual tasks with articulated
objects, which is challenging due to large action space, spatiotemporal
discontinuities, and embodiment gap between human and robot hands. We propose
DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual
object controllers with decaying strength: an object is first driven
automatically towards its target states, such that the policy can gradually
learn to take over under motion and contact guidance. We release a simulation
benchmark with a diverse set of tasks and dexterous hands, and show that
DexMachina significantly outperforms baseline methods. Our algorithm and
benchmark enable a functional comparison for hardware designs, and we present
key findings informed by quantitative and qualitative results. With the recent
surge in dexterous hand development, we hope this work will provide a useful
platform for identifying desirable hardware capabilities and lower the barrier
for contributing to future research. Videos and more at
https://project-dexmachina.github.io/

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [472] [Cascades on Constrained Multiplex Networks](https://arxiv.org/abs/2505.24631)
*Christian Kluge,Christian Kuehn*

Main category: nlin.AO

TL;DR: 论文研究了有向多重配置模型网络上的Watts级联模型，分析了级联规模、单种子级联概率和级联条件，并引入约束多重网络模型以诱导节点活动模式，发现这些模式会影响级联模型的相变。


<details>
  <summary>Details</summary>
Motivation: 研究网络结构和节点活动模式对级联行为的影响，特别是在多重网络中的动态特性。

Method: 使用有向多重配置模型网络，分析级联模型的特性，并引入约束多重网络模型以诱导节点活动模式。

Result: 发现诱导的节点活动模式会以多种方式影响级联模型的相变。

Conclusion: 网络结构和节点活动模式对级联行为有显著影响，约束多重网络模型为研究此类动态提供了新视角。

Abstract: We consider a version of the Watts cascade model on directed multiplex
configuration model networks, and present a detailed analysis of the cascade
size, single-seed cascade probability and cascade condition. We then introduce
a smaller class of network models that we call constrained multiplex networks,
which allows us to induce patterns in the node activity, i.e. in the
participation of nodes on different layers. We find that the choice of induced
patterns affects the phase transitions of the cascade model in a variety of
ways.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [473] [3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians](https://arxiv.org/abs/2505.24053)
*Zixun Huang,Cho-Ying Wu,Yuliang Guo,Xinyu Huang,Liu Ren*

Main category: cs.GR

TL;DR: 3DGEER提出了一种精确且高效的3D高斯渲染方法，解决了3DGS在大视场角下的渲染质量限制，并通过新的投影表示和优化策略提升了性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS在平衡渲染质量和效率方面取得了进展，但其近似方法在大视场角下限制了渲染质量。现有方法未能同时实现精确性和高效性。

Method: 从基本原理出发，推导了3D高斯分布沿射线的密度积分闭式表达式，提出了粒子边界视锥体和双极等角投影表示，以优化渲染和射线关联。

Result: 在多个数据集上，3DGEER表现优于现有方法，实现了实时神经渲染的最新水平。

Conclusion: 3DGEER通过精确的渲染和高效优化策略，显著提升了3D高斯渲染的质量和效率，为实时神经渲染设定了新标准。

Abstract: 3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the
quality and efficiency of differentiable rendering. However, its high
efficiency stems from an approximation of projecting 3D Gaussians onto the
image plane as 2D Gaussians, which inherently limits rendering
quality--particularly under large Field-of-View (FoV) camera inputs. While
several recent works have extended 3DGS to mitigate these approximation errors,
none have successfully achieved both exactness and high efficiency
simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient
Volumetric Gaussian Rendering method. Starting from first principles, we derive
a closed-form expression for the density integral along a ray traversing a 3D
Gaussian distribution. This formulation enables precise forward rendering with
arbitrary camera models and supports gradient-based optimization of 3D Gaussian
parameters. To ensure both exactness and real-time performance, we propose an
efficient method for computing a tight Particle Bounding Frustum (PBF) for each
3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also
introduce a novel Bipolar Equiangular Projection (BEAP) representation to
accelerate ray association under generic camera models. BEAP further provides a
more uniform ray sampling strategy to apply supervision, which empirically
improves reconstruction quality. Experiments on multiple pinhole and fisheye
datasets show that our method consistently outperforms prior methods,
establishing a new state-of-the-art in real-time neural rendering.

</details>


### [474] [TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores](https://arxiv.org/abs/2505.24796)
*Zimu Liao,Jifeng Ding,Rong Fu,Siwei Cui,Ruixuan Gong,Li Wang,Boni Hu,Yi Wang,Hengjie Li,XIngcheng Zhang,Hui Wang*

Main category: cs.GR

TL;DR: TC-GS通过将alpha计算映射为矩阵乘法，利用Tensor Core加速3D高斯渲染，提升速度2.18倍，总加速达5.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯渲染中，alpha混合是时间瓶颈，Tensor Core利用率不足。

Method: 提出TC-GS模块，将alpha计算转为矩阵乘法，并引入全局-局部坐标变换减少误差。

Result: 实验显示渲染质量不变，速度提升2.18倍，总加速5.6倍。

Conclusion: TC-GS为现有3D高斯渲染提供即插即用的加速方案。

Abstract: 3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where conditional alpha-blending dominates the time cost in the
rendering pipeline. This paper proposes TC-GS, an algorithm-independent
universal module that expands Tensor Core (TCU) applicability for 3DGS, leading
to substantial speedups and seamless integration into existing 3DGS
optimization frameworks. The key innovation lies in mapping alpha computation
to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS
implementations. TC-GS provides plug-and-play acceleration for existing
top-tier acceleration algorithms tightly coupled with rendering pipeline
designs, like Gaussian compression and redundancy elimination algorithms.
Additionally, we introduce a global-to-local coordinate transformation to
mitigate rounding errors from quadratic terms of pixel coordinates caused by
Tensor Core half-precision computation. Extensive experiments demonstrate that
our method maintains rendering quality while providing an additional 2.18x
speedup over existing Gaussian acceleration algorithms, thus reaching up to a
total 5.6x acceleration. The code is currently available at anonymous
\href{https://github.com/TensorCore3DGS/3DGSTensorCore}

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [475] [Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM](https://arxiv.org/abs/2505.24062)
*Kamyar Barakati,Yu Liu,Hiroshi Funakubo,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 论文研究了铁电材料中畴壁动力学的空间依赖性，通过机器学习控制的自动压电力显微镜技术量化了电场驱动的极应变畴结构动态行为。


<details>
  <summary>Details</summary>
Motivation: 畴壁动力学在铁电材料中具有强烈的空间依赖性，传统方法难以高效研究稀疏的钉扎中心和畴壁，因此需要自动化高分辨率成像技术。

Method: 使用机器学习控制的自动压电力显微镜技术，分析了大面积外延PbTiO3薄膜上的1500次畴壁切换事件。

Result: 畴壁位移不仅依赖于电场参数，还受局部铁电-铁弹构型影响，例如双边界和多变体边界的行为差异显著。

Conclusion: 研究结果可转化为预测性图谱，为设计铁电存储器提供了微观结构特定的规则基础。

Abstract: Domain-wall dynamics in ferroelectric materials are strongly
position-dependent since each polar interface is locked into a unique local
microstructure. This necessitates spatially resolved studies of the
wall-pinning using scanning-probe microscopy techniques. The pinning centers
and preexisting domain walls are usually sparse within image plane, precluding
the use of dense hyperspectral imaging modes and requiring time-consuming human
experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$
were investigated to quantify the electric field driven dynamics of the
polar-strain domain structures using ML-controlled automated Piezoresponse
Force Microscopy. Analysis of 1500 switching events reveals that domain wall
displacement depends not only on field parameters but also on the local
ferroelectric-ferroelastic configuration. For example, twin boundaries in
polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up
to a certain level of bias magnitude and change only marginally as the bias
increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$
$\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics
on the possible ferroelectric and ferroelastic wall orientations, together with
the automated, high-throughput AFM workflow, can be distilled into a predictive
map that links domain configurations to pulse parameters. This
microstructure-specific rule set forms the foundation for designing
ferroelectric memories.

</details>


### [476] [Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models](https://arxiv.org/abs/2505.24065)
*Isaiah A. Moses,Chen Chen,Joan M. Redwing,Wesley F. Reinhart*

Main category: cond-mat.mtrl-sci

TL;DR: 机器学习（ML）通过历史数据指导材料生长和表征，提高效率，并实现跨模态定量分析。


<details>
  <summary>Details</summary>
Motivation: 传统材料生长和表征依赖专家经验，耗时耗力，ML可提供高效、经济的替代方案。

Method: 利用生成模型从AFM和光谱数据中相互预测定量指标，并生成完整特征。

Result: 实验证明ML在跨模态表征材料中具有潜力，为加速材料发现提供基础。

Conclusion: ML为材料表征提供高效、经济的跨模态解决方案，具有广泛应用前景。

Abstract: The growth and characterization of materials using empirical optimization
typically requires a significant amount of expert time, experience, and
resources. Several complementary characterization methods are routinely
performed to determine the quality and properties of a grown sample. Machine
learning (ML) can support the conventional approaches by using historical data
to guide and provide speed and efficiency to the growth and characterization of
materials. Specifically, ML can provide quantitative information from
characterization data that is typically obtained from a different modality. In
this study, we have investigated the feasibility of projecting the quantitative
metric from microscopy measurements, such as atomic force microscopy (AFM),
using data obtained from spectroscopy measurements, like Raman spectroscopy.
Generative models were also trained to generate the full and specific features
of the Raman and photoluminescence spectra from each other and the AFM images
of the thin film MoS$_2$. The results are promising and have provided a
foundational guide for the use of ML for the cross-modal characterization of
materials for their accelerated, efficient, and cost-effective discovery.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [477] [On the Parallels Between Evolutionary Theory and the State of AI](https://arxiv.org/abs/2505.23774)
*Zeki Doruk Erden,Boi Faltings*

Main category: q-bio.NC

TL;DR: 本文批判性分析了当代AI方法的基础原则，探讨了限制其潜力的局限性，并借鉴进化生物学的现代综合理论，提出通过进化发育生物学的见解为AI设计新范式。


<details>
  <summary>Details</summary>
Motivation: 揭示当代AI方法的局限性，并借鉴进化理论的最新进展，为AI设计提供新思路。

Method: 通过比较现代AI与20世纪进化生物学的现代综合理论，结合进化发育生物学的见解，提出新的AI设计路径。

Result: 提出了一种克服AI现有局限性的新范式，以实现其更高目标。

Conclusion: 通过跨学科融合，进化理论可以为AI的未来发展提供重要启示。

Abstract: This article critically examines the foundational principles of contemporary
AI methods, exploring the limitations that hinder its potential. We draw
parallels between the modern AI landscape and the 20th-century Modern Synthesis
in evolutionary biology, and highlight how advancements in evolutionary theory
that augmented the Modern Synthesis, particularly those of Evolutionary
Developmental Biology, offer insights that can inform a new design paradigm for
AI. By synthesizing findings across AI and evolutionary theory, we propose a
pathway to overcome existing limitations, enabling AI to achieve its
aspirational goals.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [478] [Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures](https://arxiv.org/abs/2505.24117)
*Ananya Omanwar,Fady Alajaji,Tamás Linder*

Main category: cs.IT

TL;DR: 论文研究了在马尔可夫链$Y \to X \to Z$下，利用广义信息散度度量推导出超额最小风险的上界。


<details>
  <summary>Details</summary>
Motivation: 目标是估计从观测特征向量$X$或其随机退化版本$Z$中预测目标向量$Y$时的超额最小风险。

Method: 使用Rényi散度、$\alpha$-Jensen-Shannon散度和Sibson互信息等广义信息散度度量，推广了Györfi等人的基于互信息的界。

Result: 提出的界适用于更广泛的联合分布，且在特定参数$\alpha$下比基于互信息的界更紧。

Conclusion: 广义散度度量提供了更灵活且可能更紧的界，适用于非恒定次高斯参数的情况。

Abstract: Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov
chain in that order (i.e., $Y \to X \to Z$), we derive upper bounds on the
excess minimum risk using generalized information divergence measures. Here,
$Y$ is a target vector to be estimated from an observed feature vector $X$ or
its stochastically degraded version $Z$. The excess minimum risk is defined as
the difference between the minimum expected loss in estimating $Y$ from $X$ and
from $Z$. We present a family of bounds that generalize the mutual information
based bound of Gy\"orfi et al. (2023), using the R\'enyi and
$\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information.
Our bounds are similar to those developed by Modak et al. (2021) and Aminian et
al. (2024) for the generalization error of learning algorithms. However, unlike
these works, our bounds do not require the sub-Gaussian parameter to be
constant and therefore apply to a broader class of joint distributions over
$Y$, $X$, and $Z$. We also provide numerical examples under both constant and
non-constant sub-Gaussianity assumptions, illustrating that our generalized
divergence based bounds can be tighter than the one based on mutual information
for certain regimes of the parameter $\alpha$.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [479] [Searching Clinical Data Using Generative AI](https://arxiv.org/abs/2505.24090)
*Karan Hanswadkar,Anika Kanchi,Shivani Tripathi,Shi Qiao,Rony Chatterjee,Alekh Jindal*

Main category: cs.DB

TL;DR: SearchAI是一种生成式AI方法，用于提高临床数据搜索的准确性和效率，通过分层模型优化编码层次结构遍历。


<details>
  <summary>Details</summary>
Motivation: 临床数据搜索通常是一个手动且容易出错的过程，需要处理一对多的问题，传统方法无法满足需求。

Method: 采用分层模型，优化编码层次结构的遍历，支持关键词同义词、语义变体和开放式查询。

Result: 实验表明，SearchAI在准确性、鲁棒性、性能和可扩展性上优于传统方法。

Conclusion: SearchAI能提升临床数据的可访问性，优化工作流程并提高编码和诊断准确性。

Abstract: Artificial Intelligence (AI) is making a major impact on healthcare,
particularly through its application in natural language processing (NLP) and
predictive analytics. The healthcare sector has increasingly adopted AI for
tasks such as clinical data analysis and medical code assignment. However,
searching for clinical information in large and often unorganized datasets
remains a manual and error-prone process. Assisting this process with
automations can help physicians improve their operational productivity
significantly.
  In this paper, we present a generative AI approach, coined SearchAI, to
enhance the accuracy and efficiency of searching clinical data. Unlike
traditional code assignment, which is a one-to-one problem, clinical data
search is a one-to-many problem, i.e., a given search query can map to a family
of codes. Healthcare professionals typically search for groups of related
diseases, drugs, or conditions that map to many codes, and therefore, they need
search tools that can handle keyword synonyms, semantic variants, and broad
open-ended queries. SearchAI employs a hierarchical model that respects the
coding hierarchy and improves the traversal of relationships from parent to
child nodes. SearchAI navigates these hierarchies predictively and ensures that
all paths are reachable without losing any relevant nodes.
  To evaluate the effectiveness of SearchAI, we conducted a series of
experiments using both public and production datasets. Our results show that
SearchAI outperforms default hierarchical traversals across several metrics,
including accuracy, robustness, performance, and scalability. SearchAI can help
make clinical data more accessible, leading to streamlined workflows, reduced
administrative burden, and enhanced coding and diagnostic accuracy.

</details>


### [480] [Towards Scalable Schema Mapping using Large Language Models](https://arxiv.org/abs/2505.24716)
*Christopher Buss,Mahdis Safari,Arash Termehchy,Stefan Lee,David Maier*

Main category: cs.DB

TL;DR: 论文探讨了利用大语言模型（LLMs）自动化模式匹配的挑战，并提出解决输出不一致、表达性映射需求和计算成本的方案。


<details>
  <summary>Details</summary>
Motivation: 数据集成系统依赖手动编写的模式映射，成本高且难以维护，LLMs虽能辅助自动化，但仍存在关键问题。

Method: 提出采样和聚合技术解决输出不一致，优化表达性映射（如GLaV），并通过数据类型预过滤降低计算成本。

Result: 解决了LLMs在模式匹配中的三大核心问题，提升了自动化数据集成的效率和可靠性。

Conclusion: 通过技术改进，LLMs在数据集成中的应用潜力得到显著提升，但仍需进一步优化。

Abstract: The growing need to integrate information from a large number of diverse
sources poses significant scalability challenges for data integration systems.
These systems often rely on manually written schema mappings, which are
complex, source-specific, and costly to maintain as sources evolve. While
recent advances suggest that large language models (LLMs) can assist in
automating schema matching by leveraging both structural and natural language
cues, key challenges remain. In this paper, we identify three core issues with
using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to
input phrasing and structure, which we propose methods to address through
sampling and aggregation techniques; (2) the need for more expressive mappings
(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the
computational cost of repeated LLM calls, which we propose to mitigate through
strategies like data type prefiltering.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [481] [A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study](https://arxiv.org/abs/2505.24006)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed,Ashit Baran Aich,Amanda Mayeaux,Asim K. Dey,Kumer P. Das,Bruce Wade*

Main category: stat.ME

TL;DR: A2-SBNN是一种新型空间预测模型，通过嵌入A2 Copula和贝叶斯神经网络，能够捕捉典型空间模式和极端依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统高斯方法难以处理空间数据中的复杂依赖关系，尤其是极端事件。A2-SBNN旨在提供更灵活的建模方式。

Method: 模型将A2 Copula嵌入网络权重初始化，结合Wasserstein损失、矩匹配和相关惩罚进行训练。

Result: 实验表明A2-SBNN在多种依赖强度下均表现高精度，优于传统方法。

Conclusion: A2-SBNN为空间数据建模提供了新的有效解决方案，尤其适用于复杂依赖关系。

Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network
(A2-SBNN), a predictive spatial model designed to map coordinates to continuous
fields while capturing both typical spatial patterns and extreme dependencies.
By embedding the dual-tail novel Archimedean copula viz. A2 directly into the
network's weight initialization, A2-SBNN naturally models complex spatial
relationships, including rare co-movements in the data. The model is trained
through a calibration-driven process combining Wasserstein loss, moment
matching, and correlation penalties to refine predictions and manage
uncertainty. Simulation results show that A2-SBNN consistently delivers high
accuracy across a wide range of dependency strengths, offering a new, effective
solution for spatial data modeling beyond traditional Gaussian-based
approaches.

</details>


### [482] [Data Fusion for Partial Identification of Causal Effects](https://arxiv.org/abs/2505.24296)
*Quinn Lanners,Cynthia Rudin,Alexander Volfovsky,Harsh Parikh*

Main category: stat.ME

TL;DR: 论文提出了一种新的部分识别框架，用于解决因果推断中同时违反无观测混杂和反事实结果可交换性假设的问题，通过可解释的敏感性参数和双重稳健估计器，增强了因果结论的稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同时违反无观测混杂和反事实结果可交换性假设时无法识别或估计因果效应，限制了实际应用。

Method: 提出部分识别框架，引入敏感性参数量化假设违反，开发双重稳健估计器，并应用崩溃前沿分析。

Result: 应用于Project STAR研究，证明其结论对关键假设的同时违反具有稳健性。

Conclusion: 新框架增强了因果推断的可靠性，尤其在假设可能被违反的实际场景中。

Abstract: Data fusion techniques integrate information from heterogeneous data sources
to improve learning, generalization, and decision making across data sciences.
In causal inference, these methods leverage rich observational data to improve
causal effect estimation, while maintaining the trustworthiness of randomized
controlled trials. Existing approaches often relax the strong no unobserved
confounding assumption by instead assuming exchangeability of counterfactual
outcomes across data sources. However, when both assumptions simultaneously
fail - a common scenario in practice - current methods cannot identify or
estimate causal effects. We address this limitation by proposing a novel
partial identification framework that enables researchers to answer key
questions such as: Is the causal effect positive or negative? and How severe
must assumption violations be to overturn this conclusion? Our approach
introduces interpretable sensitivity parameters that quantify assumption
violations and derives corresponding causal effect bounds. We develop doubly
robust estimators for these bounds and operationalize breakdown frontier
analysis to understand how causal conclusions change as assumption violations
increase. We apply our framework to the Project STAR study, which investigates
the effect of classroom size on students' third-grade standardized test
performance. Our analysis reveals that the Project STAR results are robust to
simultaneous violations of key assumptions, both on average and across various
subgroups of interest. This strengthens confidence in the study's conclusions
despite potential unmeasured biases in the data.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [483] [Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes](https://arxiv.org/abs/2505.24132)
*Kai Fukami,Ryo Araki*

Main category: physics.flu-dyn

TL;DR: 该论文提出了一种基于深度S型流神经网络的信息理论模态分解方法，用于分析机翼周围的分离尾流，揭示了与时间变化升力响应相关的信息性涡结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过数据驱动方法提取流场快照中的信息性成分，以捕捉与未来时间目标变量相关的因果关系，从而更好地理解非定常空气动力学问题。

Method: 方法采用深度S型流神经网络，对NACA0012翼型周围的三种分离流（层流周期性尾流、强涡-翼型相互作用和湍流尾流）进行模态分解。

Result: 结果显示，周期性尾流的信息性结构随时间变化与波动水平相关；涡-翼型相互作用中，涡对升力的影响随时间变化被清晰识别；湍流尾流中，模型仅基于信息度量即可识别翼附近和涡核的信息性结构。

Conclusion: 结论表明，该方法为一系列非定常空气动力学问题提供了基于因果关系的见解，无需先验空气动力学知识或长度尺度信息。

Abstract: We perform an information-theoretic mode decomposition for separated wakes
around a wing. The current data-driven approach based on a neural network
referred to as deep sigmoidal flow enables the extraction of an informative
component from a given flow field snapshot with respect to a target variable at
a future time stamp, thereby capturing the causality as a time-varying modal
structure. We consider three examples of separated flows around a NACA0012
airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2.
strong vortex-airfoil interactions, and 3. a turbulent wake in a
spanwise-periodic domain. The present approach reveals informative vortical
structures associated with a time-varying lift response. For the periodic
shedding cases, the informative structures vary in time corresponding to the
fluctuation level from their mean values. With the second example of
vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in
the lift response over time is identified in an interpretable manner.
Furthermore, for the case of turbulent wake, the present model highlights
structures near the wing and vortex cores as informative components based
solely on the information metric without any prior knowledge of aerodynamics
and length scales. This study provides causality-based insights into a range of
unsteady aerodynamic problems.

</details>
