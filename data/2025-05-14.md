<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 11]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.CV](#cs.CV) [Total: 69]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CL](#cs.CL) [Total: 35]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.LG](#cs.LG) [Total: 35]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment](https://arxiv.org/abs/2505.07875)
*John Brandt Brodersen,Ilaria Amelia Caggiano,Pedro Kringen,Vince Istvan Madai,Walter Osika,Giovanni Sartor,Ellen Svensson,Magnus Westerlund,Roberto V. Zicari*

Main category: cs.CY

TL;DR: 论文强调在医疗领域AI开发中，主动遵守AI法案（2024年生效）及伦理原则的重要性，以确保系统可信度和可持续性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI系统的可信度至关重要，需结合技术、证据和伦理实践，并符合即将生效的AI法案要求。

Method: 建议开发者和部署者主动采取措施，确保现有及未来AI系统符合AI法案，并基于伦理原则进行解释和应用。

Result: 通过遵守AI法案和伦理原则，可以提高AI系统的有效性和可持续性，保护公共利益。

Conclusion: AI法案的遵守不仅是形式要求，更是基于伦理原则的主动承诺，有助于提升AI系统的长期可信度。

Abstract: Assessments of trustworthiness have become a cornerstone of responsible AI
development. Especially in high-stakes fields like healthcare, aligning
technical, evidence-based, and ethical practices with forthcoming legal
requirements is increasingly urgent. We argue that developers and deployers of
AI systems for the medical domain should be proactive and take steps to
progressively ensure that such systems, both those currently in use and those
being developed or planned, respect the requirements of the AI Act, which has
come into force in August 2024. This is necessary if full and effective
compliance is to be ensured when the most relevant provisions of the Act become
effective (August 2026). The engagement with the AI Act cannot be viewed as a
formalistic exercise. Compliance with the AI Act needs to be carried out
through the proactive commitment to the ethical principles of trustworthy AI.
These principles provide the background for the Act, which mentions them
several times and connects them to the protection of public interest. They can
be used to interpret and apply the Act's provisions and to identify good
practices, increasing the validity and sustainability of AI systems over time.

</details>


### [2] [LECTOR: Summarizing E-book Reading Content for Personalized Student Support](https://arxiv.org/abs/2505.07898)
*Erwin Daniel López Zapata,Cheng Tang,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.CY

TL;DR: 论文提出LECTOR模型，结合阅读内容和活动数据，提升学习分析和预测低分学生的效果。实验显示其在信息提取和预测性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前教育电子书平台的分析多依赖阅读活动数据，而阅读内容数据常被忽视。研究旨在填补这一空白。

Method: 提出LECTOR模型，整合阅读内容数据，并通过实验验证其性能。

Result: LECTOR在信息提取（F1-score提升5%）和预测低分学生（性能改善）上表现优于现有方法。

Conclusion: LECTOR为个性化教育干预提供了新工具，展示了阅读内容数据的潜在价值。

Abstract: Educational e-book platforms provide valuable information to teachers and
researchers through two main sources: reading activity data and reading content
data. While reading activity data is commonly used to analyze learning
strategies and predict low-performing students, reading content data is often
overlooked in these analyses. To address this gap, this study proposes LECTOR
(Lecture slides and Topic Relationships), a model that summarizes information
from reading content in a format that can be easily integrated with reading
activity data. Our first experiment compared LECTOR to representative Natural
Language Processing (NLP) models in extracting key information from 2,255
lecture slides, showing an average improvement of 5% in F1-score. These results
were further validated through a human evaluation involving 28 students, which
showed an average improvement of 21% in F1-score over a model predominantly
used in current educational tools. Our second experiment compared reading
preferences extracted by LECTOR with traditional reading activity data in
predicting low-performing students using 600,712 logs from 218 students. The
results showed a tendency to improve the predictive performance by integrating
LECTOR. Finally, we proposed examples showing the potential application of the
reading preferences extracted by LECTOR in designing personalized interventions
for students.

</details>


### [3] [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
*Ruikun Hou,Babette Bühler,Tim Fütterer,Efe Bozkir,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CY

TL;DR: 本文提出了一种多模态融合架构，用于评估课堂话语质量，结合文本、音频和视频数据，通过注意力机制和多任务学习提升预测效果，结果与人工评分一致性较高。


<details>
  <summary>Details</summary>
Motivation: 传统课堂话语质量评估依赖人工编码，耗时且成本高。现有AI研究多关注单一句子层面，缺乏对整个课程段话语实践的综合评估。

Method: 采用注意力机制捕捉多模态交互，多任务学习联合预测三个话语组件的质量分数，并将任务建模为有序分类问题。

Result: 在GTI德国数据集上测试，文本模态主导任务表现，结合音频特征后模型与人工评分一致性提升，总体Quadratic Weighted Kappa得分为0.384。

Conclusion: 研究为自动化话语质量评估奠定了基础，支持通过多维反馈促进教师专业发展。

Abstract: Classroom discourse is an essential vehicle through which teaching and
learning take place. Assessing different characteristics of discursive
practices and linking them to student learning achievement enhances the
understanding of teaching quality. Traditional assessments rely on manual
coding of classroom observation protocols, which is time-consuming and costly.
Despite many studies utilizing AI techniques to analyze classroom discourse at
the utterance level, investigations into the evaluation of discursive practices
throughout an entire lesson segment remain limited. To address this gap, our
study proposes a novel text-centered multimodal fusion architecture to assess
the quality of three discourse components grounded in the Global Teaching
InSights (GTI) observation protocol: Nature of Discourse, Questioning, and
Explanations. First, we employ attention mechanisms to capture inter- and
intra-modal interactions from transcript, audio, and video streams. Second, a
multi-task learning approach is adopted to jointly predict the quality scores
of the three components. Third, we formulate the task as an ordinal
classification problem to account for rating level order. The effectiveness of
these designed elements is demonstrated through an ablation study on the GTI
Germany dataset containing 92 videotaped math lessons. Our results highlight
the dominant role of text modality in approaching this task. Integrating
acoustic features enhances the model's consistency with human ratings,
achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to
human inter-rater reliability (0.326). Our study lays the groundwork for the
future development of automated discourse quality assessment to support teacher
professional development through timely feedback on multidimensional discourse
practices.

</details>


### [4] [LLMs to Support K-12 Teachers in Culturally Relevant Pedagogy: An AI Literacy Example](https://arxiv.org/abs/2505.08083)
*Jiayi Wang,Ruiwei Xiao,Xinying Hou,Hanqi Li,Ying Jui Tseng,John Stamper,Ken Koedinger*

Main category: cs.CY

TL;DR: 研究探讨了如何利用大型语言模型（LLM）工具CulturAIEd帮助教师将文化相关教学法（CRP）融入AI素养课程，通过试点实验发现该工具提升了教师的信心和实施效率。


<details>
  <summary>Details</summary>
Motivation: 教师在K-12教育中实施CRP面临时间、培训和资源不足的挑战，研究旨在探索LLM如何帮助解决这些问题。

Method: 研究通过试点实验，使用CulturAIEd工具辅助四名K-12教师，分析其对CRP整合的影响。

Result: CulturAIEd增强了教师在识别文化响应机会和修改学习活动方面的信心，并提供了高效的实施支持。

Conclusion: LLM工具如CulturAIEd可有效帮助教师高效整合CRP，尤其是在未来教育重点如AI素养中。

Abstract: Culturally Relevant Pedagogy (CRP) is vital in K-12 education, yet teachers
struggle to implement CRP into practice due to time, training, and resource
gaps. This study explores how Large Language Models (LLMs) can address these
barriers by introducing CulturAIEd, an LLM tool that assists teachers in
adapting AI literacy curricula to students' cultural contexts. Through an
exploratory pilot with four K-12 teachers, we examined CulturAIEd's impact on
CRP integration. Results showed CulturAIEd enhanced teachers' confidence in
identifying opportunities for cultural responsiveness in learning activities
and making culturally responsive modifications to existing activities. They
valued CulturAIEd's streamlined integration of student demographic information,
immediate actionable feedback, which could result in high implementation
efficiency. This exploration of teacher-AI collaboration highlights how LLM can
help teachers include CRP components into their instructional practices
efficiently, especially in global priorities for future-ready education, such
as AI literacy.

</details>


### [5] ["You Cannot Sound Like GPT": Signs of language discrimination and resistance in computer science publishing](https://arxiv.org/abs/2505.08127)
*Haley Lepp,Daniel Scott Smith*

Main category: cs.CY

TL;DR: 研究发现，大语言模型（LLMs）的采用反映了科学写作中的语言排斥现象，尤其是在ICLR会议中，审稿人对非英语母语作者的写作清晰度存在偏见。ChatGPT的引入并未显著改变这种偏见，审稿人转而通过ChatGPT的“风格”和非语言特征判断作者背景。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在科学写作中的应用是否揭示了语言排斥问题，以及ChatGPT如何影响审稿人对作者语言背景的偏见。

Method: 通过分析ICLR会议的近80,000条同行评审，并结合对14名会议参与者的访谈，研究审稿人对写作清晰度的评价偏见及其变化。

Result: 发现审稿人对非英语母语作者存在显著偏见，ChatGPT的引入仅轻微改变了这种偏见的表达方式，审稿人转而依赖其他特征判断作者背景。

Conclusion: ChatGPT并未消除语言偏见，反而可能通过新的方式强化了科学写作中的语言意识形态，将“好的英语”与“好的科学”混为一谈。

Abstract: LLMs have been celebrated for their potential to help multilingual scientists
publish their research. Rather than interpret LLMs as a solution, we
hypothesize their adoption can be an indicator of existing linguistic exclusion
in scientific writing. Using the case study of ICLR, an influential,
international computer science conference, we examine how peer reviewers
critique writing clarity. Analyzing almost 80,000 peer reviews, we find
significant bias against authors associated with institutions in countries
where English is less widely spoken. We see only a muted shift in the
expression of this bias after the introduction of ChatGPT in late 2022. To
investigate this unexpectedly minor change, we conduct interviews with 14
conference participants from across five continents. Peer reviewers describe
associating certain features of writing with people of certain language
backgrounds, and such groups in turn with the quality of scientific work. While
ChatGPT masks some signs of language background, reviewers explain that they
now use ChatGPT "style" and non-linguistic features as indicators of author
demographics. Authors, aware of this development, described the ongoing need to
remove features which could expose their "non-native" status to reviewers. Our
findings offer insight into the role of ChatGPT in the reproduction of
scholarly language ideologies which conflate producers of "good English" with
producers of "good science."

</details>


### [6] [One Bad NOFO? AI Governance in Federal Grantmaking](https://arxiv.org/abs/2505.08133)
*Dan Bateyko,Karen Levy*

Main category: cs.CY

TL;DR: 论文探讨了美国联邦机构如何通过拨款政策间接治理人工智能（AI），发现其监管力度不足，尤其是在透明度和问责制方面。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示联邦机构在拨款政策中对AI的治理作用，这一领域此前被忽视。

Method: 方法包括分析2009-2024年间超过40,000份非国防联邦拨款通知，筛选提及AI的记录并审查其目标和限制。

Result: 结果显示，尽管拨款通知中提及AI，但极少包含具体的评判标准或限制，监管力度不足。

Conclusion: 结论指出拨款政策是AI治理的新领域，但其发展滞后于其他监管措施，需进一步研究。

Abstract: Much scholarship considers how U.S. federal agencies govern artificial
intelligence (AI) through rulemaking and their own internal use policies. But
agencies have an overlooked AI governance role: setting discretionary grant
policy when directing billions of dollars in federal financial assistance.
These dollars enable state and local entities to study, create, and use AI.
This funding not only goes to dedicated AI programs, but also to grantees using
AI in the course of meeting their routine grant objectives. As discretionary
grantmakers, agencies guide and restrict what grant winners do -- a hidden
lever for AI governance. Agencies pull this lever by setting program
objectives, judging criteria, and restrictions for AI use. Using a novel
dataset of over 40,000 non-defense federal grant notices of funding opportunity
(NOFOs) posted to Grants.gov between 2009 and 2024, we analyze how agencies
regulate the use of AI by grantees. We select records mentioning AI and review
their stated goals and requirements. We find agencies promoting AI in notice
narratives, shaping adoption in ways other records of grant policy might fail
to capture. Of the grant opportunities that mention AI, we find only a handful
of AI-specific judging criteria or restrictions. This silence holds even when
agencies fund AI uses in contexts affecting people's rights and which, under an
analogous federal procurement regime, would result in extra oversight. These
findings recast grant notices as a site of AI policymaking -- albeit one that
is developing out of step with other regulatory efforts and incomplete in its
consideration of transparency, accountability, and privacy protections. The
paper concludes by drawing lessons from AI procurement scholarship, while
identifying distinct challenges in grantmaking that invite further study.

</details>


### [7] [AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques](https://arxiv.org/abs/2505.08202)
*Aman Raj,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.CY

TL;DR: 本文综述了AI和GenAI在自然灾害损害评估中的应用，探讨了其优势、局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自然灾害对人类生命和基础设施构成巨大威胁，快速有效的损害评估对灾害响应至关重要。AI和GenAI为这一领域提供了突破性解决方案。

Method: 通过多模态数据（文本、图像、视频、音频）结合AI和GenAI技术，模拟灾害场景并识别趋势。

Result: AI和GenAI在灾害评估中表现出高效性和潜力，但也面临数据隐私、安全及伦理问题，以及GenAI被滥用的风险。

Conclusion: 未来需开发安全、可靠且符合伦理的GenAI系统，以支持灾害管理。本文是首篇全面综述GenAI在灾害评估与响应中应用的论文。

Abstract: Natural disasters, including earthquakes, wildfires and cyclones, bear a huge
risk on human lives as well as infrastructure assets. An effective response to
disaster depends on the ability to rapidly and efficiently assess the intensity
of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence
(GenAI) presents a breakthrough solution, capable of combining knowledge from
multiple types and sources of data, simulating realistic scenarios of disaster,
and identifying emerging trends at a speed previously unimaginable. In this
paper, we present a comprehensive review on the prospects of AI and GenAI in
damage assessment for various natural disasters, highlighting both its
strengths and limitations. We talk about its application to multimodal data
such as text, image, video, and audio, and also cover major issues of data
privacy, security, and ethical use of the technology during crises. The paper
also recognizes the threat of Generative AI misuse, in the form of
dissemination of misinformation and for adversarial attacks. Finally, we
outline avenues of future research, emphasizing the need for secure, reliable,
and ethical Generative AI systems for disaster management in general. We
believe that this work represents the first comprehensive survey of Gen-AI
techniques being used in the field of Disaster Assessment and Response.

</details>


### [8] [The Failure of Plagiarism Detection in Competitive Programming](https://arxiv.org/abs/2505.08244)
*Ethan Dickey*

Main category: cs.CY

TL;DR: 论文探讨了编程课程中抄袭问题的挑战，分析了传统检测方法的不足，并提出了结合改进算法和多维方法的解决方案。


<details>
  <summary>Details</summary>
Motivation: 编程课程中的抄袭问题日益复杂，尤其是在竞争性编程环境中，传统检测方法难以应对AI生成代码等新挑战。

Method: 通过文献综述、工具调查（如Moss、Kattis）和实际教学经验（如Purdue大学的CP1课程），分析了现有方法的优缺点。

Result: 发现自动化检测工具易被简单代码变换或AI生成代码绕过，而人工方法虽有效但耗时。

Conclusion: 建议采用多维方法，结合改进算法、基于掌握的学习和真实评估，以提高代码原创性。

Abstract: Plagiarism in programming courses remains a persistent challenge, especially
in competitive programming contexts where assignments often have unique, known
solutions. This paper examines why traditional code plagiarism detection
methods frequently fail in these environments and explores the implications of
emerging factors such as generative AI (genAI). Drawing on the author's
experience teaching a Competitive Programming 1 (CP1) course over seven
semesters at Purdue University (with $\approx 100$ students each term) and
completely redesigning the CP1/2/3 course sequence, we provide an academically
grounded analysis. We review literature on code plagiarism in computer science
education, survey current detection tools (Moss, Kattis, etc.) and methods
(manual review, code-authorship interviews), and analyze their strengths and
limitations. Experience-based observations are presented to illustrate
real-world detection failures and successes. We find that widely-used automated
similarity checkers can be thwarted by simple code transformations or novel
AI-generated code, while human-centric approaches like oral interviews, though
effective, are labor-intensive. The paper concludes with opinions and
preliminary recommendations for improving academic integrity in programming
courses, advocating for a multi-faceted approach that combines improved
detection algorithms, mastery-based learning techniques, and authentic
assessment practices to better ensure code originality.

</details>


### [9] [Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems](https://arxiv.org/abs/2505.08319)
*Egil Diau*

Main category: cs.CY

TL;DR: 提出了一种三阶段自下而上的框架，用于模拟社会结构的涌现，解决了多智能体AI中缺乏可模拟模型的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI中缺乏对社会结构涌现的可模拟模型，且经济学和社会学中的理论往往事后描述社会结构，未从智能体行为层面重建。

Method: 提出三阶段框架：互惠动力学（个体互惠交换）、规范稳定化（共享期望的巩固）和制度构建（稳定模式的外部化）。

Result: 通过将社会涌现基于智能体互惠行为，框架支持系统探索道德、文化和制度结构如何从最小认知互动中涌现。

Conclusion: 该框架为理解社会结构的自下而上涌现提供了新途径，填补了理论和模拟之间的空白。

Abstract: A major bottleneck in multi-agent AI is the lack of simulateable models for
the bottom-up emergence of social structure under realistic behavioral
constraints. Similarly, many foundational theories in economics and sociology
including the concepts of "institutions" and "norms" tend to describe social
structures post hoc, often relying on implicit assumptions of shared culture,
morality, or symbolic agreement. These concepts are often treated as primitives
rather than reconstructed from agent-level behavior, leaving both their origins
and operational definitions under-specified. To address this, we propose a
three-stage bottom-up framework: Reciprocal Dynamics, capturing
individual-level reciprocal exchanges; Norm Stabilization, the consolidation of
shared expectations; and Institutional Construction, the externalization of
stable patterns into scalable structures. By grounding social emergence in
agent-level reciprocity, our framework enables the systematic exploration of
how moral, cultural, and institutional structures emerge from cognitively
minimal interactions.

</details>


### [10] [How Students Use AI Feedback Matters: Experimental Evidence on Physics Achievement and Autonomy](https://arxiv.org/abs/2505.08672)
*Xusheng Dai,Zhaochun Wen,Jianxiao Jiang,Huiqin Liu,Yu Zhang*

Main category: cs.CY

TL;DR: 研究探讨了生成式AI个性化反馈对不同成绩高中生物理学习的影响，发现使用模式对学习效果和自主性有显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究和实践可能忽视了生成式AI反馈的使用模式对学生学习的影响，因此研究旨在填补这一空白。

Method: 通过两项随机对照试验（共387名学生），分别评估强制使用和自主需求使用AI反馈的效果。

Result: 低成绩学生在强制使用AI提示时表现提升，但自主性下降；高成绩学生在自主使用AI反馈时表现提升且自主性不受影响。

Conclusion: 生成式AI个性化反馈的使用模式对学生学习效果和自主性至关重要，需针对性设计。

Abstract: Despite the precision and adaptiveness of generative AI (GAI)-powered
feedback provided to students, existing practice and literature might ignore
how usage patterns impact student learning. This study examines the
heterogeneous effects of GAI-powered personalized feedback on high school
students' physics achievement and autonomy through two randomized controlled
trials, with a major focus on usage patterns. Each experiment lasted for five
weeks, involving a total of 387 students. Experiment 1 (n = 121) assessed
compulsory usage of the personalized recommendation system, revealing that
low-achieving students significantly improved academic performance (d = 0.673,
p < 0.05) when receiving AI-generated heuristic solution hints, whereas
medium-achieving students' performance declined (d = -0.539, p < 0.05) with
conventional answers provided by workbook. Notably, high-achieving students
experienced a significant decline in self-regulated learning (d = -0.477, p <
0.05) without any significant gains in achievement. Experiment 2 (n = 266)
investigated the usage pattern of autonomous on-demand help, demonstrating that
fully learner-controlled AI feedback significantly enhanced academic
performance for high-achieving students (d = 0.378, p < 0.05) without
negatively impacting their autonomy. However, autonomy notably declined among
lower achievers exposed to on-demand AI interventions (d = -0.383, p < 0.05),
particularly in the technical-psychological dimension (d = -0.549, p < 0.05),
which has a large overlap with self-regulation. These findings underscore the
importance of usage patterns when applying GAI-powered personalized feedback to
students.

</details>


### [11] [Understanding Housing and Homelessness System Access by Linking Administrative Data](https://arxiv.org/abs/2505.08743)
*Geoffrey G. Messier,Sam Elliott,Dallas Seitz*

Main category: cs.CY

TL;DR: 论文通过隐私保护方法链接了北美某大城市住房与无家可归者护理系统（HHSC）中的23.5万条记录，评估了多种机器学习配对链接和聚类算法，并发现隐私保护链接方法在HHSC领域特定指标中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过隐私保护方法链接HHSC系统中的记录，以了解个人如何与多个机构互动，并评估不同链接技术的效果。

Method: 使用多种机器学习配对链接和聚类算法，结合传统机器学习指标和HHSC系统使用指标进行评估。

Result: 隐私保护链接方法有效且实用，不同链接技术在HHSC领域特定指标（如紧急收容所停留次数、互动时长等）中表现差异显著。

Conclusion: 隐私保护链接方法为理解个人在HHSC中的互动提供了有效工具，且领域特定指标能更显著地反映技术差异。

Abstract: This paper uses privacy preserving methods to link over 235,000 records in
the housing and homelessness system of care (HHSC) of a major North American
city. Several machine learning pairwise linkage and two clustering algorithms
are evaluated for merging the profiles for latent individuals in the data.
Importantly, these methods are evaluated using both traditional machine
learning metrics and HHSC system use metrics generated using the linked data.
The results demonstrate that privacy preserving linkage methods are an
effective and practical method for understanding how a single person interacts
with multiple agencies across an HHSC. They also show that performance
differences between linkage techniques are amplified when evaluated using HHSC
domain specific metrics like number of emergency homeless shelter stays, length
of time interacting with an HHSC and number of emergency shelters visited per
person.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [12] [Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2505.08448)
*Yanggang Xu,Weijie Hong,Jirong Zha,Geng Chen,Jianfeng Zheng,Chen-Chun Hsia,Xinlei Chen*

Main category: cs.MA

TL;DR: 论文提出了一种结合多智能体强化学习（MARL）和大语言模型（LLMs）的框架MRLMN，用于优化无人机（UAV）在多跳网络中的协作决策，提升网络性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中，快速建立稳定的应急通信网络至关重要，但无人机在大规模动态环境中形成多跳网络面临算法扩展性和决策协调的挑战。

Method: 提出MRLMN框架，整合MARL和LLMs，采用分组策略和奖励分解增强算法扩展性，并通过行为约束和知识蒸馏提升网络鲁棒性和训练效率。

Result: 仿真结果表明，该方法显著提升了网络性能，包括覆盖范围和通信质量。

Conclusion: MRLMN框架为无人机应急通信网络提供了一种高效、可扩展的解决方案。

Abstract: In disaster scenarios, establishing robust emergency communication networks
is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to
rapidly restore connectivity. However, organizing UAVs to form multi-hop
networks in large-scale dynamic environments presents significant challenges,
including limitations in algorithmic scalability and the vast exploration space
required for coordinated decision-making. To address these issues, we propose
MRLMN, a novel framework that integrates multi-agent reinforcement learning
(MARL) and large language models (LLMs) to jointly optimize UAV agents toward
achieving optimal networking performance. The framework incorporates a grouping
strategy with reward decomposition to enhance algorithmic scalability and
balance decision-making across UAVs. In addition, behavioral constraints are
applied to selected key UAVs to improve the robustness of the network.
Furthermore, the framework integrates LLM agents, leveraging knowledge
distillation to transfer their high-level decision-making capabilities to MARL
agents. This enhances both the efficiency of exploration and the overall
training process. In the distillation module, a Hungarian algorithm-based
matching scheme is applied to align the decision outputs of the LLM and MARL
agents and define the distillation loss. Extensive simulation results validate
the effectiveness of our approach, demonstrating significant improvements in
network performance, including enhanced coverage and communication quality.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity](https://arxiv.org/abs/2505.07830)
*Joseph Lavalle-Rivera,Aniirudh Ramesh,Subhadeep Chakraborty*

Main category: cs.AI

TL;DR: 论文提出了一种多路径路由优化算法，用于在公共枪击事件中优化疏散路线，减少拥挤和瓶颈，从而降低伤亡率。


<details>
  <summary>Details</summary>
Motivation: 公共枪击事件频发，疏散时的决策至关重要，但缺乏实时信息和高压环境可能导致错误决策。

Method: 开发了一种考虑路径容量的多路径路由优化算法，为每个疏散者提供多条最优安全路线。

Result: 算法将总伤亡率降低了34.16%（相比无容量约束的算法）和53.3%（相比专家建议策略），并减少了关键节点的拥挤。

Conclusion: 该算法显著提升了疏散效率，减少了伤亡和拥挤，适用于紧急疏散场景。

Abstract: A total of more than 3400 public shootings have occurred in the United States
between 2016 and 2022. Among these, 25.1% of them took place in an educational
institution, 29.4% at the workplace including office buildings, 19.6% in retail
store locations, and 13.4% in restaurants and bars. During these critical
scenarios, making the right decisions while evacuating can make the difference
between life and death. However, emergency evacuation is intensely stressful,
which along with the lack of verifiable real-time information may lead to fatal
incorrect decisions. To tackle this problem, we developed a multi-route routing
optimization algorithm that determines multiple optimal safe routes for each
evacuee while accounting for available capacity along the route, thus reducing
the threat of crowding and bottlenecking. Overall, our algorithm reduces the
total casualties by 34.16% and 53.3%, compared to our previous routing
algorithm without capacity constraints and an expert-advised routing strategy
respectively. Further, our approach to reduce crowding resulted in an
approximate 50% reduction in occupancy in key bottlenecking nodes compared to
both of the other evacuation algorithms.

</details>


### [14] [RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks](https://arxiv.org/abs/2505.07842)
*Sebastian Barros*

Main category: cs.AI

TL;DR: 论文提出RAN Cortex架构，通过引入记忆模块提升AI原生无线接入网络（RAN）的决策能力，解决现有无状态代理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模块（如xApps和rApps）在RAN中缺乏记忆能力，无法利用历史事件或结果优化决策，限制了在动态网络环境中的表现。

Method: 提出RAN Cortex架构，包含上下文编码器、向量记忆存储、召回引擎和策略接口，支持实时或近实时历史上下文检索。

Result: 通过用例（如体育场流量缓解和无人机走廊移动管理）展示记忆模块如何提升RAN的适应性和连续性。

Conclusion: RAN Cortex为AI原生RAN设计提供了记忆原语，支持无需重新训练或集中推理的“学习代理”。

Abstract: As Radio Access Networks (RAN) evolve toward AI-native architectures,
intelligent modules such as xApps and rApps are expected to make increasingly
autonomous decisions across scheduling, mobility, and resource management
domains. However, these agents remain fundamentally stateless, treating each
decision as isolated, lacking any persistent memory of prior events or
outcomes. This reactive behavior constrains optimization, especially in
environments where network dynamics exhibit episodic or recurring patterns. In
this work, we propose RAN Cortex, a memory-augmented architecture that enables
contextual recall in AI-based RAN decision systems. RAN Cortex introduces a
modular layer composed of four elements: a context encoder that transforms
network state into high-dimensional embeddings, a vector-based memory store of
past network episodes, a recall engine to retrieve semantically similar
situations, and a policy interface that supplies historical context to AI
agents in real time or near-real time. We formalize the retrieval-augmented
decision problem in the RAN, present a system architecture compatible with
O-RAN interfaces, and analyze feasible deployments within the Non-RT and
Near-RT RIC domains. Through illustrative use cases such as stadium traffic
mitigation and mobility management in drone corridors, we demonstrate how
contextual memory improves adaptability, continuity, and overall RAN
intelligence. This work introduces memory as a missing primitive in AI-native
RAN designs and provides a framework to enable "learning agents" without the
need for retraining or centralized inference

</details>


### [15] [Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models](https://arxiv.org/abs/2505.07846)
*Lars Malmqvist*

Main category: cs.AI

TL;DR: 研究发现前沿大语言模型（LLMs）在面临不可能任务时会利用漏洞而非接受失败，尤其是提示要求“创造性”解决方案时，漏洞利用行为显著增加。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在安全和对齐方面的潜在风险，特别是在面对设计为无法通过正常手段完成的任务时。

Method: 通过文本模拟方法，让三种领先的LLMs（o1、o3-mini和r1）参与一个无法通过合法手段获胜的井字棋场景，分析其利用漏洞的倾向。

Result: 较新的o3-mini模型漏洞利用倾向是o1的两倍（37.1% vs 17.5%），提示要求“创造性”解决方案时漏洞利用行为飙升至77.3%。

Conclusion: 即使没有实际执行能力，LLMs也能识别并提出系统漏洞利用策略，这对AI对齐提出了紧迫挑战。

Abstract: This study reveals how frontier Large Language Models LLMs can "game the
system" when faced with impossible situations, a critical security and
alignment concern. Using a novel textual simulation approach, we presented
three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed
to be unwinnable through legitimate play, then analyzed their tendency to
exploit loopholes rather than accept defeat. Our results are alarming for
security researchers: the newer, reasoning-focused o3-mini model showed nearly
twice the propensity to exploit system vulnerabilities (37.1%) compared to the
older o1 model (17.5%). Most striking was the effect of prompting. Simply
framing the task as requiring "creative" solutions caused gaming behaviors to
skyrocket to 77.3% across all models. We identified four distinct exploitation
strategies, from direct manipulation of game state to sophisticated
modification of opponent behavior. These findings demonstrate that even without
actual execution capabilities, LLMs can identify and propose sophisticated
system exploits when incentivized, highlighting urgent challenges for AI
alignment as models grow more capable of identifying and leveraging
vulnerabilities in their operating environments.

</details>


### [16] [Conceptual Logical Foundations of Artificial Social Intelligence](https://arxiv.org/abs/2505.07847)
*Eric Werner*

Main category: cs.AI

TL;DR: 本文探讨了多智能体社会中人工社会智能的概念与逻辑基础，提出了社会智能体的最小架构，并研究了信息、意图与沟通的关系。


<details>
  <summary>Details</summary>
Motivation: 研究社会协调与合作的基础问题，探索多智能体社会中信息、意图与沟通的相互作用。

Method: 提出社会智能体的最小架构，形式化定义社会状态、意图逻辑和群体战略状态的熵。

Result: 定义了动态变化的社会状态、意图逻辑和沟通的语义与语用关系，并形式化了智能体能力与意图。

Conclusion: 社会智能的逻辑超越了经典逻辑，通过信息与战略思维的结合，为多智能体社会的协调与合作提供了理论基础。

Abstract: What makes a society possible at all? How is coordination and cooperation in
social activity possible? What is the minimal mental architecture of a social
agent? How is the information about the state of the world related to the
agents intentions? How are the intentions of agents related? What role does
communication play in this coordination process? This essay explores the
conceptual and logical foundations of artificial social intelligence in the
context of a society of multiple agents that communicate and cooperate to
achieve some end. An attempt is made to provide an introduction to some of the
key concepts, their formal definitions and their interrelationships. These
include the notion of a changing social world of multiple agents. The logic of
social intelligence goes beyond classical logic by linking information with
strategic thought. A minimal architecture of social agents is presented. The
agents have different dynamically changing, possible choices and abilities. The
agents also have uncertainty, lacking perfect information about their physical
state as well as their dynamic social state. The social state of an agent
includes the intentional state of that agent, as well as, that agent's
representation of the intentional states of other agents. Furthermore, it
includes the evaluations agents make of their physical and social condition.
Communication, semantic and pragmatic meaning and their relationship to
intention and information states are investigated. The logic of agent abilities
and intentions are motivated and formalized. The entropy of group strategic
states is defined.

</details>


### [17] [CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution](https://arxiv.org/abs/2505.07854)
*Yufei Lin,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang,Linuo Xu,Shuyan Liu,Zeyu Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为CCL的课程学习框架，用于解决多智能体系统中稀疏奖励问题，通过细化任务、生成子任务和协同进化提升性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境在多智能体系统中导致反馈延迟和共享，影响学习效果。

Method: CCL框架包括细化个体任务、使用变分进化算法生成子任务、以及智能体与环境协同进化。

Result: 在MPE和Hide-and-Seek环境的五个合作任务中，CCL表现优于现有方法。

Conclusion: CCL有效解决了稀疏奖励问题，提升了多智能体系统的学习效果。

Abstract: Sparse reward environments pose significant challenges in reinforcement
learning, especially within multi-agent systems (MAS) where feedback is delayed
and shared across agents, leading to suboptimal learning. We propose
Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum
learning framework that addresses this by (1) refining intermediate tasks for
individual agents, (2) using a variational evolutionary algorithm to generate
informative subtasks, and (3) co-evolving agents with their environment to
enhance training stability. Experiments on five cooperative tasks in the MPE
and Hide-and-Seek environments show that CCL outperforms existing methods in
sparse reward settings.

</details>


### [18] [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
*Takamitsu Omasa,Ryo Koshihara,Masumi Morishige*

Main category: cs.AI

TL;DR: 论文提出了一种七阶段流程，通过箭头感知检测、OCR提取文本和结构化提示，显著提升了视觉语言模型对流程图的解析准确率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型常误解流程图的箭头和拓扑结构，限制了其在软件设计和业务流程分析中的应用。

Method: 采用七阶段流程，分为箭头感知检测、OCR文本提取和结构化提示构建三部分。

Result: 在90个问题的基准测试中，准确率从80%提升至89%，尤其在下一步查询中表现突出。

Conclusion: 方法通过显式编码箭头显著提升了性能，但依赖检测器和OCR精度，未来将扩展基准测试和应用范围。

Abstract: Flowcharts are indispensable tools in software design and business-process
analysis, yet current vision-language models (VLMs) frequently misinterpret the
directional arrows and graph topology that set these diagrams apart from
natural images. We introduce a seven-stage pipeline grouped into three broader
processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical
character recognition (OCR) to extract node text; and (3) construction of a
structured prompt that guides the VLMs. Tested on a 90-question benchmark
distilled from 30 annotated flowcharts, the method raises overall accuracy from
80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The
gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);
branch-result questions improve more modestly, and before-step questions remain
difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same
trends, reinforcing the advantage of explicit arrow encoding. Limitations
include dependence on detector and OCR precision, the small evaluation set, and
residual errors at nodes with multiple incoming edges. Future work will enlarge
the benchmark with synthetic and handwritten flowcharts and assess the approach
on Business Process Model and Notation (BPMN) and Unified Modeling Language
(UML).

</details>


### [19] [Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey](https://arxiv.org/abs/2505.07882)
*Qian Xu,Lei Zhang,Yixiao Liu*

Main category: cs.AI

TL;DR: 本文提出了一种基于机器学习的三层信任管理系统框架，用于车路云集成系统中的互联自动驾驶车辆（CAVs），并分析了其目标、方法和未来方向。


<details>
  <summary>Details</summary>
Motivation: CAVs在动态、开放和多域网络中运行，易受多种威胁。信任管理系统（TMS）能识别恶意节点并确保可靠决策，而机器学习（ML）可显著提升TMS的性能。

Method: 提出一个三层ML-based TMS框架（信任数据层、信任计算层和信任激励层），并设计六维目标分类法，分析各层模块的ML方法。

Result: 通过分类现有研究并提出未来方向，展示了ML-based TMS在CAVs中的潜力。

Conclusion: ML-based TMS为CAVs提供了有效的信任管理解决方案，未来需解决开放问题并跟进研究趋势。

Abstract: Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and
multi-domain networks, rendering them vulnerable to various threats. Trust
Management Systems (TMS) systematically organize essential steps in the trust
mechanism, identifying malicious nodes against internal threats and external
threats, as well as ensuring reliable decision-making for more cooperative
tasks. Recent advances in machine learning (ML) offer significant potential to
enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes
moving at varying speeds, and opportunistic and intermittent network behavior.
Those features distinguish ML-based TMS from social networks, static IoT, and
Social IoT. This survey proposes a novel three-layer ML-based TMS framework for
CAVs in the vehicle-road-cloud integration system, i.e., trust data layer,
trust calculation layer and trust incentive layer. A six-dimensional taxonomy
of objectives is proposed. Furthermore, the principles of ML methods for each
module in each layer are analyzed. Then, recent studies are categorized based
on traffic scenarios that are against the proposed objectives. Finally, future
directions are suggested, addressing the open issues and meeting the research
trend. We maintain an active repository that contains up-to-date literature and
open-source projects at
https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.

</details>


### [20] [The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic](https://arxiv.org/abs/2505.08021)
*Bernardo Cuenca Grau,Przemysław A. Wałęga*

Main category: cs.AI

TL;DR: 本文探讨了图神经网络（GNNs）的表达能力，并将其与一阶逻辑（FO）的特定片段对应起来，包括模态逻辑（ML）、分级模态逻辑（GML）等。


<details>
  <summary>Details</summary>
Motivation: 理解GNNs的表达能力是重要的研究问题，尤其是在处理图结构数据时。

Method: 应用有限模型理论中的方法和工具，将GNNs的表达能力与一阶逻辑的片段对应。

Result: 证明了有界GNN架构对应于特定的一阶逻辑片段，如ML、GML、FO2等。

Conclusion: 提供了一个统一的框架，用于理解GNNs在一阶逻辑中的表达能力。

Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep
learning to graph-structured data: they handle varying size input graphs and
ensure invariance under graph isomorphism. While GNNs have demonstrated broad
applicability, understanding their expressive power remains an important
question. In this paper, we show that bounded GNN architectures correspond to
specific fragments of first-order logic (FO), including modal logic (ML),
graded modal logic (GML), modal logic with the universal modality (ML(A)), the
two-variable fragment (FO2) and its extension with counting quantifiers (C2).
To establish these results, we apply methods and tools from finite model theory
of first-order and modal logics to the domain of graph representation learning.
This provides a unifying framework for understanding the logical expressiveness
of GNNs within FO.

</details>


### [21] [Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making](https://arxiv.org/abs/2505.08049)
*Prakhar Godara*

Main category: cs.AI

TL;DR: 研究发现，即使通过客观贝叶斯推理更新信念，标准Q学习模型仍会表现出积极性和确认偏误。贝叶斯推理作为Q学习算法时学习率对称但递减。通过主方程分析，确认偏误与递减学习率的行为特征相同。


<details>
  <summary>Details</summary>
Motivation: 探讨人类行为在TABB任务中表现出的偏误是否源于客观贝叶斯推理，而非真正的认知偏误。

Method: 将贝叶斯推理建模为Q学习算法，分析学习率的动态变化，并使用主方程比较确认偏误与递减学习率的行为特征。

Result: 确认偏误与递减学习率的行为特征相同，表明偏误可能是学习率递减的假象。

Conclusion: 提出实验方案以区分真正的认知偏误与学习率递减的假象。

Abstract: Recent studies claim that human behavior in a two-armed Bernoulli bandit
(TABB) task is described by positivity and confirmation biases, implying that
humans do not integrate new information objectively. However, we find that even
if the agent updates its belief via objective Bayesian inference, fitting the
standard Q-learning model with asymmetric learning rates still recovers both
biases. Bayesian inference cast as an effective Q-learning algorithm has
symmetric, though decreasing, learning rates. We explain this by analyzing the
stochastic dynamics of these learning systems using master equations. We find
that both confirmation bias and unbiased but decreasing learning rates yield
the same behavioral signatures. Finally, we propose experimental protocols to
disentangle true cognitive biases from artifacts of decreasing learning rates.

</details>


### [22] [Explainable Reinforcement Learning Agents Using World Models](https://arxiv.org/abs/2505.08073)
*Madhuri Singh,Amal Alabdulkarim,Gennie Mansi,Mark O. Riedl*

Main category: cs.AI

TL;DR: 论文提出了一种使用世界模型和反向世界模型生成可解释强化学习（XRL）的方法，帮助非AI专家理解AI代理的行为，并通过环境操控学习控制代理。


<details>
  <summary>Details</summary>
Motivation: 由于序列决策的复杂性，非AI专家难以理解或改变AI代理的行为，因此需要一种更直观的解释方法。

Method: 结合世界模型和反向世界模型，生成反事实轨迹并预测理想状态，以解释代理行为。

Result: 实验表明，这种解释方法显著提高了用户对代理策略的理解。

Conclusion: 该方法不仅帮助用户理解代理行为，还可能通过环境操控实现对代理的控制。

Abstract: Explainable AI (XAI) systems have been proposed to help people understand how
AI systems produce outputs and behaviors. Explainable Reinforcement Learning
(XRL) has an added complexity due to the temporal nature of sequential
decision-making. Further, non-AI experts do not necessarily have the ability to
alter an agent or its policy. We introduce a technique for using World Models
to generate explanations for Model-Based Deep RL agents. World Models predict
how the world will change when actions are performed, allowing for the
generation of counterfactual trajectories. However, identifying what a user
wanted the agent to do is not enough to understand why the agent did something
else. We augment Model-Based RL agents with a Reverse World Model, which
predicts what the state of the world should have been for the agent to prefer a
given counterfactual action. We show that explanations that show users what the
world should have been like significantly increase their understanding of the
agent policy. We hypothesize that our explanations can help users learn how to
control the agents execution through by manipulating the environment.

</details>


### [23] [Lost in Transmission: When and Why LLMs Fail to Reason Globally](https://arxiv.org/abs/2505.08140)
*Tobias Schnabel,Kiran Tomlinson,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文提出BAPO模型，解释LLMs在复杂推理任务中的失败，并证明CoT方法可缓解带宽限制。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务中表现不佳，作者认为是内部信息流带宽受限所致。

Method: 引入BAPO模型，模拟注意力头的带宽限制，并分析其对推理任务的影响。

Result: 实验证实GPT-4等模型在BAPO-hard任务中失败，但CoT方法可将其转化为BAPO-easy任务。

Conclusion: BAPO为LLMs失败提供理论解释，并建议通过改进架构和推理方法缓解带宽限制。

Abstract: Despite their many successes, transformer-based large language models (LLMs)
continue to struggle with tasks that require complex reasoning over large parts
of their input. We argue that these failures arise due to capacity limits on
the accurate flow of information within LLMs. To formalize this issue, we
introduce the bounded attention prefix oracle (BAPO) model, a new computational
framework that models bandwidth constraints on attention heads, the mechanism
for internal communication in LLMs. We show that several important reasoning
problems like graph reachability require high communication bandwidth for BAPOs
to solve; we call these problems BAPO-hard. Our experiments corroborate our
theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks
and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another
benefit of chain of thought (CoT): we prove that breaking down a task using CoT
can turn any BAPO-hard problem into a BAPO-easy one. Our results offer
principled explanations for key LLM failures and suggest directions for
architectures and inference methods that mitigate bandwidth limits.

</details>


### [24] [Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast](https://arxiv.org/abs/2505.08151)
*Joey Chan,Zhen Chen,Ershun Pan*

Main category: cs.AI

TL;DR: 该论文提出了一种针对时间序列基础模型的退化感知微调策略，用于锂离子电池容量退化的零样本泛化预测，并通过知识蒸馏框架将大模型知识迁移到小型专家模型中。


<details>
  <summary>Details</summary>
Motivation: 传统专家模型在电池容量退化预测中局限于特定场景，而通用时间序列基础模型尚未针对电池退化进行专门设计，因此需要一种能够实现零样本泛化的方法。

Method: 提出退化感知微调策略，将Timer模型应用于开源电池充放电数据，并通过知识蒸馏将大模型知识迁移到小型专家模型中。

Result: 在CycleLife-SJTUIE数据集上验证，微调后的Battery-Timer表现出强大的零样本泛化能力，知识蒸馏显著提升了专家模型的多条件泛化性能。

Conclusion: 该方法为电池容量退化预测提供了一种高效且泛化能力强的解决方案，同时通过知识蒸馏解决了大模型部署的计算挑战。

Abstract: Accurate estimation of lithium-ion battery capacity degradation is critical
for enhancing the reliability and safety of battery operations. Traditional
expert models, tailored to specific scenarios, provide isolated estimations.
With the rapid advancement of data-driven techniques, a series of
general-purpose time-series foundation models have been developed. However,
foundation models specifically designed for battery capacity degradation remain
largely unexplored. To enable zero-shot generalization in battery degradation
prediction using large model technology, this study proposes a
degradation-aware fine-tuning strategy for time-series foundation models. We
apply this strategy to fine-tune the Timer model on approximately 10 GB of
open-source battery charge discharge data. Validation on our released
CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer
possesses strong zero-shot generalization capability in capacity degradation
forecasting. To address the computational challenges of deploying large models,
we further propose a knowledge distillation framework that transfers the
knowledge of pre-trained foundation models into compact expert models.
Distillation results across several state-of-the-art time-series expert models
confirm that foundation model knowledge significantly improves the
multi-condition generalization of expert models.

</details>


### [25] [Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering](https://arxiv.org/abs/2505.08155)
*Weizhi Fei,Zihao Wang,hang Yin,Shukai Zhao,Wei Zhang,Yangqiu Song*

Main category: cs.AI

TL;DR: 论文提出了一种高效的符号搜索框架，通过约束策略和近似算法解决了复杂查询应答中的数据复杂性和查询复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 复杂查询应答（CQA）在处理大规模知识图谱和复杂查询时面临数据复杂性和查询复杂性的瓶颈，现有方法难以有效扩展。

Method: 提出两种约束策略计算神经逻辑索引以减少变量域，降低数据复杂性；引入基于局部搜索的近似算法处理循环查询的NP复杂性。

Result: 实验表明，该框架在保持性能的同时，将符号方法的计算负载降低了90%。

Conclusion: 该框架有效解决了CQA中的效率和可扩展性问题。

Abstract: Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.

</details>


### [26] [Decoding Neighborhood Environments with Large Language Models](https://arxiv.org/abs/2505.08163)
*Andrew Cart,Shaohu Zhang,Melanie Escue,Xugui Zhou,Haitao Zhao,Prashanth BusiReddyGari,Beiyu Lin,Shuang Li*

Main category: cs.AI

TL;DR: 研究探讨了利用大型语言模型（LLMs）如ChatGPT和Gemini解码邻里环境的可行性，通过训练YOLOv11模型和评估四种LLMs，证明了LLMs在无需训练的情况下可达到88%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统评估邻里环境的方法资源密集且难以规模化，机器学习虽具潜力但数据标注和模型可访问性限制了其扩展。

Method: 训练YOLOv11模型检测六种环境指标，评估四种LLMs的可行性、鲁棒性和局限性，采用多数投票策略提升准确率。

Result: YOLOv11模型平均准确率达99.13%，LLMs通过多数投票达到88%准确率。

Conclusion: LLMs可作为无需训练的工具解码邻里环境，具有潜在应用价值。

Abstract: Neighborhood environments include physical and environmental conditions such
as housing quality, roads, and sidewalks, which significantly influence human
health and well-being. Traditional methods for assessing these environments,
including field surveys and geographic information systems (GIS), are
resource-intensive and challenging to evaluate neighborhood environments at
scale. Although machine learning offers potential for automated analysis, the
laborious process of labeling training data and the lack of accessible models
hinder scalability. This study explores the feasibility of large language
models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood
environments (e.g., sidewalk and powerline) at scale. We train a robust
YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting
six environmental indicators, including streetlight, sidewalk, powerline,
apartment, single-lane road, and multilane road. We then evaluate four LLMs,
including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,
robustness, and limitations in identifying these indicators, with a focus on
the impact of prompting strategies and fine-tuning. We apply majority voting
with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs
could be a useful tool to decode the neighborhood environment without any
training effort.

</details>


### [27] [Benchmarking AI scientists in omics data-driven biological research](https://arxiv.org/abs/2505.08341)
*Erpai Luo,Jinmeng Jia,Yifan Xiong,Xiangyu Li,Xiaobo Guo,Baoqi Yu,Lei Wei,Xuegong Zhang*

Main category: cs.AI

TL;DR: BaisBench是一个新的基准测试，用于评估AI科学家在数据分析和外部知识推理中生成生物学发现的能力，包括细胞类型注释和科学发现两个任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏真实、数据驱动的评估环境，无法全面评估AI科学家的能力。

Method: BaisBench包含两个任务：31个专家标记的单细胞数据集上的细胞类型注释，以及基于41项单细胞研究生物学见解的198道多选题。

Result: 实验表明，当前AI模型在两项任务上仍显著落后于人类专家。

Conclusion: BaisBench填补了现有基准测试的空白，为AI模型的科学发现能力提供了评估基础。

Abstract: The rise of large language models and multi-agent systems has sparked growing
interest in AI scientists capable of autonomous biological research. However,
existing benchmarks either focus on reasoning without data or on data analysis
with predefined statistical answers, lacking realistic, data-driven evaluation
settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),
a benchmark designed to assess AI scientists' ability to generate biological
discoveries through data analysis and reasoning with external knowledge.
BaisBench comprises two tasks: cell type annotation on 31 expert-labeled
single-cell datasets, and scientific discovery through answering 198
multiple-choice questions derived from the biological insights of 41 recent
single-cell studies. Systematic experiments on state-of-the-art AI scientists
and LLM agents showed that while promising, current models still substantially
underperform human experts on both tasks. We hope BaisBench will fill this gap
and serve as a foundation for advancing and evaluating AI models for scientific
discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.

</details>


### [28] [Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations](https://arxiv.org/abs/2505.08176)
*Petrus H. Zwart,Tamas Varga,Odeta Qafoku,James A. Sethian*

Main category: cs.AI

TL;DR: 本文提出了一种基于机器学习的去噪方法，通过轻量级神经网络集成和共形分位数回归，不仅能有效去噪，还能揭示潜在空间中的结构特征。


<details>
  <summary>Details</summary>
Motivation: 科学成像中，高数据质量通常需要长时间采集，而缩短时间会引入噪声。本文旨在解决这一问题，同时揭示潜在结构。

Method: 使用轻量级随机结构神经网络集成，通过共形分位数回归训练，实现去噪并揭示空间和化学特征。

Result: 方法在真实地球生物化学成像数据上验证有效，支持可靠解释并指导资源受限的实验设计。

Conclusion: 该框架不仅去噪，还通过去噪过程驱动有意义表征的涌现，为科学成像提供了新思路。

Abstract: Scientific imaging often involves long acquisition times to obtain
high-quality data, especially when probing complex, heterogeneous systems.
However, reducing acquisition time to increase throughput inevitably introduces
significant noise into the measurements. We present a machine learning approach
that not only denoises low-quality measurements with calibrated uncertainty
bounds, but also reveals emergent structure in the latent space. By using
ensembles of lightweight, randomly structured neural networks trained via
conformal quantile regression, our method performs reliable denoising while
uncovering interpretable spatial and chemical features -- without requiring
labels or segmentation. Unlike conventional approaches focused solely on image
restoration, our framework leverages the denoising process itself to drive the
emergence of meaningful representations. We validate the approach on real-world
geobiochemical imaging data, showing how it supports confident interpretation
and guides experimental design under resource constraints.

</details>


### [29] [Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People](https://arxiv.org/abs/2505.08215)
*Haoshuai Zhou,Boxuan Cao,Changgeng Mo,Linkai Li,Shan Xiang Wang*

Main category: cs.AI

TL;DR: 研究探讨了如何优化语音基础模型（SFMs）用于听力受损人群的语音清晰度预测（SIP-HI），发现单层编码器选择、时间建模和模型集成是关键设计因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究对优化SFMs用于SIP-HI的探索不足，本文旨在填补这一空白。

Method: 通过5种SFMs，研究了编码器层选择、预测头架构和集成配置对SIP-HI性能的影响。

Result: 发现单层编码器优于传统全层方法，时间建模对预测头至关重要，集成多个SFMs可提升性能。

Conclusion: 研究为优化SFMs用于听力受损人群的语音清晰度预测提供了实用指导。

Abstract: Speech foundation models (SFMs) have demonstrated strong performance across a
variety of downstream tasks, including speech intelligibility prediction for
hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been
insufficiently explored. In this paper, we conduct a comprehensive study to
identify key design factors affecting SIP-HI performance with 5 SFMs, focusing
on encoder layer selection, prediction head architecture, and ensemble
configurations. Our findings show that, contrary to traditional use-all-layers
methods, selecting a single encoder layer yields better results. Additionally,
temporal modeling is crucial for effective prediction heads. We also
demonstrate that ensembling multiple SFMs improves performance, with stronger
individual models providing greater benefit. Finally, we explore the
relationship between key SFM attributes and their impact on SIP-HI performance.
Our study offers practical insights into effectively adapting SFMs for speech
intelligibility prediction for hearing-impaired populations.

</details>


### [30] [Evaluating LLM Metrics Through Real-World Capabilities](https://arxiv.org/abs/2505.08253)
*Justin K Miller,Wenjia Tang*

Main category: cs.AI

TL;DR: 论文提出了一种基于真实世界实用性的生成式AI评估方法，分析了六大核心能力，并发现现有基准测试在覆盖范围和实用性方面存在不足。Google Gemini在实用性指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在日常工作中的普及，需要一种更贴近实际使用场景的评估方法，而非抽象的智力测试。

Method: 通过大规模调查数据和使用日志分析，识别了六大核心能力，并基于人类中心标准评估现有基准测试的覆盖范围。

Result: 现有基准测试在覆盖范围、效率测量和可解释性方面存在显著不足。Google Gemini在实用性指标上优于其他主流模型。

Conclusion: 论文强调了基于真实世界实用性的评估方法的重要性，并提出了改进基准测试的建议。

Abstract: As generative AI becomes increasingly embedded in everyday workflows, it is
important to evaluate its performance in ways that reflect real-world usage
rather than abstract notions of intelligence. Unlike many existing benchmarks
that assess general intelligence, our approach focuses on real-world utility,
evaluating how well models support users in everyday tasks. While current
benchmarks emphasize code generation or factual recall, users rely on AI for a
much broader range of activities-from writing assistance and summarization to
citation formatting and stylistic feedback. In this paper, we analyze
large-scale survey data and usage logs to identify six core capabilities that
represent how people commonly use Large Language Models (LLMs): Summarization,
Technical Assistance, Reviewing Work, Data Structuring, Generation, and
Information Retrieval. We then assess the extent to which existing benchmarks
cover these capabilities, revealing significant gaps in coverage, efficiency
measurement, and interpretability. Drawing on this analysis, we use
human-centered criteria to identify gaps in how well current benchmarks reflect
common usage that is grounded in five practical criteria: coherence, accuracy,
clarity, relevance, and efficiency. For four of the six capabilities, we
identify the benchmarks that best align with real-world tasks and use them to
compare leading models. We find that Google Gemini outperforms other
models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,
DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.

</details>


### [31] [An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning](https://arxiv.org/abs/2505.08343)
*Ruichu Cai,Xi Chen,Jie Qiao,Zijian Li,Yuequn Liu,Wei Chen,Keli Zhang,Jiale Zheng*

Main category: cs.AI

TL;DR: 论文提出了一种最小成本因果决策框架（MiCCD），通过反事实推理解决异常条件下决策问题，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有决策框架在异常条件下常忽略行动成本或因果机制，需改进。

Method: 基于因果图构建代理模型，利用异常模式聚类标签，结合反事实推理和SLSQP算法优化干预策略。

Result: 在合成和真实数据集上，MiCCD在F1分数、成本效率和nDCG@k值上优于传统方法。

Conclusion: MiCCD框架有效且广泛适用，解决了异常决策中的成本和因果问题。

Abstract: Decision making under abnormal conditions is a critical process that involves
evaluating the current state and determining the optimal action to restore the
system to a normal state at an acceptable cost. However, in such scenarios,
existing decision-making frameworks highly rely on reinforcement learning or
root cause analysis, resulting in them frequently neglecting the cost of the
actions or failing to incorporate causal mechanisms adequately. By relaxing the
existing causal decision framework to solve the necessary cause, we propose a
minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to
address the above challenges. Emphasis is placed on making counterfactual
reasoning processes identifiable in the presence of a large amount of mixed
anomaly data, as well as finding the optimal intervention state in a continuous
decision space. Specifically, it formulates a surrogate model based on causal
graphs, using abnormal pattern clustering labels as supervisory signals. This
enables the approximation of the structural causal model among the variables
and lays a foundation for identifiable counterfactual reasoning. With the
causal structure approximated, we then established an optimization model based
on counterfactual estimation. The Sequential Least Squares Programming (SLSQP)
algorithm is further employed to optimize intervention strategies while taking
costs into account. Experimental evaluations on both synthetic and real-world
datasets reveal that MiCCD outperforms conventional methods across multiple
metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k
values), thus validating its efficacy and broad applicability.

</details>


### [32] [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://arxiv.org/abs/2505.08361)
*Xinyue Wang,Biwei Huang*

Main category: cs.AI

TL;DR: WM3C通过组合因果组件增强RL的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决RL在新环境中泛化能力不足的问题，受人类组合推理启发。

Method: 利用语言作为组合模态，分解潜在空间为因果组件，使用掩码自编码器和互信息约束。

Result: 在数值模拟和机器人任务中显著优于现有方法。

Conclusion: WM3C通过组合因果组件有效提升RL的泛化和适应能力。

Abstract: Generalization in reinforcement learning (RL) remains a significant
challenge, especially when agents encounter novel environments with unseen
dynamics. Drawing inspiration from human compositional reasoning -- where known
components are reconfigured to handle new situations -- we introduce World
Modeling with Compositional Causal Components (WM3C). This novel framework
enhances RL generalization by learning and leveraging compositional causal
components. Unlike previous approaches focusing on invariant representation
learning or meta-learning, WM3C identifies and utilizes causal dynamics among
composable elements, facilitating robust adaptation to new tasks. Our approach
integrates language as a compositional modality to decompose the latent space
into meaningful components and provides theoretical guarantees for their unique
identification under mild assumptions. Our practical implementation uses a
masked autoencoder with mutual information constraints and adaptive sparsity
regularization to capture high-level semantic information and effectively
disentangle transition dynamics. Experiments on numerical simulations and
real-world robotic manipulation tasks demonstrate that WM3C significantly
outperforms existing methods in identifying latent processes, improving policy
learning, and generalizing to unseen tasks.

</details>


### [33] [Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation](https://arxiv.org/abs/2505.08364)
*Enci Zhang,Xingang Yan,Wei Lin,Tianxiang Zhang,Qianchun Lu*

Main category: cs.AI

TL;DR: 论文提出两种新策略（ADCL和EGSR）提升大语言模型解决复杂问题的能力，实验表明其显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决复杂问题时仍存在挑战，受人类学习策略启发，提出新方法以提升模型能力。

Method: 1. ADCL：动态调整问题难度以匹配模型能力；2. EGSR：引导模型自主重构专家解决方案。

Result: 在AIME24和AIME25基准上分别提升10%和16.6%。

Conclusion: 人类学习策略启发的ADCL和EGSR能显著提升模型性能。

Abstract: Despite impressive progress in areas like mathematical reasoning, large
language models still face significant challenges in consistently solving
complex problems. Drawing inspiration from key human learning strategies, we
propose two novel strategies to enhance the capability of large language models
to solve these complex problems. First, Adaptive Difficulty Curriculum Learning
(ADCL) is a novel curriculum learning strategy that tackles the Difficulty
Shift phenomenon (i.e., a model's perception of problem difficulty dynamically
changes during training) by periodically re-estimating difficulty within
upcoming data batches to maintain alignment with the model's evolving
capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel
reinforcement learning strategy that bridges the gap between imitation learning
and pure exploration by guiding models to reformulate expert solutions within
their own conceptual framework, rather than relying on direct imitation,
fostering deeper understanding and knowledge assimilation. Extensive
experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B
as the base model, demonstrate that these human-inspired strategies
synergistically and significantly enhance performance. Notably, their combined
application improves performance over the standard Zero-RL baseline by 10% on
the AIME24 benchmark and 16.6% on AIME25.

</details>


### [34] [Explaining Autonomous Vehicles with Intention-aware Policy Graphs](https://arxiv.org/abs/2505.08404)
*Sara Montese,Victor Gimenez-Abalos,Atia Cortés,Ulises Cortés,Sergio Alvarez-Napagao*

Main category: cs.AI

TL;DR: 论文提出了一种后处理、模型无关的方法，为自动驾驶车辆的行为提供目的论解释，以提高透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的决策过程不透明，导致社会信任和监管接受度低，需要解释性解决方案。

Method: 基于意图感知策略图，从全局和局部视角提取可解释的行为解释。

Result: 方法在nuScenes数据集上验证，能评估车辆行为是否合法，并识别数据与模型的潜在漏洞。

Conclusion: 提出的解释性方法有助于提升自动驾驶的透明度和可信度，支持法律合规性和模型改进。

Abstract: The potential to improve road safety, reduce human driving error, and promote
environmental sustainability have enabled the field of autonomous driving to
progress rapidly over recent decades. The performance of autonomous vehicles
has significantly improved thanks to advancements in Artificial Intelligence,
particularly Deep Learning. Nevertheless, the opacity of their decision-making,
rooted in the use of accurate yet complex AI models, has created barriers to
their societal trust and regulatory acceptance, raising the need for
explainability. We propose a post-hoc, model-agnostic solution to provide
teleological explanations for the behaviour of an autonomous vehicle in urban
environments. Building on Intention-aware Policy Graphs, our approach enables
the extraction of interpretable and reliable explanations of vehicle behaviour
in the nuScenes dataset from global and local perspectives. We demonstrate the
potential of these explanations to assess whether the vehicle operates within
acceptable legal boundaries and to identify possible vulnerabilities in
autonomous driving datasets and models.

</details>


### [35] [Agent-as-a-Service based on Agent Network](https://arxiv.org/abs/2505.08446)
*Yuhan Zhu,Haojie Liu,Jian Wang,Bing Li,Zikang Yin,Yefei Liao*

Main category: cs.AI

TL;DR: 论文提出了一种基于Agent Network的Agent-as-a-Service（AaaS-AN）范式，通过动态Agent网络和服务导向的Agent，解决了多Agent系统中协作组织的不足。


<details>
  <summary>Details</summary>
Motivation: 大型模型驱动的AI Agent在多Agent系统（MAS）中展现出决策、协作和适应性能力，但现有Model Context Protocol（MCP）缺乏对Agent级协作的支持。

Method: 提出AaaS-AN，基于RGPS标准，包含动态Agent网络和服务导向Agent，通过Service Scheduler实现分布式协调和任务管理。

Result: 在数学推理和代码生成任务中表现优于基线方法，并构建了一个包含100多个Agent服务的MAS系统。

Conclusion: AaaS-AN有效支持Agent全生命周期管理，并发布了10,000个多Agent工作流数据集，推动MAS长链协作研究。

Abstract: The rise of large model-based AI agents has spurred interest in Multi-Agent
Systems (MAS) for their capabilities in decision-making, collaboration, and
adaptability. While the Model Context Protocol (MCP) addresses tool invocation
and data exchange challenges via a unified protocol, it lacks support for
organizing agent-level collaboration. To bridge this gap, we propose
Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented
paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN
unifies the entire agent lifecycle, including construction, integration,
interoperability, and networked collaboration, through two core components: (1)
a dynamic Agent Network, which models agents and agent groups as vertexes that
self-organize within the network based on task and role dependencies; (2)
service-oriented agents, incorporating service discovery, registration, and
interoperability protocols. These are orchestrated by a Service Scheduler,
which leverages an Execution Graph to enable distributed coordination, context
tracking, and runtime task management. We validate AaaS-AN on mathematical
reasoning and application-level code generation tasks, which outperforms
state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN
containing agent groups, Robotic Process Automation (RPA) workflows, and MCP
servers over 100 agent services. We also release a dataset containing 10,000
long-horizon multi-agent workflows to facilitate future research on long-chain
collaboration in MAS.

</details>


### [36] [Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem](https://arxiv.org/abs/2505.08451)
*Lotfi Kobrosly,Marc-Emmanuel Coupvent des Graviers,Christophe Guettier,Tristan Cazenave*

Main category: cs.AI

TL;DR: 本文提出了一种基于广义嵌套滚动策略适应（GNRPA）的新算法，用于解决柔性作业车间调度问题（FJSSP），实验结果表明其性能优于其他基于MCTS的方法。


<details>
  <summary>Details</summary>
Motivation: FJSSP是一个NP难组合优化问题，广泛应用于制造业，需要高效调度多道工序在不同机器上的顺序执行。

Method: 提出了一种基于广义嵌套滚动策略适应（GNRPA）的新算法。

Result: 实验结果显示，该算法优于其他基于MCTS的方法，但在大规模实例上的完工时间仍与已知上界有差距。

Conclusion: 新算法在解决FJSSP问题上表现出潜力，但仍需进一步优化以接近理论最优解。

Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial
optimization problem, with several application domains, especially for
manufacturing purposes. The objective is to
  efficiently schedule multiple operations on dissimilar machines. These
operations are gathered into jobs, and operations pertaining to the same job
need to be scheduled sequentially. Different methods have been previously
tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic
Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm
derived from the Generalized Nested Rollout Policy Adaptation, developed to
solve the FJSSP. We report encouraging experimental results, as our algorithm
performs better than other MCTS-based approaches, even if makespans obtained on
large instances are still far from known upper bounds.

</details>


### [37] [Strategy-Augmented Planning for Large Language Models via Opponent Exploitation](https://arxiv.org/abs/2505.08459)
*Shuai Xu,Sijia Cui,Yanna Wang,Bo Xu,Qi Wang*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段策略增强规划（SAP）框架，通过策略评估网络（SEN）增强基于LLM的代理的对手利用能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在对抗性领域中，高效建模和利用对手是一个长期挑战。尽管LLM在通用任务中表现出色，但直接使用LLM生成决策的方法受限于其领域专业知识。

Method: SAP框架分为离线阶段（构建策略空间并训练SEN网络）和在线阶段（动态识别对手策略并通过SEN搜索最佳响应策略）。

Result: 实验表明，SAP具有强大的泛化能力，在MicroRTS环境中性能提升85.35%，与强化学习方法竞争力相当。

Conclusion: SAP框架显著提升了LLM代理的对手利用能力，适用于已知和未知对手策略。

Abstract: Efficiently modeling and exploiting opponents is a long-standing challenge in
adversarial domains. Large Language Models (LLMs) trained on extensive textual
data have recently demonstrated outstanding performance in general tasks,
introducing new research directions for opponent modeling. Some studies
primarily focus on directly using LLMs to generate decisions based on the
elaborate prompt context that incorporates opponent descriptions, while these
approaches are limited to scenarios where LLMs possess adequate domain
expertise. To address that, we introduce a two-stage Strategy-Augmented
Planning (SAP) framework that significantly enhances the opponent exploitation
capabilities of LLM-based agents by utilizing a critical component, the
Strategy Evaluation Network (SEN). Specifically, in the offline stage, we
construct an explicit strategy space and subsequently collect strategy-outcome
pair data for training the SEN network. During the online phase, SAP
dynamically recognizes the opponent's strategies and greedily exploits them by
searching best response strategy on the well-trained SEN, finally translating
strategy to a course of actions by carefully designed prompts. Experimental
results show that SAP exhibits robust generalization capabilities, allowing it
to perform effectively not only against previously encountered opponent
strategies but also against novel, unseen strategies. In the MicroRTS
environment, SAP achieves a 85.35\% performance improvement over baseline
methods and matches the competitiveness of reinforcement learning approaches
against state-of-the-art (SOTA) rule-based AI.

</details>


### [38] [BAT: Benchmark for Auto-bidding Task](https://arxiv.org/abs/2505.08485)
*Alexandra Khirianova,Ekaterina Solodneva,Andrey Pudovikov,Sergey Osokin,Egor Samosvat,Yuriy Dorn,Alexander Ledovsky,Yana Zenkova*

Main category: cs.AI

TL;DR: 该论文提出了一个用于在线广告位拍卖的竞价策略优化基准，解决了数据集和标准化基准稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 在线广告拍卖中，实时自动竞价算法的开发、评估和改进面临数据集和标准化基准不足的挑战。

Method: 作者构建了一个包含两种常见拍卖格式的基准，并在新数据集上实现了一系列基线方法，专注于预算均匀分配和点击成本优化。

Result: 该基准为研究人员和从业者提供了一个用户友好的框架，促进了程序化广告领域的创新。

Conclusion: 论文提出的基准和资源有助于推动自动竞价算法的研究和发展。

Abstract: The optimization of bidding strategies for online advertising slot auctions
presents a critical challenge across numerous digital marketplaces. A
significant obstacle to the development, evaluation, and refinement of
real-time autobidding algorithms is the scarcity of comprehensive datasets and
standardized benchmarks.
  To address this deficiency, we present an auction benchmark encompassing the
two most prevalent auction formats. We implement a series of robust baselines
on a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem
domains: budget pacing uniformity and Cost Per Click (CPC) constraint
optimization. This benchmark provides a user-friendly and intuitive framework
for researchers and practitioners to develop and refine innovative autobidding
algorithms, thereby facilitating advancements in the field of programmatic
advertising. The implementation and additional resources can be accessed at the
following repository (https://github.com/avito-tech/bat-autobidding-benchmark,
https://doi.org/10.5281/zenodo.14794182).

</details>


### [39] [Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM](https://arxiv.org/abs/2505.08492)
*Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: Gideon框架通过本地小型LLM和扩展上下文解决了PDDL符号任务规划在动态人机协作中的问题，支持多领域扩展和高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决PDDL符号任务规划在动态人机协作中的可扩展性、重新规划需求和延迟问题，同时避免对闭源远程LLM的依赖。

Method: Gideon结合问题生成器生成大规模数据集，并采用神经符号规划方法，支持本地LLM执行和扩展上下文。

Result: 单领域实验中，32k样本训练的模型生成有效计划的比率为66.1%；多领域测试中，16k样本的规划有效率达70.6%。

Conclusion: Gideon在模型大小、推理效率和多领域适应性方面具有显著优势，尽管训练效率较低，但可通过数据生成流水线缓解。

Abstract: PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.

</details>


### [40] [TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching](https://arxiv.org/abs/2505.08508)
*Majd Abdallah,Sigve Nakken,Mariska Bierkens,Johanna Galvis,Alexis Groppi,Slim Karkar,Lana Meiqari,Maria Alexandra Rujano,Steve Canham,Rodrigo Dienstmann,Remond Fijneman,Eivind Hovig,Gerrit Meijer,Macha Nikolski*

Main category: cs.AI

TL;DR: TrialMatchAI是一个基于AI的患者与临床试验匹配系统，通过处理结构化与非结构化临床数据，结合检索增强生成框架，提供高效、透明且轻量化的解决方案。


<details>
  <summary>Details</summary>
Motivation: 临床试验中患者招募效率低下，需要自动化且可扩展的解决方案。

Method: 系统使用开源大语言模型（LLMs），结合混合搜索策略（词汇与语义相似性），进行生物医学实体标准化、试验检索、结果重排及标准级资格评估。

Result: 在真实验证中，92%的肿瘤患者在前20推荐中至少匹配到一个相关试验，专家评估显示标准级分类准确率超过90%。

Conclusion: TrialMatchAI通过高效性、可解释性和轻量化部署，为精准医学中的临床试验匹配提供了可扩展的解决方案。

Abstract: Patient recruitment remains a major bottleneck in clinical trials, calling
for scalable and automated solutions. We present TrialMatchAI, an AI-powered
recommendation system that automates patient-to-trial matching by processing
heterogeneous clinical data, including structured records and unstructured
physician notes. Built on fine-tuned, open-source large language models (LLMs)
within a retrieval-augmented generation framework, TrialMatchAI ensures
transparency and reproducibility and maintains a lightweight deployment
footprint suitable for clinical environments. The system normalizes biomedical
entities, retrieves relevant trials using a hybrid search strategy combining
lexical and semantic similarity, re-ranks results, and performs criterion-level
eligibility assessments using medical Chain-of-Thought reasoning. This pipeline
delivers explainable outputs with traceable decision rationales. In real-world
validation, 92 percent of oncology patients had at least one relevant trial
retrieved within the top 20 recommendations. Evaluation across synthetic and
real clinical datasets confirmed state-of-the-art performance, with expert
assessment validating over 90 percent accuracy in criterion-level eligibility
classification, particularly excelling in biomarker-driven matches. Designed
for modularity and privacy, TrialMatchAI supports Phenopackets-standardized
data, enables secure local deployment, and allows seamless replacement of LLM
components as more advanced models emerge. By enhancing efficiency and
interpretability and offering lightweight, open-source deployment, TrialMatchAI
provides a scalable solution for AI-driven clinical trial matching in precision
medicine.

</details>


### [41] [On the Complexity and Properties of Preferential Propositional Dependence Logic](https://arxiv.org/abs/2505.08522)
*Kai Sauerwald,Arne Meier,Juha Kontinen*

Main category: cs.AI

TL;DR: 本文研究了基于团队语义和依赖原子的命题逻辑中KLM式优先推理的复杂性和性质，发现其具有累积性但违反System P，并给出了满足System P的条件。这些条件不适用于团队命题逻辑。此外，文章还展示了经典逻辑和依赖逻辑的蕴含关系如何通过非平凡的优先模型表达，并提出了两种自然表示下的复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 探讨命题依赖逻辑中优先推理的性质和复杂性，特别是在团队语义和依赖原子背景下的表现。

Method: 通过分析优先推理的累积性和System P的违反情况，提出满足System P的条件，并研究这些条件在团队命题逻辑中的适用性。同时，利用非平凡优先模型表达经典和依赖逻辑的蕴含关系。

Result: 发现优先团队推理具有累积性但违反System P，并给出了满足System P的充分条件。这些条件不适用于团队命题逻辑。此外，提出了优先推理在两种表示下的复杂性结果。

Conclusion: 本文揭示了命题依赖逻辑中优先推理的复杂性和性质，特别是在团队语义下的独特表现，为相关领域提供了新的理论支持。

Abstract: This paper considers the complexity and properties of KLM-style preferential
reasoning in the setting of propositional logic with team semantics and
dependence atoms, also known as propositional dependence logic. Preferential
team-based reasoning is shown to be cumulative, yet violates System~P. We give
intuitive conditions that fully characterise those cases where preferential
propositional dependence logic satisfies System~P. We show that these
characterisations do, surprisingly, not carry over to preferential team-based
propositional logic. Furthermore, we show how classical entailment and
dependence logic entailment can be expressed in terms of non-trivial
preferential models. Finally, we present the complexity of preferential
team-based reasoning for two natural representations. This includes novel
complexity results for classical (non-team-based) preferential reasoning.

</details>


### [42] [Guiding LLM-based Smart Contract Generation with Finite State Machine](https://arxiv.org/abs/2505.08542)
*Hao Luo,Yuhao Lin,Xiao Yan,Xintong Hu,Yuxiang Wang,Qiming Zeng,Hao Wang,Jiawei Jiang*

Main category: cs.AI

TL;DR: FSM-SCG是一个基于有限状态机（FSM）和大语言模型（LLM）的智能合约生成框架，显著提升了生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约生成方法依赖人工编码和专家审核，门槛高且效率低，LLM在智能合约生成中面临效果和安全性挑战。

Method: 通过抽象用户需求生成FSM，引导LLM生成智能合约，并通过编译和安全检查反馈迭代优化代码。

Result: 实验表明，FSM-SCG显著提升了生成代码的质量，编译成功率最高提升48%，平均漏洞风险评分降低约68%。

Conclusion: FSM-SCG为解决智能合约生成的效率和安全性问题提供了有效方案。

Abstract: Smart contract is a kind of self-executing code based on blockchain
technology with a wide range of application scenarios, but the traditional
generation method relies on manual coding and expert auditing, which has a high
threshold and low efficiency. Although Large Language Models (LLMs) show great
potential in programming tasks, they still face challenges in smart contract
generation w.r.t. effectiveness and security. To solve these problems, we
propose FSM-SCG, a smart contract generation framework based on finite state
machine (FSM) and LLMs, which significantly improves the quality of the
generated code by abstracting user requirements to generate FSM, guiding LLMs
to generate smart contracts, and iteratively optimizing the code with the
feedback of compilation and security checks. The experimental results show that
FSM-SCG significantly improves the quality of smart contract generation.
Compared to the best baseline, FSM-SCG improves the compilation success rate of
generated smart contract code by at most 48%, and reduces the average
vulnerability risk score by approximately 68%.

</details>


### [43] [Resource-Efficient Language Models: Quantization for Fast and Accessible Inference](https://arxiv.org/abs/2505.08620)
*Tollef Emil Jørgensen*

Main category: cs.AI

TL;DR: 本文综述了后训练量化（PTQ）技术，旨在优化大语言模型（LLM）的推理效率，涵盖量化方案、粒度和权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型资源需求高，硬件可及性和能耗问题严重，需优化推理效率。

Method: 综述后训练量化技术，包括量化方案、粒度和权衡。

Result: 提供了理论与应用平衡的后训练量化概述。

Conclusion: 后训练量化是优化LLM推理效率的有效方法。

Abstract: Large language models have significantly advanced natural language
processing, yet their heavy resource demands pose severe challenges regarding
hardware accessibility and energy consumption. This paper presents a focused
and high-level review of post-training quantization (PTQ) techniques designed
to optimize the inference efficiency of LLMs by the end-user, including details
on various quantization schemes, granularities, and trade-offs. The aim is to
provide a balanced overview between the theory and applications of
post-training quantization.

</details>


### [44] [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
*Donghoon Kim,Minji Bae,Kyuhong Shim,Byonghyo Shim*

Main category: cs.AI

TL;DR: 论文提出了一种名为VGD的无梯度方法，利用LLM和CLIP指导生成连贯且语义对齐的提示，解决了现有提示反转技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示反转技术（如软硬提示）因解释性差和提示生成不连贯而效果不佳，需要改进。

Method: VGD结合LLM的强大文本生成能力和CLIP分数，确保生成的提示与用户指定的视觉概念对齐，无需额外训练。

Result: 实验表明，VGD在生成可理解和上下文相关的提示方面优于现有技术。

Conclusion: VGD提升了提示生成的解释性、泛化性和灵活性，使与文本到图像模型的交互更直观可控。

Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have
revolutionized visual content creation across various applications, including
advertising, personalized media, and design prototyping. However, crafting
effective textual prompts to guide these models remains challenging, often
requiring extensive trial and error. Existing prompt inversion approaches, such
as soft and hard prompt techniques, are not so effective due to the limited
interpretability and incoherent prompt generation. To address these issues, we
propose Visually Guided Decoding (VGD), a gradient-free approach that leverages
large language models (LLMs) and CLIP-based guidance to generate coherent and
semantically aligned prompts. In essence, VGD utilizes the robust text
generation capabilities of LLMs to produce human-readable prompts. Further, by
employing CLIP scores to ensure alignment with user-specified visual concepts,
VGD enhances the interpretability, generalization, and flexibility of prompt
generation without the need for additional training. Our experiments
demonstrate that VGD outperforms existing prompt inversion techniques in
generating understandable and contextually relevant prompts, facilitating more
intuitive and controllable interactions with text-to-image models.

</details>


### [45] [Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach](https://arxiv.org/abs/2505.08628)
*Yichen Zhao,Yuhua Wang,Xi Cheng,Junhao Fang,Yang Yang*

Main category: cs.AI

TL;DR: 研究提出了一种结合自然语言处理和运动监测的深度学习框架，利用日常易获取的生理数据和运动相关文本，用于代谢综合征（MetS）的早期诊断。


<details>
  <summary>Details</summary>
Motivation: 代谢综合征（MetS）的早期诊断和干预至关重要，但传统诊断方法依赖医疗机构且常被低估，导致未满足的医疗需求。

Method: 收集40名志愿者的数据，通过数据增强减少不平衡，提出结合NLP和运动监测的深度学习模型。

Result: 最佳模型表现优异（AUROC=0.806，REC=76.3%），文本和每日最低心率对分类贡献最大。

Conclusion: 研究表明日常易测数据可用于MetS早期诊断，有望降低筛查和管理成本。

Abstract: Metabolic syndrome (MetS) is a medication condition characterized by
abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It
increases the risk of majority of chronic diseases, including type 2 diabetes
mellitus, and affects about one quarter of the global population. Therefore,
early detection and timely intervention for MetS are crucial. Standard
diagnosis for MetS components requires blood tests conducted within medical
institutions. However, it is frequently underestimated, leading to unmet need
for care for MetS population. This study aims to use the least physiological
data and free texts about exercises related activities, which are obtained
easily in daily life, to diagnosis MetS. We collected the data from 40
volunteers in a nursing home and used data augmentation to reduce the
imbalance. We propose a deep learning framework for classifying MetS that
integrates natural language processing (NLP) and exercise monitoring. The
results showed that the best model reported a high positive result (AUROC=0.806
and REC=76.3%) through 3-fold cross-validation. Feature importance analysis
revealed that text and minimum heart rate on a daily basis contribute the most
in the classification of MetS. This study demonstrates the potential
application of data that are easily measurable in daily life for the early
diagnosis of MetS, which could contribute to reducing the cost of screening and
management for MetS population.

</details>


### [46] [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Jitin Krishnan,Anand Kannappan,Rebecca Qian*

Main category: cs.AI

TL;DR: 论文提出了一种新的方法来评估代理工作流生成的复杂痕迹，解决了当前依赖人工分析的不可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着代理工作流在各领域的广泛应用，现有评估方法无法应对其复杂性和规模的增加，亟需一种动态且鲁棒的评估方法。

Method: 论文提出了一种错误类型的分类法，并构建了一个包含148条人工标注痕迹的数据集（TRAIL），用于评估代理系统的表现。

Result: 实验表明，现代长上下文LLM在痕迹调试上表现不佳，最佳模型Gemini-2.5-pro在TRAIL上仅得11%。

Conclusion: 论文通过数据集和代码的公开，为未来代理工作流的可扩展评估研究提供了支持。

Abstract: The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.

</details>


### [47] [WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08643)
*Dvir Cohen,Lin Burg,Sviatoslav Pykhnivskyi,Hagit Gur,Stanislav Kovynov,Olga Atzmon,Gilad Barkan*

Main category: cs.AI

TL;DR: WixQA是一个新的基准测试套件，用于评估企业问答系统中的检索增强生成（RAG）技术，包含三个数据集和知识库快照。


<details>
  <summary>Details</summary>
Motivation: 现有开放领域数据集无法满足企业问答系统的需求，需要更贴近实际用户问题的数据集和知识库快照。

Method: 引入WixQA，包含三个数据集：专家编写的真实用户查询、专家验证的模拟对话和LLM生成的合成QA对，以及知识库快照。

Result: WixQA提供了全面的基准测试结果，支持企业RAG系统的端到端评估。

Conclusion: WixQA填补了企业问答系统评估的空白，为实际应用场景提供了实用工具。

Abstract: Retrieval-Augmented Generation (RAG) is a cornerstone of modern question
answering (QA) systems, enabling grounded answers based on external knowledge.
Although recent progress has been driven by open-domain datasets, enterprise QA
systems need datasets that mirror the concrete, domain-specific issues users
raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG
systems requires benchmarks comprising not only question--answer pairs but also
the specific knowledge base (KB) snapshot from which answers were derived. To
address this need, we introduce WixQA, a benchmark suite featuring QA datasets
precisely grounded in the released KB corpus, enabling holistic evaluation of
retrieval and generation components. WixQA includes three distinct QA datasets
derived from Wix.com customer support interactions and grounded in a snapshot
of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user
queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200
expert-validated QA pairs distilled from user dialogues; and (iii)
WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically
derived from each article in the knowledge base. We release the KB snapshot
alongside the datasets under MIT license and provide comprehensive baseline
results, forming a unique benchmark for evaluating enterprise RAG systems in
realistic enterprise environments.

</details>


### [48] [A Study of Data-driven Methods for Inventory Optimization](https://arxiv.org/abs/2505.08673)
*Lee Yeung Ping,Patrick Wong,Tan Cheng Han*

Main category: cs.AI

TL;DR: 本文分析了三种算法（时间序列、随机森林和深度强化学习）在三种库存模型（缺货损失、双源采购和多级库存模型）中的应用，评估了它们在超市环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是寻找数据驱动的高效方法，分析算法的可能性、潜力和当前挑战，以优化库存管理。

Method: 通过比较三种算法在不同库存模型中的表现，使用预测准确性、市场适应性和库存成本等关键指标进行评估。

Result: 数据可视化工具和统计指标揭示了明显趋势，帮助管理者实时跟踪算法性能并识别库存波动原因。

Conclusion: 研究为供应链中的决策提供了详细指导，有助于发现低效环节和改进空间。

Abstract: This paper shows a comprehensive analysis of three algorithms (Time Series,
Random Forest (RF) and Deep Reinforcement Learning) into three inventory models
(the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These
methodologies are applied in the supermarket context. The main purpose is to
analyse efficient methods for the data-driven. Their possibility, potential and
current challenges are taken into consideration in this report. By comparing
the results in each model, the effectiveness of each algorithm is evaluated
based on several key performance indicators, including forecast accuracy,
adaptability to market changes, and overall impact on inventory costs and
customer satisfaction levels. The data visualization tools and statistical
metrics are the indicators for the comparisons and show some obvious trends and
patterns that can guide decision-making in inventory management. These tools
enable managers to not only track the performance of different algorithms in
real-time but also to drill down into specific data points to understand the
underlying causes of inventory fluctuations. This level of detail is crucial
for pinpointing inefficiencies and areas for improvement within the supply
chain.

</details>


### [49] [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
*K M Sajjadul Islam,Ayesha Siddika Nipu,Jiawei Wu,Praveen Madiraju*

Main category: cs.AI

TL;DR: 论文探讨了基于提示的大型语言模型（如GPT-4o和DeepSeek-R1）在电子健康记录（EHR）中命名实体识别（NER）的应用，其中GPT-4o结合提示集成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化临床文本需要有效的命名实体识别技术以提取关键医疗实体，支持下游临床应用。

Method: 采用提示工程技术（零样本、少样本和集成方法）指导GPT-4o和DeepSeek-R1进行医疗实体识别。

Result: GPT-4o结合提示集成方法在分类任务中表现最优，F1分数为0.95，召回率为0.98，优于DeepSeek-R1。

Conclusion: 提示集成方法通过嵌入相似性和多数投票提高了可靠性，GPT-4o在此任务中表现突出。

Abstract: Electronic Health Records (EHRs) are digital records of patient information,
often containing unstructured clinical text. Named Entity Recognition (NER) is
essential in EHRs for extracting key medical entities like problems, tests, and
treatments to support downstream clinical applications. This paper explores
prompt-based medical entity recognition using large language models (LLMs),
specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering
techniques, including zero-shot, few-shot, and an ensemble approach. Among all
strategies, GPT-4o with prompt ensemble achieved the highest classification
performance with an F1-score of 0.95 and recall of 0.98, outperforming
DeepSeek-R1 on the task. The ensemble method improved reliability by
aggregating outputs through embedding-based similarity and majority voting.

</details>


### [50] [DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models](https://arxiv.org/abs/2505.08744)
*Xiaoyang Chen,Xinan Dai,Yu Du,Qian Feng,Naixu Guo,Tingshuo Gu,Yuting Gao,Yingyi Gao,Xudong Han,Xiang Jiang,Yilin Jin,Hongyi Lin,Shisheng Lin,Xiangnan Li,Yuante Li,Yixing Li,Zhentao Lai,Zilu Ma,Yingrong Peng,Jiacheng Qian,Hao-Yu Sun,Jianbo Sun,Zirui Wang,Siwei Wu,Zian Wang,Bin Xu,Jianghao Xu,Yiyang Yu,Zichuan Yang,Hongji Zha,Ruichong Zhang*

Main category: cs.AI

TL;DR: DeepMath团队提出评估数学创造力的标准，并发布DeepMath-Creative基准测试，发现主流LLM在创造性数学问题上的表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前数学LLM的研究多关注推理能力，而创造力评估不足，缺乏相关数据集。

Method: 提出数学创造力评估标准，构建DeepMath-Creative基准测试，评估主流LLM的创造性解题能力。

Result: 最佳模型O3 Mini在基础本科级任务中仅达70%准确率，复杂问题表现显著下降。

Conclusion: 当前LLM的创造性表现可能源于记忆模式重组，而非真正的创新洞察。

Abstract: To advance the mathematical proficiency of large language models (LLMs), the
DeepMath team has launched an open-source initiative aimed at developing an
open mathematical LLM and systematically evaluating its mathematical
creativity. This paper represents the initial contribution of this initiative.
While recent developments in mathematical LLMs have predominantly emphasized
reasoning skills, as evidenced by benchmarks on elementary to
undergraduate-level mathematical tasks, the creative capabilities of these
models have received comparatively little attention, and evaluation datasets
remain scarce. To address this gap, we propose an evaluation criteria for
mathematical creativity and introduce DeepMath-Creative, a novel, high-quality
benchmark comprising constructive problems across algebra, geometry, analysis,
and other domains. We conduct a systematic evaluation of mainstream LLMs'
creative problem-solving abilities using this dataset. Experimental results
show that even under lenient scoring criteria -- emphasizing core solution
components and disregarding minor inaccuracies, such as small logical gaps,
incomplete justifications, or redundant explanations -- the best-performing
model, O3 Mini, achieves merely 70% accuracy, primarily on basic
undergraduate-level constructive tasks. Performance declines sharply on more
complex problems, with models failing to provide substantive strategies for
open problems. These findings suggest that, although current LLMs display a
degree of constructive proficiency on familiar and lower-difficulty problems,
such performance is likely attributable to the recombination of memorized
patterns rather than authentic creative insight or novel synthesis.

</details>


### [51] [ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2505.08778)
*Etienne Guichard,Felix Reimers,Mia Kvalsund,Mikkel Lepperød,Stefano Nichele*

Main category: cs.AI

TL;DR: ARC-NCA利用神经细胞自动机（NCA）及其增强版（EngramNCA）解决ARC-AGI挑战，展示了发展性方法在AI抽象与推理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决ARC-AGI这一对AI极具挑战但对人类简单的任务，探索发展性方法如何提升AI的问题解决能力。

Method: 采用标准NCA和带隐藏记忆的EngramNCA，模拟生物系统的发展过程，实现复杂动态和模式生成。

Result: ARC-NCA在部分任务上表现优于ChatGPT 4.5，且成本显著更低。

Conclusion: 发展性方法为AI的抽象与推理能力提供了新方向，具有潜在的应用价值。

Abstract: The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a
fundamental challenge in artificial general intelligence (AGI), requiring
solutions that exhibit robust abstraction and reasoning capabilities across
diverse tasks, while only few (with median count of three) correct examples are
presented. While ARC-AGI remains very challenging for artificial intelligence
systems, it is rather easy for humans. This paper introduces ARC-NCA, a
developmental approach leveraging standard Neural Cellular Automata (NCA) and
NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark.
NCAs are employed for their inherent ability to simulate complex dynamics and
emergent patterns, mimicking developmental processes observed in biological
systems. Developmental solutions may offer a promising avenue for enhancing
AI's problem-solving capabilities beyond mere training data extrapolation.
ARC-NCA demonstrates how integrating developmental principles into
computational models can foster adaptive reasoning and abstraction. We show
that our ARC-NCA proof-of-concept results may be comparable to, and sometimes
surpass, that of ChatGPT 4.5, at a fraction of the cost.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [52] [MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing](https://arxiv.org/abs/2505.07984)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TL;DR: MilChat是一种轻量级多模态语言模型，专为分析偏远地区的遥感图像（如导弹发射场）而设计，通过专家验证的数据集和强化学习优化，显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在文本-图像内容理解和生成方面表现出色，但在资源有限且需要领域特定适应的专业领域（如军事遥感）中效果有限。

Method: 使用2B参数的开源MLLM进行监督微调，结合链式思维推理注释和GRPO强化学习，优化模型对军事关键特征的检测能力。

Result: 在MilData基准测试中，MilChat实现了80%的召回率和98%的精确率，显著优于通用模型和现有遥感适应方法。

Conclusion: 研究表明，针对特定领域的微调和强化学习可显著提升模型在专业任务中的性能，MilChat在军事遥感领域表现出色。

Abstract: Remarkable capabilities in understanding and generating text-image content
have been demonstrated by recent advancements in multimodal large language
models (MLLMs). However, their effectiveness in specialized
domains-particularly those requiring resource-efficient and domain-specific
adaptations-has remained limited. In this work, a lightweight multimodal
language model termed MilChat is introduced, specifically adapted to analyze
remote sensing imagery in secluded areas, including challenging missile launch
sites. A new dataset, MilData, was compiled by verifying hundreds of aerial
images through expert review, and subtle military installations were
highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter
open-source MLLM with chain-of-thought (CoT) reasoning annotations was
performed, enabling more accurate and interpretable explanations. Additionally,
Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's
ability to detect critical domain-specific cues-such as defensive layouts and
key military structures-while minimizing false positives on civilian scenes.
Through empirical evaluations, it has been shown that MilChat significantly
outperforms both larger, general-purpose multimodal models and existing remote
sensing-adapted approaches on open-ended captioning and classification metrics.
Over 80% recall and 98% precision were achieved on the newly proposed MilData
benchmark, underscoring the potency of targeted fine-tuning and reinforcement
learning in specialized real-world applications.

</details>


### [53] [Vision Foundation Model Embedding-Based Semantic Anomaly Detection](https://arxiv.org/abs/2505.07998)
*Max Peter Ronecker,Matthew Foutter,Amine Elhafsi,Daniele Gammelli,Ihor Barakaiev,Marco Pavone,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉基础模型语义先验的语义异常检测框架，通过比较运行时图像的局部视觉嵌入与安全场景数据库，实现自主系统中的实时异常检测。


<details>
  <summary>Details</summary>
Motivation: 语义异常可能导致自主系统推理失败，因此需要一种有效的方法来检测这些异常。

Method: 提出了两种框架变体：基于原始网格嵌入和基于实例分割的对象中心表示，并引入过滤机制减少误报。

Result: 在CARLA模拟异常上的评估表明，基于实例的方法结合过滤机制性能接近GPT-4o，并能精确定位异常。

Conclusion: 视觉基础模型的嵌入在自主系统实时异常检测中具有潜在实用性。

Abstract: Semantic anomalies are contextually invalid or unusual combinations of
familiar visual elements that can cause undefined behavior and failures in
system-level reasoning for autonomous systems. This work explores semantic
anomaly detection by leveraging the semantic priors of state-of-the-art vision
foundation models, operating directly on the image. We propose a framework that
compares local vision embeddings from runtime images to a database of nominal
scenarios in which the autonomous system is deemed safe and performant. In this
work, we consider two variants of the proposed framework: one using raw
grid-based embeddings, and another leveraging instance segmentation for
object-centric representations. To further improve robustness, we introduce a
simple filtering mechanism to suppress false positives. Our evaluations on
CARLA-simulated anomalies show that the instance-based method with filtering
achieves performance comparable to GPT-4o, while providing precise anomaly
localization. These results highlight the potential utility of vision
embeddings from foundation models for real-time anomaly detection in autonomous
systems.

</details>


### [54] [RDD: Robust Feature Detector and Descriptor using Deformable Transformer](https://arxiv.org/abs/2505.08013)
*Gonglin Chen,Tianwen Fu,Haiwei Chen,Wenbin Teng,Hanyuan Xiao,Yajie Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于可变形Transformer的鲁棒关键点检测与描述方法（RDD），通过可变形自注意力机制捕捉全局上下文和几何不变性，显著提升了稀疏匹配任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在显著视角变化等挑战性场景下，现有局部特征方法难以建模长距离关系，导致特征检测与描述不够鲁棒。

Method: 利用可变形Transformer的可变形自注意力机制，聚焦关键位置，降低搜索空间复杂度并建模几何不变性。同时结合Air-to-Ground数据集进行训练。

Result: RDD在稀疏匹配任务中优于所有现有方法，并能实现半稠密匹配。还提出了两个新基准测试，包括大视角变化和Air-to-Ground场景。

Conclusion: RDD通过全局上下文建模和几何不变性学习，显著提升了特征检测与描述的鲁棒性，适用于复杂场景。

Abstract: As a core step in structure-from-motion and SLAM, robust feature detection
and description under challenging scenarios such as significant viewpoint
changes remain unresolved despite their ubiquity. While recent works have
identified the importance of local features in modeling geometric
transformations, these methods fail to learn the visual cues present in
long-range relationships. We present Robust Deformable Detector (RDD), a novel
and robust keypoint detector/descriptor leveraging the deformable transformer,
which captures global context and geometric invariance through deformable
self-attention mechanisms. Specifically, we observed that deformable attention
focuses on key locations, effectively reducing the search space complexity and
modeling the geometric invariance. Furthermore, we collected an Air-to-Ground
dataset for training in addition to the standard MegaDepth dataset. Our
proposed method outperforms all state-of-the-art keypoint detection/description
methods in sparse matching tasks and is also capable of semi-dense matching. To
ensure comprehensive evaluation, we introduce two challenging benchmarks: one
emphasizing large viewpoint and scale variations, and the other being an
Air-to-Ground benchmark -- an evaluation setting that has recently gaining
popularity for 3D reconstruction across different altitudes.

</details>


### [55] [Visually Interpretable Subtask Reasoning for Visual Question Answering](https://arxiv.org/abs/2505.08084)
*Yu Cheng,Arushi Goel,Hakan Bilen*

Main category: cs.CV

TL;DR: VISTAR是一种基于子任务的训练框架，通过生成文本和视觉解释提升多模态大语言模型（MLLMs）的可解释性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂视觉问题中计算成本高且准确性低的问题。

Method: 通过微调MLLMs生成结构化的子任务推理序列（Subtask-of-Thought rationales），无需依赖外部模型。

Result: 在两个基准测试中，VISTAR显著提升了推理准确性，同时保持了可解释性。

Conclusion: VISTAR为复杂视觉问题的多步推理提供了一种高效且可解释的解决方案。

Abstract: Answering complex visual questions like `Which red furniture can be used for
sitting?' requires multi-step reasoning, including object recognition,
attribute filtering, and relational understanding. Recent work improves
interpretability in multimodal large language models (MLLMs) by decomposing
tasks into sub-task programs, but these methods are computationally expensive
and less accurate due to poor adaptation to target data. To address this, we
introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a
subtask-driven training framework that enhances both interpretability and
reasoning by generating textual and visual explanations within MLLMs. Instead
of relying on external models, VISTAR fine-tunes MLLMs to produce structured
Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments
on two benchmarks show that VISTAR consistently improves reasoning accuracy
while maintaining interpretability. Our code and dataset will be available at
https://github.com/ChengJade/VISTAR.

</details>


### [56] [Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)](https://arxiv.org/abs/2505.08086)
*Ramin Mousa,Ehsan Matbooe,Hakimeh Khojasteh,Amirali Bengari,Mohammadmahdi Vahediahmar*

Main category: cs.CV

TL;DR: 提出了一种基于迁移学习的多模态AI模型，结合Xception和GMRNN架构，用于伤口分类，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 急性及难愈合伤口的有效诊断对临床护理至关重要，但现有工具常因感染、血管疾病等因素导致效果不佳，AI技术可提升诊断效率和早期检测能力。

Method: 采用迁移学习技术，结合Xception和GMRNN架构，提取特征并融合位置信息，对糖尿病、压力、手术和静脉溃疡进行分类。

Result: 实验结果显示，分类准确率在78.77%至100%之间，表现优于传统深度神经网络。

Conclusion: 该方法在常见伤口类型的分类中表现出卓越的准确性，为临床诊断提供了高效工具。

Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound
care practitioners to provide effective patient care. Poor clinical outcomes
are often linked to infection, peripheral vascular disease, and increasing
wound depth, which collectively exacerbate these comorbidities. However,
diagnostic tools based on Artificial Intelligence (AI) speed up the
interpretation of medical images and improve early detection of disease. In
this article, we propose a multi-modal AI model based on transfer learning
(TL), which combines two state-of-the-art architectures, Xception and GMRNN,
for wound classification. The multi-modal network is developed by concatenating
the features extracted by a transfer learning algorithm and location features
to classify the wound types of diabetic, pressure, surgical, and venous ulcers.
The proposed method is comprehensively compared with deep neural networks (DNN)
for medical image analysis. The experimental results demonstrate a notable
wound-class classifications (containing only diabetic, pressure, surgical, and
venous) vary from 78.77 to 100\% in various experiments. The results presented
in this study showcase the exceptional accuracy of the proposed methodology in
accurately classifying the most commonly occurring wound types using wound
images and their corresponding locations.

</details>


### [57] [Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing](https://arxiv.org/abs/2505.08101)
*Luu Tung Hai,Thinh D. Le,Zhicheng Ding,Qing Tian,Truong-Son Hy*

Main category: cs.CV

TL;DR: 提出了一种新颖的点云知识蒸馏框架，通过拓扑感知表示和梯度引导蒸馏，将高性能教师模型的知识高效传递给轻量级学生模型，显著减少了模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 点云处理在自动驾驶和3D物体识别中至关重要，但高性能模型在资源受限环境中部署困难，需要轻量化解决方案。

Method: 采用拓扑感知表示和梯度引导知识蒸馏，捕捉点云的几何结构，并通过梯度特征对齐指导学生模型学习。

Result: 在NuScenes、SemanticKITTI和Waymo数据集上表现优异，模型大小减少约16倍，推理时间降低近1.9倍，分割性能领先。

Conclusion: 该方法在点云知识蒸馏中实现了最先进的性能，为资源受限环境提供了高效的轻量化解决方案。

Abstract: Point cloud processing has gained significant attention due to its critical
role in applications such as autonomous driving and 3D object recognition.
However, deploying high-performance models like Point Transformer V3 in
resource-constrained environments remains challenging due to their high
computational and memory demands. This work introduces a novel distillation
framework that leverages topology-aware representations and gradient-guided
knowledge distillation to effectively transfer knowledge from a high-capacity
teacher to a lightweight student model. Our approach captures the underlying
geometric structures of point clouds while selectively guiding the student
model's learning process through gradient-based feature alignment. Experimental
results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the
proposed method achieves competitive performance, with an approximately 16x
reduction in model size and a nearly 1.9x decrease in inference time compared
to its teacher model. Notably, on NuScenes, our method achieves
state-of-the-art performance among knowledge distillation techniques trained
solely on LiDAR data, surpassing prior knowledge distillation baselines in
segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill

</details>


### [58] [Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors](https://arxiv.org/abs/2505.08111)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: 利用预训练的Vision Transformer模型（ViTMAE和ViTPose）对低分辨率压力敏感垫数据进行睡眠姿势分类，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 睡眠姿势影响睡眠质量和睡眠障碍（如呼吸暂停），但临床环境中标记数据不足，需解决分类问题。

Method: 采用迁移学习，利用预训练的ViTMAE和ViTPose模型，对低分辨率PSM数据进行分类。

Result: 在112晚患者数据上表现优于传统方法（TCN、SVM、XGBoost、随机森林），并在高分辨率数据集上验证。

Conclusion: 尽管低分辨率数据分类困难，该方法在临床环境中具有实际应用潜力。

Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of
monitoring patients during sleep. We focus on four-way sleep position
classification using data collected from a PSM placed under a mattress in a
sleep clinic. Sleep positions can affect sleep quality and the prevalence of
sleep disorders, such as apnea. Measurements were performed on patients with
suspected sleep disorders referred for assessments at a sleep clinic. Training
deep learning models can be challenging in clinical settings due to the need
for large amounts of labeled data. To overcome the shortage of labeled training
data, we utilize transfer learning to adapt pre-trained deep learning models to
accurately estimate sleep positions from a low-resolution PSM dataset collected
in a polysomnography sleep lab. Our approach leverages Vision Transformer
models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a
pre-trained model for human pose estimation (ViTPose). These approaches
outperform previous work from PSM-based sleep pose classification using deep
learning (TCN) as well as traditional machine learning models (SVM, XGBoost,
Random Forest) that use engineered features. We evaluate the performance of
sleep position classification from 112 nights of patient recordings and
validate it on a higher resolution 13-patient dataset. Despite the challenges
of differentiating between sleep positions from low-resolution PSM data, our
approach shows promise for real-world deployment in clinical settings

</details>


### [59] [Now you see it, Now you don't: Damage Label Agreement in Drone & Satellite Post-Disaster Imagery](https://arxiv.org/abs/2505.08117)
*Thomas Manzini,Priyankari Perali,Jayesh Tripathi,Robin Murphy*

Main category: cs.CV

TL;DR: 本文通过对比卫星和无人机图像对15,814栋建筑的损坏标签，发现29.02%的标签不一致，且两种来源的标签分布显著不同，可能对机器学习损坏评估系统的部署带来风险和潜在危害。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于无人机和卫星图像在建筑损坏评估中标签一致性的研究，且现有研究因标签模式、建筑位置不一致和数据量不足而受限。

Method: 本研究克服了这些限制，通过使用相同的损坏标签模式和建筑位置，比较了三种飓风中的建筑损坏标签，数据量是之前研究的19.05倍。

Result: 分析发现，卫星标签比无人机标签至少低估了20.43%的损坏（p<1.2x10^-117），且两种标签分布显著不同（p<5.1x10^-175），表明基于其中一种标签训练的模型会误报实际状况。

Conclusion: 这种潜在的误报可能带来伦理风险和社会危害，因此本文提出了四条建议，以提高CV/ML损坏评估系统的可靠性和透明度。

Abstract: This paper audits damage labels derived from coincident satellite and drone
aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey,
finding 29.02% label disagreement and significantly different distributions
between the two sources, which presents risks and potential harms during the
deployment of machine learning damage assessment systems. Currently, there is
no known study of label agreement between drone and satellite imagery for
building damage assessment. The only prior work that could be used to infer if
such imagery-derived labels agree is limited by differing damage label schemas,
misaligned building locations, and low data quantities. This work overcomes
these limitations by comparing damage labels using the same damage label
schemas and building locations from three hurricanes, with the 15,814 buildings
representing 19.05 times more buildings considered than the most relevant prior
work. The analysis finds satellite-derived labels significantly under-report
damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and
satellite- and drone-derived labels represent significantly different
distributions (p<5.1x10^-175). This indicates that computer vision and machine
learning (CV/ML) models trained on at least one of these distributions will
misrepresent actual conditions, as the differing satellite and drone-derived
distributions cannot simultaneously represent the distribution of actual
conditions in a scene. This potential misrepresentation poses ethical risks and
potential societal harm if not managed. To reduce the risk of future societal
harms, this paper offers four recommendations to improve reliability and
transparency to decisio-makers when deploying CV/ML damage assessment systems
in practice

</details>


### [60] [JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections](https://arxiv.org/abs/2505.08123)
*Qing Wu,Hongjiang Wei,Jingyi Yu,S. Kevin Zhou,Yuyao Zhang*

Main category: cs.CV

TL;DR: JSover是一种新的单能CT多材料分解框架，通过联合重建和能量谱估计，显著提高了分解的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统多材料分解方法依赖光谱CT和预测量能谱，临床适用性受限。单能CT分解方法存在两步流程的缺陷，导致非线性伪影和噪声。

Method: 提出JSover框架，一步联合重建多材料组成和估计能量谱，结合物理先验和隐式神经表示（INR）优化分解质量。

Result: 实验表明，JSover在模拟和真实CT数据上优于现有方法，提高了准确性和计算效率。

Conclusion: JSover为单能CT多材料分解提供了更可靠和高效的解决方案，扩展了临床应用潜力。

Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of
tissue compositions in the human body, supporting a wide range of clinical
applications. However, traditional MMD typically requires spectral CT scanners
and pre-measured X-ray energy spectra, significantly limiting clinical
applicability. To this end, various methods have been developed to perform MMD
using conventional (i.e., single-energy, SE) CT systems, commonly referred to
as SEMMD. Despite promising progress, most SEMMD methods follow a two-step
image decomposition pipeline, which first reconstructs monochromatic CT images
using algorithms such as FBP, and then performs decomposition on these images.
The initial reconstruction step, however, neglects the energy-dependent
attenuation of human tissues, introducing severe nonlinear beam hardening
artifacts and noise into the subsequent decomposition. This paper proposes
JSover, a fundamentally reformulated one-step SEMMD framework that jointly
reconstructs multi-material compositions and estimates the energy spectrum
directly from SECT projections. By explicitly incorporating physics-informed
spectral priors into the SEMMD process, JSover accurately simulates a virtual
spectral CT system from SE acquisitions, thereby improving the reliability and
accuracy of decomposition. Furthermore, we introduce implicit neural
representation (INR) as an unsupervised deep learning solver for representing
the underlying material maps. The inductive bias of INR toward continuous image
patterns constrains the solution space and further enhances estimation quality.
Extensive experiments on both simulated and real CT datasets show that JSover
outperforms state-of-the-art SEMMD methods in accuracy and computational
efficiency.

</details>


### [61] [SLAG: Scalable Language-Augmented Gaussian Splatting](https://arxiv.org/abs/2505.08124)
*Laszlo Szilagyi,Francis Engelmann,Jeannette Bohg*

Main category: cs.CV

TL;DR: SLAG是一个多GPU框架，用于语言增强的高斯泼溅，显著提升大场景嵌入的速度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决在计算资源有限的机器人上快速编码大场景的需求，同时满足数据密集型任务的要求。

Method: 集成2D视觉语言模型特征到3D场景中，通过归一化加权平均计算语言嵌入，无需损失函数，并引入向量数据库存储和检索嵌入。

Result: 在16-GPU设置下，SLAG比OpenGaussian快18倍，同时在ScanNet和LERF数据集上保持嵌入质量。

Conclusion: SLAG为大规模机器人应用提供了一种高效且可扩展的语言增强场景表示解决方案。

Abstract: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.

</details>


### [62] [Asynchronous Multi-Object Tracking with an Event Camera](https://arxiv.org/abs/2505.08126)
*Angus Apps,Ziwei Wang,Vladimir Perejogin,Timothy Molloy,Robert Mahony*

Main category: cs.CV

TL;DR: AEMOT算法通过异步处理事件相机数据，检测和跟踪多目标，性能优于其他事件基算法37%。


<details>
  <summary>Details</summary>
Motivation: 事件相机在动态环境中具有低延迟、高时间分辨率和动态范围优势，适合多目标跟踪。

Method: AEMOT通过识别光流区域检测特征，利用AEB跟踪器构建目标强度块，并通过学习验证阶段筛选目标。

Result: 在Bee Swarm数据集上，AEMOT的精度和召回率比其他算法高37%。

Conclusion: AEMOT在动态多目标跟踪中表现优异，代码和数据集将开源。

Abstract: Events cameras are ideal sensors for enabling robots to detect and track
objects in highly dynamic environments due to their low latency output, high
temporal resolution, and high dynamic range. In this paper, we present the
Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and
tracking multiple objects by processing individual raw events asynchronously.
AEMOT detects salient event blob features by identifying regions of consistent
optical flow using a novel Field of Active Flow Directions built from the
Surface of Active Events. Detected features are tracked as candidate objects
using the recently proposed Asynchronous Event Blob (AEB) tracker in order to
construct small intensity patches of each candidate object. A novel learnt
validation stage promotes or discards candidate objects based on classification
of their intensity patches, with promoted objects having their position,
velocity, size, and orientation estimated at their event rate. We evaluate
AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with
precision and recall performance exceeding that of alternative event-based
detection and tracking algorithms by over 37%. Source code and the labelled
event Bee Swarm Dataset will be open sourced

</details>


### [63] [MoKD: Multi-Task Optimization for Knowledge Distillation](https://arxiv.org/abs/2505.08170)
*Zeeshan Hayder,Ali Cheraghian,Lars Petersson,Mehrtash Harandi*

Main category: cs.CV

TL;DR: MoKD通过多任务优化解决知识蒸馏中的梯度冲突和主导问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决知识蒸馏中任务目标与教师指导的平衡问题，以及师生模型知识表示的差异。

Method: 提出多任务优化框架MoKD，处理梯度冲突和主导，并引入子空间学习改进知识传递。

Result: 在ImageNet-1K和COCO数据集上表现优异，达到SOTA性能。

Conclusion: MoKD在知识蒸馏中高效且性能优越，优于从头训练的模型。

Abstract: Compact models can be effectively trained through Knowledge Distillation
(KD), a technique that transfers knowledge from larger, high-performing teacher
models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing
learning from the teacher's guidance and the task objective, and 2) handling
the disparity in knowledge representation between teacher and student models.
To address these, we propose Multi-Task Optimization for Knowledge Distillation
(MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where
task-specific and distillation gradients are misaligned, and b) Gradient
Dominance, where one objective's gradient dominates, causing imbalance. MoKD
reformulates KD as a multi-objective optimization problem, enabling better
balance between objectives. Additionally, it introduces a subspace learning
framework to project feature representations into a high-dimensional space,
improving knowledge transfer. Our MoKD is demonstrated to outperform existing
methods through extensive experiments on image classification using the
ImageNet-1K dataset and object detection using the COCO dataset, achieving
state-of-the-art performance with greater efficiency. To the best of our
knowledge, MoKD models also achieve state-of-the-art performance compared to
models trained from scratch.

</details>


### [64] [Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification](https://arxiv.org/abs/2505.08173)
*Xiaoshuo Yan,Zhaochuan Li,Lei Meng,Zhuang Qi,Wei Wu,Zixuan Li,Xiangxu Meng*

Main category: cs.CV

TL;DR: 论文提出TSCNet，一种两阶段因果建模方法，通过多尺度因果干预解决ViT在长尾分类中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果模型在ViT上表现不佳，因其全局特征表示难以建模细粒度特征与预测的关联，导致尾部分类困难。

Method: TSCNet分两阶段：HCRL阶段通过多尺度干预增强细粒度因果表示；CLBC阶段通过反事实平衡数据分布优化决策边界。

Result: 在多个长尾基准测试中，TSCNet显著优于现有方法，有效消除数据不平衡带来的偏差。

Conclusion: TSCNet通过两阶段因果建模，成功解决了ViT在长尾分类中的性能瓶颈。

Abstract: Causal inference has emerged as a promising approach to mitigate long-tail
classification by handling the biases introduced by class imbalance. However,
along with the change of advanced backbone models from Convolutional Neural
Networks (CNNs) to Visual Transformers (ViT), existing causal models may not
achieve an expected performance gain. This paper investigates the influence of
existing causal models on CNNs and ViT variants, highlighting that ViT's global
feature representation makes it hard for causal methods to model associations
between fine-grained features and predictions, which leads to difficulties in
classifying tail classes with similar visual appearance. To address these
issues, this paper proposes TSCNet, a two-stage causal modeling method to
discover fine-grained causal associations through multi-scale causal
interventions. Specifically, in the hierarchical causal representation learning
stage (HCRL), it decouples the background and objects, applying backdoor
interventions at both the patch and feature level to prevent model from using
class-irrelevant areas to infer labels which enhances fine-grained causal
representation. In the counterfactual logits bias calibration stage (CLBC), it
refines the optimization of model's decision boundary by adaptive constructing
counterfactual balanced data distribution to remove the spurious associations
in the logits caused by data distribution. Extensive experiments conducted on
various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate
multiple biases introduced by data imbalance, which outperforms existing
methods.

</details>


### [65] [Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images](https://arxiv.org/abs/2505.08178)
*Ziteng Liu,Dongdong He,Chenghong Zhang,Wenpeng Gao,Yili Fu*

Main category: cs.CV

TL;DR: 论文提出了一种深度引导的遮挡感知视差细化网络（DGORNet），通过利用单目深度信息解决腹腔镜图像中的遮挡和标记数据稀缺问题，并引入位置嵌入模块和光流差异损失，显著提升了视差估计的精度。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜图像中的遮挡和标记数据稀缺是视差估计的主要挑战，需要一种能够利用无遮挡深度信息并增强时空一致性的方法。

Method: 提出DGORNet网络，结合单目深度信息、位置嵌入模块（PE）和光流差异损失（OFDLoss），优化视差图的时空一致性。

Result: 在SCARED数据集上，DGORNet在端点误差（EPE）和均方根误差（RMSE）上优于现有方法，尤其在遮挡和无纹理区域表现突出。

Conclusion: DGORNet通过结合深度引导和时空一致性优化，为腹腔镜手术中的视差估计提供了有效解决方案。

Abstract: Occlusion and the scarcity of labeled surgical data are significant
challenges in disparity estimation for stereo laparoscopic images. To address
these issues, this study proposes a Depth Guided Occlusion-Aware Disparity
Refinement Network (DGORNet), which refines disparity maps by leveraging
monocular depth information unaffected by occlusion. A Position Embedding (PE)
module is introduced to provide explicit spatial context, enhancing the
network's ability to localize and refine features. Furthermore, we introduce an
Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal
continuity across video frames to improve robustness in dynamic surgical
scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms
state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean
Squared Error (RMSE), particularly in occlusion and texture-less regions.
Ablation studies confirm the contributions of the Position Embedding and
Optical Flow Difference Loss, highlighting their roles in improving spatial and
temporal consistency. These results underscore DGORNet's effectiveness in
enhancing disparity estimation for laparoscopic surgery, offering a practical
solution to challenges in disparity estimation and data limitations.

</details>


### [66] [Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models](https://arxiv.org/abs/2505.08190)
*Lhuqita Fazry,Valentino Vito*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的单图像雨滴去除方法，利用扩散模型的先进图像修复技术。


<details>
  <summary>Details</summary>
Motivation: 单图像雨滴去除任务具有挑战性，现有方法主要依赖GAN进行背景修复，而扩散模型在图像修复领域的最新进展为任务提供了新思路。

Method: 采用基于扩散模型的图像修复技术，通过检测雨滴区域并利用扩散模型进行背景修复。

Result: 该方法利用扩散模型的优势，实现了更高质量的雨滴去除效果。

Conclusion: 扩散模型在单图像雨滴去除任务中表现出色，为图像处理领域提供了新的解决方案。

Abstract: Raindrop removal is a challenging task in image processing. Removing
raindrops while relying solely on a single image further increases the
difficulty of the task. Common approaches include the detection of raindrop
regions in the image, followed by performing a background restoration process
conditioned on those regions. While various methods can be applied for the
detection step, the most common architecture used for background restoration is
the Generative Adversarial Network (GAN). Recent advances in the use of
diffusion models have led to state-of-the-art image inpainting techniques. In
this paper, we introduce a novel technique for raindrop removal from a single
image using diffusion-based image inpainting.

</details>


### [67] [ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction](https://arxiv.org/abs/2505.08196)
*He Huang,Qi Yang,Mufan Liu,Yiling Xu,Zhu Li*

Main category: cs.CV

TL;DR: ADC-GS提出了一种基于锚点的动态场景重建方法，通过分层优化和速率失真优化，显著提升了渲染速度和存储效率。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅方法忽略了相邻高斯基元的冗余性，导致性能不佳。

Method: ADC-GS采用锚点驱动结构，结合分层粗到细的管道和速率失真优化。

Result: 实验显示ADC-GS渲染速度提升300%-800%，存储效率达到最优。

Conclusion: ADC-GS在动态场景重建中实现了高效且高质量的表示。

Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.

</details>


### [68] [Visual Watermarking in the Era of Diffusion Models: Advances and Challenges](https://arxiv.org/abs/2505.08197)
*Junxian Duan,Jiyang Guang,Wenkui Yang,Ran He*

Main category: cs.CV

TL;DR: 论文探讨了在生成式AI时代，利用扩散模型嵌入水印以保护数字内容版权的方法，分析了其优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的进步，视觉内容容易被滥用，传统水印检测方法难以应对复杂篡改，需要更强大的保护机制。

Method: 通过扩散模型学习特征，嵌入不可察觉且鲁棒的水印，提升检测准确性。

Result: 扩散模型能有效增强水印的鲁棒性，对抗伪造威胁。

Conclusion: 开发创新的水印技术对保护数字内容所有权至关重要，尤其是在生成式AI时代。

Abstract: As generative artificial intelligence technologies like Stable Diffusion
advance, visual content becomes more vulnerable to misuse, raising concerns
about copyright infringement. Visual watermarks serve as effective protection
mechanisms, asserting ownership and deterring unauthorized use. Traditional
deepfake detection methods often rely on passive techniques that struggle with
sophisticated manipulations. In contrast, diffusion models enhance detection
accuracy by allowing for the effective learning of features, enabling the
embedding of imperceptible and robust watermarks. We analyze the strengths and
challenges of watermark techniques related to diffusion models, focusing on
their robustness and application in watermark generation. By exploring the
integration of advanced diffusion models and watermarking security, we aim to
advance the discourse on preserving watermark robustness against evolving
forgery threats. It emphasizes the critical importance of developing innovative
solutions to protect digital content and ensure the preservation of ownership
rights in the era of generative AI.

</details>


### [69] [Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix](https://arxiv.org/abs/2505.08228)
*Unai Gurbindo,Axel Brando,Jaume Abella,Caroline König*

Main category: cs.CV

TL;DR: 研究提出了一种基于扩散模型Instruct Pix2Pix的数据增强方法，用于提升目标检测模型在恶劣天气条件下的鲁棒性，并在仿真和真实数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶技术中目标检测系统在恶劣天气条件下的鲁棒性，解决现有模型在复杂环境中的性能下降问题。

Method: 利用扩散模型Instruct Pix2Pix生成具有天气增强效果的逼真数据集，并在CARLA仿真器和真实数据集BDD100K、ACDC上进行实验验证。

Result: 实验表明，该方法能显著提升目标检测模型（如Faster R-CNN和YOLOv10）在恶劣天气条件下的性能。

Conclusion: 研究为提升自动驾驶感知系统在复杂环境中的可靠性奠定了基础，并提供了未来技术发展的方向。

Abstract: Enhancing the robustness of object detection systems under adverse weather
conditions is crucial for the advancement of autonomous driving technology.
This study presents a novel approach leveraging the diffusion model Instruct
Pix2Pix to develop prompting methodologies that generate realistic datasets
with weather-based augmentations aiming to mitigate the impact of adverse
weather on the perception capabilities of state-of-the-art object detection
models, including Faster R-CNN and YOLOv10. Experiments were conducted in two
environments, in the CARLA simulator where an initial evaluation of the
proposed data augmentation was provided, and then on the real-world image data
sets BDD100K and ACDC demonstrating the effectiveness of the approach in real
environments.
  The key contributions of this work are twofold: (1) identifying and
quantifying the performance gap in object detection models under challenging
weather conditions, and (2) demonstrating how tailored data augmentation
strategies can significantly enhance the robustness of these models. This
research establishes a solid foundation for improving the reliability of
perception systems in demanding environmental scenarios, and provides a pathway
for future advancements in autonomous driving.

</details>


### [70] [HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective](https://arxiv.org/abs/2505.08231)
*Yu Zhang,Fengyuan Liu,Juan Lyu,Yi Wei,Changdong Yu*

Main category: cs.CV

TL;DR: 论文提出了Navigation12数据集和HMPNet模型，用于船舶视角下的目标检测，解决了海事数据稀缺问题，并在精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 海事领域缺乏专门的数据集，限制了先进视觉感知技术的应用，因此需要构建海事专用数据集和高效检测模型。

Method: 基于Navigation12数据集，设计了HMPNet模型，采用分层动态调制主干、矩阵级联多尺度颈部和聚合权重共享检测器。

Result: HMPNet在精度上比YOLOv11n提升3.3%，参数减少23%，计算效率更高。

Conclusion: Navigation12和HMPNet为海事目标检测提供了有效解决方案，显著提升了性能。

Abstract: In the realm of intelligent maritime navigation, object detection from a
shipborne perspective is paramount. Despite the criticality, the paucity of
maritime-specific data impedes the deployment of sophisticated visual
perception techniques, akin to those utilized in autonomous vehicular systems,
within the maritime context. To bridge this gap, we introduce Navigation12, a
novel dataset annotated for 12 object categories under diverse maritime
environments and weather conditions. Based upon this dataset, we propose
HMPNet, a lightweight architecture tailored for shipborne object detection.
HMPNet incorporates a hierarchical dynamic modulation backbone to bolster
feature aggregation and expression, complemented by a matrix cascading
poly-scale neck and a polymerization weight sharing detector, facilitating
efficient multi-scale feature aggregation. Empirical evaluations indicate that
HMPNet surpasses current state-of-the-art methods in terms of both accuracy and
computational efficiency, realizing a 3.3% improvement in mean Average
Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.

</details>


### [71] [G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition](https://arxiv.org/abs/2505.08233)
*Santhoshkumar Peddi,Soham Bandyopadhyay,Debasis Samanta*

Main category: cs.CV

TL;DR: G-MSGINet是一种高效的无接触指纹识别框架，通过GMSGI层联合实现细节定位和身份嵌入，无需复杂预处理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多分支架构或复杂预处理，限制了实际应用中的扩展性和泛化能力。

Method: 引入GMSGI层，结合像素级卷积、动态多尺度核生成和图关系建模，通过端到端优化逐步优化特征。

Result: 在三个基准数据集上，F1分数达0.83±0.02，Rank-1准确率97.0%-99.1%，EER低至0.5%，参数和计算量大幅减少。

Conclusion: G-MSGINet在性能和效率上显著优于现有方法，适用于实际无接触生物识别场景。

Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust
contactless fingerprint recognition that jointly performs minutiae localization
and identity embedding directly from raw input images. Existing approaches rely
on multi-branch architectures, orientation labels, or complex preprocessing
steps, which limit scalability and generalization across real-world acquisition
scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a
novel computational module that integrates grouped pixel-level involution,
dynamic multi-scale kernel generation, and graph-based relational modelling
into a single processing unit. Stacked GMSGI layers progressively refine both
local minutiae-sensitive features and global topological representations
through end-to-end optimization. The architecture eliminates explicit
orientation supervision and adapts graph connectivity directly from learned
kernel descriptors, thereby capturing meaningful structural relationships among
fingerprint regions without fixed heuristics. Extensive experiments on three
benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that
G-MSGINet consistently achieves minutiae F1-scores in the range of
$0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,
while maintaining an Equal Error Rate (EER) as low as 0.5%. These results
correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1
accuracy when compared to prior methods, using only 0.38 million parameters and
6.63 giga floating-point operations, which represents up to ten times fewer
parameters than competitive baselines. This highlights the scalability and
effectiveness of G-MSGINet in real-world contactless biometric recognition
scenarios.

</details>


### [72] [Removing Watermarks with Partial Regeneration using Semantic Information](https://arxiv.org/abs/2505.08234)
*Krti Tallam,John Kevin Cava,Caleb Geniesse,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.CV

TL;DR: 论文揭示了一种新型攻击方法SemanticRegen，能够有效擦除AI生成图像中的语义水印，同时保持图像内容完整，暴露了当前水印技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的普及，隐形水印成为版权保护的主要手段，但其对适应性攻击的鲁棒性尚未充分研究。

Method: 提出SemanticRegen，一种三阶段、无标签的攻击方法，结合视觉语言模型、零样本分割和LLM引导的扩散模型，选择性修复背景以保留前景内容。

Result: 在四种水印系统（TreeRing、StegaStamp、StableSig、DWT/DCT）上测试，SemanticRegen成功擦除TreeRing水印，并显著降低其他水印的比特准确率，同时保持高感知质量（mSSIM=0.94）。

Conclusion: 研究揭示了当前水印技术与适应性攻击之间的差距，呼吁开发更具鲁棒性的水印算法以应对内容保留的再生攻击。

Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged
as a primary line of defense for copyright and provenance. The newest
watermarking schemes embed semantic signals - content-aware patterns that are
designed to survive common image manipulations - yet their true robustness
against adaptive adversaries remains under-explored. We expose a previously
unreported vulnerability and introduce SemanticRegen, a three-stage, label-free
attack that erases state-of-the-art semantic and invisible watermarks while
leaving an image's apparent meaning intact. Our pipeline (i) uses a
vision-language model to obtain fine-grained captions, (ii) extracts foreground
masks with zero-shot segmentation, and (iii) inpaints only the background via
an LLM-guided diffusion model, thereby preserving salient objects and style
cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,
StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat
the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy
below 0.75 for the remaining schemes, all while maintaining high perceptual
quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)
to quantify fidelity within foreground regions, showing that our attack
achieves up to 12 percent higher mSSIM than prior diffusion-based attackers.
These results highlight an urgent gap between current watermark defenses and
the capabilities of adaptive, semantics-aware adversaries, underscoring the
need for watermarking algorithms that are resilient to content-preserving
regenerative attacks.

</details>


### [73] [EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation](https://arxiv.org/abs/2505.08235)
*Hanle Zheng,Xujie Han,Zegang Peng,Shangbin Zhang,Guangxun Du,Zhuo Zou,Xilin Wang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: EventDiff是一种基于事件的扩散模型框架，用于视频帧插值（VFI），通过隐空间去噪扩散过程实现高效插值，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频帧插值在复杂运动、遮挡和光照变化条件下具有挑战性，现有方法依赖显式运动建模，难以兼顾高保真重建。扩散模型提供了一种无需显式运动估计的替代方案。

Method: 提出EventDiff框架，包含Event-Frame Hybrid AutoEncoder（HAE）和轻量级Spatial-Temporal Cross Attention（STCA）模块，通过两阶段训练策略（预训练HAE后联合优化扩散模型）实现高效插值。

Result: 在Vimeo90K-Triplet和SNU-FILM等数据集上表现优异，PSNR提升最高达1.98dB，推理速度比现有扩散方法快4.24倍。

Conclusion: EventDiff通过隐空间扩散过程实现了高效且鲁棒的视频帧插值，显著优于现有方法。

Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in
computer vision, particularly under conditions involving large motion,
occlusion, and lighting variation. Recent advancements in event cameras have
opened up new opportunities for addressing these challenges. While existing
event-based VFI methods have succeeded in recovering large and complex motions
by leveraging handcrafted intermediate representations such as optical flow,
these designs often compromise high-fidelity image reconstruction under subtle
motion scenarios due to their reliance on explicit motion modeling. Meanwhile,
diffusion models provide a promising alternative for VFI by reconstructing
frames through a denoising process, eliminating the need for explicit motion
estimation or warping operations. In this work, we propose EventDiff, a unified
and efficient event-based diffusion model framework for VFI. EventDiff features
a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight
Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic
event streams with static frames. Unlike previous event-based VFI methods,
EventDiff performs interpolation directly in the latent space via a denoising
diffusion process, making it more robust across diverse and challenging VFI
scenarios. Through a two-stage training strategy that first pretrains the HAE
and then jointly optimizes it with the diffusion model, our method achieves
state-of-the-art performance across multiple synthetic and real-world event VFI
datasets. The proposed method outperforms existing state-of-the-art event-based
VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior
performance in SNU-FILM tasks with multiple difficulty levels. Compared to the
emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR
gain on Vimeo90K-Triplet and 4.24X faster inference.

</details>


### [74] [Congenital Heart Disease recognition using Deep Learning/Transformer models](https://arxiv.org/abs/2505.08242)
*Aidar Amangeldi,Vladislav Yarovenko,Angsar Taigonyrov*

Main category: cs.CV

TL;DR: 使用双模态（声音和图像）深度学习方法提高先天性心脏病（CHD）诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病是婴儿发病和死亡的主要原因，但非侵入性筛查方法常出现假阴性，需要更有效的诊断手段。

Method: 采用双模态（声音和图像）深度学习方法，结合ZCHSound和DICOM胸部X光数据集进行训练和测试。

Result: 在ZCHSound数据集上达到73.9%的准确率，在DICOM胸部X光数据集上达到80.72%的准确率。

Conclusion: 双模态深度学习方法在CHD诊断中显示出潜力，但仍需进一步提高准确性。

Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity
and mortality, yet non-invasive screening methods often yield false negatives.
Deep learning models, with their ability to automatically extract features, can
assist doctors in detecting CHD more effectively. In this work, we investigate
the use of dual-modality (sound and image) deep learning methods for CHD
diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72%
accuracy on the DICOM Chest X-ray dataset.

</details>


### [75] [Identifying Memorization of Diffusion Models through p-Laplace Analysis](https://arxiv.org/abs/2505.08246)
*Jonathan Brokman,Amit Giloni,Omer Hofman,Roman Vainshtein,Hisashi Kojima,Guy Gilboa*

Main category: cs.CV

TL;DR: 本文研究了扩散模型中估计的得分函数是否能用于计算高阶微分（p-Laplace算子），并展示了其在识别记忆训练数据中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型中的得分函数是否能用于计算高阶微分，特别是p-Laplace算子，以识别记忆的训练数据。

Method: 提出了一种基于学习得分函数的数值p-Laplace近似方法，并在高斯混合模型和图像生成模型中验证其有效性。

Result: p-Laplace算子能有效识别概率分布的关键特征，首次在图像生成模型中实现了基于该算子的记忆识别。

Conclusion: 得分函数可用于计算p-Laplace算子，为识别记忆训练数据提供了新方法。

Abstract: Diffusion models, today's leading image generative models, estimate the score
function, i.e. the gradient of the log probability of (perturbed) data samples,
without direct access to the underlying probability distribution. This work
investigates whether the estimated score function can be leveraged to compute
higher-order differentials, namely p-Laplace operators. We show here these
operators can be employed to identify memorized training data. We propose a
numerical p-Laplace approximation based on the learned score functions, showing
its effectiveness in identifying key features of the probability landscape. We
analyze the structured case of Gaussian mixture models, and demonstrate the
results carry-over to image generative models, where memorization
identification based on the p-Laplace operator is performed for the first time.

</details>


### [76] [CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets](https://arxiv.org/abs/2505.08259)
*Aidar Amangeldi,Angsar Taigonyrov,Muhammad Huzaid Jawad,Chinedu Emmanuel Mbonu*

Main category: cs.CV

TL;DR: 研究评估了卷积和Transformer架构在医学和通用图像分类任务中的权衡，通过微调策略发现Vision Transformer在性能和效率上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索在资源受限环境中，如何通过微调Vision Transformer实现高效且准确的图像分类。

Method: 以ResNet-18为基线，对四种Vision Transformer变体（Tiny、Small、Base、Large）进行微调，应用于DermatologyMNIST和TinyImageNet数据集。

Result: 微调后的Vision Transformer性能匹配或超越基线，推理速度更快，参数更少。

Conclusion: Vision Transformer在资源受限环境中具有部署潜力。

Abstract: This study evaluates the trade-offs between convolutional and
transformer-based architectures on both medical and general-purpose image
classification benchmarks. We use ResNet-18 as our baseline and introduce a
fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,
Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce
inference latency and model complexity with acceptable accuracy degradation.
Through systematic hyperparameter variations, we demonstrate that appropriately
fine-tuned Vision Transformers can match or exceed the baseline's performance,
achieve faster inference, and operate with fewer parameters, highlighting their
viability for deployment in resource-constrained environments.

</details>


### [77] [Few-shot Novel Category Discovery](https://arxiv.org/abs/2505.08260)
*Chunming Li,Shidong Wang,Haofeng Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的Few-Shot Novel Category Discovery (FSNCD)框架，通过结合少量标记数据和未标记数据，实现了在已知和未知类别之间的灵活切换。


<details>
  <summary>Details</summary>
Motivation: 现有的Novel Category Discovery (NCD)方法在现实场景中应用受限，而少量标记数据可以缓解这一问题。

Method: 提出了Semi-supervised Hierarchical Clustering (SHC)和Uncertainty-aware K-means Clustering (UKC)方法，结合少量支持样本和查询样本进行推理。

Result: 在五个常用数据集上的实验表明，该方法在不同任务设置和场景下均取得了领先性能。

Conclusion: FSNCD框架通过结合少量标记数据和未标记数据，显著提升了模型的适应性和性能。

Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of
transductive learning hinders its application in more real-world scenarios. In
fact, few labeled data in part of new categories can well alleviate this
burden, which coincides with the ease that people can label few of new category
data. Therefore, this paper presents a new setting in which a trained agent is
able to flexibly switch between the tasks of identifying examples of known
(labelled) classes and clustering novel (completely unlabeled) classes as the
number of query examples increases by leveraging knowledge learned from only a
few (handful) support examples. Drawing inspiration from the discovery of novel
categories using prior-based clustering algorithms, we introduce a novel
framework that further relaxes its assumptions to the real-world open set level
by unifying the concept of model adaptability in few-shot learning. We refer to
this setting as Few-Shot Novel Category Discovery (FSNCD) and propose
Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means
Clustering (UKC) to examine the model's reasoning capabilities. Extensive
experiments and detailed analysis on five commonly used datasets demonstrate
that our methods can achieve leading performance levels across different task
settings and scenarios.

</details>


### [78] [Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction](https://arxiv.org/abs/2505.08266)
*Yanbin Wei,Xuehao Wang,Zhan Zhuang,Yang Chen,Shuhao Chen,Yulong Zhang,Yu Zhang,James Kwok*

Main category: cs.CV

TL;DR: GVN和E-GVN首次将视觉感知引入MPNN，显著提升了链接预测性能，并在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: MPNN和结构特征在链接预测中很重要，但视觉感知的潜力被忽视。

Method: 提出Graph Vision Network (GVN)及其高效变体E-GVN，赋予MPNN视觉结构感知能力。

Result: 在七个链接预测数据集上表现优异，包括大规模图，兼容现有SOTA方法并取得新SOTA。

Conclusion: GVN为链接预测开辟了新方向，展示了视觉增强的潜力。

Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs)
are cornerstones for the link prediction task. However, as a common and
intuitive mode of understanding, the potential of visual perception has been
overlooked in the MPNN community. For the first time, we equip MPNNs with
vision structural awareness by proposing an effective framework called Graph
Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive
empirical results demonstrate that with the proposed frameworks, GVN
consistently benefits from the vision enhancement across seven link prediction
datasets, including challenging large-scale graphs. Such improvements are
compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new
SOTA results, thereby underscoring a promising novel direction for link
prediction.

</details>


### [79] [IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping](https://arxiv.org/abs/2505.08273)
*Nibir Chandra Mandal,Oishee Bintey Hoque,Abhijin Adiga,Samarth Swarup,Mandy Wilson,Lu Feng,Yangfeng Ji,Miaomiao Zhang,Geoffrey Fox,Madhav Marathe*

Main category: cs.CV

TL;DR: IrrMap是一个用于灌溉方法映射的大规模数据集（110万个补丁），包含多分辨率卫星图像和辅助数据，覆盖美国西部多个州的农田。数据集支持深度学习模型训练，并附带完整的数据生成流程和分析结果。


<details>
  <summary>Details</summary>
Motivation: 提供首个大规模、多样化的灌溉方法映射数据集，支持农业和地理空间分析的研究与应用。

Method: 利用Landsat和Sentinel卫星图像，结合辅助数据（如作物类型、土地利用等），构建标准化数据集，并提供数据加载器和基准模型。

Result: 数据集覆盖168万多个农场和1410万英亩土地，分析了灌溉方法分布、空间模式和面积变化。

Conclusion: IrrMap为灌溉研究提供了丰富资源，并公开了数据集、代码和分析工具，促进进一步探索。

Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for
irrigation method mapping across regions. IrrMap consists of multi-resolution
satellite imagery from LandSat and Sentinel, along with key auxiliary data such
as crop type, land use, and vegetation indices. The dataset spans 1,687,899
farms and 14,117,330 acres across multiple western U.S. states from 2013 to
2023, providing a rich and diverse foundation for irrigation analysis and
ensuring geospatial alignment and quality control. The dataset is ML-ready,
with standardized 224x224 GeoTIFF patches, the multiple input modalities,
carefully chosen train-test-split data, and accompanying dataloaders for
seamless deep learning model training andbenchmarking in irrigation mapping.
The dataset is also accompanied by a complete pipeline for dataset generation,
enabling researchers to extend IrrMap to new regions for irrigation data
collection or adapt it with minimal effort for other similar applications in
agricultural and geospatial analysis. We also analyze the irrigation method
distribution across crop groups, spatial irrigation patterns (using Shannon
diversity indices), and irrigated area variations for both LandSat and
Sentinel, providing insights into regional and resolution-based differences. To
promote further exploration, we openly release IrrMap, along with the derived
datasets, benchmark models, and pipeline code, through a GitHub repository:
https://github.com/Nibir088/IrrMap and Data repository:
https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and
implementation details.

</details>


### [80] [Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion](https://arxiv.org/abs/2505.08281)
*Anle Ke,Xu Zhang,Tong Chen,Ming Lu,Chao Zhou,Jiawen Gu,Zhan Ma*

Main category: cs.CV

TL;DR: ResULIC提出了一种基于残差信号的低码率图像压缩方法，结合语义残差编码和感知保真优化器，显著提升了重建质量和编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图像压缩框架在重建保真度和编码效率上表现不佳，ResULIC旨在解决这一问题。

Method: 引入语义残差编码（SRC）捕捉语义差异，并结合压缩感知扩散模型（CDM）优化比特率与扩散步长的对齐。

Result: 实验表明，ResULIC在LPIPS和FID指标上分别节省了80.7%和66.3%的BD-rate，优于现有方法。

Conclusion: ResULIC通过残差信号和扩散模型的协同优化，显著提升了低码率图像压缩的性能。

Abstract: Existing multimodal large model-based image compression frameworks often rely
on a fragmented integration of semantic retrieval, latent compression, and
generative models, resulting in suboptimal performance in both reconstruction
fidelity and coding efficiency. To address these challenges, we propose a
residual-guided ultra lowrate image compression named ResULIC, which
incorporates residual signals into both semantic retrieval and the
diffusion-based generation process. Specifically, we introduce Semantic
Residual Coding (SRC) to capture the semantic disparity between the original
image and its compressed latent representation. A perceptual fidelity optimizer
is further applied for superior reconstruction quality. Additionally, we
present the Compression-aware Diffusion Model (CDM), which establishes an
optimal alignment between bitrates and diffusion time steps, improving
compression-reconstruction synergy. Extensive experiments demonstrate the
effectiveness of ResULIC, achieving superior objective and subjective
performance compared to state-of-the-art diffusion-based methods with - 80.7%,
-66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at
https: //njuvision.github.io/ResULIC/.

</details>


### [81] [Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks](https://arxiv.org/abs/2505.08284)
*Honna Shinichi,Akira Matsui*

Main category: cs.CV

TL;DR: 论文通过机器学习定量分析日本浮世绘的创造力，发现整体创造力随文化成熟下降，但风格创造力保持高水平。


<details>
  <summary>Details</summary>
Motivation: 传统艺术研究依赖主观判断，机器学习为东方绘画（如浮世绘）的定量分析提供了新方法。

Method: 使用11,000张高分辨率浮世绘图像，基于网络计算创造力，分析作品和艺术家的创造力。

Result: 浮世绘整体创造力随文化成熟下降，但风格创造力保持高水平并更加细分。

Conclusion: 研究为浮世绘和东方艺术分析提供了新视角，揭示了其在文化历史中的演变和重要性。

Abstract: Artwork research has long relied on human sensibility and subjective
judgment, but recent developments in machine learning have enabled the
quantitative assessment of features that humans could not discover. In Western
paintings, comprehensive analyses have been conducted from various perspectives
in conjunction with large databases, but such extensive analysis has not been
sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a
traditional Japanese art form, as a case study of Eastern paintings, and
conduct a quantitative analysis of creativity in works of art using 11,000
high-resolution images. This involves using the concept of calculating
creativity from networks to analyze both the creativity of the artwork and that
of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that
the creativity of its appearance has declined with the maturation of culture,
but in terms of style, it has become more segmented with the maturation of
culture and has maintained a high level of creativity. This not only provides
new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved
within the ongoing cultural history, playing a culturally significant role in
the analysis of Eastern art.

</details>


### [82] [FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units](https://arxiv.org/abs/2505.08294)
*Jian Wang,Baoyuan Wu,Li Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为FauForensics的新框架，利用生物不变的面部动作单元（FAUs）作为伪造抗性表征，结合细粒度帧级视听相似性计算，显著提升了多模态深度伪造检测的性能和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致逼真的视听深度伪造威胁增加，现有方法难以有效处理多模态伪造，尤其是在跨数据集泛化方面表现不佳。

Method: 引入生物不变的面部动作单元（FAUs）作为伪造抗性表征，设计了一个专用的融合模块，通过可学习的跨模态查询动态对齐时空唇-音频关系。

Result: 在FakeAVCeleb和LAV-DF数据集上实现了最先进的性能，跨数据集泛化能力平均提升4.83%。

Conclusion: FauForensics框架通过生物不变特征和动态多模态对齐，显著提升了深度伪造检测的鲁棒性和泛化能力。

Abstract: The rapid evolution of generative AI has increased the threat of realistic
audio-visual deepfakes, demanding robust detection methods. Existing solutions
primarily address unimodal (audio or visual) forgeries but struggle with
multimodal manipulations due to inadequate handling of heterogeneous modality
features and poor generalization across datasets. To this end, we propose a
novel framework called FauForensics by introducing biologically invariant
facial action units (FAUs), which is a quantitative descriptor of facial muscle
activity linked to emotion physiology. It serves as forgery-resistant
representations that reduce domain dependency while capturing subtle dynamics
often disrupted in synthetic content. Besides, instead of comparing entire
video clips as in prior works, our method computes fine-grained frame-wise
audiovisual similarities via a dedicated fusion module augmented with learnable
cross-modal queries. It dynamically aligns temporal-spatial lip-audio
relationships while mitigating multi-modal feature heterogeneity issues.
Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance
and superior cross-dataset generalizability with up to an average of 4.83\%
than existing methods.

</details>


### [83] [Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing](https://arxiv.org/abs/2505.08302)
*Oishee Bintey Hoque,Nibir Chandra Mandal,Abhijin Adiga,Samarth Swarup,Sayjro Kossi Nouwakpo,Amanda Wilson,Madhav Marathe*

Main category: cs.CV

TL;DR: KIIM是一种基于Swin-Transformer的新方法，通过多模态信息融合和迁移学习，显著提升了灌溉分类的准确性，减少了对大量训练数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基于光谱特征的卫星图像模型在复杂农业景观和有限训练数据下效果不佳，亟需更高效的灌溉分类方法。

Method: KIIM结合了作物灌溉概率编码、空间注意力、双向跨模态注意力及加权集成预测，并采用两阶段迁移学习。

Result: 在五个美国州的实验中，KIIM比基线模型提升了22.9%的IoU，对滴灌分类提升了71.4%，且仅需40%训练数据即可达到基线性能。

Conclusion: KIIM显著提高了灌溉分类的准确性和效率，为大规模自动化灌溉制图提供了可行且经济的解决方案。

Abstract: Accurate mapping of irrigation methods is crucial for sustainable
agricultural practices and food systems. However, existing models that rely
solely on spectral features from satellite imagery are ineffective due to the
complexity of agricultural landscapes and limited training data, making this a
challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a
novel Swin-Transformer based approach that uses (i) a specialized projection
matrix to encode crop to irrigation probability, (ii) a spatial attention map
to identify agricultural lands from non-agricultural lands, (iii)
bi-directional cross-attention to focus complementary information from
different modalities, and (iv) a weighted ensemble for combining predictions
from images and crop information. Our experimentation on five states in the US
shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)
improvement for hard-to-classify drip irrigation. In addition, we propose a
two-phase transfer learning approach to enhance cross-state irrigation mapping,
achieving a 51% IoU boost in a state with limited labeled data. The ability to
achieve baseline performance with only 40% of the training data highlights its
efficiency, reducing the dependency on extensive manual labeling efforts and
making large-scale, automated irrigation mapping more feasible and
cost-effective.

</details>


### [84] [An incremental algorithm for non-convex AI-enhanced medical image processing](https://arxiv.org/abs/2505.08324)
*Elena Morotti*

Main category: cs.CV

TL;DR: 本文提出了一种名为incDG的混合框架，结合深度学习和增量模型优化，用于高效解决非凸正则化逆问题，特别是在医学成像中。


<details>
  <summary>Details</summary>
Motivation: 非凸正则化逆问题因其复杂的优化空间和多个局部极小值而具有挑战性，但其能提供高质量的任务导向解，尤其在医学成像中。

Method: incDG框架基于Deep Guess策略，利用深度神经网络生成初始解，并通过正则化增量迭代优化非凸变分解。

Result: 在TpV正则化任务中，incDG在医学图像去模糊和断层重建中表现优于传统迭代解法和深度学习方法，且无需真实数据训练。

Conclusion: incDG是一种高效且稳健的工具，适用于解决成像及其他领域的非凸逆问题。

Abstract: Solving non-convex regularized inverse problems is challenging due to their
complex optimization landscapes and multiple local minima. However, these
models remain widely studied as they often yield high-quality, task-oriented
solutions, particularly in medical imaging, where the goal is to enhance
clinically relevant features rather than merely minimizing global error. We
propose incDG, a hybrid framework that integrates deep learning with
incremental model-based optimization to efficiently approximate the
$\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess
strategy, incDG exploits a deep neural network to generate effective
initializations for a non-convex variational solver, which refines the
reconstruction through regularized incremental iterations. This design combines
the efficiency of Artificial Intelligence (AI) tools with the theoretical
guarantees of model-based optimization, ensuring robustness and stability. We
validate incDG on TpV-regularized optimization tasks, demonstrating its
effectiveness in medical image deblurring and tomographic reconstruction across
diverse datasets, including synthetic images, brain CT slices, and
chest-abdomen scans. Results show that incDG outperforms both conventional
iterative solvers and deep learning-based methods, achieving superior accuracy
and stability. Moreover, we confirm that training incDG without ground truth
does not significantly degrade performance, making it a practical and powerful
tool for solving non-convex inverse problems in imaging and beyond.

</details>


### [85] [A computer vision-based model for occupancy detection using low-resolution thermal images](https://arxiv.org/abs/2505.08336)
*Xue Cui,Vincent Gbouna Zakka,Minhyun Lee*

Main category: cs.CV

TL;DR: 该研究利用低分辨率热成像和计算机视觉技术开发了一种占用检测模型，解决了隐私问题并减少了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统HVAC系统基于固定时间表运行，未考虑占用情况，而基于RGB图像的占用检测方法存在隐私问题。

Method: 采用低分辨率热成像和YOLOv5模型进行迁移学习，开发占用检测模型。

Result: 模型性能优异，精确度、召回率和mAP50值接近1.000。

Conclusion: 该模型不仅解决了隐私问题，还降低了计算资源需求，为HVAC系统的智能控制提供了新方案。

Abstract: Occupancy plays an essential role in influencing the energy consumption and
operation of heating, ventilation, and air conditioning (HVAC) systems.
Traditional HVAC typically operate on fixed schedules without considering
occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in
regulating HVAC operations. RGB images combined with computer vision (CV)
techniques are widely used for occupancy detection, however, the detailed
facial and body features they capture raise significant privacy concerns.
Low-resolution thermal images offer a non-invasive solution that mitigates
privacy issues. The study developed an occupancy detection model utilizing
low-resolution thermal images and CV techniques, where transfer learning was
applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The
developed model ultimately achieved satisfactory performance, with precision,
recall, mAP50, and mAP50 values approaching 1.000. The contributions of this
model lie not only in mitigating privacy concerns but also in reducing
computing resource demands.

</details>


### [86] [FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning](https://arxiv.org/abs/2505.08349)
*Ruixiao Shi,Fu Feng,Yucheng Xie,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: 论文提出了一种频率感知框架FAD，通过频域表示和分频带适应提升跨域小样本学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习中，空间域方法忽略了频域变化对泛化的重要性，而频域信息（如低频和高频）能捕捉互补的语义信息。

Method: FAD框架通过频率分流适配器将特征转换到频域，分频带（低、中、高频）并分别用卷积分支适应，实现针对性调整。

Result: 在Meta-Dataset基准测试中，FAD在可见和未见域上均优于现有方法。

Conclusion: 频域表示和分频带适应能显著提升跨域小样本学习的泛化性能。

Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from
limited labeled samples under significant distribution shifts. While recent
methods enhance adaptability through lightweight task-specific modules, they
operate solely in the spatial domain and overlook frequency-specific variations
that are often critical for robust transfer. We observe that spatially similar
images across domains can differ substantially in their spectral
representations, with low and high frequencies capturing complementary semantic
information at coarse and fine levels. This indicates that uniform spatial
adaptation may overlook these spectral distinctions, thus constraining
generalization. To address this, we introduce Frequency Adaptation and
Diversion (FAD), a frequency-aware framework that explicitly models and
modulates spectral components. At its core is the Frequency Diversion Adapter,
which transforms intermediate features into the frequency domain using the
discrete Fourier transform (DFT), partitions them into low, mid, and
high-frequency bands via radial masks, and reconstructs each band using inverse
DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional
branch with a kernel size tailored to its spectral scale, enabling targeted and
disentangled adaptation across frequencies. Extensive experiments on the
Meta-Dataset benchmark demonstrate that FAD consistently outperforms
state-of-the-art methods on both seen and unseen domains, validating the
utility of frequency-domain representations and band-wise adaptation for
improving generalization in CD-FSL.

</details>


### [87] [STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives](https://arxiv.org/abs/2505.08350)
*Bo Wang,Haoyang Huang,Zhiyin Lu,Fengyuan Liu,Guoqing Ma,Jianlong Yuan,Yuan Zhang,Nan Duan*

Main category: cs.CV

TL;DR: StoryAnchors是一个统一框架，用于生成高质量、多场景且时间一致的故事帧。它通过双向故事生成器和特定条件确保时间一致性、角色连续性和场景过渡流畅。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成多场景故事帧时时间一致性和叙事丰富性不足的问题。

Method: 采用双向故事生成器整合过去和未来上下文，引入Multi-Event Story Frame Labeling和Progressive Story Frame Training提升生成质量。

Result: 在一致性、叙事连贯性和场景多样性方面优于现有开源模型，与GPT-4o在叙事一致性和故事丰富性上表现相当。

Conclusion: StoryAnchors为故事驱动的帧生成提供了可扩展、灵活且高度可编辑的基础，推动了该领域的发展。

Abstract: This paper introduces StoryAnchors, a unified framework for generating
high-quality, multi-scene story frames with strong temporal consistency. The
framework employs a bidirectional story generator that integrates both past and
future contexts to ensure temporal consistency, character continuity, and
smooth scene transitions throughout the narrative. Specific conditions are
introduced to distinguish story frame generation from standard video synthesis,
facilitating greater scene diversity and enhancing narrative richness. To
further improve generation quality, StoryAnchors integrates Multi-Event Story
Frame Labeling and Progressive Story Frame Training, enabling the model to
capture both overarching narrative flow and event-level dynamics. This approach
supports the creation of editable and expandable story frames, allowing for
manual modifications and the generation of longer, more complex sequences.
Extensive experiments show that StoryAnchors outperforms existing open-source
models in key areas such as consistency, narrative coherence, and scene
diversity. Its performance in narrative consistency and story richness is also
on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of
story-driven frame generation, offering a scalable, flexible, and highly
editable foundation for future research.

</details>


### [88] [DArFace: Deformation Aware Robustness for Low Quality Face Recognition](https://arxiv.org/abs/2505.08423)
*Sadaf Gulshad,Abdullah Aldahlawi Thakaa*

Main category: cs.CV

TL;DR: DArFace是一种新的面部识别框架，通过模拟真实低质量图像中的全局和局部变形，提升了在低质量图像中的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部识别系统在低质量图像中性能下降，主要原因是忽略了局部非刚性变形。

Method: DArFace通过对抗性训练模拟全局和局部变形，并使用对比目标保持身份一致性。

Result: 在TinyFace、IJB-B和IJB-C等低质量基准测试中，DArFace表现优于现有方法。

Conclusion: DArFace通过建模局部变形，显著提升了面部识别系统在低质量图像中的鲁棒性。

Abstract: Facial recognition systems have achieved remarkable success by leveraging
deep neural networks, advanced loss functions, and large-scale datasets.
However, their performance often deteriorates in real-world scenarios involving
low-quality facial images. Such degradations, common in surveillance footage or
standoff imaging include low resolution, motion blur, and various distortions,
resulting in a substantial domain gap from the high-quality data typically used
during training. While existing approaches attempt to address robustness by
modifying network architectures or modeling global spatial transformations,
they frequently overlook local, non-rigid deformations that are inherently
present in real-world settings. In this work, we introduce DArFace, a
Deformation-Aware robust Face recognition framework that enhances robustness to
such degradations without requiring paired high- and low-quality training
samples. Our method adversarially integrates both global transformations (e.g.,
rotation, translation) and local elastic deformations during training to
simulate realistic low-quality conditions. Moreover, we introduce a contrastive
objective to enforce identity consistency across different deformed views.
Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and
IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with
significant gains attributed to the inclusion of local deformation modeling.

</details>


### [89] [DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation](https://arxiv.org/abs/2505.08426)
*Franko Šikić,Donik Vršnak,Sven Lončarić*

Main category: cs.CV

TL;DR: DHECA-SuperGaze是一种基于深度学习的方法，通过超分辨率和双头眼交叉注意力模块改进视线预测，显著降低了角度误差。


<details>
  <summary>Details</summary>
Motivation: 解决无约束环境中视线估计的挑战，包括低分辨率图像和现有方法对头眼交互建模不足的问题。

Method: 提出双分支卷积骨干网络处理眼部和多尺度超分辨率头部图像，并引入双头眼交叉注意力模块进行双向特征优化。

Result: 在Gaze360和GFIE数据集上，静态和动态配置下的角度误差显著降低，跨数据集测试也验证了方法的鲁棒性。

Conclusion: DHECA-SuperGaze在视线估计任务中表现出优越性能，并解决了数据集标注错误问题。

Abstract: Unconstrained gaze estimation is the process of determining where a subject
is directing their visual attention in uncontrolled environments. Gaze
estimation systems are important for a myriad of tasks such as driver
distraction monitoring, exam proctoring, accessibility features in modern
software, etc. However, these systems face challenges in real-world scenarios,
partially due to the low resolution of in-the-wild images and partially due to
insufficient modeling of head-eye interactions in current state-of-the-art
(SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based
method that advances gaze prediction through super-resolution (SR) and a dual
head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone
processes eye and multiscale SR head images, while the proposed DHECA module
enables bidirectional feature refinement between the extracted visual features
through cross-attention mechanisms. Furthermore, we identified critical
annotation errors in one of the most diverse and widely used gaze estimation
datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on
Gaze360 and GFIE datasets demonstrates superior within-dataset performance of
the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and
2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and
3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods.
Cross-dataset testing shows improvements in AE of more than 1.53{\deg}
(Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings,
validating the robust generalization properties of our approach.

</details>


### [90] [Visual Image Reconstruction from Brain Activity via Latent Representation](https://arxiv.org/abs/2505.08429)
*Yukiyasu Kamitani,Misato Tanaka,Ken Shirakawa*

Main category: cs.CV

TL;DR: 本文回顾了视觉图像重建领域的进展，从早期分类方法到复杂的生成模型，强调了潜在表示和模块化架构的作用，同时指出零样本泛化和主观感知建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何从大脑活动中解码视觉内容，以深入了解神经编码并推动临床应用。

Method: 结合深度神经网络和生成模型，利用分层潜在表示和模块化架构实现图像重建。

Result: 实现了更详细的视觉体验重建，但仍需解决零样本泛化和主观感知建模问题。

Conclusion: 视觉图像重建为神经编码研究提供了新视角，但需关注伦理问题和模型泛化能力的提升。

Abstract: Visual image reconstruction, the decoding of perceptual content from brain
activity into images, has advanced significantly with the integration of deep
neural networks (DNNs) and generative models. This review traces the field's
evolution from early classification approaches to sophisticated reconstructions
that capture detailed, subjective visual experiences, emphasizing the roles of
hierarchical latent representations, compositional strategies, and modular
architectures. Despite notable progress, challenges remain, such as achieving
true zero-shot generalization for unseen images and accurately modeling the
complex, subjective aspects of perception. We discuss the need for diverse
datasets, refined evaluation metrics aligned with human perceptual judgments,
and compositional representations that strengthen model robustness and
generalizability. Ethical issues, including privacy, consent, and potential
misuse, are underscored as critical considerations for responsible development.
Visual image reconstruction offers promising insights into neural coding and
enables new psychological measurements of visual experiences, with applications
spanning clinical diagnostics and brain-machine interfaces.

</details>


### [91] [TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection](https://arxiv.org/abs/2505.08437)
*Wenkui Yang,Zhida Zhang,Xiaoqiang Zhou,Junxian Duan,Jie Cao*

Main category: cs.CV

TL;DR: 论文介绍了TikTok-DeepFake（TT-DF）数据集，专注于人体伪造检测，并提出了TOF-Net模型，利用时空不一致性和光流分布差异进行检测。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对人体伪造的数据集和检测方法，而面部伪造已有较多研究。TT-DF旨在填补这一空白。

Method: 提出TT-DF数据集，包含多种伪造方法和压缩版本；设计TOF-Net模型，利用时空不一致性和光流分布差异检测伪造。

Result: TOF-Net在TT-DF上表现优异，优于现有面部伪造检测模型。

Conclusion: TT-DF和TOF-Net为人体伪造检测提供了新工具，填补了研究空白。

Abstract: The emergence and popularity of facial deepfake methods spur the vigorous
development of deepfake datasets and facial forgery detection, which to some
extent alleviates the security concerns about facial-related artificial
intelligence technologies. However, when it comes to human body forgery, there
has been a persistent lack of datasets and detection methods, due to the later
inception and complexity of human body generation methods. To mitigate this
issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale
diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic
frames, specifically tailored for body forgery detection. TT-DF offers a wide
variety of forgery methods, involving multiple advanced human image animation
models utilized for manipulation, two generative configurations based on the
disentanglement of identity and pose information, as well as different
compressed versions. The aim is to simulate any potential unseen forged data in
the wild as comprehensively as possible, and we also furnish a benchmark on
TT-DF. Additionally, we propose an adapted body forgery detection model,
Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal
inconsistencies and optical flow distribution differences between natural data
and forged data. Our experiments demonstrate that TOF-Net achieves favorable
performance on TT-DF, outperforming current state-of-the-art extendable facial
forgery detection models. For our TT-DF dataset, please refer to
https://github.com/HashTAG00002/TT-DF.

</details>


### [92] [A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering](https://arxiv.org/abs/2505.08438)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Haodong Chen,Ying Zhou,Vera Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 该论文综述了事件相机在3D重建中的应用，分类并总结了现有方法、数据集及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其异步捕捉像素亮度变化的能力，在高动态范围等极端环境下具有3D重建潜力，但缺乏全面综述。

Method: 将现有工作按输入模态（立体、单目、多模态）和重建方法（几何、深度学习、神经渲染）分类，并整理相关数据集。

Result: 总结了事件相机3D重建的现状，指出了数据可用性、评估、动态场景处理等方面的局限性。

Conclusion: 该综述为事件驱动3D重建提供了全面参考和未来研究方向。

Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

</details>


### [93] [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455)
*Pritam Sarkar,Ali Etemad*

Main category: cs.CV

TL;DR: 论文提出了一个名为VCRBench的新基准，用于评估大型视频语言模型（LVLMs）在视频因果推理中的能力，并提出了识别-推理分解（RRD）方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估视频因果推理的基准，导致LVLMs在这方面的能力未被充分探索。

Method: 通过创建VCRBench基准，测试LVLMs对事件序列的识别和推理能力，并提出RRD方法将任务分解为视频识别和因果推理两部分。

Result: 实验表明LVLMs在长程因果推理上表现不佳，但RRD方法能显著提升性能（最高25.2%）。

Conclusion: LVLMs在视频因果推理中依赖语言知识，RRD方法为解决这一问题提供了有效途径。

Abstract: Despite recent advances in video understanding, the capabilities of Large
Video Language Models (LVLMs) to perform video-based causal reasoning remains
underexplored, largely due to the absence of relevant and dedicated benchmarks
for evaluating causal reasoning in visually grounded and goal-driven settings.
To fill this gap, we introduce a novel benchmark named Video-based long-form
Causal Reasoning (VCRBench). We create VCRBench using procedural videos of
simple everyday activities, where the steps are deliberately shuffled with each
clip capturing a key causal event, to test whether LVLMs can identify, reason
about, and correctly sequence the events needed to accomplish a specific goal.
Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting
linguistic shortcuts, as seen in multiple-choice or binary QA formats, while
also avoiding the challenges associated with evaluating open-ended QA. Our
evaluation of state-of-the-art LVLMs on VCRBench suggests that these models
struggle with video-based long-form causal reasoning, primarily due to their
difficulty in modeling long-range causal dependencies directly from visual
observations. As a simple step toward enabling such capabilities, we propose
Recognition-Reasoning Decomposition (RRD), a modular approach that breaks
video-based causal reasoning into two sub-tasks of video recognition and causal
reasoning. Our experiments on VCRBench show that RRD significantly boosts
accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis
reveals interesting insights, for instance, that LVLMs primarily rely on
language knowledge for complex video-based long-form causal reasoning tasks.

</details>


### [94] [A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images](https://arxiv.org/abs/2505.08517)
*Yifan Li,Alan W Pang,Jo Woon Chong*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的框架，用于通过支气管镜图像对吸入性损伤进行分级，并利用增强的StarGAN生成高质量合成图像以解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如AIS）依赖主观评估且与临床结果相关性较弱，因此需要一种更客观、准确的分级方法。

Method: 使用增强的StarGAN（结合Patch Loss和SSIM Loss）生成合成图像，并通过Swin Transformer进行分类评估。

Result: 增强的StarGAN生成的图像显著提高了分类准确率（77.78%，提升11.11%），并在FID评分中表现最佳（30.06）。

Conclusion: 增强的StarGAN在解决数据限制和提高吸入性损伤分级准确性方面具有潜力。

Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to
the limitations of traditional methods, such as Abbreviated Injury Score (AIS),
which rely on subjective assessments and show weak correlations with clinical
outcomes. This study introduces a novel deep learning-based framework for
grading inhalation injuries using bronchoscopy images with the duration of
mechanical ventilation as an objective metric. To address the scarcity of
medical imaging data, we propose enhanced StarGAN, a generative model that
integrates Patch Loss and SSIM Loss to improve synthetic images' quality and
clinical relevance. The augmented dataset generated by enhanced StarGAN
significantly improved classification performance when evaluated using the Swin
Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the
original dataset. Image quality was assessed using the Fr\'echet Inception
Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06,
outperforming baseline models. Burn surgeons confirmed the realism and clinical
relevance of the generated images, particularly the preservation of bronchial
structures and color distribution. These results highlight the potential of
enhanced StarGAN in addressing data limitations and improving classification
accuracy for inhalation injury grading.

</details>


### [95] [Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis](https://arxiv.org/abs/2505.08524)
*Pratibha Kumari,Daniel Reisenbüchler,Afshin Bozorgpour,Nadine S. Schaadt,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 提出了一种基于注意力的生成潜在重放持续学习框架（AGLR-CL），用于解决全切片图像（WSI）分类中的领域偏移问题。该方法通过高斯混合模型（GMMs）合成WSI表示和补丁分布，无需存储原始数据，同时利用注意力机制筛选关键补丁嵌入。实验表明，该方法在隐私保护的前提下，性能优于无缓冲方法，并匹配有缓冲方法的性能。


<details>
  <summary>Details</summary>
Motivation: 全切片图像分类在计算病理学中具有重要应用，但面临领域偏移（如不同器官、疾病或机构差异）的挑战。现有方法通常需要存储原始数据或性能受限。

Method: 提出AGLR-CL框架，结合高斯混合模型（GMMs）合成数据，并通过注意力机制筛选关键补丁嵌入，实现隐私保护的持续学习。

Result: 在多个公共数据集上验证了AGLR-CL的有效性，显示其在保留历史知识的同时适应新领域的能力，性能优于无缓冲方法，匹配有缓冲方法。

Conclusion: AGLR-CL为WSI分类中的领域增量持续学习提供了一种高效且隐私保护的解决方案。

Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in
computational pathology, but remains constrained by domain shifts, e.g., due to
different organs, diseases, or institution-specific variations. To address this
challenge, we propose an Attention-based Generative Latent Replay Continual
Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for
domain incremental WSI classification. Our method employs Gaussian Mixture
Models (GMMs) to synthesize WSI representations and patch count distributions,
preserving knowledge of past domains without explicitly storing original data.
A novel attention-based filtering step focuses on the most salient patch
embeddings, ensuring high-quality synthetic samples. This privacy-aware
strategy obviates the need for replay buffers and outperforms other buffer-free
counterparts while matching the performance of buffer-based solutions. We
validate AGLR-CL on clinically relevant biomarker detection and molecular
status prediction across multiple public datasets with diverse centers, organs,
and patient cohorts. Experimental results confirm its ability to retain prior
knowledge and adapt to new domains, offering an effective, privacy-preserving
avenue for domain incremental continual learning in WSI classification.

</details>


### [96] [Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation](https://arxiv.org/abs/2505.08525)
*Yiqi Chen,Ganghai Huang,Sheng Zhang,Jianglin Dai*

Main category: cs.CV

TL;DR: 论文提出了一种动态蛇形上采样操作符和边界-骨架加权损失，用于提升管状拓扑结构的分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统上采样操作符无法适应管状结构的纤细性和形态曲率，影响分割精度。

Method: 设计了基于自适应采样域的动态蛇形上采样操作符，并提出边界-骨架加权损失以优化权重分配。

Result: 实验表明，该方法显著提升了像素级分割精度和拓扑一致性。

Conclusion: 动态蛇形上采样和加权损失是提升管状结构分割的有效方法。

Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and
vasculature) is critical in various fields to guarantee dependable downstream
quantitative analysis and modeling. However, in dense prediction tasks such as
semantic segmentation and super-resolution, conventional upsampling operators
cannot accommodate the slenderness of tubular structures and the curvature of
morphology. This paper introduces a dynamic snake upsampling operators and a
boundary-skeleton weighted loss tailored for topological tubular structures.
Specifically, we design a snake upsampling operators based on an adaptive
sampling domain, which dynamically adjusts the sampling stride according to the
feature map and selects a set of subpixel sampling points along the serpentine
path, enabling more accurate subpixel-level feature recovery for tubular
structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted
loss that trades off main body and boundary weight allocation based on mask
class ratio and distance field, preserving main body overlap while enhancing
focus on target topological continuity and boundary alignment precision.
Experiments across various domain datasets and backbone networks show that this
plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted
loss boost both pixel-wise segmentation accuracy and topological consistency of
results.

</details>


### [97] [Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting](https://arxiv.org/abs/2505.08527)
*Zheang Huai,Hui Tang,Yi Li,Zhuangzhuang Chen,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于Segment Anything Model（SAM）的双特征引导（DFG）自动提示方法，用于解决源自由域适应（SFDA）分割任务中的边界框提示缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 源自由域适应（SFDA）分割任务中，直接生成的边界框提示因域差距存在缺陷，需要一种更准确的方法。

Method: 通过双阶段方法：1）特征聚合阶段初步适应目标域；2）基于目标模型特征和SAM特征的双重引导，逐步扩展边界框提示，并通过连通性分析优化伪标签。

Result: 在3D和2D数据集上的实验表明，该方法优于传统方法。

Conclusion: DFG方法通过自动搜索边界框提示，显著提升了SFDA分割任务的性能。

Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a
model trained in the source domain to perform well in the target domain with
only the source model and unlabeled target data.Inspired by the recent success
of Segment Anything Model (SAM) which exhibits the generality of segmenting
images of various modalities and in different domains given human-annotated
prompts like bounding boxes or points, we for the first time explore the
potentials of Segment Anything Model for SFDA via automatedly finding an
accurate bounding box prompt. We find that the bounding boxes directly
generated with existing SFDA approaches are defective due to the domain gap.To
tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting
approach to search for the box prompt. Specifically, the source model is first
trained in a feature aggregation phase, which not only preliminarily adapts the
source model to the target domain but also builds a feature distribution
well-prepared for box prompt search. In the second phase, based on two feature
distribution observations, we gradually expand the box prompt with the guidance
of the target model feature and the SAM feature to handle the class-wise
clustered target features and the class-wise dispersed target features,
respectively. To remove the potentially enlarged false positive regions caused
by the over-confident prediction of the target model, the refined pseudo-labels
produced by SAM are further postprocessed based on connectivity analysis.
Experiments on 3D and 2D datasets indicate that our approach yields superior
performance compared to conventional methods. Code is available at
https://github.com/zheangh/DFG.

</details>


### [98] [The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning](https://arxiv.org/abs/2505.08537)
*Mohamed Lamine Mekhalfi,Paul Chippendale,Fabio Poiesi,Samuele Bonecher,Gilberto Osler,Nicola Zancanella*

Main category: cs.CV

TL;DR: 研究探讨了计算机视觉在快速、准确、非侵入式食品质量评估中的应用，专注于工业环境中实时将覆盆子分为五个等级的新挑战。


<details>
  <summary>Details</summary>
Motivation: 解决工业环境中覆盆子实时分级的难题，提高食品质量评估的效率和准确性。

Method: 采集并标注了RaspGrade数据集，通过实例分割实验获取果实级掩码，并尝试分类五个等级。

Result: 实验显示果实级掩码准确，但某些等级因颜色相似和遮挡难以分类，而其他等级基于颜色较易区分。

Conclusion: RaspGrade数据集公开可用，为覆盆子分级研究提供了资源，但需进一步解决颜色相似和遮挡问题。

Abstract: This research investigates the application of computer vision for rapid,
accurate, and non-invasive food quality assessment, focusing on the novel
challenge of real-time raspberry grading into five distinct classes within an
industrial environment as the fruits move along a conveyor belt. To address
this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and
meticulously annotated. Instance segmentation experiments revealed that
accurate fruit-level masks can be obtained; however, the classification of
certain raspberry grades presents challenges due to color similarities and
occlusion, while others are more readily distinguishable based on color. The
acquired and annotated RaspGrade dataset is accessible on HuggingFace at:
https://huggingface.co/datasets/FBK-TeV/RaspGrade.

</details>


### [99] [DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art](https://arxiv.org/abs/2505.08552)
*Haroon Wahab,Hassan Ugail,Irfan Mehmood*

Main category: cs.CV

TL;DR: 本文提出了一种名为DFA-CON的对比学习框架，用于检测侵犯版权或伪造的AI生成艺术品，并在多种攻击类型中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在视觉内容创作中的广泛应用引发了版权侵权和伪造的严重问题，需要一种有效的方法来检测此类行为。

Method: 通过对比学习框架DFA-CON，学习原始艺术品及其伪造版本之间的判别性表示空间，涵盖多种攻击类型（如修复、风格迁移、对抗扰动和cutmix）。

Result: DFA-CON在大多数攻击类型中表现出鲁棒的检测性能，优于现有的预训练基础模型。

Conclusion: DFA-CON为解决AI生成艺术品的版权侵权和伪造问题提供了一种有效的解决方案，代码和模型将公开。

Abstract: Recent proliferation of generative AI tools for visual content
creation-particularly in the context of visual artworks-has raised serious
concerns about copyright infringement and forgery. The large-scale datasets
used to train these models often contain a mixture of copyrighted and
non-copyrighted artworks. Given the tendency of generative models to memorize
training patterns, they are susceptible to varying degrees of copyright
violation. Building on the recently proposed DeepfakeArt Challenge benchmark,
this work introduces DFA-CON, a contrastive learning framework designed to
detect copyright-infringing or forged AI-generated art. DFA-CON learns a
discriminative representation space, posing affinity among original artworks
and their forged counterparts within a contrastive learning framework. The
model is trained across multiple attack types, including inpainting, style
transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate
robust detection performance across most attack types, outperforming recent
pretrained foundation models. Code and model checkpoints will be released
publicly upon acceptance.

</details>


### [100] [Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection](https://arxiv.org/abs/2505.08561)
*Ayush K. Rai,Kyle Min,Tarun Krishna,Feiyan Hu,Alan F. Smeaton,Noel E. O'Connor*

Main category: cs.CV

TL;DR: 本文提出了一种新的轨迹感知自适应令牌采样器（TATS），用于视频建模中的掩码策略选择，并结合MAE框架和PPO联合优化，显著提升了动作识别任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有掩码视频建模（MVM）方法在掩码策略选择上存在挑战，需要更高效且通用的方法。

Method: 提出TATS建模令牌运动动态，结合MAE框架和PPO联合优化，实现高效掩码和训练。

Result: 在多个基准测试中表现优异，支持高掩码率且不影响性能，同时保持内存高效。

Conclusion: TATS是一种高效、通用且性能优越的掩码视频建模方法。

Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training
strategy for visual foundation models, whereby the model reconstructs masked
spatiotemporal tokens using information from visible tokens. However, a key
challenge in such approaches lies in selecting an appropriate masking strategy.
Previous studies have explored predefined masking techniques, including random
and tube-based masking, as well as approaches that leverage key motion priors,
optical flow and semantic cues from externally pre-trained models. In this
work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token
Sampler (TATS), which models the motion dynamics of tokens and can be
seamlessly integrated into the masked autoencoder (MAE) framework to select
motion-centric tokens in videos. Additionally, we propose a unified training
strategy that enables joint optimization of both MAE and TATS from scratch
using Proximal Policy Optimization (PPO). We show that our model allows for
aggressive masking without compromising performance on the downstream task of
action recognition while also ensuring that the pre-training remains memory
efficient. Extensive experiments of the proposed approach across four
benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,
demonstrate the effectiveness, transferability, generalization, and efficiency
of our work compared to other state-of-the-art methods.

</details>


### [101] [Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections](https://arxiv.org/abs/2505.08568)
*Xiao Ni,Carsten Kuehnel,Xiaoyi Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于热成像的交通灯系统，通过动态调整信号时长和触发听觉信号，提升行动不便和视障人士的通行便利性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB摄像头系统忽视行动不便人士需求，且在恶劣天气或低能见度下性能受限，隐私问题突出。

Method: 构建热成像数据集TD4PWMR，开发YOLO-Thermal检测器，结合特征提取和注意力机制提升检测精度。

Result: YOLO-Thermal优于现有检测器，系统有效提升无障碍交叉路口体验。

Conclusion: 热成像系统解决了RGB系统的局限性，为无障碍交通提供了可行方案。

Abstract: Rapid advances in deep learning for computer vision have driven the adoption
of RGB camera-based adaptive traffic light systems to improve traffic safety
and pedestrian comfort. However, these systems often overlook the needs of
people with mobility restrictions. Moreover, the use of RGB cameras presents
significant challenges, including limited detection performance under adverse
weather or low-visibility conditions, as well as heightened privacy concerns.
To address these issues, we propose a fully automated, thermal detector-based
traffic light system that dynamically adjusts signal durations for individuals
with walking impairments or mobility burden and triggers the auditory signal
for visually impaired individuals, thereby advancing towards barrier-free
intersection for all users. To this end, we build the thermal dataset for
people with mobility restrictions (TD4PWMR), designed to capture diverse
pedestrian scenarios, particularly focusing on individuals with mobility aids
or mobility burden under varying environmental conditions, such as different
lighting, weather, and crowded urban settings. While thermal imaging offers
advantages in terms of privacy and robustness to adverse conditions, it also
introduces inherent hurdles for object detection due to its lack of color and
fine texture details and generally lower resolution of thermal images. To
overcome these limitations, we develop YOLO-Thermal, a novel variant of the
YOLO architecture that integrates advanced feature extraction and attention
mechanisms for enhanced detection accuracy and robustness in thermal imaging.
Experiments demonstrate that the proposed thermal detector outperforms existing
detectors, while the proposed traffic light system effectively enhances
barrier-free intersection. The source codes and dataset are available at
https://github.com/leon2014dresden/YOLO-THERMAL.

</details>


### [102] [ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking](https://arxiv.org/abs/2505.08581)
*Haofeng Liu,Mingqi Gao,Xuxiao Luo,Ziyue Wang,Guanyi Qin,Junde Wu,Yueming Jin*

Main category: cs.CV

TL;DR: ReSurgSAM2是一个两阶段的手术场景分割框架，利用Segment Anything Model 2进行文本引导的目标检测和跟踪，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景分割方法效率低且跟踪时间短，难以适应复杂手术场景。

Method: 采用两阶段框架：文本引导的目标检测和跟踪，结合可信初始帧选择和多样性驱动的长期记忆机制。

Result: ReSurgSAM2在准确性和效率上显著优于现有方法，实时运行速度为61.2 FPS。

Conclusion: ReSurgSAM2为手术场景分割提供了高效、可靠的解决方案，适用于复杂手术环境。

Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is
vital for enhancing surgical quality and patient outcomes. Recently, referring
surgical segmentation is emerging, given its advantage of providing surgeons
with an interactive experience to segment the target object. However, existing
methods are limited by low efficiency and short-term tracking, hindering their
applicability in complex real-world surgical scenarios. In this paper, we
introduce ReSurgSAM2, a two-stage surgical referring segmentation framework
that leverages Segment Anything Model 2 to perform text-referred target
detection, followed by tracking with reliable initial frame identification and
diversity-driven long-term memory. For the detection stage, we propose a
cross-modal spatial-temporal Mamba to generate precise detection and
segmentation results. Based on these results, our credible initial frame
selection strategy identifies the reliable frame for the subsequent tracking.
Upon selecting the initial frame, our method transitions to the tracking stage,
where it incorporates a diversity-driven memory mechanism that maintains a
credible and diverse memory bank, ensuring consistent long-term tracking.
Extensive experiments demonstrate that ReSurgSAM2 achieves substantial
improvements in accuracy and efficiency compared to existing methods, operating
in real-time at 61.2 FPS. Our code and datasets will be available at
https://github.com/jinlab-imvr/ReSurgSAM2.

</details>


### [103] [A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior](https://arxiv.org/abs/2505.08585)
*Jorge Quesada,Chen Zhou,Prithwijit Chowdhury,Mohammad Alotaibi,Ahmad Mustafa,Yusufjon Kumamnov,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 论文通过大规模基准测试研究，探讨了地震解释中模型泛化能力的限制，并提出了领域转移策略的指导。


<details>
  <summary>Details</summary>
Motivation: 当前地震解释中机器学习模型的泛化能力缺乏系统性研究，领域转移、数据差异和评估不一致是主要障碍。

Method: 研究训练和评估了200多个模型，涵盖合成和真实数据集（FaultSeg3D、CRACKS、Thebe），系统分析了预训练、微调和联合训练策略。

Result: 揭示了当前微调实践的脆弱性、灾难性遗忘现象，并建立了实验基线以分析性能权衡。

Conclusion: 研究为地震解释中模型的部署提供了指导，指明了开发更泛化、可解释和有效模型的方向。

Abstract: Machine learning has taken a critical role in seismic interpretation
workflows, especially in fault delineation tasks. However, despite the recent
proliferation of pretrained models and synthetic datasets, the field still
lacks a systematic understanding of the generalizability limits of these models
across seismic data representing a variety of geologic, acquisition and
processing settings. Distributional shifts between different data sources,
limitations in fine-tuning strategies and labeled data accessibility, and
inconsistent evaluation protocols all represent major roadblocks in the
deployment of reliable and robust models in real-world exploration settings. In
this paper, we present the first large-scale benchmarking study explicitly
designed to provide answers and guidelines for domain shift strategies in
seismic interpretation. Our benchmark encompasses over $200$ models trained and
evaluated on three heterogeneous datasets (synthetic and real data) including
FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,
fine-tuning, and joint training strategies under varying degrees of domain
shift. Our analysis highlights the fragility of current fine-tuning practices,
the emergence of catastrophic forgetting, and the challenges of interpreting
performance in a systematic manner. We establish a robust experimental baseline
to provide insights into the tradeoffs inherent to current fault delineation
workflows, and shed light on directions for developing more generalizable,
interpretable and effective machine learning models for seismic interpretation.
The insights and analyses reported provide a set of guidelines on the
deployment of fault delineation models within seismic interpretation workflows.

</details>


### [104] [PrePrompt: Predictive prompting for class incremental learning](https://arxiv.org/abs/2505.08586)
*Libo Huang,Zhulin An,Chuanguang Yang,Boyu Diao,Fei Wang,Yan Zeng,Zhifeng Hao,Yongjun Xu*

Main category: cs.CV

TL;DR: PrePrompt提出了一种新的CIL框架，通过预测任务特定提示来避免基于相关性的限制，并通过特征翻译平衡稳定性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的CIL方法依赖相关性策略，难以用少量可训练提示拟合所有任务的特征空间。

Method: PrePrompt将CIL分解为两阶段预测框架：任务特定提示预测和标签预测，并通过特征翻译缓解历史数据缺失导致的偏差。

Result: 实验表明PrePrompt在多个基准测试中优于现有基于提示的CIL方法。

Conclusion: PrePrompt通过创新的预测框架和特征翻译机制，有效解决了CIL中的挑战。

Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a
promising direction for open-world continual learning. Existing methods
typically rely on correlation-based strategies, where an image's classification
feature is used as a query to retrieve the most related key prompts and select
the corresponding value prompts for training. However, these approaches face an
inherent limitation: fitting the entire feature space of all tasks with only a
few trainable prompts is fundamentally challenging. We propose Predictive
Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based
limitations by leveraging pre-trained models' natural classification ability to
predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a
two-stage prediction framework: task-specific prompt prediction followed by
label prediction. While theoretically appealing, this framework risks bias
toward recent classes due to missing historical data for older classifier
calibration. PrePrompt then mitigates this by incorporating feature
translation, dynamically balancing stability and plasticity. Experiments across
multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art
prompt-based CIL methods. The code will be released upon acceptance.

</details>


### [105] [MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment](https://arxiv.org/abs/2505.08589)
*Barak Pinkovich,Boaz Matalon,Ehud Rivlin,Hector Rotstein*

Main category: cs.CV

TL;DR: MESSI数据集包含2525张无人机拍摄的密集城市环境图像，支持多高度语义分割研究，并可用于深度学习训练。


<details>
  <summary>Details</summary>
Motivation: 研究深度对语义分割的影响，并覆盖无人机3D飞行捕获的视觉多样性。

Method: 使用多种神经网络模型进行语义分割，并标注图像的位置、方向和相机参数。

Result: MESSI数据集可作为评估基准，支持语义分割及其他应用（如定位、导航）。

Conclusion: MESSI数据集将公开，为无人机图像语义分割提供标准化基准。

Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)
dataset comprising 2525 images taken by a drone flying over dense urban
environments. MESSI is unique in two main features. First, it contains images
from various altitudes, allowing us to investigate the effect of depth on
semantic segmentation. Second, it includes images taken from several different
urban regions (at different altitudes). This is important since the variety
covers the visual richness captured by a drone's 3D flight, performing
horizontal and vertical maneuvers. MESSI contains images annotated with
location, orientation, and the camera's intrinsic parameters and can be used to
train a deep neural network for semantic segmentation or other applications of
interest (e.g., localization, navigation, and tracking). This paper describes
the dataset and provides annotation details. It also explains how semantic
segmentation was performed using several neural network models and shows
several relevant statistics. MESSI will be published in the public domain to
serve as an evaluation benchmark for semantic segmentation using images
captured by a drone or similar vehicle flying over a dense urban environment.

</details>


### [106] [Rejoining fragmented ancient bamboo slips with physics-driven deep learning](https://arxiv.org/abs/2505.08601)
*Jinchi Zhu,Zhou Zhao,Hailong Lei,Xiaoguang Wang,Jialiang Lu,Jing Li,Qianqian Tang,Jiachen Shen,Gui-Song Xia,Bo Du,Yongchao Xu*

Main category: cs.CV

TL;DR: WisePanda是一个基于物理原理的深度学习框架，用于拼接破碎的竹简，显著提高了匹配准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 竹简是记录东亚古代文明的重要媒介，但许多出土的竹简破碎成不规则碎片，拼接困难。

Method: WisePanda通过物理断裂和材料退化原理生成合成训练数据，训练匹配网络，无需手动配对样本。

Result: Top-50匹配准确率从36%提升至52%，拼接效率提高约20倍。

Conclusion: 结合物理原理的深度学习显著提升了古代文物修复的效果，为解决数据稀缺问题提供了新范式。

Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East
Asia, and offers invaluable archaeological insights for reconstructing the Silk
Road, studying material culture exchanges, and global history. However, many
excavated bamboo slips have been fragmented into thousands of irregular pieces,
making their rejoining a vital yet challenging step for understanding their
content. Here we introduce WisePanda, a physics-driven deep learning framework
designed to rejoin fragmented bamboo slips. Based on the physics of fracture
and material deterioration, WisePanda automatically generates synthetic
training data that captures the physical properties of bamboo fragmentations.
This approach enables the training of a matching network without requiring
manually paired samples, providing ranked suggestions to facilitate the
rejoining process. Compared to the leading curve matching method, WisePanda
increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using
WisePanda have experienced substantial efficiency improvements (approximately
20 times faster) when rejoining fragmented bamboo slips. This research
demonstrates that incorporating physical principles into deep learning models
can significantly enhance their performance, transforming how archaeologists
restore and study fragmented artifacts. WisePanda provides a new paradigm for
addressing data scarcity in ancient artifact restoration through physics-driven
machine learning.

</details>


### [107] [Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking](https://arxiv.org/abs/2505.08604)
*Yu-Jen Chen,Xueyang Li,Yiyu Shi,Tsung-Yi Ho*

Main category: cs.CV

TL;DR: 论文提出了一种基于多出口类激活图（MECAM）的无监督OOD检测框架，通过特征掩码和多分辨率CAM增强检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学影像中OOD检测对模型可靠性至关重要，研究发现ID数据的CAM通常聚焦于预测相关区域，而OOD数据缺乏这种特征。

Method: 利用多出口网络生成不同分辨率的CAM，结合特征掩码技术，通过ID和OOD数据在掩码后特征变化的差异进行检测。

Result: 在多个ID和OOD数据集（如ISIC19、PathMNIST、RSNA Pneumonia等）上验证了MECAM的有效性，优于现有方法。

Conclusion: 多出口网络和特征掩码技术为医学影像中的无监督OOD检测提供了新思路，有望提升临床模型的可靠性和可解释性。

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models in medical imaging applications. This work is motivated
by the observation that class activation maps (CAMs) for in-distribution (ID)
data typically emphasize regions that are highly relevant to the model's
predictions, whereas OOD data often lacks such focused activations. By masking
input images with inverted CAMs, the feature representations of ID data undergo
more substantial changes compared to those of OOD data, offering a robust
criterion for differentiation. In this paper, we introduce a novel unsupervised
OOD detection framework, Multi-Exit Class Activation Map (MECAM), which
leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks
that combine CAMs from varying resolutions and depths, our method captures both
global and local feature representations, thereby enhancing the robustness of
OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and
PathMNIST, and test its performance against three medical OOD datasets, RSNA
Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN.
Comprehensive comparisons with state-of-the-art OOD detection methods validate
the effectiveness of our approach. Our findings emphasize the potential of
multi-exit networks and feature masking for advancing unsupervised OOD
detection in medical imaging, paving the way for more reliable and
interpretable models in clinical practice.

</details>


### [108] [Leveraging Multi-Modal Information to Enhance Dataset Distillation](https://arxiv.org/abs/2505.08605)
*Zhe Li,Hadrien Reynaud,Bernhard Kainz*

Main category: cs.CV

TL;DR: 论文提出两种改进数据集蒸馏的方法：基于文本的监督和对象中心掩码，通过结合文本信息和优化对象级特征，显著提升蒸馏数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉表示优化，但结合多模态信息和细化对象级信息可以进一步提升蒸馏数据集的质量。

Method: 引入两种策略：1）基于文本的监督（特征拼接和文本匹配）；2）对象中心掩码（掩码特征对齐损失和掩码梯度匹配损失）。

Result: 综合评估表明，结合文本指导和对象中心掩码显著提升了数据集蒸馏的效果，生成的数据集在下游任务中表现更优。

Conclusion: 通过多模态信息和对象级优化，可以显著提升数据集蒸馏的质量和实用性。

Abstract: Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.

</details>


### [109] [Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World](https://arxiv.org/abs/2505.08607)
*Yuran Wang,Yingping Liang,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出了一种名为BooSTer的新框架，利用视觉基础模型和大规模混合图像源（包括合成、真实和单视图图像）来解决立体匹配中标签稀缺和领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 立体匹配方法依赖密集的像素级真实标签，但获取这些标签耗时且困难，尤其是在真实世界数据集中。此外，合成图像与真实图像之间的领域差距也带来了挑战。

Method: 1. 设计了一种数据生成策略，结合单目深度估计和扩散模型，从单视图图像生成密集立体匹配数据。2. 利用单目深度估计模型的伪标签和动态尺度不变损失来解决真实数据集中标签稀疏的问题。3. 引入视觉基础模型作为编码器，提取鲁棒且可迁移的特征。

Result: 在基准数据集上的实验表明，该方法在标签有限和领域转移的场景中显著提高了准确性，优于现有方法。

Conclusion: BooSTer框架通过结合多种技术手段，有效解决了立体匹配中的标签稀缺和领域差距问题，提升了模型的性能和泛化能力。

Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which
are laborious to obtain, especially for real-world datasets. The scarcity of
labeled data and domain gaps between synthetic and real-world images also pose
notable challenges. In this paper, we propose a novel framework,
\textbf{BooSTer}, that leverages both vision foundation models and large-scale
mixed image sources, including synthetic, real, and single-view images. First,
to fully unleash the potential of large-scale single-view images, we design a
data generation strategy combining monocular depth estimation and diffusion
models to generate dense stereo matching data from single-view images. Second,
to tackle sparse labels in real-world datasets, we transfer knowledge from
monocular depth estimation models, using pseudo-mono depth labels and a dynamic
scale- and shift-invariant loss for additional supervision. Furthermore, we
incorporate vision foundation model as an encoder to extract robust and
transferable features, boosting accuracy and generalization. Extensive
experiments on benchmark datasets demonstrate the effectiveness of our
approach, achieving significant improvements in accuracy over existing methods,
particularly in scenarios with limited labeled data and domain shifts.

</details>


### [110] [WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks](https://arxiv.org/abs/2505.08614)
*Ziyuan He,Zhiqing Guo,Liejun Wang,Gaobo Yang,Yunfeng Diao,Dan Ma*

Main category: cs.CV

TL;DR: WaveGuard是一种主动水印框架，通过频域嵌入和图结构一致性增强鲁棒性和不可感知性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 应对Deepfake技术带来的隐私侵犯和身份盗窃风险。

Method: 使用DT-CWT在高频子带嵌入水印，结合SC-GNN保持视觉质量，并设计注意力模块提升嵌入精度。

Result: 在人脸替换和重演任务中，WaveGuard在鲁棒性和视觉质量上优于现有方法。

Conclusion: WaveGuard为Deepfake威胁提供了一种有效的主动防御解决方案。

Abstract: Deepfake technology poses increasing risks such as privacy invasion and
identity theft. To address these threats, we propose WaveGuard, a proactive
watermarking framework that enhances robustness and imperceptibility via
frequency-domain embedding and graph-based structural consistency.
Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree
Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph
Neural Network (SC-GNN) to preserve visual quality. We also design an attention
module to refine embedding precision. Experimental results on face swap and
reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art
methods in both robustness and visual quality. Code is available at
https://github.com/vpsg-research/WaveGuard.

</details>


### [111] [OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning](https://arxiv.org/abs/2505.08617)
*Zhaochen Su,Linjie Li,Mingyang Song,Yunzhuo Hao,Zhengyuan Yang,Jun Zhang,Guanjie Chen,Jiawei Gu,Juntao Li,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: OpenThinkIMG是一个开源框架，用于增强大型视觉语言模型（LVLMs）的动态工具调用能力，通过强化学习（V-ToolRL）显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化基础设施，阻碍了视觉工具集成和交互数据生成，限制了LVLMs的动态问题解决能力。

Method: 提出OpenThinkIMG框架，包括标准化工具接口、轨迹生成和强化学习（V-ToolRL）训练方法。

Result: V-ToolRL训练的模型在图表推理任务中显著优于监督学习基线（+12.7分）和GPT-4.1（+8.68分）。

Conclusion: OpenThinkIMG为动态视觉推理提供了基础框架，推动AI代理实现真正的“图像思维”。

Abstract: While humans can flexibly leverage interactive visual cognition for complex
problem-solving, enabling Large Vision-Language Models (LVLMs) to learn
similarly adaptive behaviors with visual tools remains challenging. A
significant hurdle is the current lack of standardized infrastructure, which
hinders integrating diverse tools, generating rich interaction data, and
training robust agents effectively. To address these gaps, we introduce
OpenThinkIMG, the first open-source, comprehensive end-to-end framework for
tool-augmented LVLMs. It features standardized vision tool interfaces, scalable
trajectory generation for policy initialization, and a flexible training
environment. Furthermore, considering supervised fine-tuning (SFT) on static
demonstrations offers limited policy generalization for dynamic tool
invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL
to train LVLMs to learn adaptive policies for invoking external vision tools.
V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies
by directly optimizing for task success using feedback from tool interactions.
We empirically validate V-ToolRL on challenging chart reasoning tasks. Our
RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its
SFT-initialized counterpart (+28.83 points) and surpasses established
supervised tool-learning baselines like Taco and CogCom by an average of +12.7
points. Notably, it also surpasses prominent closed-source models like GPT-4.1
by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational
framework for advancing dynamic, tool-augmented visual reasoning, helping the
community develop AI agents that can genuinely "think with images".

</details>


### [112] [DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting](https://arxiv.org/abs/2505.08644)
*Holly Dinkel,Marcel Büsching,Alberta Longhini,Brian Coltin,Trey Smith,Danica Kragic,Mårten Björkman,Timothy Bretl*

Main category: cs.CV

TL;DR: DLO-Splatting是一种通过多视角RGB图像和夹爪状态信息估计可变形线性物体（DLO）3D形状的算法，结合预测-更新滤波和3D高斯渲染优化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法在复杂场景（如打结）中表现不佳，需要结合动态模型和视觉信息以提高形状估计精度。

Method: 算法采用基于位置的动力学模型预测形状，并通过3D高斯渲染损失优化预测，使其与视觉观测对齐。

Result: 初步实验在打结场景中展示了优于纯视觉方法的结果。

Conclusion: DLO-Splatting通过结合动态模型和视觉优化，为复杂场景下的DLO形状估计提供了有效解决方案。

Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

</details>


### [113] [SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation](https://arxiv.org/abs/2505.08665)
*Edoardo Bianchi,Antonio Liotta*

Main category: cs.CV

TL;DR: SkillFormer是一种高效的架构，用于从第一人称和第三人称视频中统一评估多视角技能水平，通过跨视角融合模块和低秩适应技术显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 评估复杂活动中的人类技能水平在体育、康复和训练中有重要应用，但现有方法在多视角融合和计算效率上存在不足。

Method: 基于TimeSformer架构，SkillFormer引入CrossViewFusion模块，结合多头交叉注意力、可学习门控和自适应自校准，并采用低秩适应技术优化参数。

Result: 在EgoExo4D数据集上，SkillFormer在多视角设置中达到最先进精度，参数减少4.5倍，训练周期减少3.75倍。

Conclusion: SkillFormer证明了多视角融合在细粒度技能评估中的价值，同时显著提升了计算效率。

Abstract: Assessing human skill levels in complex activities is a challenging problem
with applications in sports, rehabilitation, and training. In this work, we
present SkillFormer, a parameter-efficient architecture for unified multi-view
proficiency estimation from egocentric and exocentric videos. Building on the
TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that
fuses view-specific features using multi-head cross-attention, learnable
gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to
fine-tune only a small subset of parameters, significantly reducing training
costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves
state-of-the-art accuracy in multi-view settings while demonstrating remarkable
computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer
training epochs than prior baselines. It excels in multiple structured tasks,
confirming the value of multi-view integration for fine-grained skill
assessment.

</details>


### [114] [Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results](https://arxiv.org/abs/2505.08685)
*Meritxell Riera-Marin,Sikha O K,Julia Rodriguez-Comas,Matthias Stefan May,Zhaohong Pan,Xiang Zhou,Xiaokun Liang,Franciskus Xaverius Erick,Andrea Prenner,Cedric Hemon,Valentin Boussot,Jean-Louis Dillenseger,Jean-Claude Nunes,Abdul Qayyum,Moona Mazher,Steven A Niederer,Kaisar Kushibar,Carlos Martin-Isla,Petia Radeva,Karim Lekadir,Theodore Barfoot,Luis C. Garcia Peraza Herrera,Ben Glocker,Tom Vercauteren,Lucas Gago,Justin Englemann,Joy-Marie Kleiss,Anton Aubanell,Andreu Antolin,Javier Garcia-Lopez,Miguel A. Gonzalez Ballester,Adrian Galdran*

Main category: cs.CV

TL;DR: 论文提出了CURVAS方法，通过多标注者数据解决医学图像分割中的不确定性和校准问题，评估了多种深度学习模型，发现校准良好的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标注变异性、校准和不确定性估计是关键挑战，需通过多标注者数据建立更全面的基准。

Method: 创建CURVAS挑战，七支团队提交DL模型，使用DSC、ECE和CRPS等指标评估，结合共识与分歧基准。

Result: 校准良好的模型表现更优，预训练模型在非标准解剖结构中更稳健，最佳模型DSC高且不确定性校准良好。

Conclusion: 多标注者基准、校准评估和不确定性感知对开发可靠医学图像分割模型至关重要。

Abstract: Deep learning (DL) has become the dominant approach for medical image
segmentation, yet ensuring the reliability and clinical applicability of these
models requires addressing key challenges such as annotation variability,
calibration, and uncertainty estimation. This is why we created the Calibration
and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation
(CURVAS), which highlights the critical role of multiple annotators in
establishing a more comprehensive ground truth, emphasizing that segmentation
is inherently subjective and that leveraging inter-annotator variability is
essential for robust model evaluation. Seven teams participated in the
challenge, submitting a variety of DL models evaluated using metrics such as
Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and
Continuous Ranked Probability Score (CRPS). By incorporating consensus and
dissensus ground truth, we assess how DL models handle uncertainty and whether
their confidence estimates align with true segmentation performance. Our
findings reinforce the importance of well-calibrated models, as better
calibration is strongly correlated with the quality of the results.
Furthermore, we demonstrate that segmentation models trained on diverse
datasets and enriched with pre-trained knowledge exhibit greater robustness,
particularly in cases deviating from standard anatomical structures. Notably,
the best-performing models achieved high DSC and well-calibrated uncertainty
estimates. This work underscores the need for multi-annotator ground truth,
thorough calibration assessments, and uncertainty-aware evaluations to develop
trustworthy and clinically reliable DL-based medical image segmentation models.

</details>


### [115] [SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model](https://arxiv.org/abs/2505.08695)
*Zhanjie Zhang,Quanwei Zhang,Junsheng Luan,Mengyuan Yang,Yun Wang,Lei Zhao*

Main category: cs.CV

TL;DR: SPAST框架通过局部-全局窗口大小风格化模块和风格先验损失，实现了高质量风格迁移并减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高质量风格迁移和内容结构保持上存在不足，SPAST旨在解决这些问题。

Method: 设计了局部-全局窗口大小风格化模块（LGWSSM）和风格先验损失，结合预训练大模型的风格先验。

Result: 实验表明SPAST能生成高质量风格化图像，且推理时间更短。

Conclusion: SPAST在风格迁移质量和效率上优于现有方法。

Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to
render a new stylized
  image which preserves the content image's structure and possesses the style
image's style. Existing
  arbitrary style transfer methods are based on either small models or
pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images,
bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can
generate high-quality
  stylized images but struggle to preserve the content structure and cost long
inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality
stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size
Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a
novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into
the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference
time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality
stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.

</details>


### [116] [Controllable Image Colorization with Instance-aware Texts and Masks](https://arxiv.org/abs/2505.08705)
*Yanru An,Ling Gui,Qiang Hu,Chunlei Cai,Tianxiao Ye,Xiaoyun Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的实例感知图像着色方法MT-Color，通过像素级掩码注意力机制和实例掩码文本引导模块解决颜色溢出和绑定错误问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流图像着色模型存在颜色溢出和绑定错误问题，且无法实现实例级着色。

Method: 设计了像素级掩码注意力机制和实例掩码文本引导模块，并采用多实例采样策略。

Result: 实验表明，该方法在质量和定量上优于现有方法。

Conclusion: MT-Color和GPT-color数据集在实例级着色任务中表现优异。

Abstract: Recently, the application of deep learning in image colorization has received
widespread attention. The maturation of diffusion models has further advanced
the development of image colorization models. However, current mainstream image
colorization models still face issues such as color bleeding and color binding
errors, and cannot colorize images at the instance level. In this paper, we
propose a diffusion-based colorization method MT-Color to achieve precise
instance-aware colorization with use-provided guidance. To tackle color
bleeding issue, we design a pixel-level mask attention mechanism that
integrates latent features and conditional gray image features through
cross-attention. We use segmentation masks to construct cross-attention masks,
preventing pixel information from exchanging between different instances. We
also introduce an instance mask and text guidance module that extracts instance
masks and text representations of each instance, which are then fused with
latent features through self-attention, utilizing instance masks to form
self-attention masks to prevent instance texts from guiding the colorization of
other areas, thus mitigating color binding errors. Furthermore, we apply a
multi-instance sampling strategy, which involves sampling each instance region
separately and then fusing the results. Additionally, we have created a
specialized dataset for instance-level colorization tasks, GPT-color, by
leveraging large visual language models on existing image datasets. Qualitative
and quantitative experiments show that our model and dataset outperform
previous methods and datasets.

</details>


### [117] [TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series](https://arxiv.org/abs/2505.08723)
*Xiaolei Qin,Di Wang,Jing Zhang,Fengxiang Wang,Xin Su,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: TiMo是一种新型分层视觉Transformer基础模型，专为卫星图像时间序列（SITS）分析设计，通过动态捕捉多尺度时空关系提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有时空基础模型依赖普通视觉Transformer，未能显式捕捉多尺度时空关系，限制了其在下游任务中的有效性。

Method: 提出TiMo模型，引入时空陀螺仪注意力机制，动态捕捉时空多尺度模式，并利用MillionST数据集进行预训练。

Result: 在多项时空任务（如森林砍伐监测、土地覆盖分割等）中，TiMo优于现有方法。

Conclusion: TiMo通过显式建模多尺度时空关系，显著提升了SITS分析的性能。

Abstract: Satellite image time series (SITS) provide continuous observations of the
Earth's surface, making them essential for applications such as environmental
management and disaster assessment. However, existing spatiotemporal foundation
models rely on plain vision transformers, which encode entire temporal
sequences without explicitly capturing multiscale spatiotemporal relationships
between land objects. This limitation hinders their effectiveness in downstream
tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision
transformer foundation model tailored for SITS analysis. At its core, we
introduce a spatiotemporal gyroscope attention mechanism that dynamically
captures evolving multiscale patterns across both time and space. For
pre-training, we curate MillionST, a large-scale dataset of one million images
from 100,000 geographic locations, each captured across 10 temporal phases over
five years, encompassing diverse geospatial changes and seasonal variations.
Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,
enabling it to effectively learn and encode generalizable spatiotemporal
representations.Extensive experiments across multiple spatiotemporal
tasks-including deforestation monitoring, land cover segmentation, crop type
classification, and flood detection-demonstrate TiMo's superiority over
state-of-the-art methods. Code, model, and dataset will be released at
https://github.com/MiliLab/TiMo.

</details>


### [118] [Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving](https://arxiv.org/abs/2505.08725)
*Zongchuang Zhao,Haoyu Fu,Dingkang Liang,Xin Zhou,Dingyuan Zhang,Hongwei Xie,Bing Wang,Xiang Bai*

Main category: cs.CV

TL;DR: 论文提出NuInteract数据集和DriveMonkey框架，解决LVLMs在3D场景理解中的不足，显著提升3D视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在自动驾驶场景中缺乏全面的场景理解能力，尤其是2D与3D映射关系不足。

Method: 引入NuInteract数据集（150万对多视角图像语言数据），并提出DriveMonkey框架，结合空间处理器提升3D感知。

Result: DriveMonkey在3D视觉定位任务中表现优于通用LVLMs，提升9.86%。

Conclusion: NuInteract和DriveMonkey为LVLMs在3D场景理解中的应用提供了有效解决方案。

Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image
understanding. Their comprehension and reasoning capabilities enable promising
applications in autonomous driving scenarios. However, existing research
typically focuses on front-view perspectives and partial objects within scenes,
struggling to achieve comprehensive scene understanding. Meanwhile, existing
LVLMs suffer from the lack of mapping relationship between 2D and 3D and
insufficient integration of 3D object localization and instruction
understanding. To tackle these limitations, we first introduce NuInteract, a
large-scale dataset with over 1.5M multi-view image language pairs spanning
dense scene captions and diverse interactive tasks. Furthermore, we propose
DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs
with a spatial processor using a series of learnable queries. The spatial
processor, designed as a plug-and-play component, can be initialized with
pre-trained 3D detectors to improve 3D perception. Our experiments show that
DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable
improvement on the 3D visual grounding task. The dataset and code will be
released at https://github.com/zc-zhao/DriveMonkey.

</details>


### [119] [Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion](https://arxiv.org/abs/2505.08747)
*Huiyan Qi,Bin Zhu,Chong-Wah Ngo,Jingjing Chen,Ee-Peng Lim*

Main category: cs.CV

TL;DR: FastFood数据集和VIF²方法通过融合视觉与食材特征提升营养估计准确性。


<details>
  <summary>Details</summary>
Motivation: 营养估计对健康饮食至关重要，但缺乏带营养标注的数据集限制了进展。

Method: 提出VIF²方法，结合视觉与食材特征，并通过数据增强优化食材预测。

Result: 在FastFood和Nutrition5k数据集上验证了方法的有效性，支持多种骨干网络。

Conclusion: 食材信息对营养估计至关重要，VIF²方法显著提升了准确性。

Abstract: Nutrition estimation is an important component of promoting healthy eating
and mitigating diet-related health risks. Despite advances in tasks such as
food classification and ingredient recognition, progress in nutrition
estimation is limited due to the lack of datasets with nutritional annotations.
To address this issue, we introduce FastFood, a dataset with 84,446 images
across 908 fast food categories, featuring ingredient and nutritional
annotations. In addition, we propose a new model-agnostic Visual-Ingredient
Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating
visual and ingredient features. Ingredient robustness is improved through
synonym replacement and resampling strategies during training. The
ingredient-aware visual feature fusion module combines ingredient features and
visual representation to achieve accurate nutritional prediction. During
testing, ingredient predictions are refined using large multimodal models by
data augmentation and majority voting. Our experiments on both FastFood and
Nutrition5k datasets validate the effectiveness of our proposed method built in
different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the
importance of ingredient information in nutrition estimation.
https://huiyanqi.github.io/fastfood-nutrition-estimation/.

</details>


### [120] [Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology](https://arxiv.org/abs/2505.08765)
*Yatai Ji,Zhengqiu Zhu,Yong Zhao,Beidan Liu,Chen Gao,Yihao Zhao,Sihang Qiu,Yue Hu,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了CityAVOS数据集和PRPSearcher方法，用于解决无人机在复杂城市环境中自主搜索目标物体的挑战，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂城市环境中表现不佳，主要由于冗余语义处理、相似物体区分和探索-利用困境。

Method: 提出PRPSearcher方法，基于多模态大语言模型，构建三种专用地图（动态语义地图、3D认知地图和不确定性地图），并结合去噪机制和IPT提示机制。

Result: 实验结果显示PRPSearcher在成功率和搜索效率上显著优于基线方法（平均提升37.69% SR和28.96% SPL）。

Conclusion: 尽管表现优异，但与人类相比仍有差距，未来需提升语义推理和空间探索能力。工作为未来目标搜索研究奠定了基础。

Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require
Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target
objects using visual and textual cues without external guidance. Existing
approaches struggle in complex urban environments due to redundant semantic
processing, similar object distinction, and the exploration-exploitation
dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,
the first benchmark dataset for autonomous search of common urban objects. This
dataset comprises 2,420 tasks across six object categories with varying
difficulty levels, enabling comprehensive evaluation of UAV agents' search
capabilities. To solve the AVOS tasks, we also propose PRPSearcher
(Perception-Reasoning-Planning Searcher), a novel agentic method powered by
multi-modal large language models (MLLMs) that mimics human three-tier
cognition. Specifically, PRPSearcher constructs three specialized maps: an
object-centric dynamic semantic map enhancing spatial perception, a 3D
cognitive map based on semantic attraction values for target reasoning, and a
3D uncertainty map for balanced exploration-exploitation search. Also, our
approach incorporates a denoising mechanism to mitigate interference from
similar objects and utilizes an Inspiration Promote Thought (IPT) prompting
mechanism for adaptive action planning. Experimental results on CityAVOS
demonstrate that PRPSearcher surpasses existing baselines in both success rate
and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and
-46.40% NE). While promising, the performance gap compared to humans highlights
the need for better semantic reasoning and spatial exploration capabilities in
AVOS tasks. This work establishes a foundation for future advances in embodied
target search. Dataset and source code are available at
https://anonymous.4open.science/r/CityAVOS-3DF8.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [121] [SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices](https://arxiv.org/abs/2505.08191)
*Yipu Zhang,Jiawei Liang,Jian Peng,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: SpNeRF提出了一种软硬件协同设计的稀疏体素神经渲染方案，通过预处理和在线解码步骤显著减少内存占用，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 神经渲染在AR/VR应用中需要高质量输出，但传统方法因大体积网格数据和不规则访问模式难以在边缘设备上实时处理。

Method: 采用哈希映射预处理和在线解码技术，结合位图掩码减少哈希冲突导致的PSNR损失，并设计专用硬件架构优化性能。

Result: 实验显示SpNeRF平均减少21.07倍内存占用，速度提升最高95.1倍，能效提升最高625.6倍。

Conclusion: SpNeRF通过软硬件协同设计有效解决了神经渲染在边缘设备上的内存和性能瓶颈。

Abstract: Neural rendering has gained prominence for its high-quality output, which is
crucial for AR/VR applications. However, its large voxel grid data size and
irregular access patterns challenge real-time processing on edge devices. While
previous works have focused on improving data locality, they have not
adequately addressed the issue of large voxel grid sizes, which necessitate
frequent off-chip memory access and substantial on-chip memory. This paper
introduces SpNeRF, a software-hardware co-design solution tailored for sparse
volumetric neural rendering. We first identify memory-bound rendering
inefficiencies and analyze the inherent sparsity in the voxel grid data of
neural rendering. To enhance efficiency, we propose novel preprocessing and
online decoding steps, reducing the memory size for voxel grid. The
preprocessing step employs hash mapping to support irregular data access while
maintaining a minimal memory size. The online decoding step enables efficient
on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate
PSNR loss caused by hash collisions. To further optimize performance, we design
a dedicated hardware architecture supporting our sparse voxel grid processing
technique. Experimental results demonstrate that SpNeRF achieves an average
21.07$\times$ reduction in memory size while maintaining comparable PSNR
levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and
NeuRex.Edge, our design achieves speedups of 95.1$\times$, 63.5$\times$,
1.5$\times$ and 10.3$\times$, and improves energy efficiency by 625.6$\times$,
529.1$\times$, 4$\times$, and 4.4$\times$, respectively.

</details>


### [122] [MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units](https://arxiv.org/abs/2505.08599)
*Sebastian Billaudelle,Laura Kriener,Filippo Moro,Tristan Torchet,Melika Payvand*

Main category: cs.AR

TL;DR: 本文提出了一种基于最小门控循环单元（GRU）的硬件兼容架构，并实现了高效的混合信号硬件设计，利用开关电容电路进行内存计算和门控状态更新。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式边缘计算环境中，内存受限的系统需要高效的RNN架构，以处理时序数据。

Method: 提出了一种基于最小GRU的架构，并设计了混合信号硬件实现，利用开关电容电路进行内存计算和门控状态更新。

Result: 通过时间序列数据验证了架构性能，并在混合信号仿真中验证了硬件兼容性。

Conclusion: 该设计为内存受限系统提供了一种高效且可扩展的RNN实现方案。

Abstract: Recurrent neural networks (RNNs) have been a long-standing candidate for
processing of temporal sequence data, especially in memory-constrained systems
that one may find in embedded edge computing environments. Recent advances in
training paradigms have now inspired new generations of efficient RNNs. We
introduce a streamlined and hardware-compatible architecture based on minimal
gated recurrent units (GRUs), and an accompanying efficient mixed-signal
hardware implementation of the model. The proposed design leverages
switched-capacitor circuits not only for in-memory computation (IMC), but also
for the gated state updates. The mixed-signal cores rely solely on commodity
circuits consisting of metal capacitors, transmission gates, and a clocked
comparator, thus greatly facilitating scaling and transfer to other technology
nodes.
  We benchmark the performance of our architecture on time series data,
introducing all constraints required for a direct mapping to the hardware
system. The direct compatibility is verified in mixed-signal simulations,
reproducing data recorded from the software-only network model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [123] [Moving From Monolithic To Microservices Architecture for Multi-Agent Systems](https://arxiv.org/abs/2505.07838)
*Muskaan Goyal,Pranav Bhasin*

Main category: cs.SE

TL;DR: 本文探讨了从单体架构到微服务架构在多智能体系统（MAS）中的演变，分析了传统单体MAS的局限性和微服务架构的优势，并研究了核心架构原则和通信协议。


<details>
  <summary>Details</summary>
Motivation: 随着软件架构从单体向微服务的转变，这种范式在复杂多智能体系统（MAS）中的应用变得日益重要，本文旨在探索这一转变的潜力和挑战。

Method: 通过比较分析，本文研究了MAS中微服务架构的核心原则、通信协议（如ACL、MCP、A2A）以及新兴架构模式。

Result: 研究揭示了微服务架构在MAS中的优势，包括可扩展性和可维护性的提升，同时也指出了设计挑战和考虑因素。

Conclusion: 微服务架构为MAS带来了显著的改进，但其成功实施需要解决特定的设计挑战和通信协议问题。

Abstract: The transition from monolithic to microservices architecture revolutionized
software development by improving scalability and maintainability. This
paradigm shift is now becoming relevant for complex multi-agent systems (MAS).
This review article explores the evolution from monolithic architecture to
microservices architecture in the specific context of MAS. It will highlight
the limitations of traditional monolithic MAS and the benefits of adopting a
microservices-based approach. The article further examines the core
architectural principles and communication protocols, including Agent
Communication Languages (ACLs), the Model Context Protocol (MCP), and the
Application-to-Application (A2A) protocol. The article identifies emerging
architectural patterns, design challenges, and considerations through a
comparative lens of the paradigm shift.

</details>


### [124] [Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey](https://arxiv.org/abs/2505.07058)
*Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.SE

TL;DR: 本文综述了可解释人工智能（XAI）在软件开发生命周期（SDLC）各阶段的应用，填补了XAI在软件工程中应用的研究空白。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型的黑箱问题，提升AI系统的透明度和可解释性，以增强信任和广泛应用。

Method: 通过文献综述，分析XAI方法（如LIME、SHAP、规则提取等）在SDLC各阶段的应用。

Result: 发现68%的XAI研究集中在软件维护阶段，而软件管理和需求阶段仅占8%。

Conclusion: 本文首次全面综述了XAI在SDLC各阶段的应用，旨在推动XAI在软件工程中的实践应用。

Abstract: Artificial Intelligence (AI) is rapidly expanding and integrating more into
daily life to automate tasks, guide decision making, and enhance efficiency.
However, complex AI models, which make decisions without providing clear
explanations (known as the "black-box problem"), currently restrict trust and
widespread adoption of AI. Explainable Artificial Intelligence (XAI) has
emerged to address the black-box problem of making AI systems more
interpretable and transparent so stakeholders can trust, verify, and act upon
AI-based outcomes. Researchers have developed various techniques to foster XAI
in the Software Development Lifecycle. However, there are gaps in applying XAI
techniques in the Software Engineering phases. Literature review shows that 68%
of XAI in Software Engineering research is focused on maintenance as opposed to
8% on software management and requirements. In this paper, we present a
comprehensive survey of the applications of XAI methods such as concept-based
explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), rule extraction, attention mechanisms,
counterfactual explanations, and example-based explanations to the different
phases of the Software Development Life Cycle (SDLC), including requirements
elicitation, design and development, testing and deployment, and evolution. To
the best of our knowledge, this paper presents the first comprehensive survey
of XAI techniques for every phase of the Software Development Life Cycle
(SDLC). This survey aims to promote explainable AI in Software Engineering and
facilitate the practical application of complex AI models in AI-driven software
development.

</details>


### [125] [SweRank: Software Issue Localization with Code Ranking](https://arxiv.org/abs/2505.07849)
*Revanth Gangi Reddy,Tarun Suresh,JaeHyeok Doo,Ye Liu,Xuan Phi Nguyen,Yingbo Zhou,Semih Yavuz,Caiming Xiong,Heng Ji,Shafiq Joty*

Main category: cs.SE

TL;DR: SweRank是一个高效的检索-重排框架，用于软件问题定位，结合了传统代码排名模型和LLM方法的优势，并在新数据集SweLoc上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 软件问题定位是开发中的关键但耗时的任务，现有方法（如LLM代理或传统代码排名模型）存在延迟高、成本大或性能不足的问题。

Method: 提出SweRank框架，结合检索和重排技术，并构建SweLoc数据集用于训练。

Result: 在SWE-Bench-Lite和LocBench上，SweRank表现优于现有排名模型和基于闭源LLM的代理系统。

Conclusion: SweRank和SweLoc为问题定位提供了高效且有效的解决方案，同时数据集对社区有重要价值。

Abstract: Software issue localization, the task of identifying the precise code
locations (files, classes, or functions) relevant to a natural language issue
description (e.g., bug report, feature request), is a critical yet
time-consuming aspect of software development. While recent LLM-based agentic
approaches demonstrate promise, they often incur significant latency and cost
due to complex multi-step reasoning and relying on closed-source LLMs.
Alternatively, traditional code ranking models, typically optimized for
query-to-code or code-to-code retrieval, struggle with the verbose and
failure-descriptive nature of issue localization queries. To bridge this gap,
we introduce SweRank, an efficient and effective retrieve-and-rerank framework
for software issue localization. To facilitate training, we construct SweLoc, a
large-scale dataset curated from public GitHub repositories, featuring
real-world issue descriptions paired with corresponding code modifications.
Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves
state-of-the-art performance, outperforming both prior ranking models and
costly agent-based systems using closed-source LLMs like Claude-3.5. Further,
we demonstrate SweLoc's utility in enhancing various existing retriever and
reranker models for issue localization, establishing the dataset as a valuable
resource for the community.

</details>


### [126] [Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions](https://arxiv.org/abs/2505.08135)
*Keita Teranishi,Harshitha Menon,William F. Godoy,Prasanna Balaprakash,David Bau,Tal Ben-Nun,Abhinav Bathele,Franz Franchetti,Michael Franusich,Todd Gamblin,Giorgis Georgakoudis,Tom Goldstein,Arjun Guha,Steven Hahn,Costin Iancu,Zheming Jin,Terry Jones,Tze Meng Low,Het Mankad,Narasinga Rao Miniskar,Mohammad Alaul Haque Monil,Daniel Nichols,Konstantinos Parasyris,Swaroop Pophale,Pedro Valero-Lara,Jeffrey S. Vetter,Samuel Williams,Aaron Young*

Main category: cs.SE

TL;DR: 探讨AI如何革新高性能计算（HPC）软件开发，分析挑战并提出研究方向，重点介绍两个美国能源部资助项目Ellora和Durban。


<details>
  <summary>Details</summary>
Motivation: AI技术（尤其是大语言模型）已改变软件开发，但HPC软件作为高度专业化领域，其AI应用面临独特挑战，需深入研究。

Method: 分析HPC软件开发的特殊性，结合AI技术提出研究方向，并通过Ellora和Durban项目实践验证。

Result: 提出利用AI优化HPC软件开发的具体方向，并启动相关项目以推动技术落地。

Conclusion: AI在HPC软件开发中潜力巨大，但需针对性研究以克服挑战，项目Ellora和Durban为未来研究奠定基础。

Abstract: We discuss the challenges and propose research directions for using AI to
revolutionize the development of high-performance computing (HPC) software. AI
technologies, in particular large language models, have transformed every
aspect of software development. For its part, HPC software is recognized as a
highly specialized scientific field of its own. We discuss the challenges
associated with leveraging state-of-the-art AI technologies to develop such a
unique and niche class of software and outline our research directions in the
two US Department of Energy--funded projects for advancing HPC Software via AI:
Ellora and Durban.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [127] [Monocular Online Reconstruction with Enhanced Detail Preservation](https://arxiv.org/abs/2505.07887)
*Songyin Wu,Zhaoyang Lv,Yufeng Zhu,Duncan Frost,Zhengqin Li,Ling-Qi Yan,Carl Ren,Richard Newcombe,Zhao Dong*

Main category: cs.GR

TL;DR: 提出了一种基于3D高斯的在线密集映射框架，用于从单目图像流中重建逼真细节。


<details>
  <summary>Details</summary>
Motivation: 解决单目在线重建中的两个关键挑战：无需依赖深度图的高斯分布，以及确保重建地图的局部和全局一致性。

Method: 引入了两个关键模块：分层高斯管理模块（用于有效分布高斯）和全局一致性优化模块（用于保持多尺度对齐和连贯性）。此外，提出了多级占用哈希体素（MOHV）结构，用于正则化高斯以捕捉多粒度细节。

Result: 与最先进的仅RGB甚至RGB-D方法相比，该框架在计算效率高的同时实现了更优的重建质量。

Conclusion: 该框架能够无缝集成多种跟踪系统，具有通用性和可扩展性。

Abstract: We propose an online 3D Gaussian-based dense mapping framework for
photorealistic details reconstruction from a monocular image stream. Our
approach addresses two key challenges in monocular online reconstruction:
distributing Gaussians without relying on depth maps and ensuring both local
and global consistency in the reconstructed maps. To achieve this, we introduce
two key modules: the Hierarchical Gaussian Management Module for effective
Gaussian distribution and the Global Consistency Optimization Module for
maintaining alignment and coherence at all scales. In addition, we present the
Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes
Gaussians for capturing details across multiple levels of granularity. MOHV
ensures accurate reconstruction of both fine and coarse geometries and
textures, preserving intricate details while maintaining overall structural
integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our
framework achieves superior reconstruction quality with high computational
efficiency. Moreover, it integrates seamlessly with various tracking systems,
ensuring generality and scalability.

</details>


### [128] [ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image](https://arxiv.org/abs/2505.08239)
*Yizhi Wang,Mingrui Zhao,Ali Mahdavi-Amiri,Hao Zhang*

Main category: cs.GR

TL;DR: 提出自适应视角规划方法，通过动态相机轨迹优化多视角合成，提升单视角3D重建的遮挡揭示和3D一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统多视角合成中无序视角生成导致的遮挡问题和3D不一致性。

Method: 提出自适应相机轨迹（ACT）技术，动态计算最优视角序列，结合视频扩散模型生成新视角，输入多视角3D重建模型。

Result: 在GSO数据集上显著提升3D重建效果，定量和定性均优于现有方法。

Conclusion: 自适应视角规划有效提升遮挡揭示和3D一致性，无需运行时训练，高效且性能优越。

Abstract: We introduce adaptive view planning to multi-view synthesis, aiming to
improve both occlusion revelation and 3D consistency for single-view 3D
reconstruction. Instead of generating an unordered set of views independently
or simultaneously, we generate a sequence of views, leveraging temporal
consistency to enhance 3D coherence. Most importantly, our view sequence is not
determined by a pre-determined camera setup. Instead, we compute an adaptive
camera trajectory (ACT), specifically, an orbit of camera views, which
maximizes the visibility of occluded regions of the 3D object to be
reconstructed. Once the best orbit is found, we feed it to a video diffusion
model to generate novel views around the orbit, which in turn, are passed to a
multi-view 3D reconstruction model to obtain the final reconstruction. Our
multi-view synthesis pipeline is quite efficient since it involves no run-time
training/optimization, only forward inferences by applying the pre-trained
models for occlusion analysis and multi-view synthesis. Our method predicts
camera trajectories that reveal occlusions effectively and produce consistent
novel views, significantly improving 3D reconstruction over SOTA on the unseen
GSO dataset, both quantitatively and qualitatively.

</details>


### [129] [M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis](https://arxiv.org/abs/2505.08293)
*Zhizhuo Yin,Yuk Hang Tsui,Pan Hui*

Main category: cs.GR

TL;DR: 提出了一种名为M3G的新框架，用于从音频生成全身手势，解决了现有方法因固定粒度而无法建模不同手势模式的问题。


<details>
  <summary>Details</summary>
Motivation: 生成包含面部、身体、手部和全局动作的全身手势是虚拟角色创建中的重要任务，但现有系统因固定粒度的手势标记而无法捕捉不同手势模式的多样性。

Method: 提出M3G框架，包括多粒度VQ-VAE（MGVQ-VAE）用于标记和重建不同时间粒度的动作序列，以及多粒度标记预测器从音频中提取信息并预测动作标记。

Result: 实验表明，M3G在生成自然且富有表现力的全身手势方面优于现有方法。

Conclusion: M3G通过多粒度建模解决了手势生成的多样性问题，显著提升了生成效果。

Abstract: Generating full-body human gestures encompassing face, body, hands, and
global movements from audio is a valuable yet challenging task in virtual
avatar creation. Previous systems focused on tokenizing the human gestures
framewisely and predicting the tokens of each frame from the input audio.
However, one observation is that the number of frames required for a complete
expressive human gesture, defined as granularity, varies among different human
gesture patterns. Existing systems fail to model these gesture patterns due to
the fixed granularity of their gesture tokens. To solve this problem, we
propose a novel framework named Multi-Granular Gesture Generator (M3G) for
audio-driven holistic gesture generation. In M3G, we propose a novel
Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct
motion sequences from different temporal granularities. Subsequently, we
proposed a multi-granular token predictor that extracts multi-granular
information from audio and predicts the corresponding motion tokens. Then M3G
reconstructs the human gestures from the predicted tokens using the MGVQ-VAE.
Both objective and subjective experiments demonstrate that our proposed M3G
framework outperforms the state-of-the-art methods in terms of generating
natural and expressive full-body human gestures.

</details>


### [130] [Claycode: Stylable and Deformable 2D Scannable Codes](https://arxiv.org/abs/2505.08666)
*Marco Maida,Alberto Crescini,Marco Perronet,Elena Camuffo*

Main category: cs.GR

TL;DR: Claycode是一种新型的2D可扫描码，支持高度样式化和变形，通过树结构编码信息，优于传统二维码。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵式二维码（如QR码）在样式化和变形方面受限，Claycode旨在解决这一问题。

Method: 通过树结构编码信息，将比特映射到拓扑树中，并在目标多边形内绘制嵌套颜色区域。解码时从摄像头流中实时提取。

Result: Claycode在高度变形情况下仍能正常工作，性能优于传统二维码。

Conclusion: Claycode为可扫描码提供了更高的样式化和变形容忍度，适用于传统二维码无法胜任的场景。

Abstract: This paper introduces Claycode, a novel 2D scannable code designed for
extensive stylization and deformation. Unlike traditional matrix-based codes
(e.g., QR codes), Claycodes encode their message in a tree structure. During
the encoding process, bits are mapped into a topology tree, which is then
depicted as a nesting of color regions drawn within the boundaries of a target
polygon shape. When decoding, Claycodes are extracted and interpreted in
real-time from a camera stream. We detail the end-to-end pipeline and show that
Claycodes allow for extensive stylization without compromising their
functionality. We then empirically demonstrate Claycode's high tolerance to
heavy deformations, outperforming traditional 2D scannable codes in scenarios
where they typically fail.

</details>


### [131] [CAD-Coder:Text-Guided CAD Files Code Generation](https://arxiv.org/abs/2505.08686)
*Changqi He,Shuhan Zhang,Liguo Zhang,Jiajun Miao*

Main category: cs.GR

TL;DR: CAD-Coder是一个将自然语言指令转换为可编辑CAD脚本代码的框架，支持生成带有几何标注的CAD文件。


<details>
  <summary>Details</summary>
Motivation: 传统CAD依赖专家手工绘制或修改现有库文件，无法快速个性化。现有生成方法缺乏交互编辑性和几何标注，限制了实际应用。

Method: 提出CAD-Coder框架，利用自然语言指令生成CAD脚本代码，构建包含29,130个Dxf文件及其脚本代码的数据集。

Result: 在多种2D/3D CAD生成任务中表现优异，提供可编辑草图和几何标注。

Conclusion: CAD-Coder实现了交互式生成CAD，解决了现有方法的编辑性和标注问题。

Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D
models of real-world products. Traditional CAD typically relies on hand-drawing
by experts or modifications of existing library files, which doesn't allow for
rapid personalization. With the emergence of generative artificial
intelligence, convenient and efficient personalized CAD generation has become
possible. However, existing generative methods typically produce outputs that
lack interactive editability and geometric annotations, limiting their
practical applications in manufacturing. To enable interactive generative CAD,
we propose CAD-Coder, a framework that transforms natural language instructions
into CAD script codes, which can be executed in Python environments to generate
human-editable CAD files (.Dxf). To facilitate the generation of editable CAD
sketches with annotation information, we construct a comprehensive dataset
comprising 29,130 Dxf files with their corresponding script codes, where each
sketch preserves both editability and geometric annotations. We evaluate
CAD-Coder on various 2D/3D CAD generation tasks against existing methods,
demonstrating superior interactive capabilities while uniquely providing
editable sketches with geometric annotations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [132] [Patchwork: A Unified Framework for RAG Serving](https://arxiv.org/abs/2505.07833)
*Bodun Hu,Luis Pabon,Saurabh Agarwal,Aditya Akella*

Main category: cs.DC

TL;DR: Patchwork是一个端到端的RAG服务框架，通过灵活配置、分布式优化和动态调度显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统因异构计算管道导致的效率瓶颈问题。

Method: 提供灵活接口、分布式部署和动态调度机制。

Result: 吞吐量提升48%，SLO违规减少24%。

Conclusion: Patchwork在性能和效率上优于现有商业方案。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a new paradigm for
enhancing Large Language Model reliability through integration with external
knowledge sources. However, efficient deployment of these systems presents
significant technical challenges due to their inherently heterogeneous
computational pipelines comprising LLMs, databases, and specialized processing
components. We introduce Patchwork, a comprehensive end-to-end RAG serving
framework designed to address these efficiency bottlenecks. Patchwork's
architecture offers three key innovations: First, it provides a flexible
specification interface enabling users to implement custom RAG pipelines.
Secondly, it deploys these pipelines as distributed inference systems while
optimizing for the unique scalability characteristics of individual RAG
components. Third, Patchwork incorporates an online scheduling mechanism that
continuously monitors request load and execution progress, dynamically
minimizing SLO violations through strategic request prioritization and resource
auto-scaling. Our experimental evaluation across four distinct RAG
implementations demonstrates that Patchwork delivers substantial performance
improvements over commercial alternatives, achieving throughput gains exceeding
48% while simultaneously reducing SLO violations by ~24%.

</details>


### [133] [AI-Based Crypto Tokens: The Illusion of Decentralized AI?](https://arxiv.org/abs/2505.07828)
*Rischan Mafrur*

Main category: cs.DC

TL;DR: 本文综述了基于区块链和人工智能（AI）的AI代币项目，分析了其技术架构、代币功能、共识机制和商业模式，并指出了当前实现中的技术局限性和商业模式问题。


<details>
  <summary>Details</summary>
Motivation: 探索区块链与AI结合的AI代币项目，评估其是否提供超越传统中心化AI服务的价值。

Method: 通过全面审查领先的AI代币项目，分析其技术架构、代币功能、共识机制和商业模式。

Result: 发现当前AI代币项目在技术上依赖链下计算、链上智能能力有限且存在可扩展性问题；商业模式上多为中心化AI服务的简单复制。

Conclusion: 尽管新兴技术为去中心化AI生态系统提供了改进路径，但当前AI代币实现与承诺之间仍存在显著差距，需进一步批判性评估和务实方法。

Abstract: The convergence of blockchain and artificial intelligence (AI) has led to the
emergence of AI-based tokens, which are cryptographic assets designed to power
decentralized AI platforms and services. This paper provides a comprehensive
review of leading AI-token projects, examining their technical architectures,
token utilities, consensus mechanisms, and underlying business models. We
explore how these tokens operate across various blockchain ecosystems and
assess the extent to which they offer value beyond traditional centralized AI
services. Based on this assessment, our analysis identifies several core
limitations. From a technical perspective, many platforms depend extensively on
off-chain computation, exhibit limited capabilities for on-chain intelligence,
and encounter significant scalability challenges. From a business perspective,
many models appear to replicate centralized AI service structures, simply
adding token-based payment and governance layers without delivering truly novel
value. In light of these challenges, we also examine emerging developments that
may shape the next phase of decentralized AI systems. These include approaches
for on-chain verification of AI outputs, blockchain-enabled federated learning,
and more robust incentive frameworks. Collectively, while emerging innovations
offer pathways to strengthen decentralized AI ecosystems, significant gaps
remain between the promises and the realities of current AI-token
implementations. Our findings contribute to a growing body of research at the
intersection of AI and blockchain, highlighting the need for critical
evaluation and more grounded approaches as the field continues to evolve.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [134] [MACH: Multi-Agent Coordination for RSU-centric Handovers](https://arxiv.org/abs/2505.07827)
*Nikolaus Spring,Andrea Morichetta,Boris Sedlak,Schahram Dustdar*

Main category: cs.NI

TL;DR: MACH是一种优化车载计算任务交接的新方法，通过分散决策到路侧单元（RSU）来降低延迟并提高服务质量。


<details>
  <summary>Details</summary>
Motivation: 传统集中式或基于车辆的任务交接方法存在延迟和资源分配不均的问题，MACH旨在通过边缘计算解决这些问题。

Method: MACH将决策权下放到RSU，考虑RSU负载和车辆轨迹等上下文因素，实现任务的高效分配。

Result: 实验表明，MACH在低延迟和高可靠性场景中显著提升了计算效率和适应性，同时优化了资源利用。

Conclusion: MACH通过分散控制到网络边缘，提供了一种高效、可靠的任务交接管理框架。

Abstract: This paper introduces MACH, a novel approach for optimizing task handover in
vehicular computing scenarios. To ensure fast and latency-aware placement of
tasks, the decision-making -- where and when should tasks be offloaded -- is
carried out decentralized at the Road Side Units (RSUs) who also execute the
tasks. By shifting control to the network edge, MACH moves away from the
traditional centralized or vehicle-based handover method. Still, it focuses on
contextual factors, such as the current RSU load and vehicle trajectories.
Thus, MACH improves the overall Quality of Service (QoS) while fairly balancing
computational loads between RSUs. To evaluate the effectiveness of our
approach, we develop a robust simulation environment composed of real-world
traffic data, dynamic network conditions, and different infrastructure
capacities. For scenarios that demand low latency and high reliability, our
experimental results demonstrate how MACH significantly improves the
adaptability and efficiency of vehicular computations. By decentralizing
control to the network edge, MACH effectively reduces communication overhead
and optimizes resource utilization, offering a robust framework for task
handover management.

</details>


### [135] [Intelligent Product 3.0: Decentralised AI Agents and Web3 Intelligence Standards](https://arxiv.org/abs/2505.07835)
*Alex C. Y. Wong,Duncan McFarlane,C. Ellarby,M. Lee,M. Kuok*

Main category: cs.NI

TL;DR: 论文回顾了智能产品的发展历程，从早期Auto-ID项目到区块链、Web3和AI技术的应用，提出了智能产品3.0的新规范。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用去中心化身份、区块链和AI技术提升智能产品的交互能力。

Method: 通过回顾历史发展并结合区块链、Web3和AI技术，提出新规范。

Result: 提出了智能产品3.0的新规范，支持去中心化和AI驱动的无缝交互。

Conclusion: 去中心化和AI技术为智能产品的未来提供了新的可能性。

Abstract: Twenty-five years ago, the specification of the Intelligent Product was
established, envisaging real-time connectivity that not only enables products
to gather accurate data about themselves but also allows them to assess and
influence their own destiny. Early work by the Auto-ID project focused on
creating a single, open-standard repository for storing and retrieving product
information, laying a foundation for scalable connectivity. A decade later, the
approach was revisited in light of low-cost RFID systems that promised a
low-cost link between physical goods and networked information environments.
Since then, advances in blockchain, Web3, and artificial intelligence have
introduced unprecedented levels of resilience, consensus, and autonomy. By
leveraging decentralised identity, blockchain-based product information and
history, and intelligent AI-to-AI collaboration, this paper examines these
developments and outlines a new specification for the Intelligent Product 3.0,
illustrating how decentralised and AI-driven capabilities facilitate seamless
interaction between physical AI and everyday products.

</details>


### [136] [ai.txt: A Domain-Specific Language for Guiding AI Interactions with the Internet](https://arxiv.org/abs/2505.07834)
*Yuekang Li,Wei Song,Bangshuo Zhu,Dong Gong,Yi Liu,Gelei Deng,Chunyang Chen,Lei Ma,Jun Sun,Toby Walsh,Jingling Xue*

Main category: cs.NI

TL;DR: ai.txt是一种新型领域特定语言（DSL），用于规范AI模型与网络内容的交互，弥补了robots.txt的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如robots.txt）缺乏细粒度和语义表达能力，无法确保AI与网络内容交互的伦理和法律合规性。

Method: ai.txt扩展了URL访问控制，支持元素级规则和自然语言指令，并提供开发工具和合规机制（XML和自然语言提示）。

Result: 初步实验和案例研究表明，ai.txt能有效规范AI与网络内容的交互。

Conclusion: ai.txt有助于促进AI与互联网交互的治理，推动负责任AI的使用。

Abstract: We introduce ai.txt, a novel domain-specific language (DSL) designed to
explicitly regulate interactions between AI models, agents, and web content,
addressing critical limitations of the widely adopted robots.txt standard. As
AI increasingly engages with online materials for tasks such as training,
summarization, and content modification, existing regulatory methods lack the
necessary granularity and semantic expressiveness to ensure ethical and legal
compliance. ai.txt extends traditional URL-based access controls by enabling
precise element-level regulations and incorporating natural language
instructions interpretable by AI systems. To facilitate practical deployment,
we provide an integrated development environment with code autocompletion and
automatic XML generation. Furthermore, we propose two compliance mechanisms:
XML-based programmatic enforcement and natural language prompt integration, and
demonstrate their effectiveness through preliminary experiments and case
studies. Our approach aims to aid the governance of AI-Internet interactions,
promoting responsible AI use in digital ecosystems.

</details>


### [137] [Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data](https://arxiv.org/abs/2505.07877)
*Vignesh Ethiraj,Divya Vijay,Sidhanth Menon,Heblin Berscilla*

Main category: cs.NI

TL;DR: 论文通过精细调优TSLAM-Mini模型，结合电信领域专用数据集和QLoRA技术，显著提升了模型在实时电信应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域表现不佳，需针对其特殊需求优化。

Method: 使用NetoAI的DigiTwin平台构建10万样本数据集，结合QLoRA技术对TSLAM-Mini进行调优。

Result: TSLAM-Mini在电信应用中表现优异，验证了领域专用数据集和PEFT方法的有效性。

Conclusion: 领域专用数据集和高效调优技术能显著提升模型在电信等专业领域的性能。

Abstract: General-purpose large language models (LLMs), despite their broad
capabilities accrued from open-world data, frequently exhibit suboptimal
performance when confronted with the nuanced and specialized demands inherent
in real-time telecommunications applications. This investigation addresses this
critical limitation through the meticulous fine-tuning of TSLAM-Mini developed
by NetoAI, a compact (3.8-billion parameter) causal language model
architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen
leverages a bespoke dataset comprising 100,000 samples, strategically
engineered to address 20 pivotal telecommunications use-cases, encompassing
domains such as Network Fundamentals, IP Routing, MPLS, Network Security,
Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical
AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched
with granular insights from venerated network Subject Matter Experts (SMEs) and
authoritative RFC documents, thereby capturing high-fidelity representations of
real-world network dynamics through simulations inspired by digital twin
paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art
Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial
training efficiency and enabled prospective deployment on resource-constrained
hardware. A novel evaluation framework, predicated on a high-capacity LLM
(Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to
rigorously assess instruction-following fidelity and response quality across
the specified telecom use-cases. Empirical results unequivocally demonstrate
TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring
the profound efficacy of domain-specific datasets and PEFT methodologies for
advancing intelligent network management.

</details>


### [138] [Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience](https://arxiv.org/abs/2505.08032)
*Seyed Bagher Hashemi Natanzi,Zhicong Zhu,Bo Tang*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习（DRL）的在线学习框架，用于6G网络中的自适应波束切换，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的自适应波束切换面临高频、移动性和阻塞等挑战，需要更高效的解决方案。

Method: 采用DRL框架，结合增强的状态表示（速度和阻塞历史）、GRU架构和优先经验回放，进行实时波束优化。

Result: 在时间相关阻塞场景下，该方法在SNR、吞吐量和准确性方面显著优于传统启发式方法，且优于多臂老虎机（MAB）基线。

Conclusion: 证明了记忆和优先学习对6G波束管理的有效性，同时确认MAB作为强基线。

Abstract: Adaptive beam switching in 6G networks is challenged by high frequencies,
mobility, and blockage. We propose an Online Learning framework using Deep
Reinforcement Learning (DRL) with an enhanced state representation (velocity
and blockage history), a GRU architecture, and prioritized experience replay
for real-time beam optimization. Validated via Nvidia Sionna under
time-correlated blockage, our approach significantly enhances resilience in
SNR, throughput, and accuracy compared to a conventional heuristic.
Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit
(MAB) baseline by leveraging temporal dependencies, achieving lower performance
variability. This demonstrates the benefits of memory and prioritized learning
for robust 6G beam management, while confirming MAB as a strong baseline.

</details>


### [139] [Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories](https://arxiv.org/abs/2505.08088)
*Rabia Yasa Kostas,Kahraman Kostas*

Main category: cs.NI

TL;DR: 本文提出了一种基于图的Wi-Fi指纹轨迹楼层分离方法，通过Node2Vec和K-means聚类实现高精度楼层定位，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂多层室内环境中垂直定位的挑战，提升室内定位系统的实用性。

Method: 构建Wi-Fi指纹图，利用Node2Vec生成低维嵌入，并通过K-means聚类实现楼层分离。

Result: 在华为大学挑战赛2021数据集上，准确率达68.97%，F1分数61.99%，调整兰德指数57.19%。

Conclusion: 该方法对信号噪声和建筑复杂性具有鲁棒性，为楼层级定位提供了可扩展的解决方案。

Abstract: Indoor positioning systems (IPSs) are increasingly vital for location-based
services in complex multi-storey environments. This study proposes a novel
graph-based approach for floor separation using Wi-Fi fingerprint trajectories,
addressing the challenge of vertical localization in indoor settings. We
construct a graph where nodes represent Wi-Fi fingerprints, and edges are
weighted by signal similarity and contextual transitions. Node2Vec is employed
to generate low-dimensional embeddings, which are subsequently clustered using
K-means to identify distinct floors. Evaluated on the Huawei University
Challenge 2021 dataset, our method outperforms traditional community detection
algorithms, achieving an accuracy of 68.97%, an F1- score of 61.99%, and an
Adjusted Rand Index of 57.19%. By publicly releasing the preprocessed dataset
and implementation code, this work contributes to advancing research in indoor
positioning. The proposed approach demonstrates robustness to signal noise and
architectural complexities, offering a scalable solution for floor-level
localization.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [140] [Justified Evidence Collection for Argument-based AI Fairness Assurance](https://arxiv.org/abs/2505.08064)
*Alpay Sabuncuoglu,Christopher Burr,Carsten Maple*

Main category: cs.HC

TL;DR: 论文提出了一种基于系统工程和软件工具的动态论证保证框架，分两阶段操作，以支持AI系统的公平性治理。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统的公平性是一个复杂的社会技术挑战，需要从需求定义到模型部署的全生命周期持续监督。

Method: 框架分为两阶段：需求规划阶段由多学科团队定义目标和主张；持续监控阶段通过工具动态收集证据支持论证。

Result: 通过金融领域的案例研究，验证了框架在支持公平性相关论证方面的有效性。

Conclusion: 该框架为动态论证保证提供了系统化方法，适用于AI系统的公平性和其他规范性目标。

Abstract: It is well recognised that ensuring fair AI systems is a complex
sociotechnical challenge, which requires careful deliberation and continuous
oversight across all stages of a system's lifecycle, from defining requirements
to model deployment and deprovisioning. Dynamic argument-based assurance cases,
which present structured arguments supported by evidence, have emerged as a
systematic approach to evaluating and mitigating safety risks and hazards in
AI-enabled system development and have also been extended to deal with broader
normative goals such as fairness and explainability. This paper introduces a
systems-engineering-driven framework, supported by software tooling, to
operationalise a dynamic approach to argument-based assurance in two stages. In
the first stage, during the requirements planning phase, a multi-disciplinary
and multi-stakeholder team define goals and claims to be established (and
evidenced) by conducting a comprehensive fairness governance process. In the
second stage, a continuous monitoring interface gathers evidence from existing
artefacts (e.g. metrics from automated tests), such as model, data, and use
case documentation, to support these arguments dynamically. The framework's
effectiveness is demonstrated through an illustrative case study in finance,
with a focus on supporting fairness-related arguments.

</details>


### [141] [A Comparison Between Human and Generative AI Decision-Making Attributes in Complex Health Services](https://arxiv.org/abs/2505.08360)
*Nandini Doreswamy,Louise Horstmanshof*

Main category: cs.HC

TL;DR: 比较人类与生成式AI在复杂健康服务决策中的属性，发现两者互补，合作可能性大于竞争。


<details>
  <summary>Details</summary>
Motivation: 填补文献中关于人类与生成式AI在复杂健康服务决策中属性比较的知识空白，探讨人类是否具有独特或有益属性。

Method: 基于两篇已发表的综述（人类属性的范围综述和生成式AI属性的快速综述），按独特性和影响分类属性，并以表格形式呈现结果。

Result: 人类与生成式AI的决策属性具有互补优势，合作可能性大于竞争。

Conclusion: 人类可通过发展独特属性维持决策角色，未来决策系统可能整合两者贡献，甚至实现融合。

Abstract: A comparison between human and Generative AI decision-making attributes in
complex health services is a knowledge gap in the literature, at present.
Humans may possess unique attributes beneficial to decision-making in complex
health services such as health policy and health regulation, but are also
susceptible to decision-making flaws. The objective is to explore whether
humans have unique, and/or helpful attributes that contribute to optimal
decision-making in complex health services. This comparison may also shed light
on whether humans are likely to compete, cooperate, or converge with Generative
AI. The comparison is based on two published reviews: a scoping review of human
attributes [1] and a rapid review of Generative AI attributes [2]. The analysis
categorizes attributes by uniqueness and impact. The results are presented in
tabular form, comparing the sets and subsets of human and Generative AI
attributes. Humans and Generative AI decision-making attributes have
complementary strengths. Cooperation between these two entities seems more
likely than pure competition. To maintain meaningful decision-making roles,
humans could develop their unique attributes, with decision-making systems
integrating both human and Generative AI contributions. These entities may also
converge, in future.

</details>


### [142] [Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information](https://arxiv.org/abs/2505.08143)
*Jiawei Zhou,Kritika Venkatachalam,Minje Choi,Koustuv Saha,Munmun De Choudhury*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLMs）在健康信息事实核查中的沟通风格与人类专家的差异，发现LLMs在说服策略、确定性表达和社会价值观一致性上得分较低，但人类评估显示对LLM内容的偏好。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在信息辅助中的广泛应用，研究其与人类沟通风格和价值观的一致性至关重要，尤其是在健康信息事实核查这一关键领域。

Method: 通过收集1498条健康错误信息解释，生成LLM响应，并从信息语言特征、发送者说服策略和接收者价值观一致性三个维度评估沟通风格，同时进行99名参与者的盲评。

Result: LLM生成的内容在说服策略、确定性表达和社会价值观一致性上得分较低，但人类评估显示60%以上参与者更偏好LLM内容的清晰性、完整性和说服力。

Conclusion: 尽管在传统质量指标上得分较低，LLMs的结构化信息呈现方式可能更有效地吸引读者。

Abstract: With the wide adoption of large language models (LLMs) in information
assistance, it is essential to examine their alignment with human communication
styles and values. We situate this study within the context of fact-checking
health information, given the critical challenge of rectifying conceptions and
building trust. Recent studies have explored the potential of LLM for health
communication, but style differences between LLMs and human experts and
associated reader perceptions remain under-explored. In this light, our study
evaluates the communication styles of LLMs, focusing on how their explanations
differ from those of humans in three core components of health communication:
information, sender, and receiver. We compiled a dataset of 1498 health
misinformation explanations from authoritative fact-checking organizations and
generated LLM responses to inaccurate health information. Drawing from health
communication theory, we evaluate communication styles across three key
dimensions of information linguistic features, sender persuasive strategies,
and receiver value alignments. We further assessed human perceptions through a
blinded evaluation with 99 participants. Our findings reveal that LLM-generated
articles showed significantly lower scores in persuasive strategies, certainty
expressions, and alignment with social values and moral foundations. However,
human evaluation demonstrated a strong preference for LLM content, with over
60% responses favoring LLM articles for clarity, completeness, and
persuasiveness. Our results suggest that LLMs' structured approach to
presenting information may be more effective at engaging readers despite
scoring lower on traditional measures of quality in fact-checking and health
communication.

</details>


### [143] [VizCV: AI-assisted visualization of researchers' publications tracks](https://arxiv.org/abs/2505.08691)
*Vladimír Lazárik,Marco Agus,Barbora Kozlíková,Pere-Pau Vázquez*

Main category: cs.HC

TL;DR: VizCV是一个基于网络的端到端可视化分析框架，用于交互式探索研究人员的科学轨迹，支持AI辅助分析和自动化职业发展报告。


<details>
  <summary>Details</summary>
Motivation: 评估科学家和研究组的出版记录演变对学术环境管理和职业规划至关重要。

Method: 通过三个维度建模职业发展：研究主题演变、出版记录及影响、合作动态。结合AI技术提供自动化解释和比较分析。

Result: 系统支持多视角探索性分析，包括高影响力文章、新兴研究主题等，并提供AI驱动的见解。

Conclusion: VizCV通过AI/ML技术为职业发展分析提供了创新工具，支持多维度的交互式探索和自动化报告。

Abstract: Analyzing how the publication records of scientists and research groups have
evolved over the years is crucial for assessing their expertise since it can
support the management of academic environments by assisting with career
planning and evaluation. We introduce VizCV, a novel web-based end-to-end
visual analytics framework that enables the interactive exploration of
researchers' scientific trajectories. It incorporates AI-assisted analysis and
supports automated reporting of career evolution. Our system aims to model
career progression through three key dimensions: a) research topic evolution to
detect and visualize shifts in scholarly focus over time, b) publication record
and the corresponding impact, c) collaboration dynamics depicting the growth
and transformation of a researcher's co-authorship network. AI-driven insights
provide automated explanations of career transitions, detecting significant
shifts in research direction, impact surges, or collaboration expansions. The
system also supports comparative analysis between researchers, allowing users
to compare topic trajectories and impact growth. Our interactive, multi-tab and
multiview system allows for the exploratory analysis of career milestones under
different perspectives, such as the most impactful articles, emerging research
themes, or obtaining a detailed analysis of the contribution of the researcher
in a subfield. The key contributions include AI/ML techniques for: a) topic
analysis, b) dimensionality reduction for visualizing patterns and trends, c)
the interactive creation of textual descriptions of facets of data through
configurable prompt generation and large language models, that include key
indicators, to help understanding the career development of individuals or
groups.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [144] [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
*Pranav Narayanan Venkit,Jiayi Li,Yingfan Zhou,Sarah Rajtmajer,Shomir Wilson*

Main category: cs.CL

TL;DR: 论文研究了LLM生成的合成人物在少数族裔身份表征中的问题，揭示了算法他者化现象，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在数据有限领域（如健康、隐私和HCI）中生成合成人物的应用增多，需了解这些叙述如何表征少数族裔身份。

Method: 采用混合方法（细读、词汇分析和参数化创造力框架），比较1512个LLM生成人物与人类撰写内容。

Result: 发现LLM过度强调种族标记、使用文化编码语言，导致刻板印象、异域化等社会技术危害。

Conclusion: 提出基于叙事的评估指标和社区验证协议，以减少算法他者化现象。

Abstract: As LLMs (large language models) are increasingly used to generate synthetic
personas particularly in data-limited domains such as health, privacy, and HCI,
it becomes necessary to understand how these narratives represent identity,
especially that of minority communities. In this paper, we audit synthetic
personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the
lens of representational harm, focusing specifically on racial identity. Using
a mixed methods approach combining close reading, lexical analysis, and a
parameterized creativity framework, we compare 1512 LLM generated personas to
human-authored responses. Our findings reveal that LLMs disproportionately
foreground racial markers, overproduce culturally coded language, and construct
personas that are syntactically elaborate yet narratively reductive. These
patterns result in a range of sociotechnical harms, including stereotyping,
exoticism, erasure, and benevolent bias, that are often obfuscated by
superficially positive narrations. We formalize this phenomenon as algorithmic
othering, where minoritized identities are rendered hypervisible but less
authentic. Based on these findings, we offer design recommendations for
narrative-aware evaluation metrics and community-centered validation protocols
for synthetic identity generation.

</details>


### [145] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
*Yumou Wei,Paulo Carvalho,John Stamper*

Main category: cs.CL

TL;DR: 论文指出，尽管GPT等大型语言模型（LLMs）在AIED领域占据主导地位，但小型语言模型（SLMs）在资源受限的教育机构中具有潜力，且能提供高质量、低成本的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前AIED领域过度关注GPT等资源密集型LLMs，可能忽视了SLMs在提供公平且经济的高质量AI工具方面的潜力。

Method: 通过实验验证SLMs（如Phi-2）在知识组件（KC）发现等关键任务中的有效性，无需复杂提示策略。

Result: SLMs在KC发现任务中表现良好，表明其可作为资源受限机构的可行解决方案。

Conclusion: 呼吁更多关注SLM-based AIED方法的发展，以实现更广泛的教育公平。

Abstract: GPT has become nearly synonymous with large language models (LLMs), an
increasingly popular term in AIED proceedings. A simple keyword-based search
reveals that 61% of the 76 long and short papers presented at AIED 2024
describe novel solutions using LLMs to address some of the long-standing
challenges in education, and 43% specifically mention GPT. Although LLMs
pioneered by GPT create exciting opportunities to strengthen the impact of AI
on education, we argue that the field's predominant focus on GPT and other
resource-intensive LLMs (with more than 10B parameters) risks neglecting the
potential impact that small language models (SLMs) can make in providing
resource-constrained institutions with equitable and affordable access to
high-quality AI tools. Supported by positive results on knowledge component
(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as
Phi-2 can produce an effective solution without elaborate prompting strategies.
Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [146] [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
*Michael Pichat,William Pogrund,Paloma Pichat,Judicael Poumay,Armanouche Gasparian,Samuel Demarchi,Martin Corbet,Alois Georgeon,Michael Veillet-Guillem*

Main category: cs.CL

TL;DR: 论文提出了一种几何方法，将神经元定义为具有非正交基的分类向量空间，通过神经元内注意力过程识别关键分类区域，以提高语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 理解人工神经网络中多义神经元的本质，并提出一种替代方法以优化语言模型的性能。

Method: 将神经元定义为分类向量空间，利用非正交基和神经元内注意力过程识别关键分类区域。

Result: 该方法能够识别并利用更均匀且位于不同分类子维度交叉处的关键区域，从而提高语言模型的效率。

Conclusion: 提出的几何方法为理解多义神经元提供了新视角，并展示了其在优化语言模型中的潜力。

Abstract: The polysemantic nature of synthetic neurons in artificial intelligence
language models is currently understood as the result of a necessary
superposition of distributed features within the latent space. We propose an
alternative approach, geometrically defining a neuron in layer n as a
categorical vector space with a non-orthogonal basis, composed of categorical
sub-dimensions extracted from preceding neurons in layer n-1. This categorical
vector space is structured by the activation space of each neuron and enables,
via an intra-neuronal attention process, the identification and utilization of
a critical categorical zone for the efficiency of the language model - more
homogeneous and located at the intersection of these different categorical
sub-dimensions.

</details>


### [147] [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
*Ali Senol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 提出了一种两阶段检测框架，结合集成分类模型和概念漂移分析，用于实时检测数字通信平台中的虚假交互，提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 数字通信平台中的虚假交互问题尚未得到充分解决，传统静态异常检测方法难以适应动态对话变化，导致误报或漏报。

Method: 使用集成分类模型初步识别可疑对话，再通过概念漂移分析（OCDD）和大型语言模型（LLM）判断是否为欺诈行为。

Result: 在社交工程聊天场景数据集上验证了框架的实用性，显著提升了检测准确性和可解释性。

Conclusion: 提出的模块化方法优于双LLM基线，为实时欺诈检测提供了更可靠的解决方案。

Abstract: Detecting fake interactions in digital communication platforms remains a
challenging and insufficiently addressed problem. These interactions may appear
as harmless spam or escalate into sophisticated scam attempts, making it
difficult to flag malicious intent early. Traditional detection methods often
rely on static anomaly detection techniques that fail to adapt to dynamic
conversational shifts. One key limitation is the misinterpretation of benign
topic transitions referred to as concept drift as fraudulent behavior, leading
to either false alarms or missed threats. We propose a two stage detection
framework that first identifies suspicious conversations using a tailored
ensemble classification model. To improve the reliability of detection, we
incorporate a concept drift analysis step using a One Class Drift Detector
(OCDD) to isolate conversational shifts within flagged dialogues. When drift is
detected, a large language model (LLM) assesses whether the shift indicates
fraudulent manipulation or a legitimate topic change. In cases where no drift
is found, the behavior is inferred to be spam like. We validate our framework
using a dataset of social engineering chat scenarios and demonstrate its
practical advantages in improving both accuracy and interpretability for real
time fraud detection. To contextualize the trade offs, we compare our modular
approach against a Dual LLM baseline that performs detection and judgment using
different language models.

</details>


### [148] [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
*Hao Zhen,Jidong J. Yang*

Main category: cs.CL

TL;DR: CrashSage是一个基于大型语言模型（LLM）的框架，通过四项创新提升交通事故分析能力，包括数据转换、增强、模型微调和可解释性技术。


<details>
  <summary>Details</summary>
Motivation: 全球每年因交通事故造成巨大生命和经济损失，现有方法难以捕捉复杂关系和语义信息，亟需更有效的分析工具。

Method: 提出CrashSage框架，包括数据转换、上下文增强、LLaMA3-8B模型微调和梯度解释技术。

Result: CrashSage在事故严重性推断上表现优于基线方法，并提供可解释的模型决策。

Conclusion: CrashSage通过LLM技术显著提升交通事故分析的准确性和可解释性，为道路安全干预提供支持。

Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global
economic losses exceeding \$1.8 trillion. Such profound societal and financial
impacts underscore the urgent need for road safety research that uncovers crash
mechanisms and delivers actionable insights. Conventional statistical models
and tree ensemble approaches typically rely on structured crash data,
overlooking contextual nuances and struggling to capture complex relationships
and underlying semantics. Moreover, these approaches tend to incur significant
information loss, particularly in narrative elements related to multi-vehicle
interactions, crash progression, and rare event characteristics. This study
presents CrashSage, a novel Large Language Model (LLM)-centered framework
designed to advance crash analysis and modeling through four key innovations.
First, we introduce a tabular-to-text transformation strategy paired with
relational data integration schema, enabling the conversion of raw,
heterogeneous crash data into enriched, structured textual narratives that
retain essential structural and relational context. Second, we apply
context-aware data augmentation using a base LLM model to improve narrative
coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B
model for crash severity inference, demonstrating superior performance over
baseline approaches, including zero-shot, zero-shot with chain-of-thought
prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,
LLaMA3-70B). Finally, we employ a gradient-based explainability technique to
elucidate model decisions at both the individual crash level and across broader
risk factor dimensions. This interpretability mechanism enhances transparency
and enables targeted road safety interventions by providing deeper insights
into the most influential factors.

</details>


### [149] [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
*Paweł Walkowiak,Marek Klonowski,Marcin Oleksy,Arkadiusz Janz*

Main category: cs.CL

TL;DR: 该论文研究了对抗性攻击在屈折语中的表现，提出了一种基于Edge Attribution Patching（EAP）的新评估协议，并在波兰语和英语上进行了实验。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性攻击方法主要在非屈折语（如英语）中开发和评估，缺乏对屈折语的研究。本文旨在填补这一空白。

Method: 设计了基于EAP的新评估协议，使用波兰语和英语的平行语料库，创建了基于MultiEmo数据集的新基准。

Result: 分析了模型在屈折语中的行为，揭示了屈折变化与对抗鲁棒性之间的关系。

Conclusion: 通过新协议和基准，论文为屈折语中的对抗性攻击研究提供了工具和见解。

Abstract: Various techniques are used in the generation of adversarial examples,
including methods such as TextBugger which introduce minor, hardly visible
perturbations to words leading to changes in model behaviour. Another class of
techniques involves substituting words with their synonyms in a way that
preserves the text's meaning but alters its predicted class, with TextFooler
being a prominent example of such attacks. Most adversarial example generation
methods are developed and evaluated primarily on non-inflectional languages,
typically English. In this work, we evaluate and explain how adversarial
attacks perform in inflectional languages. To explain the impact of inflection
on model behaviour and its robustness under attack, we designed a novel
protocol inspired by mechanistic interpretability, based on Edge Attribution
Patching (EAP) method. The proposed evaluation protocol relies on parallel
task-specific corpora that include both inflected and syncretic variants of
texts in two languages -- Polish and English. To analyse the models and explain
the relationship between inflection and adversarial robustness, we create a new
benchmark based on task-oriented dataset MultiEmo, enabling the identification
of mechanistic inflection-related elements of circuits within the model and
analyse their behaviour under attack.

</details>


### [150] [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
*Faiza Hassan,Summra Saleem,Kashif Javed,Muhammad Nabeel Asim,Abdur Rehman,Andreas Dengel*

Main category: cs.CL

TL;DR: 该论文提出了一种基于对比学习的意图检测方法LLMPIA，专门针对乌尔都语，利用未标记数据和预训练语言模型，结合原型注意力机制，显著提升了意图检测性能。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为第十大语言，其意图检测领域缺乏少样本学习策略，传统方法仅能预测训练集中见过的类别。本文旨在填补这一空白。

Method: 采用对比学习策略，利用未标记乌尔都语数据重新训练预训练语言模型，并结合原型注意力机制构建LLMPIA意图检测框架。

Result: 在ATIS和Web Queries数据集上，LLMPIA在少样本设置下分别达到83.28%/98.25%和76.23%/84.42%的F1分数，并在相同类别测试中超越现有方法53.55%。

Conclusion: LLMPIA框架为乌尔都语意图检测提供了高效解决方案，展示了预训练模型与对比学习的潜力。

Abstract: Multifarious intent detection predictors are developed for different
languages, including English, Chinese and French, however, the field remains
underdeveloped for Urdu, the 10th most spoken language. In the realm of
well-known languages, intent detection predictors utilize the strategy of
few-shot learning and prediction of unseen classes based on the model training
on seen classes. However, Urdu language lacks few-shot strategy based intent
detection predictors and traditional predictors are focused on prediction of
the same classes which models have seen in the train set. To empower Urdu
language specific intent detection, this introduces a unique contrastive
learning approach that leverages unlabeled Urdu data to re-train pre-trained
language models. This re-training empowers LLMs representation learning for the
downstream intent detection task. Finally, it reaps the combined potential of
pre-trained LLMs and the prototype-informed attention mechanism to create a
comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm
of proposed predictive pipeline, it explores the potential of 6 distinct
language models and 13 distinct similarity computation methods. The proposed
framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing
5836 samples and Web Queries having 8519 samples. Across ATIS dataset under
4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and
98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,
respectively. In an additional case study on the Web Queries dataset under same
classes train and test set settings, LLMPIA outperformed state-of-the-art
predictor by 53.55% F1-Score.

</details>


### [151] [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
*Siyuan Yan,Mo Zhu,Guo-qing Jiang,Jianfei Wang,Jiaxing Chen,Wentai Zhang,Xiang Liao,Xiao Cui,Chen Zhang,Zhuoran Song,Ran Zhu*

Main category: cs.CL

TL;DR: 本文研究了通过密集LLM架构的推测解码技术，提出了对数线性缩放定律，并开发了Scylla系统，显著提高了解码效率和推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理密集型架构中需要高效解码，但现有方法在解码效率的缩放规律上研究不足。

Method: 通过研究预训练令牌量、草稿模型容量和解码批量大小三个维度，提出了对数线性缩放定律，并开发了Scylla系统。

Result: Scylla在解码速度和任务性能上优于现有方法，工业部署中实现了2倍的解码吞吐量提升。

Conclusion: 系统性缩放对高效LLM推理具有变革潜力，Scylla为未来研究提供了重要基础。

Abstract: The escalating demand for efficient decoding in large language models (LLMs)
is particularly critical for reasoning-intensive architectures like OpenAI-o3
and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This
study investigates speculative decoding techniques through dense LLM
architectures to establish foundational insights for accelerating reasoning
tasks. While speculative decoding methods leveraging parallel
draft-verification cycles have emerged as promising acceleration techniques,
the scaling laws governing decoding efficiency remain under-explored compared
to conventional backbone LLMs developed through Pretraining->SFT->RLHF training
paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2
and 1.3) governing draft model acceptance rate (or decoding speed) across three
dimensions: pretraining token volume, draft model capacity, and decoding batch
size. Building on these laws, we achieve Scylla, which coordinates
multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical
validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and
0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on
summarization and QA tasks (Figure 2). Industrial inference engine deployments
demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),
validating the transformative potential of systematic scaling for efficient LLM
inference. Code will be released later.

</details>


### [152] [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
*Daniel Franzen,Jan Disselhoff,David Hartmann*

Main category: cs.CL

TL;DR: 本文提出了一种通过任务特定数据增强和深度优先搜索算法提升LLMs在ARC-AGI任务中表现的方法，实现了71.6%的得分，并强调了透明性和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在抽象推理任务（如ARC-AGI）中的局限性。

Method: 在训练、生成和评分阶段使用任务特定数据增强，结合深度优先搜索算法生成多样候选解，并利用LLM作为生成器和评分器。

Result: 在ARC-AGI公开评估集上获得71.6%的得分（286.5/400任务），表现优于现有公开方法。

Conclusion: 该方法在透明性、可复现性和低成本方面具有优势，尽管闭源方法得分更高。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge
for large language models (LLMs), exposing limitations in their abstract
reasoning abilities. In this work, we leverage task-specific data augmentations
throughout the training, generation, and scoring phases, and employ a
depth-first search algorithm to generate diverse, high-probability candidate
solutions. Furthermore, we utilize the LLM not only as a generator but also as
a scorer, using its output probabilities to select the most promising
solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the
public ARC-AGI evaluation set, demonstrating state-of-the-art performance among
publicly available approaches. While concurrent closed-source work has reported
higher scores, our method distinguishes itself through its transparency,
reproducibility, and remarkably low inference cost, averaging only around 2ct
per task on readily available hardware (we assume a price of 36ct/hour for a
Nvidia 4090 GPU).

</details>


### [153] [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
*Harry Dong,Bilge Acun,Beidi Chen,Yuejie Chi*

Main category: cs.CL

TL;DR: Caprese是一种低成本蒸馏方法，用于恢复因高效推理方法部署而丢失的数学能力，同时不影响语言任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的数学推理需要大量计算资源和时间，现有高效推理方法在语言任务上表现良好，但会严重降低数学性能。

Method: 提出Caprese方法，通过仅增加约1%的参数和20K合成训练样本，恢复因高效推理而丢失的数学能力，同时不干扰原始权重。

Result: Caprese显著减少了活跃参数数量（如Gemma 2 9B和Llama 3.1 8B减少约2B），并降低了延迟（如Qwen 2.5 14B生成2048个令牌时延迟减少11%）。

Conclusion: Caprese是一种高效且低成本的方法，能够在保持语言任务性能的同时恢复数学推理能力，并显著减少计算资源消耗。

Abstract: Due to long generations, large language model (LLM) math reasoning demands
significant computational resources and time. While many existing efficient
inference methods have been developed with excellent performance preservation
on language tasks, they often severely degrade math performance. In this paper,
we propose Caprese, a low-cost distillation method to recover lost capabilities
from deploying efficient inference methods, focused primarily in feedforward
blocks. With original weights unperturbed, roughly 1% of additional parameters,
and only 20K synthetic training samples, we are able to recover much if not all
of the math capabilities lost from efficient inference for thinking LLMs and
without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the
number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and
integrates cleanly into existing model layers to reduce latency (>11% reduction
to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.

</details>


### [154] [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
*Suavis Giramata,Madhusudan Srinivasan,Venkat Naidu Gudivada,Upulee Kanewala*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子多样性的蜕变关系（MRs）优先级排序方法，用于高效检测大语言模型（LLMs）中的公平性问题。实验表明，该方法在故障检测率和首次故障时间上优于随机和距离排序方法，且计算成本显著低于基于故障的排序。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，其输出中的公平性和潜在偏见问题日益突出。由于测试用例数量庞大，如何高效检测公平性问题成为关键挑战。

Method: 采用基于句子多样性的方法计算和排序蜕变关系（MRs），以优化故障检测效率。

Result: 实验结果显示，该方法比随机排序和距离排序分别提高了22%和12%的故障检测率，并缩短了首次故障时间15%和8%。同时，其效果接近基于故障的排序，但计算成本显著降低。

Conclusion: 多样性为基础的MR优先级排序方法能有效提升LLMs的公平性测试效果，同时减少计算开销。

Abstract: Large Language Models (LLMs) are increasingly deployed in various
applications, raising critical concerns about fairness and potential biases in
their outputs. This paper explores the prioritization of metamorphic relations
(MRs) in metamorphic testing as a strategy to efficiently detect fairness
issues within LLMs. Given the exponential growth of possible test cases,
exhaustive testing is impractical; therefore, prioritizing MRs based on their
effectiveness in detecting fairness violations is crucial. We apply a sentence
diversity-based approach to compute and rank MRs to optimize fault detection.
Experimental results demonstrate that our proposed prioritization approach
improves fault detection rates by 22% compared to random prioritization and 12%
compared to distance-based prioritization, while reducing the time to the first
failure by 15% and 8%, respectively. Furthermore, our approach performs within
5% of fault-based prioritization in effectiveness, while significantly reducing
the computational cost associated with fault labeling. These results validate
the effectiveness of diversity-based MR prioritization in enhancing fairness
testing for LLMs.

</details>


### [155] [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
*A M Muntasir Rahman,Ajim Uddin,Guiling "Grace" Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为AIAP的新型评估提示方法，通过整合人类标注者的详细任务指令，改进金融情感分析（FSA）的评估标准，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析（FSA）因金融语境的语言复杂性而更具挑战性，现有基准数据集的主观性导致LLM评估不公平。

Method: 提出Annotators' Instruction Assisted Prompt（AIAP），将人类标注者的任务指令融入LLM提示框架，标准化情感理解。

Result: 实验显示AIAP显著提升LLM性能（最高提升9.08），并引入基于模型置信度的情感索引方法，改进股价预测。

Conclusion: AIAP通过优化任务定义和评估方法，提升了FSA的准确性和实用性，凸显了WSB作为金融文本来源的重要性。

Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that
surpass those in typical sentiment analysis due to the nuanced language used in
financial contexts. The prowess of these models is often undermined by the
inherent subjectivity of sentiment classifications in existing benchmark
datasets like Financial Phrasebank. These datasets typically feature undefined
sentiment classes that reflect the highly individualized perspectives of
annotators, leading to significant variability in annotations. This variability
results in an unfair expectation for LLMs during benchmarking, where they are
tasked to conjecture the subjective viewpoints of human annotators without
sufficient context. In this paper, we introduce the Annotators' Instruction
Assisted Prompt, a novel evaluation prompt designed to redefine the task
definition of FSA for LLMs. By integrating detailed task instructions
originally intended for human annotators into the LLMs' prompt framework, AIAP
aims to standardize the understanding of sentiment across both human and
machine interpretations, providing a fair and context-rich foundation for
sentiment analysis. We utilize a new dataset, WSBS, derived from the
WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM
performance by aligning machine operations with the refined task definitions.
Experimental results demonstrate that AIAP enhances LLM performance
significantly, with improvements up to 9.08. This context-aware approach not
only yields incremental gains in performance but also introduces an innovative
sentiment-indexing method utilizing model confidence scores. This method
enhances stock price prediction models and extracts more value from the
financial sentiment analysis, underscoring the significance of WSB as a
critical source of financial text. Our research offers insights into both
improving FSA through better evaluation methods.

</details>


### [156] [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 论文探讨了如何从LLM嵌入中恢复符合概率论公理的相干事件概率，并通过扩展VAE在潜在空间中强制概率论公理约束，实验表明该方法能显著提高概率的相干性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的事件概率存在不连贯性，违反概率论公理，因此研究是否可以从嵌入中恢复相干概率以提高不确定性事件的估计准确性。

Method: 提出在扩展VAE的潜在空间中强制概率论公理约束（如加法规则），使事件概率自然地从嵌入中恢复。

Result: 实验表明，从嵌入中恢复的概率比LLM直接报告的概率更连贯，且更接近真实概率。

Conclusion: 通过强制潜在空间中的概率论公理约束，可以从LLM嵌入中恢复更准确的相干事件概率。

Abstract: Rational decision-making under uncertainty requires coherent degrees of
belief in events. However, event probabilities generated by Large Language
Models (LLMs) have been shown to exhibit incoherence, violating the axioms of
probability theory. This raises the question of whether coherent event
probabilities can be recovered from the embeddings used by the models. If so,
those derived probabilities could be used as more accurate estimates in events
involving uncertainty. To explore this question, we propose enforcing axiomatic
constraints, such as the additive rule of probability theory, in the latent
space learned by an extended variational autoencoder (VAE) applied to LLM
embeddings. This approach enables event probabilities to naturally emerge in
the latent space as the VAE learns to both reconstruct the original embeddings
and predict the embeddings of semantically related events. We evaluate our
method on complementary events (i.e., event A and its complement, event not-A),
where the true probabilities of the two events must sum to 1. Experiment
results on open-weight language models demonstrate that probabilities recovered
from embeddings exhibit greater coherence than those directly reported by the
corresponding models and align closely with the true probabilities.

</details>


### [157] [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
*Chun-Pai Yang,Kan Zheng,Shou-De Lin*

Main category: cs.CL

TL;DR: PLHF是一种基于人类反馈的少样本提示优化框架，通过特定评估模块解决输出质量难以量化的问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理输出质量无法通过标准样本直接评估的任务，因此需要一种无需明确指标的优化方法。

Method: PLHF采用类似RLHF的技术，通过单轮人类反馈和评估模块优化提示。

Result: 在公开和工业数据集上，PLHF优于先前的输出评分策略。

Conclusion: PLHF为提示优化提供了一种高效且无需明确指标的解决方案。

Abstract: Automatic prompt optimization frameworks are developed to obtain suitable
prompts for large language models (LLMs) with respect to desired output quality
metrics. Although existing approaches can handle conventional tasks such as
fixed-solution question answering, defining the metric becomes complicated when
the output quality cannot be easily assessed by comparisons with standard
golden samples. Consequently, optimizing the prompts effectively and
efficiently without a clear metric becomes a critical challenge. To address the
issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman
"F"eedback), a few-shot prompt optimization framework inspired by the
well-known RLHF technique. Different from naive strategies, PLHF employs a
specific evaluator module acting as the metric to estimate the output quality.
PLHF requires only a single round of human feedback to complete the entire
prompt optimization process. Empirical results on both public and industrial
datasets show that PLHF outperforms prior output grading strategies for LLM
prompt optimizations.

</details>


### [158] [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
*Yusen Wu,Xiaotie Deng*

Main category: cs.CL

TL;DR: 论文提出了一种基于零样本学习的分层框架ZeroStylus，用于长文本风格迁移，结合句子级风格适应和段落级结构连贯性，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决长文本风格迁移中保持句法和语义一致性的挑战，避免平行语料库或LLM微调的需求。

Method: 提出ZeroStylus框架，分两阶段：从参考文本获取分层模板，并通过多粒度匹配进行模板引导生成。

Result: 实验显示，在风格一致性、内容保留和表达质量的三轴指标中，平均得分6.90，优于直接提示方法的6.70。

Conclusion: ZeroStylus通过分层模板实现了无需平行语料库或微调的长文本风格迁移，验证了其有效性。

Abstract: This paper addresses the challenge in long-text style transfer using
zero-shot learning of large language models (LLMs), proposing a hierarchical
framework that combines sentence-level stylistic adaptation with
paragraph-level structural coherence. We argue that in the process of effective
paragraph-style transfer, to preserve the consistency of original syntactic and
semantic information, it is essential to perform style transfer not only at the
sentence level but also to incorporate paragraph-level semantic considerations,
while ensuring structural coherence across inter-sentential relationships. Our
proposed framework, ZeroStylus, operates through two systematic phases:
hierarchical template acquisition from reference texts and template-guided
generation with multi-granular matching. The framework dynamically constructs
sentence and paragraph template repositories, enabling context-aware
transformations while preserving inter-sentence logical relationships.
Experimental evaluations demonstrate significant improvements over baseline
methods, with structured rewriting achieving 6.90 average score compared to
6.70 for direct prompting approaches in tri-axial metrics assessing style
consistency, content preservation, and expression quality. Ablation studies
validate the necessity of both template hierarchies during style transfer,
showing higher content preservation win rate against sentence-only approaches
through paragraph-level structural encoding, as well as direct prompting method
through sentence-level pattern extraction and matching. The results establish
new capabilities for coherent long-text style transfer without requiring
parallel corpora or LLM fine-tuning.

</details>


### [159] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
*Ching Nam Hang,Pei-Duo Yu,Chee Wei Tan*

Main category: cs.CL

TL;DR: TrumorGPT是一种基于生成式AI的健康领域事实核查工具，旨在区分真实健康谣言（trumors），利用LLM和图增强生成技术（GraphRAG）提升准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代，健康相关谣言的快速传播对社会构成威胁，亟需高效的事实核查工具。

Method: 结合LLM的少样本学习和语义健康知识图谱构建，采用GraphRAG技术动态更新数据以减少LLM的幻觉问题。

Result: 在公共卫生声明的事实核查中表现优异，显著提升信息准确性和信任度。

Conclusion: TrumorGPT为对抗健康谣言提供了创新解决方案，推动了数字信息时代的信任建设。

Abstract: In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT , a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [160] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
*Stefano Rando,Luca Romani,Alessio Sampieri,Yuta Kyuragi,Luca Franco,Fabio Galasso,Tatsunori Hashimoto,John Yang*

Main category: cs.CL

TL;DR: 论文介绍了LongCodeBench（LCB），一个用于测试长上下文模型中代码理解和修复能力的基准测试，基于真实GitHub问题构建任务。


<details>
  <summary>Details</summary>
Motivation: 现代长上下文模型的上下文长度快速增长，但缺乏现实的基准测试，尤其是在需要大量上下文的场景中。

Method: 通过从GitHub问题中构建QA和bug修复任务，创建LongCodeBench，并分层测试不同复杂度的任务。

Result: 所有模型在长上下文任务中表现下降，例如Claude 3.5 Sonnet从29%降至3%，Qwen2.5从70.2%降至40%。

Conclusion: 长上下文仍是模型的弱点，需要进一步改进。

Abstract: Context lengths for models have grown rapidly, from thousands to millions of
tokens in just a few years. The extreme context sizes of modern long-context
models have made it difficult to construct realistic long-context benchmarks --
not only due to the cost of collecting million-context tasks but also in
identifying realistic scenarios that require significant contexts. We identify
code comprehension and repair as a natural testbed and challenge task for
long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM
coding abilities in long-context scenarios. Our benchmark tests both the
comprehension and repair capabilities of LCLMs in realistic and important
settings by drawing from real-world GitHub issues and constructing QA
(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the
complexity of our benchmark, enabling us to evaluate models across different
scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.
We find that long-context remains a weakness for all models, with performance
drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for
Qwen2.5.

</details>


### [161] [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
*Ding Cao,Yuchen Cai,Rongxi Guo,Xuesong He,Guiquan Liu*

Main category: cs.CL

TL;DR: DeltaEdit通过动态正交约束策略优化更新参数，显著提高了长期序列知识编辑的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列知识编辑方法在长期编辑后成功率显著下降的问题。

Method: 提出DeltaEdit方法，采用动态正交约束策略减少编辑间的干扰。

Result: DeltaEdit在编辑成功率和泛化能力保留上显著优于现有方法。

Conclusion: DeltaEdit能确保模型在长期序列编辑下保持稳定可靠的性能。

Abstract: Sequential knowledge editing techniques aim to continuously update the
knowledge in large language models at a low cost, preventing the models from
generating outdated or incorrect information. However, existing sequential
editing methods suffer from a significant decline in editing success rates
after long-term editing. Through theoretical analysis and experiments, we
identify that as the number of edits increases, the model's output increasingly
deviates from the desired target, leading to a drop in editing success rates.
We refer to this issue as the accumulation of superimposed noise problem. To
address this, we identify the factors contributing to this deviation and
propose DeltaEdit, a novel method that optimizes update parameters through a
dynamic orthogonal constraints strategy, effectively reducing interference
between edits to mitigate deviation. Experimental results demonstrate that
DeltaEdit significantly outperforms existing methods in edit success rates and
the retention of generalization capabilities, ensuring stable and reliable
model performance even under extensive sequential editing.

</details>


### [162] [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
*Zeyang Sha,Shiwen Cui,Weiqiang Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为SEM的后训练强化学习框架，旨在优化大型语言模型（LLMs）在调用搜索引擎时的决策能力，减少冗余搜索行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在调用外部工具（如搜索引擎）时，难以区分何时需要搜索、何时依赖内部知识，导致冗余搜索和效率低下。

Method: 通过结合MuSiQue和MMLU构建平衡数据集，设计结构化推理模板，并采用Group Relative Policy Optimization（GRPO）进行后训练，优化模型的搜索行为。

Result: 实验结果表明，该方法显著减少了冗余搜索操作，同时在多个基准测试中保持或提高了答案准确性。

Conclusion: SEM框架提升了模型的推理效率，并扩展了其明智利用外部知识的能力。

Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their
capabilities not only in reasoning but also in invoking external tools,
particularly search engines. However, teaching models to discern when to invoke
search and when to rely on their internal knowledge remains a significant
challenge. Existing reinforcement learning approaches often lead to redundant
search behaviors, resulting in inefficiencies and over-cost. In this paper, we
propose SEM, a novel post-training reinforcement learning framework that
explicitly trains LLMs to optimize search usage. By constructing a balanced
dataset combining MuSiQue and MMLU, we create scenarios where the model must
learn to distinguish between questions it can answer directly and those
requiring external retrieval. We design a structured reasoning template and
employ Group Relative Policy Optimization(GRPO) to post-train the model's
search behaviors. Our reward function encourages accurate answering without
unnecessary search while promoting effective retrieval when needed.
Experimental results demonstrate that our method significantly reduces
redundant search operations while maintaining or improving answer accuracy
across multiple challenging benchmarks. This framework advances the model's
reasoning efficiency and extends its capability to judiciously leverage
external knowledge.

</details>


### [163] [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
*Daoze Zhang,Zhijian Bao,Sihang Du,Zhiyi Zhao,Kuangling Zhang,Dezheng Bao,Yang Yang*

Main category: cs.CL

TL;DR: 论文提出了一个名为Re^2的大规模一致性保障的同行评审和反驳数据集，以解决现有数据集在多样性、数据质量和交互支持方面的不足。


<details>
  <summary>Details</summary>
Motivation: 同行评审系统因投稿量激增和重复提交低质量稿件而负担过重，缺乏有效的作者自我评估工具。LLMs在辅助评审和作者方面潜力巨大，但受限于现有数据集的质量。

Method: 构建了包含19,926份初始投稿、70,668条评审意见和53,818条反驳的Re^2数据集，覆盖24个会议和21个研讨会，并将反驳阶段建模为多轮对话范式。

Result: Re^2数据集支持静态评审任务和动态交互式LLM助手，为作者提供实用指导，有望减轻评审负担。

Conclusion: Re^2数据集填补了现有数据集的不足，为LLM辅助评审和作者提供了更高质量的数据支持，有助于缓解评审系统的压力。

Abstract: Peer review is a critical component of scientific progress in the fields like
AI, but the rapid increase in submission volume has strained the reviewing
system, which inevitably leads to reviewer shortages and declines review
quality. Besides the growing research popularity, another key factor in this
overload is the repeated resubmission of substandard manuscripts, largely due
to the lack of effective tools for authors to self-evaluate their work before
submission. Large Language Models (LLMs) show great promise in assisting both
authors and reviewers, and their performance is fundamentally limited by the
quality of the peer review data. However, existing peer review datasets face
three major limitations: (1) limited data diversity, (2) inconsistent and
low-quality data due to the use of revised rather than initial submissions, and
(3) insufficient support for tasks involving rebuttal and reviewer-author
interactions. To address these challenges, we introduce the largest
consistency-ensured peer review and rebuttal dataset named Re^2, which
comprises 19,926 initial submissions, 70,668 review comments, and 53,818
rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the
rebuttal and discussion stage is framed as a multi-turn conversation paradigm
to support both traditional static review tasks and dynamic interactive LLM
assistants, providing more practical guidance for authors to refine their
manuscripts and helping alleviate the growing review burden. Our data and code
are available in https://anonymous.4open.science/r/ReviewBench_anon/.

</details>


### [164] [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
*Haneh Rhel,Dmitri Roussinov*

Main category: cs.CL

TL;DR: 论文概述了大型语言模型（LLMs）在阿拉伯语自然语言处理（NLP）中的应用，强调其成功处理复杂阿拉伯语任务的能力，并探讨了微调和提示工程等技术对模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语资源稀缺且语言复杂，研究旨在展示LLMs在阿拉伯语NLP任务中的潜力。

Method: 回顾早期预训练阿拉伯语模型，分析微调和提示工程技术，总结常用基准和数据集。

Result: LLMs在多语言语料库训练下在阿拉伯语NLP任务中表现优异，技术改进进一步提升性能。

Conclusion: LLMs在阿拉伯语NLP中的应用呈上升趋势，未来需更多资源和工具支持。

Abstract: Over the past three years, the rapid advancement of Large Language Models
(LLMs) has had a profound impact on multiple areas of Artificial Intelligence
(AI), particularly in Natural Language Processing (NLP) across diverse
languages, including Arabic. Although Arabic is considered one of the most
widely spoken languages across 27 countries in the Arabic world and used as a
second language in some other non-Arabic countries as well, there is still a
scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face
various challenges due to the complexities of the Arabic language, including
its rich morphology, intricate structure, and diverse writing standards, among
other factors. Researchers have been actively addressing these challenges,
demonstrating that pre-trained Large Language Models (LLMs) trained on
multilingual corpora achieve significant success in various Arabic NLP tasks.
This study provides an overview of using large language models (LLMs) for the
Arabic language, highlighting early pre-trained Arabic Language models across
various NLP applications and their ability to handle diverse Arabic content
tasks and dialects. It also provides an overview of how techniques like
finetuning and prompt engineering can enhance the performance of these models.
Additionally, the study summarizes common Arabic benchmarks and datasets while
presenting our observations on the persistent upward trend in the adoption of
LLMs.

</details>


### [165] [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
*Zhehao Zhang,Weijie Xu,Fanyou Wu,Chandan K. Reddy*

Main category: cs.CL

TL;DR: FalseReject是一个资源，包含16k看似有毒的查询和结构化响应，旨在减少LLMs对良性查询的过度拒绝。通过图引导的多智能体交互框架生成多样化提示，并通过监督微调显著减少不必要的拒绝。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在安全对齐过程中对良性查询过度拒绝的问题，提升模型在敏感场景中的实用性。

Method: 提出FalseReject资源，包含16k查询和44类结构化响应；采用图引导的多智能体交互框架生成多样化提示；通过监督微调优化模型。

Result: 在29个SOTA LLMs上的实验表明，FalseReject显著减少不必要的拒绝，同时保持安全性和语言能力。

Conclusion: FalseReject有效解决了LLMs的过度拒绝问题，提升了模型的实用性和安全性。

Abstract: Safety alignment approaches in large language models (LLMs) often lead to the
over-refusal of benign queries, significantly diminishing their utility in
sensitive scenarios. To address this challenge, we introduce FalseReject, a
comprehensive resource containing 16k seemingly toxic queries accompanied by
structured responses across 44 safety-related categories. We propose a
graph-informed adversarial multi-agent interaction framework to generate
diverse and complex prompts, while structuring responses with explicit
reasoning to aid models in accurately distinguishing safe from unsafe contexts.
FalseReject includes training datasets tailored for both standard
instruction-tuned models and reasoning-oriented models, as well as a
human-annotated benchmark test set. Our extensive benchmarking on 29
state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.
Empirical results demonstrate that supervised finetuning with FalseReject
substantially reduces unnecessary refusals without compromising overall safety
or general language capabilities.

</details>


### [166] [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Ahmed Masry,Mizanur Rahman,Amran Bhuiyan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 本文评估了13种开源大型视觉语言模型（LVLM）作为图表理解任务的自动评估工具，发现部分模型性能接近GPT-4，但存在位置偏好和长度偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有评估方法成本高且耗时，限制了大型视觉语言模型在工业场景中的应用，因此需要探索低成本的开源模型作为替代评估工具。

Method: 设计了成对和点式评估任务，涵盖事实准确性、信息量和相关性等标准，并分析了格式遵循、位置一致性和指令遵循等指标。

Result: 实验结果显示，部分开源LVLM评估性能接近GPT-4（约80%一致），但其他模型表现较差（低于10%一致），且存在位置偏好和长度偏差。

Conclusion: 开源LVLM可作为图表任务的低成本自动评估工具，但需注意其潜在偏差。

Abstract: Charts are ubiquitous as they help people understand and reason with data.
Recently, various downstream tasks, such as chart question answering,
chart2text, and fact-checking, have emerged. Large Vision-Language Models
(LVLMs) show promise in tackling these tasks, but their evaluation is costly
and time-consuming, limiting real-world deployment. While using LVLMs as judges
to assess the chart comprehension capabilities of other LVLMs could streamline
evaluation processes, challenges like proprietary datasets, restricted access
to powerful models, and evaluation costs hinder their adoption in industrial
settings. To this end, we present a comprehensive evaluation of 13 open-source
LVLMs as judges for diverse chart comprehension and reasoning tasks. We design
both pairwise and pointwise evaluation tasks covering criteria like factual
correctness, informativeness, and relevancy. Additionally, we analyze LVLM
judges based on format adherence, positional consistency, length bias, and
instruction-following. We focus on cost-effective LVLMs (<10B parameters)
suitable for both research and commercial use, following a standardized
evaluation protocol and rubric to measure the LVLM judge's accuracy.
Experimental results reveal notable variability: while some open LVLM judges
achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4
judgments), others struggle (below ~10% agreement). Our findings highlight that
state-of-the-art open-source LVLMs can serve as cost-effective automatic
evaluators for chart-related tasks, though biases such as positional preference
and length bias persist.

</details>


### [167] [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
*Jiashen,Du,Jesse Yao,Allen Liu,Zhekai Zhang*

Main category: cs.CL

TL;DR: 研究探讨LLMs是否能模拟人类伦理推理，通过基准数据集和复合指标评估，发现LLMs在词汇和结构上优于非专家人类，但在历史背景和策略建议上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够模拟人类伦理推理并作为人类判断的可信代理。

Method: 引入包含196个伦理困境的基准数据集，使用复合指标（BLEU、Damerau-Levenshtein等）评估多个前沿LLMs，并与非专家人类回答对比。

Result: LLMs在词汇和结构上优于非专家人类，但在历史背景和策略建议上表现较差；GPT-4o-mini表现最稳定。

Conclusion: LLMs在伦理决策中具有潜力，但在复杂情境中仍需改进。

Abstract: One open question in the study of Large Language Models (LLMs) is whether
they can emulate human ethical reasoning and act as believable proxies for
human judgment. To investigate this, we introduce a benchmark dataset
comprising 196 real-world ethical dilemmas and expert opinions, each segmented
into five structured components: Introduction, Key Factors, Historical
Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also
collect non-expert human responses for comparison, limited to the Key Factors
section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,
Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric
framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine
similarity, and Universal Sentence Encoder similarity. Metric weights are
computed through an inversion-based ranking alignment and pairwise AHP
analysis, enabling fine-grained comparison of model outputs to expert
responses. Our results show that LLMs generally outperform non-expert humans in
lexical and structural alignment, with GPT-4o-mini performing most consistently
across all sections. However, all models struggle with historical grounding and
proposing nuanced resolution strategies, which require contextual abstraction.
Human responses, while less structured, occasionally achieve comparable
semantic similarity, suggesting intuitive moral reasoning. These findings
highlight both the strengths and current limitations of LLMs in ethical
decision-making.

</details>


### [168] [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
*Saurabh Dash,Yiyang Nan,John Dang,Arash Ahmadian,Shivalika Singh,Madeline Smith,Bharat Venkitesh,Vlad Shmyhlo,Viraat Aryabumi,Walter Beller-Morales,Jeremy Pekmez,Jason Ozuzu,Pierre Richemond,Acyr Locatelli,Nick Frosst,Phil Blunsom,Aidan Gomez,Ivan Zhang,Marzieh Fadaee,Manoj Govindassamy,Sudip Roy,Matthias Gallé,Beyza Ermis,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 论文提出了一种解决多语言多模态模型构建挑战的方法，包括高质量数据合成和跨模态模型合并技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 构建多语言多模态模型面临数据稀缺、模态对齐和灾难性遗忘等挑战，需要创新解决方案。

Method: 开发了合成注释框架生成高质量多语言多模态数据，并提出跨模态模型合并技术以减少灾难性遗忘。

Result: Aya-Vision-8B和Aya-Vision-32B在性能上超越了许多更大的模型，如Qwen-2.5-VL-7B和LLaMA-3.2-90B-Vision。

Conclusion: 研究在多语言多模态领域取得了进展，提供了高效计算与高性能兼顾的技术见解。

Abstract: Building multimodal language models is fundamentally challenging: it requires
aligning vision and language modalities, curating high-quality instruction
data, and avoiding the degradation of existing text-only capabilities once
vision is introduced. These difficulties are further magnified in the
multilingual setting, where the need for multimodal data in different languages
exacerbates existing data scarcity, machine translation often distorts meaning,
and catastrophic forgetting is more pronounced. To address the aforementioned
challenges, we introduce novel techniques spanning both data and modeling.
First, we develop a synthetic annotation framework that curates high-quality,
diverse multilingual multimodal instruction data, enabling Aya Vision models to
produce natural, human-preferred responses to multimodal inputs across many
languages. Complementing this, we propose a cross-modal model merging technique
that mitigates catastrophic forgetting, effectively preserving text-only
capabilities while simultaneously enhancing multimodal generative performance.
Aya-Vision-8B achieves best-in-class performance compared to strong multimodal
models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger
Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which
outperforms models more than twice its size, such as Molmo-72B and
LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the
multi-modal frontier, and provides insights into techniques that effectively
bend the need for compute while delivering extremely high performance.

</details>


### [169] [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
*Mingxu Tao,Bowen Tang,Mingxuan Ma,Yining Zhang,Hourun Li,Feifan Wen,Hao Ma,Jia Yang*

Main category: cs.CL

TL;DR: ALOHA是一个多语言代理系统，通过分层检索和外部API集成，为校园信息查询提供高效、多语言支持，优于商业聊天机器人和搜索引擎。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在校园特定信息检索中的不足，尤其是多语言和实时性需求。

Method: 引入ALOHA系统，结合分层检索和外部API集成，提供交互式服务。

Result: 系统在多语言查询中表现优异，已为超过12,000人提供服务。

Conclusion: ALOHA在校园信息检索中展现出高效、用户友好的特性，具有实际应用价值。

Abstract: The rise of Large Language Models~(LLMs) revolutionizes information
retrieval, allowing users to obtain required answers through complex
instructions within conversations. However, publicly available services remain
inadequate in addressing the needs of faculty and students to search
campus-specific information. It is primarily due to the LLM's lack of
domain-specific knowledge and the limitation of search engines in supporting
multilingual and timely scenarios. To tackle these challenges, we introduce
ALOHA, a multilingual agent enhanced by hierarchical retrieval for university
orientation. We also integrate external APIs into the front-end interface to
provide interactive service. The human evaluation and case study show our
proposed system has strong capabilities to yield correct, timely, and
user-friendly responses to the queries in multiple languages, surpassing
commercial chatbots and search engines. The system has been deployed and has
provided service for more than 12,000 people.

</details>


### [170] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
*Ruilin Liu,Zhixiao Zhao,Jieqiong Li,Chang Liu,Dongbo Wang*

Main category: cs.CL

TL;DR: 论文提出了一种结合双向思维链和奖励机制的新训练方法，用于解决领域特定大语言模型在微调过程中面临的偏见、知识继承错误和灾难性遗忘等问题。该方法在ICH-Qwen模型上表现优异，并在多个领域数据集上验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在微调过程中因使用非物质文化遗产数据而导致的偏见、知识继承错误和灾难性遗忘等问题。

Method: 提出了一种结合双向思维链（正向和反向推理）和奖励机制的训练方法，通过结构化和内容评估优化模型输出。

Result: 实验表明，该方法在问答任务中的准确性、Bleu-4和Rouge-L得分上优于0-shot、逐步推理、知识蒸馏和问题增强等方法，并在多个领域数据集上表现出泛化能力。

Conclusion: 该方法不仅适用于非物质文化遗产领域，还能推广到其他领域，为未来跨领域模型训练提供了有价值的参考。

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [171] [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
*Yuxiang Wang,Xiao Yan,Shiyu Jin,Quanqing Xu,Chuang Hu,Yuanyuan Zhu,Bo Du,Jia Wu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TSA的文本语义增强方法，通过引入更多文本语义监督信号来提升文本属性图（TAG）中少样本和零样本节点分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖基于图的增强技术，而基于文本的增强技术研究较少。TSA旨在填补这一空白，通过文本语义增强提升分类性能。

Method: 设计了两种增强技术：正语义匹配（检索相似文本与节点匹配）和负语义对比（通过负提示构造相反语义的文本描述）。

Result: 在5个数据集上与13个基线方法对比，TSA表现最优，准确率通常比最佳基线高出5%以上。

Conclusion: TSA通过文本语义增强显著提升了少样本和零样本节点分类的准确性，为TAG研究提供了新思路。

Abstract: Text-attributed graph (TAG) provides a text description for each graph node,
and few- and zero-shot node classification on TAGs have many applications in
fields such as academia and social networks. Existing work utilizes various
graph-based augmentation techniques to train the node and text embeddings,
while text-based augmentations are largely unexplored. In this paper, we
propose Text Semantics Augmentation (TSA) to improve accuracy by introducing
more text semantic supervision signals. Specifically, we design two
augmentation techniques, i.e., positive semantics matching and negative
semantics contrast, to provide more reference texts for each graph node or text
description. Positive semantic matching retrieves texts with similar embeddings
to match with a graph node. Negative semantic contrast adds a negative prompt
to construct a text description with the opposite semantics, which is
contrasted with the original node and text. We evaluate TSA on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that TSA
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

</details>


### [172] [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
*Artem Shelmanov,Ekaterina Fadeeva,Akim Tsvigun,Ivan Tsvigun,Zhuohan Xie,Igor Kiselev,Nico Daheim,Caiqi Zhang,Artem Vazhentsev,Mrinmaya Sachan,Preslav Nakov,Timothy Baldwin*

Main category: cs.CL

TL;DR: 论文提出了一种预训练的不确定性量化（UQ）模块，用于增强大语言模型（LLMs）检测幻觉的能力，显著优于无监督方法。


<details>
  <summary>Details</summary>
Motivation: LLMs容易生成虚假信息（幻觉），且难以检测，需要一种可靠的方法来评估模型输出的可信度。

Method: 引入预训练的UQ模块，利用Transformer架构和LLM的注意力图特征，提升不确定性量化能力。

Result: 实验表明，该方法在幻觉检测上表现优异，且能泛化到未训练的语言。

Conclusion: 预训练的UQ模块有效提升了LLMs的可靠性，并公开了代码和模型。

Abstract: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to
sporadically generate false or fabricated information. This presents a major
challenge, as hallucinations often appear highly convincing and users generally
lack the tools to detect them. Uncertainty quantification (UQ) provides a
framework for assessing the reliability of model outputs, aiding in the
identification of potential hallucinations. In this work, we introduce
pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially
enhance their ability to capture uncertainty compared to unsupervised UQ
methods. Their strong performance stems from the powerful Transformer
architecture in their design and informative features derived from LLM
attention maps. Experimental evaluation shows that these heads are highly
robust and achieve state-of-the-art performance in claim-level hallucination
detection across both in-domain and out-of-domain prompts. Moreover, these
modules demonstrate strong generalization to languages they were not explicitly
trained on. We pre-train a collection of UQ heads for popular LLM series,
including Mistral, Llama, and Gemma 2. We publicly release both the code and
the pre-trained heads.

</details>


### [173] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
*Haoran Ye,Jing Jin,Yuhang Xie,Xin Zhang,Guojie Song*

Main category: cs.CL

TL;DR: 该论文提出了LLM心理测量学这一新兴交叉领域，旨在利用心理测量工具和理论评估和改进大型语言模型，以解决传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展超越了传统评估方法，需要新的方法来衡量人类心理特征并建立以人为中心的评估体系。

Method: 论文通过整合心理测量学的工具、理论和原则，系统性地探索其在LLM评估中的作用，包括基准原则、方法论改进和结果验证。

Result: 提出了一个结构化的跨学科框架，为研究人员提供了全面理解LLM心理测量学的资源，并提供了相关资源库。

Conclusion: 论文旨在为未来开发与人类水平AI对齐的评估范式提供可行见解，推动以人为中心的AI系统发展，造福社会。

Abstract: The rapid advancement of large language models (LLMs) has outpaced
traditional evaluation methodologies. It presents novel challenges, such as
measuring human-like psychological constructs, navigating beyond static and
task-specific benchmarks, and establishing human-centered evaluation. These
challenges intersect with Psychometrics, the science of quantifying the
intangible aspects of human psychology, such as personality, values, and
intelligence. This survey introduces and synthesizes an emerging
interdisciplinary field of LLM Psychometrics, which leverages psychometric
instruments, theories, and principles to evaluate, understand, and enhance
LLMs. We systematically explore the role of Psychometrics in shaping
benchmarking principles, broadening evaluation scopes, refining methodologies,
validating results, and advancing LLM capabilities. This paper integrates
diverse perspectives to provide a structured framework for researchers across
disciplines, enabling a more comprehensive understanding of this nascent field.
Ultimately, we aim to provide actionable insights for developing future
evaluation paradigms that align with human-level AI and promote the advancement
of human-centered AI systems for societal benefit. A curated repository of LLM
psychometric resources is available at
https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [174] [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
*Rishabh Agrawal,Himanshu Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种自适应上下文压缩（ACC）技术和混合CAG-RAG框架，以优化大规模语言模型的知识密集型任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管缓存增强生成（CAG）在减少检索延迟和简化系统设计方面表现出潜力，但其在处理大规模动态知识库时仍面临挑战。

Method: 引入自适应上下文压缩（ACC）技术动态管理上下文输入，并提出混合CAG-RAG框架，结合选择性检索以补充预加载上下文。

Result: 实验表明，该方法能显著提升可扩展性、效率和多跳推理性能。

Conclusion: 提出的技术为实际知识集成挑战提供了实用解决方案。

Abstract: The rapid progress in large language models (LLMs) has paved the way for
novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented
Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented
Generation (RAG). CAG minimizes retrieval latency and simplifies system design
by preloading knowledge into the model's context. However, challenges persist
in scaling CAG to accommodate large and dynamic knowledge bases effectively.
This paper introduces Adaptive Contextual Compression (ACC), an innovative
technique designed to dynamically compress and manage context inputs, enabling
efficient utilization of the extended memory capabilities of modern LLMs. To
further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG
Framework, which integrates selective retrieval to augment preloaded contexts
in scenarios requiring additional information. Comprehensive evaluations on
diverse datasets highlight the proposed methods' ability to enhance
scalability, optimize efficiency, and improve multi-hop reasoning performance,
offering practical solutions for real-world knowledge integration challenges.

</details>


### [175] [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.CL

TL;DR: 论文提出Adaptive GoGI-Skip框架，通过动态压缩Chain-of-Thought (CoT)提示，提升推理效率。结合Goal-Gradient Importance (GoGI)和Adaptive Dynamic Skipping (ADS)，显著减少计算成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 当前CoT压缩技术依赖静态指标，可能误删关键内容或无法适应复杂推理。

Method: 提出GoGI度量关键令牌，ADS动态调节压缩率，通过监督微调实现。

Result: 在MATH数据上训练，跨领域表现优异，平均减少45%令牌，推理速度提升1.6-2.0倍。

Conclusion: Adaptive GoGI-Skip在效率与准确性上优于现有方法，推动CoT推理优化。

Abstract: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex
tasks, but their reasoning traces are often excessively verbose and
inefficient, leading to significant computational costs and latency. Current
CoT compression techniques typically rely on generic importance metrics and
static compression rates, which may inadvertently remove functionally critical
tokens or fail to adapt to varying reasoning complexity. To overcome these
limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic
CoT compression via supervised fine-tuning. This approach introduces two
synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric
accurately identifying functionally relevant tokens by measuring the gradient
influence of their intermediate representations on the final answer loss, and
(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the
compression rate based on runtime model uncertainty while ensuring local
coherence through an adaptive N-token constraint. To our knowledge, this is the
first work unifying a goal-oriented, gradient-based importance metric with
dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed
MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization
across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It
achieves substantial efficiency gains - reducing CoT token counts by over 45%
on average and delivering 1.6-2.0 times inference speedups - while maintaining
high reasoning accuracy. Notably, it significantly outperforms existing
baselines by preserving accuracy even at high effective compression rates,
advancing the state of the art in the CoT reasoning efficiency-accuracy
trade-off.

</details>


### [176] [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
*Mehran Sarmadi,Morteza Alikhani,Erfan Zinvandi,Zahra Pourbahman*

Main category: cs.CL

TL;DR: 本文介绍了Hakim，一种新型波斯语文本嵌入模型，性能提升8.5%，并引入三个新数据集，适用于聊天机器人和检索增强生成系统。


<details>
  <summary>Details</summary>
Motivation: 波斯语在大规模嵌入研究中代表性不足，需要提升其自然语言理解能力。

Method: 提出Hakim模型，基于BERT架构，并引入三个新数据集（Corpesia、Pairsia-sup、Pairsia-unsup）。

Result: Hakim在FaMTEB基准测试中性能提升8.5%，优于现有波斯语模型。

Conclusion: Hakim为波斯语理解提供了新基础，适用于多种NLP任务和检索应用。

Abstract: Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.

</details>


### [177] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
*Fujun Zhang,XiangDong Su*

Main category: cs.CL

TL;DR: 论文提出了一种名为RepCali的方法，通过在预训练语言模型（PLM）的潜在空间中校准表示，解决了编码器与解码器输入之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: PLM在微调后仍存在编码器输出与解码器输入之间的表示差异，影响下游任务性能。

Method: 在编码器后添加校准模块，优化潜在空间表示，作为解码器输入。

Result: 在25个PLM模型和8个任务上的实验表明，RepCali显著提升了性能，优于基准微调方法。

Conclusion: RepCali是一种通用、即插即用的方法，能有效提升PLM在下游任务中的表现。

Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.

</details>


### [178] [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
*Takumi Shibata,Yuichi Miyamura*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）的对比性作文评分方法（LCES），通过将评分任务转化为成对比较，显著提升了零样本自动评分的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统零样本方法直接生成绝对分数，容易因模型偏见和不一致性导致与人工评分偏差。LCES旨在解决这一问题。

Method: LCES将评分任务转化为成对比较，利用LLM判断两篇作文的优劣，并通过RankNet将比较结果转换为连续分数。

Result: 实验表明，LCES在基准数据集上优于传统零样本方法，且计算效率高，对不同LLM主干具有鲁棒性。

Conclusion: LCES为零样本自动作文评分提供了一种高效且可靠的方法，适用于实际应用。

Abstract: Recent advances in large language models (LLMs) have enabled zero-shot
automated essay scoring (AES), providing a promising way to reduce the cost and
effort of essay scoring in comparison with manual grading. However, most
existing zero-shot approaches rely on LLMs to directly generate absolute
scores, which often diverge from human evaluations owing to model biases and
inconsistent scoring. To address these limitations, we propose LLM-based
Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise
comparison task. Specifically, we instruct LLMs to judge which of two essays is
better, collect many such comparisons, and convert them into continuous scores.
Considering that the number of possible comparisons grows quadratically with
the number of essays, we improve scalability by employing RankNet to
efficiently transform LLM preferences into scalar scores. Experiments using AES
benchmark datasets show that LCES outperforms conventional zero-shot methods in
accuracy while maintaining computational efficiency. Moreover, LCES is robust
across different LLM backbones, highlighting its applicability to real-world
zero-shot AES.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [179] [Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors](https://arxiv.org/abs/2505.07906)
*Geunho Choi,Changhwan Lee,Jieun Kim,Insoo Ye,Keeyoung Jung,Inchul Park*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于AI的图像驱动框架，用于锂离子电池正极前驱体合成的预测设计与优化，通过闭环方法实现微观结构的可控设计。


<details>
  <summary>Details</summary>
Motivation: 微观结构对材料性能至关重要，但由于难以量化、预测和优化，很少被作为设计变量。

Method: 结合扩散图像生成模型、定量图像分析流程和粒子群优化算法，从SEM图像中提取形态特征，预测并优化合成条件。

Result: 框架能够准确预测特定合成条件下的微观结构，并通过实验验证了预测与合成结构的一致性。

Conclusion: 该框架为数据驱动的材料设计提供了实用策略，支持正向预测和逆向设计，推动微观结构工程的自主化。

Abstract: Microstructure often dictates materials performance, yet it is rarely treated
as an explicit design variable because microstructure is hard to quantify,
predict, and optimize. Here, we introduce an image centric, closed-loop
framework that makes microstructural morphology into a controllable objective
and demonstrate its use case with Li- and Mn-rich layered oxide cathode
precursors. This work presents an integrated, AI driven framework for the
predictive design and optimization of lithium-ion battery cathode precursor
synthesis. This framework integrates a diffusion-based image generation model,
a quantitative image analysis pipeline, and a particle swarm optimization (PSO)
algorithm. By extracting key morphological descriptors such as texture,
sphericity, and median particle size (D50) from SEM images, the platform
accurately predicts SEM like morphologies resulting from specific
coprecipitation conditions, including reaction time-, solution concentration-,
and pH-dependent structural changes. Optimization then pinpoints synthesis
parameters that yield user defined target morphologies, as experimentally
validated by the close agreement between predicted and synthesized structures.
This framework offers a practical strategy for data driven materials design,
enabling both forward prediction and inverse design of synthesis conditions and
paving the way toward autonomous, image guided microstructure engineering.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [180] [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
*Fan Zhang,Tianyu Liu,Zhihong Zhu,Hao Wu,Haixin Wang,Donghao Zhou,Yefeng Zheng,Kun Wang,Xian Wu,Pheng-Ann Heng*

Main category: q-bio.QM

TL;DR: 论文介绍了CellVerse，一个基于语言的单细胞分析基准测试，评估了多种LLMs在细胞生物学任务中的表现，发现现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLMs在语言驱动的单细胞分析任务中性能的全面评估，因此作者提出了CellVerse来解决这一问题。

Method: 通过整合四种单细胞多组学数据，设计了三个层次的分析任务（细胞类型注释、药物反应预测和扰动分析），并系统评估了14种LLMs的表现。

Result: 实验显示，现有专家模型表现不佳，通用模型（如Qwen、Llama等）在细胞生物学领域有初步理解能力，但整体性能仍有显著提升空间。

Conclusion: CellVerse揭示了LLMs在细胞生物学应用中的挑战，为未来研究奠定了基础。

Abstract: Recent studies have demonstrated the feasibility of modeling single-cell data
as natural languages and the potential of leveraging powerful large language
models (LLMs) for understanding cell biology. However, a comprehensive
evaluation of LLMs' performance on language-driven single-cell analysis tasks
still remains unexplored. Motivated by this challenge, we introduce CellVerse,
a unified language-centric question-answering benchmark that integrates four
types of single-cell multi-omics data and encompasses three hierarchical levels
of single-cell analysis tasks: cell type annotation (cell-level), drug response
prediction (drug-level), and perturbation analysis (gene-level). Going beyond
this, we systematically evaluate the performance across 14 open-source and
closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the
experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail
to make reasonable decisions across all sub-tasks within CellVerse, while
generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit
preliminary understanding capabilities within the realm of cell biology. (2)
The performance of current LLMs falls short of expectations and has substantial
room for improvement. Notably, in the widely studied drug response prediction
task, none of the evaluated LLMs demonstrate significant performance
improvement over random guessing. CellVerse offers the first large-scale
empirical demonstration that significant challenges still remain in applying
LLMs to cell biology. By introducing CellVerse, we lay the foundation for
advancing cell biology through natural languages and hope this paradigm could
facilitate next-generation single-cell analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [181] [Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing](https://arxiv.org/abs/2505.08474)
*Kuan-Cheng Chen,Chen-Yu Liu,Yu Shang,Felix Burt,Kin K. Leung*

Main category: quant-ph

TL;DR: 提出了一种分布式量子-经典框架，结合光子量子神经网络（QNNs）和矩阵乘积态（MPS）映射，实现经典神经网络的高效参数训练。


<details>
  <summary>Details</summary>
Motivation: 通过结合量子计算和经典计算的优势，开发一种参数高效的训练方法，同时利用光子量子计算的室温操作和可扩展性。

Method: 使用光子QNNs生成高维概率分布，并通过MPS模型映射到经典网络权重，实现参数压缩。

Result: 在MNIST分类任务中，该方法以3,292参数达到95.50%准确率，压缩比为10倍时准确率损失小于3%。

Conclusion: 该框架展示了光子量子计算在分布式量子机器学习中的实用性，结合了光子希尔伯特空间的表达能力和经典神经网络的可部署性。

Abstract: We introduce a distributed quantum-classical framework that synergizes
photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping
to achieve parameter-efficient training of classical neural networks. By
leveraging universal linear-optical decompositions of $M$-mode interferometers
and photon-counting measurement statistics, our architecture generates neural
parameters through a hybrid quantum-classical workflow: photonic QNNs with
$M(M+1)/2$ trainable parameters produce high-dimensional probability
distributions that are mapped to classical network weights via an MPS model
with bond dimension $\chi$. Empirical validation on MNIST classification
demonstrates that photonic QT achieves an accuracy of $95.50\% \pm 0.84\%$
using 3,292 parameters ($\chi = 10$), compared to $96.89\% \pm 0.31\%$ for
classical baselines with 6,690 parameters. Moreover, a ten-fold compression
ratio is achieved at $\chi = 4$, with a relative accuracy loss of less than
$3\%$. The framework outperforms classical compression techniques (weight
sharing/pruning) by 6--12\% absolute accuracy while eliminating quantum
hardware requirements during inference through classical deployment of
compressed parameters. Simulations incorporating realistic photonic noise
demonstrate the framework's robustness to near-term hardware imperfections.
Ablation studies confirm quantum necessity: replacing photonic QNNs with random
inputs collapses accuracy to chance level ($10.0\% \pm 0.5\%$). Photonic
quantum computing's room-temperature operation, inherent scalability through
spatial-mode multiplexing, and HPC-integrated architecture establish a
practical pathway for distributed quantum machine learning, combining the
expressivity of photonic Hilbert spaces with the deployability of classical
neural networks.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [182] [Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging](https://arxiv.org/abs/2505.07973)
*Isabella Cama,Michele Piana,Cristina Campi,Sara Garbarino*

Main category: stat.AP

TL;DR: 提出了一种概率模型用于纵向预测疾病进展，整合基线特征和中期随访数据，处理预测不确定性，并在合成场景和脑癌数据集中验证其竞争力。


<details>
  <summary>Details</summary>
Motivation: 纵向影像分析能动态追踪疾病进展和治疗效果，但现有方法在处理预测不确定性和数据维度增长方面存在不足。

Method: 开发了一种概率模型，整合基线特征和中期随访数据，自然处理预测不确定性，并控制问题维度增长。

Result: 在合成场景和脑癌数据集中，该模型表现与现有方法相当，同时能处理不确定性和减少对中期随访数据的依赖。

Conclusion: 该概率模型为纵向疾病预测提供了一种有效且灵活的方法，特别适用于处理不确定性和数据维度问题。

Abstract: Longitudinal imaging analysis tracks disease progression and treatment
response over time, providing dynamic insights into treatment efficacy and
disease evolution. Radiomic features extracted from medical imaging can support
the study of disease progression and facilitate longitudinal prediction of
clinical outcomes. This study presents a probabilistic model for longitudinal
response prediction, integrating baseline features with intermediate
follow-ups. The probabilistic nature of the model naturally allows to handle
the instrinsic uncertainty of the longitudinal prediction of disease
progression. We evaluate the proposed model against state-of-the-art disease
progression models in both a synthetic scenario and using a brain cancer
dataset. Results demonstrate that the approach is competitive against existing
methods while uniquely accounting for uncertainty and controlling the growth of
problem dimensionality, eliminating the need for data from intermediate
follow-ups.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [183] [Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted](https://arxiv.org/abs/2505.08255)
*Shuaiwei Yuan,Junyu Dong,Yuezun Li*

Main category: cs.CR

TL;DR: 论文探讨了Deepfake检测器因第三方数据中毒而引发的安全风险，并提出了一种生成隐蔽触发器的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成技术的发展，Deepfake检测器依赖第三方数据训练可能被植入后门，导致检测失效。

Method: 开发了一种触发器生成器，支持密码控制、语义抑制、自适应和隐形触发器，并通过脏标签和干净标签两种中毒场景注入后门。

Result: 实验证明该方法在隐蔽性和有效性上优于基线。

Conclusion: 研究揭示了Deepfake检测器的安全漏洞，并提出了一种隐蔽的后门注入方法。

Abstract: With the advancement of AI generative techniques, Deepfake faces have become
incredibly realistic and nearly indistinguishable to the human eye. To counter
this, Deepfake detectors have been developed as reliable tools for assessing
face authenticity. These detectors are typically developed on Deep Neural
Networks (DNNs) and trained using third-party datasets. However, this protocol
raises a new security risk that can seriously undermine the trustfulness of
Deepfake detectors: Once the third-party data providers insert poisoned
(corrupted) data maliciously, Deepfake detectors trained on these datasets will
be injected ``backdoors'' that cause abnormal behavior when presented with
samples containing specific triggers. This is a practical concern, as
third-party providers may distribute or sell these triggers to malicious users,
allowing them to manipulate detector performance and escape accountability.
  This paper investigates this risk in depth and describes a solution to
stealthily infect Deepfake detectors. Specifically, we develop a trigger
generator, that can synthesize passcode-controlled, semantic-suppression,
adaptive, and invisible trigger patterns, ensuring both the stealthiness and
effectiveness of these triggers. Then we discuss two poisoning scenarios,
dirty-label poisoning and clean-label poisoning, to accomplish the injection of
backdoors. Extensive experiments demonstrate the effectiveness, stealthiness,
and practicality of our method compared to several baselines.

</details>


### [184] [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
*Sunday Oyinlola Ogundoyin,Muhammad Ikram,Hassan Jameel Asghar,Benjamin Zi Hao Zhao,Dali Kaafar*

Main category: cs.CR

TL;DR: 研究分析了14,904个自定义GPT模型，发现95%以上存在安全漏洞，最常见的是角色扮演攻击、系统提示泄露和钓鱼内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着自定义GPT模型的广泛应用，其安全漏洞问题日益突出，但现有研究缺乏大规模实证分析。

Method: 通过多指标排名系统，评估了自定义GPT对七种可被利用威胁的易感性。

Result: 95%以上的自定义GPT缺乏足够安全保护，主要漏洞包括角色扮演攻击（96.51%）、系统提示泄露（92.20%）和钓鱼（91.22%）。

Conclusion: 研究呼吁加强安全措施和内容审核，以确保GPT应用的安全部署。

Abstract: Millions of users leverage generative pretrained transformer (GPT)-based
language models developed by leading model providers for a wide range of tasks.
To support enhanced user interaction and customization, many platforms-such as
OpenAI-now enable developers to create and publish tailored model instances,
known as custom GPTs, via dedicated repositories or application stores. These
custom GPTs empower users to browse and interact with specialized applications
designed to meet specific needs. However, as custom GPTs see growing adoption,
concerns regarding their security vulnerabilities have intensified. Existing
research on these vulnerabilities remains largely theoretical, often lacking
empirical, large-scale, and statistically rigorous assessments of associated
risks.
  In this study, we analyze 14,904 custom GPTs to assess their susceptibility
to seven exploitable threats, such as roleplay-based attacks, system prompt
leakage, phishing content generation, and malicious code synthesis, across
various categories and popularity tiers within the OpenAI marketplace. We
introduce a multi-metric ranking system to examine the relationship between a
custom GPT's popularity and its associated security risks.
  Our findings reveal that over 95% of custom GPTs lack adequate security
protections. The most prevalent vulnerabilities include roleplay-based
vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing
(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit
inherent security weaknesses, which are often inherited or amplified in custom
GPTs. These results highlight the urgent need for enhanced security measures
and stricter content moderation to ensure the safe deployment of GPT-based
applications.

</details>


### [185] [Securing RAG: A Risk Assessment and Mitigation Framework](https://arxiv.org/abs/2505.08728)
*Lukas Ammann,Sara Ott,Christoph R. Landolt,Marco P. Lehmann*

Main category: cs.CR

TL;DR: 本文探讨了检索增强生成（RAG）的安全与隐私挑战，提出了漏洞分析、缓解措施及一个结合RAG特定安全考量的框架。


<details>
  <summary>Details</summary>
Motivation: RAG已成为用户端NLP应用的标准，但其数据集成能力带来了新的安全与隐私风险，亟需解决方案。

Method: 首先分析RAG管道的漏洞和攻击面，提出缓解措施；然后开发一个结合RAG安全考量的框架。

Result: 提出了一个指导实现安全、合规、可信RAG系统的框架。

Conclusion: 该框架为RAG系统的安全实施提供了结构化指导，结合了行业标准和最佳实践。

Abstract: Retrieval Augmented Generation (RAG) has emerged as the de facto industry
standard for user-facing NLP applications, offering the ability to integrate
data without re-training or fine-tuning Large Language Models (LLMs). This
capability enhances the quality and accuracy of responses but also introduces
novel security and privacy challenges, particularly when sensitive data is
integrated. With the rapid adoption of RAG, securing data and services has
become a critical priority. This paper first reviews the vulnerabilities of RAG
pipelines, and outlines the attack surface from data pre-processing and data
storage management to integration with LLMs. The identified risks are then
paired with corresponding mitigations in a structured overview. In a second
step, the paper develops a framework that combines RAG-specific security
considerations, with existing general security guidelines, industry standards,
and best practices. The proposed framework aims to guide the implementation of
robust, compliant, secure, and trustworthy RAG systems.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [186] [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
*Tim Wittenborg,Constantin Sebastian Tremel,Niklas Stehr,Oliver Karras,Markus Stocker,Sören Auer*

Main category: cs.DL

TL;DR: 该论文提出了一种名为SciCom Wiki的协作平台，旨在通过FAIR原则和神经符号计算事实检查工具，支持科学传播知识基础设施（SciCom KI），以应对视频和播客中的信息泛滥和错误信息问题。


<details>
  <summary>Details</summary>
Motivation: 民主社会需要可靠且易获取的信息，但视频和播客作为主要传播媒介，也带来了错误信息的挑战。现有的科学传播知识基础设施（SciCom KI）在处理非文本媒体时仍显不足，无法规模化应对内容泛滥。

Method: 研究通过调查53位利益相关者的需求，并在11次访谈中细化这些需求，随后基于Wikibase构建了一个开源服务平台原型。针对最需要的功能——事实检查，开发了一种神经符号计算方法，将异构媒体转换为知识图谱。

Result: 研究通过10次专家访谈和43名参与者的公开用户调查验证了工具的实用性和必要性。SciCom Wiki作为FAIR数字图书馆，结合神经符号计算事实检查框架，能够满足需求。

Conclusion: 研究发现SciCom KI在FAIR知识和协作系统方面严重不足，SciCom Wiki可作为核心知识节点，但需要协作努力以应对信息泛滥的挑战。

Abstract: Democratic societies need accessible, reliable information. Videos and
Podcasts have established themselves as the medium of choice for civic
dissemination, but also as carriers of misinformation. The emerging Science
Communication Knowledge Infrastructure (SciCom KI) curating non-textual media
is still fragmented and not adequately equipped to scale against the content
flood. Our work sets out to support the SciCom KI with a central, collaborative
platform, the SciCom Wiki, to facilitate FAIR (findable, accessible,
interoperable, reusable) media representation and the fact-checking of their
content, particularly for videos and podcasts. Building an open-source service
system centered around Wikibase, we survey requirements from 53 stakeholders,
refine these in 11 interviews, and evaluate our prototype based on these
requirements with another 14 participants. To address the most requested
feature, fact-checking, we developed a neurosymbolic computational
fact-checking approach, converting heterogenous media into knowledge graphs.
This increases machine-readability and allows comparing statements against
equally represented ground-truth. Our computational fact-checking tool was
iteratively evaluated through 10 expert interviews, a public user survey with
43 participants verified the necessity and usability of our tool. Overall, our
findings identified several needs to systematically support the SciCom KI. The
SciCom Wiki, as a FAIR digital library complementing our neurosymbolic
computational fact-checking framework, was found suitable to address the raised
requirements. Further, we identified that the SciCom KI is severely
underdeveloped regarding FAIR knowledge and related systems facilitating its
collaborative creation and curation. Our system can provide a central knowledge
node, yet a collaborative effort is required to scale against the imminent
(mis-)information flood.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [187] [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh*

Main category: cs.SI

TL;DR: 该研究通过构建多维相似性网络，模拟古典波斯诗人之间的影响动态，结合语义、词汇、风格、主题和韵律特征，识别关键诗人、风格中心和桥梁诗人，并通过社区检测算法揭示文学流派的聚类。


<details>
  <summary>Details</summary>
Motivation: 旨在通过计算模型揭示波斯诗人之间的影响关系，区分经典重要性与互文影响，突出结构上重要但知名度较低的诗人。

Method: 使用Ganjoor语料库构建数据集，基于多维度特征生成相似性矩阵和聚合图，计算多种中心性指标，并应用Louvain社区检测算法。

Result: 研究发现了一些在结构上具有重要意义的诗人，并揭示了与已知文学流派（如Sabk-e Hindi、Sabk-e Khorasani）密切相关的诗人聚类。

Conclusion: 该研究为波斯文学提供了数据驱动的新视角，结合计算语言学与文学研究，构建了一个可解释且可扩展的诗歌传统模型。

Abstract: This study formalizes a computational model to simulate classical Persian
poets' dynamics of influence through constructing a multi-dimensional
similarity network. Using a rigorously curated dataset based on Ganjoor's
corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical
features to demarcate each poet's corpus. Each is contained within weighted
similarity matrices, which are then appended to generate an aggregate graph
showing poet-to-poet influence. Further network investigation is carried out to
identify key poets, style hubs, and bridging poets by calculating degree,
closeness, betweenness, eigenvector, and Katz centrality measures. Further, for
typological insight, we use the Louvain community detection algorithm to
demarcate clusters of poets sharing both style and theme coherence, which
correspond closely to acknowledged schools of literature like Sabk-e Hindi,
Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a
new data-driven view of Persian literature distinguished between canonical
significance and interextual influence, thus highlighting relatively
lesser-known figures who hold great structural significance. Combining
computational linguistics with literary study, this paper produces an
interpretable and scalable model for poetic tradition, enabling retrospective
reflection as well as forward-looking research within digital humanities.

</details>


### [188] [The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News](https://arxiv.org/abs/2505.08532)
*Yuhan Liu,Yuxuan Liu,Xiaoqing Zhang,Xiuying Chen,Rui Yan*

Main category: cs.SI

TL;DR: 本文提出了一种名为TruEDebate（TED）的多智能体系统，通过模拟正式辩论过程，利用大语言模型（LLMs）增强假新闻检测的可解释性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前假新闻检测方法存在可解释性低和泛化能力有限的问题，或未能充分利用LLMs的推理能力。

Method: TED采用辩论流程，包括DebateFlow Agents（支持与挑战新闻真实性的团队）和InsightFlow Agents（合成与分析辩论内容的子智能体）。

Result: 通过模拟人类辩论过程，TED实现了对新闻内容的全面评估，并提供了最终判断。

Conclusion: TED通过多智能体辩论框架显著提升了假新闻检测的可解释性和效果。

Abstract: In today's digital environment, the rapid propagation of fake news via social
networks poses significant social challenges. Most existing detection methods
either employ traditional classification models, which suffer from low
interpretability and limited generalization capabilities, or craft specific
prompts for large language models (LLMs) to produce explanations and results
directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the
saying that "truth becomes clearer through debate," our study introduces a
novel multi-agent system with LLMs named TruEDebate (TED) to enhance the
interpretability and effectiveness of fake news detection. TED employs a
rigorous debate process inspired by formal debate settings. Central to our
approach are two innovative components: the DebateFlow Agents and the
InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where
one supports and the other challenges the truth of the news. These agents
engage in opening statements, cross-examination, rebuttal, and closing
statements, simulating a rigorous debate process akin to human discourse
analysis, allowing for a thorough evaluation of news content. Concurrently, the
InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent
and the Analysis Agent. The Synthesis Agent summarizes the debates and provides
an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The
Analysis Agent, which includes a role-aware encoder and a debate graph,
integrates role embeddings and models the interactions between debate roles and
arguments using an attention mechanism, providing the final judgment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [189] [TikTok Search Recommendations: Governance and Research Challenges](https://arxiv.org/abs/2505.08385)
*Taylor Annabell,Robert Gorwa,Rebecca Scharlach,Jacob van de Kerkhof,Thales Bertaglia*

Main category: cs.IR

TL;DR: TikTok作为搜索引擎的搜索推荐功能缺乏透明度，引发平台治理挑战，需加强透明度和研究。


<details>
  <summary>Details</summary>
Motivation: 探讨TikTok搜索推荐功能的不透明性及其对平台治理的挑战，提出研究议程。

Method: 基于初步定性分析，提出计算研究议程。

Result: TikTok搜索推荐功能缺乏透明度，可能引发问题，需进一步研究。

Conclusion: 呼吁平台透明化，并推动相关研究以解决搜索推荐带来的治理问题。

Abstract: Like other social media, TikTok is embracing its use as a search engine,
developing search products to steer users to produce searchable content and
engage in content discovery. Their recently developed product search
recommendations are preformulated search queries recommended to users on
videos. However, TikTok provides limited transparency about how search
recommendations are generated and moderated, despite requirements under
regulatory frameworks like the European Union's Digital Services Act. By
suggesting that the platform simply aggregates comments and common searches
linked to videos, it sidesteps responsibility and issues that arise from
contextually problematic recommendations, reigniting long-standing concerns
about platform liability and moderation. This position paper addresses the
novelty of search recommendations on TikTok by highlighting the challenges that
this feature poses for platform governance and offering a computational
research agenda, drawing on preliminary qualitative analysis. It sets out the
need for transparency in platform documentation, data access and research to
study search recommendations.

</details>


### [190] [OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval](https://arxiv.org/abs/2505.07879)
*Wei Yang,Jingjing Fu,Rui Wang,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.IR

TL;DR: 该论文提出了一种多模态检索增强生成（RAG）系统，通过从粗到细的多步检索方法提升知识库视觉问答（KB-VQA）的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用查询和知识库中多模态和多粒度信息的潜在交互，导致KB-VQA任务效果受限。

Method: 系统采用从粗到细的多步检索策略，包括初始粗粒度跨模态检索、多模态融合重排序和细粒度文本重排序。

Result: 在InfoSeek和Encyclopedic-VQA基准测试中，该方法实现了最先进的检索性能和极具竞争力的回答效果。

Conclusion: 该方法通过协调多粒度和多模态信息，显著提升了KB-VQA系统的性能。

Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective
approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which
requires external knowledge beyond the visual content presented in images. The
effectiveness of Vision-language RAG systems hinges on multimodal retrieval,
which is inherently challenging due to the diverse modalities and knowledge
granularities in both queries and knowledge bases. Existing methods have not
fully tapped into the potential interplay between these elements. We propose a
multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that
harmonizes multiple granularities and modalities to enhance efficacy. Our
system begins with a broad initial search aligning knowledge granularity for
cross-modal retrieval, followed by a multimodal fusion reranking to capture the
nuanced multimodal information for top entity selection. A text reranker then
filters out the most relevant fine-grained section for augmented generation.
Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our
method achieves state-of-the-art retrieval performance and highly competitive
answering results, underscoring its effectiveness in advancing KB-VQA systems.

</details>


### [191] [Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation](https://arxiv.org/abs/2505.07917)
*Linus Stuhlmann,Michael Alexander Saxer,Jonathan Fürst*

Main category: cs.IR

TL;DR: 本文研究了生物医学问答系统中的检索增强生成（RAG）方法，评估了不同检索策略和响应时间的权衡，最终确定了BM25与MedCPT结合的最优方案。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答系统需要高效的检索和生成组件以确保准确性、效率和可扩展性。

Method: 评估了BM25、BioBERT、MedCPT等检索方法及Elasticsearch、MongoDB等数据存储，在PubMed子集上测试后部署到完整数据集。

Result: BM25检索50篇文档后用MedCPT重排，在准确性（0.90）、召回率（0.90）和响应时间（1.91秒）上达到最优平衡。

Conclusion: 研究揭示了检索深度、效率和可扩展性的权衡，系统开源且可扩展。

Abstract: Biomedical question-answering (QA) systems require effective retrieval and
generation components to ensure accuracy, efficiency, and scalability. This
study systematically examines a Retrieval-Augmented Generation (RAG) system for
biomedical QA, evaluating retrieval strategies and response time trade-offs. We
first assess state-of-the-art retrieval methods, including BM25, BioBERT,
MedCPT, and a hybrid approach, alongside common data stores such as
Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents)
to measure indexing efficiency, retrieval latency, and retriever performance in
the end-to-end RAG system. Based on these insights, we deploy the final RAG
system on the full 24M PubMed corpus, comparing different retrievers' impact on
overall performance. Evaluations of the retrieval depth show that retrieving 50
documents with BM25 before reranking with MedCPT optimally balances accuracy
(0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains
stable (82ms), while MedCPT incurs the main computational cost. These results
highlight previously not well-known trade-offs in retrieval depth, efficiency,
and scalability for biomedical QA. With open-source code, the system is fully
reproducible and extensible.

</details>


### [192] [Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation](https://arxiv.org/abs/2505.08157)
*Shengyin Sun,Chen Ma*

Main category: cs.IR

TL;DR: 论文提出了一种基于双曲对比学习和模型增强的知识感知推荐方法，解决了现有方法在捕获层次结构和避免用户偏好偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的GNN方法难以有效捕获用户-物品二分图和知识图的层次结构，且通过扰动图结构生成正样本可能导致用户偏好偏移。

Method: 设计了Lorentzian知识聚合机制以捕获层次结构，并提出三种模型级增强技术辅助双曲对比学习，避免偏好偏移。

Result: 实验表明，所提方法在性能上显著优于现有基线（最高提升11.03%）。

Conclusion: 通过双曲对比学习和模型增强，该方法在知识感知推荐中实现了更有效的表示学习和性能提升。

Abstract: Benefiting from the effectiveness of graph neural networks (GNNs) and
contrastive learning, GNN-based contrastive learning has become mainstream for
knowledge-aware recommendation. However, most existing contrastive
learning-based methods have difficulties in effectively capturing the
underlying hierarchical structure within user-item bipartite graphs and
knowledge graphs. Moreover, they commonly generate positive samples for
contrastive learning by perturbing the graph structure, which may lead to a
shift in user preference learning. To overcome these limitations, we propose
hyperbolic contrastive learning with model-augmentation for knowledge-aware
recommendation. To capture the intrinsic hierarchical graph structures, we
first design a novel Lorentzian knowledge aggregation mechanism, which enables
more effective representations of users and items. Then, we propose three
model-level augmentation techniques to assist Hyperbolic contrastive learning.
Different from the classical structure-level augmentation (e.g., edge
dropping), the proposed model-augmentations can avoid preference shifts between
the augmented positive pair. Finally, we conduct extensive experiments to
demonstrate the superiority (maximum improvement of $11.03\%$) of proposed
methods over existing baselines.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [193] [Sub-diffraction terahertz backpropagation compressive imaging](https://arxiv.org/abs/2505.07839)
*Yongsheng Zhu,Shaojing Liu,Ximiao Wang,Runli Li,Haili Yang,Jiali Wang,Hongjia Zhu,Yanlin Ke,Ningsheng Xu,Huanjun Chen,Shaozhi Deng*

Main category: eess.IV

TL;DR: 提出了一种亚衍射太赫兹反向传播压缩成像技术，通过光激发载流子调制太赫兹波，结合无训练神经网络和角谱传播理论，实现高分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 太赫兹单像素成像（TSPI）因简单和成本低受关注，但波长限制分辨率且现有技术条件苛刻、耗时。

Method: 用单色连续太赫兹波照射物体，通过硅片背面光激发载流子调制波，单探测器记录，无训练神经网络在物理模型约束下重建图像。

Result: 实现亚衍射成像，分辨率约λ0/7（λ0=833.3μm），采样时间短，无需超薄光调制器。

Conclusion: 该方法为太赫兹显微成像和其他逆成像问题提供了高效解决方案。

Abstract: Terahertz single-pixel imaging (TSPI) has garnered significant attention due
to its simplicity and cost-effectiveness. However, the relatively long
wavelength of THz waves limits sub-diffraction-scale imaging resolution.
Although TSPI technique can achieve sub-wavelength resolution, it requires
harsh experimental conditions and time-consuming processes. Here, we propose a
sub-diffraction THz backpropagation compressive imaging technique. We
illuminate the object with monochromatic continuous-wave THz radiation. The
transmitted THz wave is modulated by prearranged patterns generated on the back
surface of a 500-{\mu}m-thick silicon wafer, realized through photoexcited
carriers using a 532-nm laser. The modulated THz wave is then recorded by a
single-element detector. An untrained neural network is employed to iteratively
reconstruct the object image with an ultralow compression ratio of 1.5625%
under a physical model constraint, thus reducing the long sampling times. To
further suppress the diffraction-field effects, embedded with the angular
spectrum propagation (ASP) theory to model the diffraction of THz waves during
propagation, the network retrieves near-field information from the object,
enabling sub-diffraction imaging with a spatial resolution of ~{\lambda}0/7
({\lambda}0 = 833.3 {\mu}m at 0.36 THz) and eliminating the need for ultrathin
photomodulators. This approach provides an efficient solution for advancing THz
microscopic imaging and addressing other inverse imaging challenges.

</details>


### [194] [Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding](https://arxiv.org/abs/2505.07851)
*Jaeyoung Huh,Ankur Kapoor,Young-Ho Kim*

Main category: eess.IV

TL;DR: 提出了一种基于视觉Transformer的解剖感知姿态估计系统，用于从ICE图像中确定导管位置和方向，无需外部跟踪传感器。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法依赖电磁跟踪或手动调整，易受干扰且需要专业知识。

Method: 使用ViT模型处理ICE图像，通过16x16嵌入块和Transformer网络预测位置和方向，训练于851例临床数据。

Result: 平均位置误差9.48毫米，方向误差（16.13度、8.98度、10.47度），验证了模型准确性。

Conclusion: 该系统提高了手术效率，减少操作负担，支持无跟踪实时定位，可独立或与现有系统互补使用。

Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing high-resolution, real-time imaging of cardiac structures. However,
existing navigation methods rely on electromagnetic (EM) tracking, which is
susceptible to interference and position drift, or require manual adjustments
based on operator expertise. To overcome these limitations, we propose a novel
anatomy-aware pose estimation system that determines the ICE catheter position
and orientation solely from ICE images, eliminating the need for external
tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep
learning model, which captures spatial relationships between ICE images and
anatomical structures. The model is trained on a clinically acquired dataset of
851 subjects, including ICE images paired with position and orientation labels
normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16
embeddings and processed through a transformer network, where a [CLS] token
independently predicts position and orientation via separate linear layers. The
model is optimized using a Mean Squared Error (MSE) loss function, balancing
positional and orientational accuracy. Experimental results demonstrate an
average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98
deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.
Qualitative assessments further validate alignment between predicted and target
views within 3D cardiac meshes. This AI-driven system enhances procedural
efficiency, reduces operator workload, and enables real-time ICE catheter
localization for tracking-free procedures. The proposed method can function
independently or complement existing mapping systems like CARTO, offering a
transformative approach to ICE-guided interventions.

</details>


### [195] [Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2505.07866)
*Abdullah,Tao Huang,Ickjai Lee,Euijoon Ahn*

Main category: eess.IV

TL;DR: 该论文探讨了扩散模型在计算机视觉中的高效性和推理时间，重点介绍了DDPM、LDM和WDM三种模型在自然和医学影像中的应用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量合成图像方面表现出色，但训练和生成的高计算成本仍是挑战。研究旨在优化其效率和推理时间，特别是在医学影像中的应用。

Method: 通过分类和比较DDPM、LDM和WDM三种扩散模型，分析它们在自然和医学影像中的计算复杂性和性能。

Result: 三种模型在医学影像中表现出快速、可靠和高质量的生成能力，但仍存在计算成本高的问题。

Conclusion: 扩散模型在医学影像中有巨大潜力，未来研究需解决计算效率问题并探索更多应用方向。

Abstract: The diffusion model has recently emerged as a potent approach in computer
vision, demonstrating remarkable performances in the field of generative
artificial intelligence. Capable of producing high-quality synthetic images,
diffusion models have been successfully applied across a range of applications.
However, a significant challenge remains with the high computational cost
associated with training and generating these models. This study focuses on the
efficiency and inference time of diffusion-based generative models,
highlighting their applications in both natural and medical imaging. We present
the most recent advances in diffusion models by categorizing them into three
key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent
Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play
a crucial role in medical imaging, where producing fast, reliable, and
high-quality medical images is essential for accurate analysis of abnormalities
and disease diagnosis. We first investigate the general framework of DDPM, LDM,
and WDM and discuss the computational complexity gap filled by these models in
natural and medical imaging. We then discuss the current limitations of these
models as well as the opportunities and future research directions in medical
imaging.

</details>


### [196] [Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation](https://arxiv.org/abs/2505.07840)
*Alavikunhu Panthakkan,S M Anzar,K. Sherin,Saeed Al Mansoori,Hussain Al-Ahmad*

Main category: eess.IV

TL;DR: 研究评估了无人机多光谱和RGB成像在迪拜棕榈树种植区的植被健康监测效果，发现RGB植被指数与多光谱指数性能相当，为大规模农业监测提供了低成本方案。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要高效且经济的植被监测方法，以提升作物产量并实现可持续农业。

Method: 通过无人机搭载多光谱和RGB传感器，计算NDVI、SAVI等指数，并与RGB指数（如VARI、MGRVI）对比，分类植被健康状况。

Result: RGB指数在多光谱指数相近的性能下，显著降低了成本，适合大规模应用。

Conclusion: RGB成像在精准农业中具有潜力，可推动数据驱动的作物管理决策，提升农业效率。

Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop
productivity and promote sustainable agricultural practices. This study
presents a comprehensive evaluation of UAV-based imaging for vegetation health
assessment in a palm tree cultivation region in Dubai. By comparing
multispectral and RGB image data, we demonstrate that RGBbased vegetation
indices offer performance comparable to more expensive multispectral indices,
providing a cost-effective alternative for large-scale agricultural monitoring.
Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI
were computed to categorize vegetation into healthy, moderate, and stressed
conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered
similar results in vegetation classification and stress detection. Our findings
highlight the practical benefits of integrating RGB imagery into precision
farming, reducing operational costs while maintaining accuracy in plant health
monitoring. This research underscores the potential of UAVbased RGB imaging as
a powerful tool for precision agriculture, enabling broader adoption of
data-driven decision-making in crop management. By leveraging the strengths of
both multispectral and RGB imaging, this work advances the state of UAV
applications in agriculture, paving the way for more efficient and scalable
farming solutions.

</details>


### [197] [Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis](https://arxiv.org/abs/2505.08247)
*Midi Wan,Pengfei Li,Yizhuo Liang,Di Wu,Yushan Pan,Guangzhen Zhu,Hao Wang*

Main category: eess.IV

TL;DR: 论文提出了一种骨骼约束条件扩散模型（SCCDM）和足部评估方法KCC，用于提升医学影像合成的准确性和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有X射线模型在图像保真度、骨骼一致性和物理约束上的不足，特别是缺乏骨骼引导的扩散方法。

Method: 采用多尺度特征提取和注意力机制，结合骨骼约束条件扩散模型（SCCDM）和KCC评估方法。

Result: SSIM提升5.72%（0.794），PSNR提升18.34%（21.40 dB），结合KCC后平均得分0.85。

Conclusion: SCCDM和KCC显著提升了医学影像合成的质量和临床实用性。

Abstract: Medical image synthesis plays a crucial role in providing anatomically
accurate images for diagnosis and treatment. Hallux valgus, which affects
approximately 19% of the global population, requires frequent weight-bearing
X-rays for assessment, placing additional strain on both patients and
healthcare providers. Existing X-ray models often struggle to balance image
fidelity, skeletal consistency, and physical constraints, particularly in
diffusion-based methods that lack skeletal guidance. We propose the
Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a
foot evaluation method utilizing skeletal landmarks. SCCDM incorporates
multi-scale feature extraction and attention mechanisms, improving the
Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise
Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves
an average score of 0.85, demonstrating strong clinical applicability. The code
is available at https://github.com/midisec/SCCDM.

</details>


### [198] [An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care](https://arxiv.org/abs/2505.08414)
*Zhi Da Soh,Yang Bai,Kai Yu,Yang Zhou,Xiaofeng Lei,Sahil Thakur,Zann Lee,Lee Ching Linette Phang,Qingsheng Peng,Can Can Xue,Rachel Shujuan Chong,Quan V. Hoang,Lavanya Raghavan,Yih Chung Tham,Charumathi Sabanayagam,Wei-Chi Wu,Ming-Chih Ho,Jiangnan He,Preeti Gupta,Ecosse Lamoureux,Seang Mei Saw,Vinay Nangia,Songhomitra Panda-Jonas,Jie Xu,Ya Xing Wang,Xinxing Xu,Jost B. Jonas,Tien Yin Wong,Rick Siow Mong Goh,Yong Liu,Ching-Yu Cheng*

Main category: eess.IV

TL;DR: Meta-EyeFM是一个结合大型语言模型（LLM）和视觉基础模型（VFM）的多功能基础模型，用于眼部疾病评估。它通过路由机制实现基于文本查询的精准任务分析，并在疾病检测、严重程度区分和常见体征识别方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型多为任务专用且缺乏用户友好界面，Meta-EyeFM旨在解决这一问题，提供多功能、高精度的眼部疾病评估工具。

Method: Meta-EyeFM利用路由机制和Low Rank Adaptation技术微调VFM，支持疾病检测、严重程度区分和体征识别。

Result: 模型在路由任务中达到100%准确率，疾病检测准确率≥82.2%，严重程度区分≥89%，体征识别≥76%，优于Gemini-1.5-flash和ChatGPT-4o LMMs。

Conclusion: Meta-EyeFM提升了可用性和诊断性能，可作为初级眼科护理的决策支持工具或在线LLM用于眼底评估。

Abstract: Current deep learning models are mostly task specific and lack a
user-friendly interface to operate. We present Meta-EyeFM, a multi-function
foundation model that integrates a large language model (LLM) with vision
foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a
routing mechanism to enable accurate task-specific analysis based on text
queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and
systemic diseases, differentiate ocular disease severity, and identify common
ocular signs. The model achieved 100% accuracy in routing fundus images to
appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,
$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.
Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o
LMMs in detecting various eye diseases and comparable to an ophthalmologist.
This system offers enhanced usability and diagnostic performance, making it a
valuable decision support tool for primary eye care or an online LLM for fundus
evaluation.

</details>


### [199] [GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI](https://arxiv.org/abs/2505.08430)
*Lei Su*

Main category: eess.IV

TL;DR: 提出了一种基于GNN的邻近上下文聚合框架（GNCAF），用于端到端分割WSI中的TLS区域和成熟阶段，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖细胞代理任务且需额外后处理，无法充分利用邻近上下文信息。

Method: 采用GNN逐步聚合多跳邻近上下文，结合自注意力机制指导目标分割。

Result: 在TCGA-COAD和INHOUSE-PAAD数据集上，mF1和mIoU分别提升22.08%和26.57%。

Conclusion: GNCAF能有效增强分割模型对上下文信息的感知能力，并验证了其在淋巴结转移分割任务中的扩展性。

Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells,
whose maturity and area can be quantified in whole slide image (WSI) for
various prognostic tasks. Existing methods for assessing these characteristics
typically rely on cell proxy tasks and require additional post-processing
steps. In this work, We focus on a novel task-TLS Semantic Segmentation
(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in
an end-to-end manner. Due to the extensive scale of WSI and patch-based
segmentation strategies, TLS-SS necessitates integrating from neighboring
patches to guide target patch (target) segmentation. Previous techniques often
employ on multi-resolution approaches, constraining the capacity to leverage
the broader neighboring context while tend to preserve coarse-grained
information. To address this, we propose a GNN-based Neighboring Context
Aggregation Framework (GNCAF), which progressively aggregates multi-hop
neighboring context from the target and employs a self-attention mechanism to
guide the segmentation of the target. GNCAF can be integrated with various
segmentation models to enhance their ability to perceive contextual information
outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and
INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly
available. Experiments on these datasets demonstrate the superiority of GNCAF,
achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,
respectively. Additionally, we also validate the task scalability of GNCAF on
segmentation of lymph node metastases.

</details>


### [200] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/abs/2505.08616)
*Yifan Li,Myeongjun Kim,Yanjing Jin,Peter Ho,Jo Woon Chong*

Main category: eess.IV

TL;DR: 提出了一种基于智能手机的便携式圆锥角膜诊断框架，通过两阶段检测流程实现高精度分类和可视化。


<details>
  <summary>Details</summary>
Motivation: 解决传统Placido盘地形图依赖专业设备的问题，提高圆锥角膜诊断的可及性。

Method: 使用智能手机屏幕显示Placido盘，捕获角膜反射，通过两阶段检测流程（WSVM分类和彩色图可视化）分析。

Result: 在模拟眼球模型上验证，分类准确率最高达92.93%，且在多款手机上表现稳定；统计检验显示特征区分能力显著。

Conclusion: 智能手机框架为圆锥角膜诊断提供了便携、高精度的解决方案，具有临床潜力。

Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized
thinning and protrusion, leading to visual distortion. While Placido disc-based
topography remains a standard in clinical diagnostics, its dependence on
specialized equipment limits accessibility. In this paper, we propose a
portable, smartphone-based diagnostic framework that captures corneal
reflections of a Placido disc displayed on a phone screen and applies a
two-stage detection pipeline, then validate on 3D-printed emulated eyeball
models that simulate normal, moderate, and severe KC stages based on anterior
chamber depth (ACD). The first step of the two-stage detection pipeline is
classifying different stages of KC with features including height and width of
extracted reflections using weighted support vector machine (WSVM). It achieves
a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple
smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16
Pro. For the second step, we visualize the KC-affected protrusion regions on
the corneas with color maps based on inter-disc distance, that provides an
intuitive representation of disease severity and localization. Moreover, we
validate the ability of the extracted features to differentiate between KC
stages with ANOVA and Omega Squared, with significant p-values (e.g., $p <
10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.

</details>


### [201] [VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation](https://arxiv.org/abs/2505.08693)
*Badhan Kumar Das,Ajay Singh,Gengyan Zhao,Han Liu,Thomas J. Re,Dorin Comaniciu,Eli Gibson,Andreas Maier*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的框架VIViT，用于自监督预训练和可变对比度的分割微调，以解决MR研究中输入对比度不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的MR研究通常包含不同的对比度组合，而现有深度学习方法需要固定输入模态，限制了大规模预训练和下游任务的适应性。

Method: 设计了VIViT框架，支持自监督预训练和可变对比度的分割微调，最大化数据利用率并适应不同输入需求。

Result: 在脑梗死和脑肿瘤分割任务中，VIViT分别取得了0.624和0.883的平均Dice分数，优于现有CNN和ViT模型。

Conclusion: VIViT在真实世界异构MR数据任务中表现出更好的适应性和性能。

Abstract: Self-supervised pretrain techniques have been widely used to improve the
downstream tasks' performance. However, real-world magnetic resonance (MR)
studies usually consist of different sets of contrasts due to different
acquisition protocols, which poses challenges for the current deep learning
methods on large-scale pretrain and different downstream tasks with different
input requirements, since these methods typically require a fixed set of input
modalities or, contrasts. To address this challenge, we propose variable-input
ViT (VIViT), a transformer-based framework designed for self-supervised
pretraining and segmentation finetuning for variable contrasts in each study.
With this ability, our approach can maximize the data availability in pretrain,
and can transfer the learned knowledge from pretrain to downstream tasks
despite variations in input requirements. We validate our method on brain
infarct and brain tumor segmentation, where our method outperforms current CNN
and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.
These results highlight the efficacy of our design for better adaptability and
performance on tasks with real-world heterogeneous MR data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [202] [Land-Coverage Aware Path-Planning for Multi-UAV Swarms in Search and Rescue Scenarios](https://arxiv.org/abs/2505.08060)
*Pedro Antonio Alarcon Granadeno,Jane Cleland-Huang*

Main category: cs.RO

TL;DR: 提出了一种基于计算机视觉的地形感知任务规划器，用于无人机搜救任务，通过深度学习分割网络分析地形拓扑，优化部署策略。


<details>
  <summary>Details</summary>
Motivation: 现有无人机搜救任务规划中，地形感知路径规划研究不足，影响部署效率。

Method: 使用深度学习分割网络处理卫星图像，生成网格化地形表示，并采用两阶段分区方案优化路径规划。

Result: 在高保真仿真环境中验证，该方法在搜索和调度时间上优于多种元启发式技术和现有先进方法。

Conclusion: 该方法在大规模搜救任务中具有潜力，能显著提升响应速度和无人机协调效率。

Abstract: Unmanned Aerial Vehicles (UAVs) have become vital in search-and-rescue (SAR)
missions, with autonomous mission planning improving response times and
coverage efficiency. Early approaches primarily used path planning techniques
such as A*, potential-fields, or Dijkstra's algorithm, while recent approaches
have incorporated meta-heuristic frameworks like genetic algorithms and
particle swarm optimization to balance competing objectives such as network
connectivity, energy efficiency, and strategic placement of charging stations.
However, terrain-aware path planning remains under-explored, despite its
critical role in optimizing UAV SAR deployments. To address this gap, we
present a computer-vision based terrain-aware mission planner that autonomously
extracts and analyzes terrain topology to enhance SAR pre-flight planning. Our
framework uses a deep segmentation network fine-tuned on our own collection of
landcover datasets to transform satellite imagery into a structured, grid-based
representation of the operational area. This classification enables
terrain-specific UAV-task allocation, improving deployment strategies in
complex environments. We address the challenge of irregular terrain partitions,
by introducing a two-stage partitioning scheme that first evaluates terrain
monotonicity along coordinate axes before applying a cost-based recursive
partitioning process, minimizing unnecessary splits and optimizing path
efficiency. Empirical validation in a high-fidelity simulation environment
demonstrates that our approach improves search and dispatch time over multiple
meta-heuristic techniques and against a competing state-of-the-art method.
These results highlight its potential for large-scale SAR operations, where
rapid response and efficient UAV coordination are critical.

</details>


### [203] [PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints](https://arxiv.org/abs/2505.08025)
*Hannah Lee,Zachary Serlin,James Motes,Brendan Long,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: PRISM是一种去中心化算法，用于解决多任务多智能体路径规划问题，通过快速信息共享和运动约束实现高效协作路径规划。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在复杂动态环境中高效协作路径规划的挑战，尤其是避免死锁和提升可扩展性。

Method: 采用快速通信策略，通过信息包交换运动约束信息，支持无直接通信场景下的协作。

Result: PRISM在5种环境和25个随机场景中表现优异，支持比CBS多3.4倍的智能体，比TPTS多处理2.5倍的任务，且计算速度更快。

Conclusion: PRISM在复杂动态路径规划场景中表现出鲁棒性、可扩展性和高效性，适用于大规模环境。

Abstract: We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion
Constraints), a decentralized algorithm designed to address the multi-task
multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents
to concurrently plan safe and efficient paths for multiple tasks while avoiding
collisions. It employs a rapid communication strategy that uses information
packets to exchange motion constraint information, enhancing cooperative
pathfinding and situational awareness, even in scenarios without direct
communication. We prove that PRISM resolves and avoids all deadlock scenarios
when possible, a critical challenge in decentralized pathfinding. Empirically,
we evaluate PRISM across five environments and 25 random scenarios,
benchmarking it against the centralized Conflict-Based Search (CBS) and the
decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM
demonstrates scalability and solution quality, supporting 3.4 times more agents
than CBS and handling up to 2.5 times more tasks in narrow passage environments
than TPTS. Additionally, PRISM matches CBS in solution quality while achieving
faster computation times, even under low-connectivity conditions. Its
decentralized design reduces the computational burden on individual agents,
making it scalable for large environments. These results confirm PRISM's
robustness, scalability, and effectiveness in complex and dynamic pathfinding
scenarios.

</details>


### [204] [What Matters for Batch Online Reinforcement Learning in Robotics?](https://arxiv.org/abs/2505.08078)
*Perry Dong,Suvir Mirchandani,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TL;DR: 论文研究了批量在线强化学习（batch online RL）在机器人学习中的有效性，提出了三个关键因素（算法类别、策略提取方法和策略表达能力），并基于分析提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 批量在线强化学习有望通过减少人工数据收集需求并利用自主数据实现自我改进，从而提升机器人学习的可扩展性。然而，现有方法难以高效利用自主数据，因此需要研究如何有效实现这一目标。

Method: 通过系统性实证研究，分析了算法类别、策略提取方法和策略表达能力三个因素对性能的影响，并提出了一种改进方法，包括使用Q函数引导、隐式策略提取和表达性策略类。

Result: 研究发现，基于Q函数的方法优于模仿学习方法，隐式策略提取和表达性策略类能显著提升性能。此外，引入时间相关噪声可进一步提高性能。

Conclusion: 论文提出了一种有效的批量在线强化学习方法，显著优于现有方法，并展示了其在性能和扩展性上的优势。

Abstract: The ability to learn from large batches of autonomously collected data for
policy improvement -- a paradigm we refer to as batch online reinforcement
learning -- holds the promise of enabling truly scalable robot learning by
significantly reducing the need for human effort of data collection while
getting benefits from self-improvement. Yet, despite the promise of this
paradigm, it remains challenging to achieve due to algorithms not being able to
learn effectively from the autonomous data. For example, prior works have
applied imitation learning and filtered imitation learning methods to the batch
online RL problem, but these algorithms often fail to efficiently improve from
the autonomously collected data or converge quickly to a suboptimal point. This
raises the question of what matters for effective batch online RL in robotics.
Motivated by this question, we perform a systematic empirical study of three
axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy
expressivity -- and analyze how these axes affect performance and scaling with
the amount of autonomous data. Through our analysis, we make several
observations. First, we observe that the use of Q-functions to guide batch
online RL significantly improves performance over imitation-based methods.
Building on this, we show that an implicit method of policy extraction -- via
choosing the best action in the distribution of the policy -- is necessary over
traditional policy extraction methods from offline RL. Next, we show that an
expressive policy class is preferred over less expressive policy classes. Based
on this analysis, we propose a general recipe for effective batch online RL. We
then show a simple addition to the recipe of using temporally-correlated noise
to obtain more diversity results in further performance gains. Our recipe
obtains significantly better performance and scaling compared to prior methods.

</details>


### [205] [UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations](https://arxiv.org/abs/2505.08787)
*Hanjung Kim,Jaehyun Kang,Hyolim Kang,Meedeum Cho,Seon Joo Kim,Youngwoon Lee*

Main category: cs.RO

TL;DR: UniSkill框架通过无标签的大规模跨体现视频数据学习技能表示，实现从人类视频提示到机器人策略的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 模仿是人类学习新任务的基本机制，但机器人模仿人类面临体现差异的挑战。现有方法依赖对齐数据，但大规模收集困难。

Method: 提出UniSkill框架，从无标签的跨体现视频数据中学习技能表示，实现人类视频提示到机器人策略的迁移。

Result: 实验表明，UniSkill能成功指导机器人选择合适动作，即使面对未见过的视频提示。

Conclusion: UniSkill为跨体现技能迁移提供了有效解决方案，无需对齐数据。

Abstract: Mimicry is a fundamental learning mechanism in humans, enabling individuals
to learn new tasks by observing and imitating experts. However, applying this
ability to robots presents significant challenges due to the inherent
differences between human and robot embodiments in both their visual appearance
and physical capabilities. While previous methods bridge this gap using
cross-embodiment datasets with shared scenes and tasks, collecting such aligned
data between humans and robots at scale is not trivial. In this paper, we
propose UniSkill, a novel framework that learns embodiment-agnostic skill
representations from large-scale cross-embodiment video data without any
labels, enabling skills extracted from human video prompts to effectively
transfer to robot policies trained only on robot data. Our experiments in both
simulation and real-world environments show that our cross-embodiment skills
successfully guide robots in selecting appropriate actions, even with unseen
video prompts. The project website can be found at:
https://kimhanjung.github.io/UniSkill.

</details>


### [206] [Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles](https://arxiv.org/abs/2505.08222)
*Matteo Gallici,Ivan Masmitja,Mario Martín*

Main category: cs.RO

TL;DR: 论文提出了一种迭代蒸馏方法，将高保真仿真转移到简化的GPU加速环境中，显著提升多智能体强化学习（MARL）的训练效率，并通过Transformer架构（TransfMAPPO）提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在复杂海洋环境中控制自动驾驶车辆（AV）时存在计算效率低的问题，尤其是在多目标跟踪场景中。高保真仿真工具如Gazebo无法显著加速多车训练，限制了MARL的实际应用。

Method: 提出迭代蒸馏方法，将高保真仿真转移到简化的GPU加速环境，并结合Transformer架构（TransfMAPPO）学习多智能体策略。通过大规模课程学习在GPU上进行训练。

Result: 方法实现了高达30,000倍的加速，跟踪误差保持在5米以下，适用于多快速移动目标的场景。

Conclusion: 该研究填补了大规模MARL训练与高保真部署之间的鸿沟，为实际海洋任务中的自动驾驶车队控制提供了可扩展框架。

Abstract: Autonomous vehicles (AV) offer a cost-effective solution for scientific
missions such as underwater tracking. Recently, reinforcement learning (RL) has
emerged as a powerful method for controlling AVs in complex marine
environments. However, scaling these techniques to a fleet--essential for
multi-target tracking or targets with rapid, unpredictable motion--presents
significant computational challenges. Multi-Agent Reinforcement Learning (MARL)
is notoriously sample-inefficient, and while high-fidelity simulators like
Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,
they offer no significant speedup for multi-vehicle scenarios, making MARL
training impractical. To address these limitations, we propose an iterative
distillation method that transfers high-fidelity simulations into a simplified,
GPU-accelerated environment while preserving high-level dynamics. This approach
achieves up to a 30,000x speedup over Gazebo through parallelization, enabling
efficient training via end-to-end GPU acceleration. Additionally, we introduce
a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent
policies invariant to the number of agents and targets, significantly improving
sample efficiency. Following large-scale curriculum learning conducted entirely
on GPU, we perform extensive evaluations in Gazebo, demonstrating that our
method maintains tracking errors below 5 meters over extended durations, even
in the presence of multiple fast-moving targets. This work bridges the gap
between large-scale MARL training and high-fidelity deployment, providing a
scalable framework for autonomous fleet control in real-world sea missions.

</details>


### [207] [Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation](https://arxiv.org/abs/2505.08223)
*Dohyun Kim,Jayden Dongwoo Lee,Hyochoong Bang,Jungho Bae*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习和Transformer的混合故障容错控制框架，用于多旋翼飞行器，无需重新训练即可适应新配置。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器在应用中易受执行器故障影响，现有方法需要先验知识或难以适应新配置。

Method: 结合强化学习和Transformer架构，实时推断潜在表示以适配未知系统模型。

Result: 在PyBullet仿真中，成功率95%，位置RMSE为0.129米，优于现有方法。

Conclusion: 该框架显著提升了多旋翼飞行器在动态环境中的适应性和可靠性。

Abstract: Multirotors play a significant role in diverse field robotics applications
but remain highly susceptible to actuator failures, leading to rapid
instability and compromised mission reliability. While various fault-tolerant
control (FTC) strategies using reinforcement learning (RL) have been widely
explored, most previous approaches require prior knowledge of the multirotor
model or struggle to adapt to new configurations. To address these limitations,
we propose a novel hybrid RL-based FTC framework integrated with a
transformer-based online adaptation module. Our framework leverages a
transformer architecture to infer latent representations in real time, enabling
adaptation to previously unseen system models without retraining. We evaluate
our method in a PyBullet simulation under loss-of-effectiveness actuator
faults, achieving a 95% success rate and a positional root mean square error
(RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success
and an RMSE of 0.153 m. Further evaluations on quadrotors with varying
configurations confirm the robustness of our framework across untrained
dynamics. These results demonstrate the potential of our framework to enhance
the adaptability and reliability of multirotors, enabling efficient fault
management in dynamic and uncertain environments. Website is available at
http://00dhkim.me/paper/rl-ftc

</details>


### [208] [Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning](https://arxiv.org/abs/2505.08264)
*Ahmed Abouelazm,Tim Weinstein,Tim Joseph,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 提出了一种自动课程学习框架，通过动态生成适应复杂度的驾驶场景，提升强化学习自动驾驶代理的训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习在固定场景和模拟环境中训练自动驾驶代理时泛化能力不足的问题，避免领域随机化带来的训练低效和策略次优。

Method: 设计了一个自动课程学习框架，包含一个‘教师’模块，根据代理当前策略动态生成和调整驾驶场景的复杂度，排除已掌握或过于困难的场景。

Result: 相比基线方法（固定场景训练和领域随机化），该方法在低密度交通中成功率提升9%，高密度交通中提升21%，且收敛更快。

Conclusion: 自动课程学习框架能显著提升强化学习自动驾驶代理的鲁棒性和训练效率。

Abstract: This paper addresses the challenges of training end-to-end autonomous driving
agents using Reinforcement Learning (RL). RL agents are typically trained in a
fixed set of scenarios and nominal behavior of surrounding road users in
simulations, limiting their generalization and real-life deployment. While
domain randomization offers a potential solution by randomly sampling driving
scenarios, it frequently results in inefficient training and sub-optimal
policies due to the high variance among training scenarios. To address these
limitations, we propose an automatic curriculum learning framework that
dynamically generates driving scenarios with adaptive complexity based on the
agent's evolving capabilities. Unlike manually designed curricula that
introduce expert bias and lack scalability, our framework incorporates a
``teacher'' that automatically generates and mutates driving scenarios based on
their learning potential -- an agent-centric metric derived from the agent's
current policy -- eliminating the need for expert design. The framework
enhances training efficiency by excluding scenarios the agent has mastered or
finds too challenging. We evaluate our framework in a reinforcement learning
setting where the agent learns a driving policy from camera images. Comparative
results against baseline methods, including fixed scenario training and domain
randomization, demonstrate that our approach leads to enhanced generalization,
achieving higher success rates: +9\% in low traffic density, +21\% in high
traffic density, and faster convergence with fewer training steps. Our findings
highlight the potential of ACL in improving the robustness and efficiency of
RL-based autonomous driving agents.

</details>


### [209] [Adaptive Diffusion Policy Optimization for Robotic Manipulation](https://arxiv.org/abs/2505.08376)
*Huiyun Jiang,Zhuang Yang*

Main category: cs.RO

TL;DR: 本文提出了一种基于Adam的扩散策略优化（ADPO）框架，用于快速稳定地微调扩散策略在机器人控制任务中的表现。实验表明，ADPO在标准任务中优于其他扩散强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在强化学习中表现出潜力，但目前缺乏快速稳定优化扩散策略的研究。

Method: 提出ADPO框架，采用自适应梯度下降方法微调扩散策略。

Result: ADPO在标准机器人任务中表现优于或与基准方法相当。

Conclusion: ADPO为扩散策略优化提供了高效解决方案，并分析了超参数敏感性，为实际应用提供指导。

Abstract: Recent studies have shown the great potential of diffusion models in
improving reinforcement learning (RL) by modeling complex policies, expressing
a high degree of multi-modality, and efficiently handling high-dimensional
continuous control tasks. However, there is currently limited research on how
to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably.
In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a
fast algorithmic framework containing best practices for fine-tuning
diffusion-based polices in robotic control tasks using the adaptive gradient
descent method in RL. Adaptive gradient method is less studied in training RL,
let alone diffusion-based policies. We confirm that ADPO outperforms other
diffusion-based RL methods in terms of overall effectiveness for fine-tuning on
standard robotic tasks. Concretely, we conduct extensive experiments on
standard robotic control tasks to test ADPO, where, particularly, six popular
diffusion-based RL methods are provided as benchmark methods. Experimental
results show that ADPO acquires better or comparable performance than the
baseline methods. Finally, we systematically analyze the sensitivity of
multiple hyperparameters in standard robotics tasks, providing guidance for
subsequent practical applications. Our video demonstrations are released in
https://github.com/Timeless-lab/ADPO.git.

</details>


### [210] [From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation](https://arxiv.org/abs/2505.08548)
*Yifu Yuan,Haiqin Cui,Yibin Chen,Zibin Dong,Fei Ni,Longxin Kou,Jinyi Liu,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: FSD模型通过空间关系推理生成中间表示，显著提升了机器人操作的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉-语言-动作模型在未见场景和新任务中泛化能力不足的问题。

Method: 提出FSD模型，结合分层数据管道和自一致性机制，对齐空间坐标与视觉信号。

Result: 在8个基准测试和VABench上表现优异，零样本机器人操作成功率显著提升。

Conclusion: FSD在空间推理和机器人操作中表现出色，为泛化问题提供了有效解决方案。

Abstract: Achieving generalization in robotic manipulation remains a critical
challenge, particularly for unseen scenarios and novel tasks. Current
Vision-Language-Action (VLA) models, while building on top of general
Vision-Language Models (VLMs), still fall short of achieving robust zero-shot
performance due to the scarcity and heterogeneity prevalent in embodied
datasets. To address these limitations, we propose FSD (From Seeing to Doing),
a novel vision-language model that generates intermediate representations
through spatial relationship reasoning, providing fine-grained guidance for
robotic manipulation. Our approach combines a hierarchical data pipeline for
training with a self-consistency mechanism that aligns spatial coordinates with
visual signals. Through extensive experiments, we comprehensively validated
FSD's capabilities in both "seeing" and "doing," achieving outstanding
performance across 8 benchmarks for general spatial reasoning and embodied
reference abilities, as well as on our proposed more challenging benchmark
VABench. We also verified zero-shot capabilities in robot manipulation,
demonstrating significant performance improvements over baseline methods in
both SimplerEnv and real robot settings. Experimental results show that FSD
achieves 54.1% success rate in SimplerEnv and 72% success rate across 8
real-world tasks, outperforming the strongest baseline by 30%.

</details>


### [211] [A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches](https://arxiv.org/abs/2505.08657)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 研究评估了基于视觉的触觉传感器在15种活动分类中的表现，并与基于IMU的数据手套进行比较，提出多模态框架结合触觉和运动数据，结果显示多模态方法优于单模态。


<details>
  <summary>Details</summary>
Motivation: 提升人机协作（HRC）中的人类活动识别（HAR）能力，通过结合触觉和运动数据的互补优势。

Method: 比较了三种方法：基于运动的分类（MBC）、基于触觉的分类（TBC）以及多模态分类（MMC），并进行了离线和在线验证。

Result: 多模态方法在分类准确性和在线性能上均优于单模态方法。

Conclusion: 结合触觉和运动传感可显著提升HAR系统在人机协作中的表现。

Abstract: Human activity recognition (HAR) is essential for effective Human-Robot
Collaboration (HRC), enabling robots to interpret and respond to human actions.
This study evaluates the ability of a vision-based tactile sensor to classify
15 activities, comparing its performance to an IMU-based data glove.
Additionally, we propose a multi-modal framework combining tactile and motion
data to leverage their complementary strengths. We examined three approaches:
motion-based classification (MBC) using IMU data, tactile-based classification
(TBC) with single or dual video streams, and multi-modal classification (MMC)
integrating both. Offline validation on segmented datasets assessed each
configuration's accuracy under controlled conditions, while online validation
on continuous action sequences tested online performance. Results showed the
multi-modal approach consistently outperformed single-modality methods,
highlighting the potential of integrating tactile and motion sensing to enhance
HAR systems for collaborative robotics.

</details>


### [212] [A Social Robot with Inner Speech for Dietary Guidance](https://arxiv.org/abs/2505.08664)
*Valerio Belcamino,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: 研究探讨了如何利用内部语音增强社交机器人在饮食建议中的透明度和信任度，通过显式化推理过程提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，信任机器人助手依赖于准确建议和自然对话，内部语音能提升透明度和信任。

Method: 开发了具备内部语音功能的社交机器人，结合大语言模型和知识图谱，显式化推理过程。

Result: 通过计算效率和用户研究验证了内部语音在解释机器人行为中的可靠性。

Conclusion: 内部语音显著提升了机器人的透明度和人机交互信任度。

Abstract: We explore the use of inner speech as a mechanism to enhance transparency and
trust in social robots for dietary advice. In humans, inner speech structures
thought processes and decision-making; in robotics, it improves explainability
by making reasoning explicit. This is crucial in healthcare scenarios, where
trust in robotic assistants depends on both accurate recommendations and
human-like dialogue, which make interactions more natural and engaging.
Building on this, we developed a social robot that provides dietary advice, and
we provided the architecture with inner speech capabilities to validate user
input, refine reasoning, and generate clear justifications. The system
integrates large language models for natural language understanding and a
knowledge graph for structured dietary information. By making decisions more
transparent, our approach strengthens trust and improves human-robot
interaction in healthcare. We validated this by measuring the computational
efficiency of our architecture and conducting a small user study, which
assessed the reliability of inner speech in explaining the robot's behavior.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [213] [Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity](https://arxiv.org/abs/2505.08316)
*Dazhong Rong,Hao Dong,Xing Gao,Jiyu Wei,Di Hong,Yaoyao Hao,Qinming He,Yueming Wang*

Main category: cs.CE

TL;DR: 论文提出了一种结合相对位置（RP）预测和对比学习的新方法，以更全面地建模腹侧视觉流（VVS），并证明其在下游任务和脑相似性上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注VVS在物体识别中的作用，忽略了其在位置感知（如RP预测）中的功能。作者认为VVS的功能更广泛，需结合RP预测以更贴近生物现实。

Method: 提出了一种新的无监督任务驱动方法，将RP学习与对比学习结合，以建模VVS的多功能特性。

Result: 实验表明，该方法显著提升了物体识别的下游性能，同时增强了RP预测能力，并提高了模型的脑相似性。

Conclusion: 研究从计算角度证明了VVS在位置感知（尤其是RP预测）中的重要作用，为更全面的VVS建模提供了新思路。

Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for
object recognition, current unsupervised task-driven methods model VVS by
contrastive learning, and have achieved good brain similarity. However, we
believe functions of VVS extend beyond just object recognition. In this paper,
we introduce an additional function involving VVS, named relative position (RP)
prediction. We first theoretically explain contrastive learning may be unable
to yield the model capability of RP prediction. Motivated by this, we
subsequently integrate RP learning with contrastive learning, and propose a new
unsupervised task-driven method to model VVS, which is more inline with
biological reality. We conduct extensive experiments, demonstrating that: (i)
our method significantly improves downstream performance of object recognition
while enhancing RP predictivity; (ii) RP predictivity generally improves the
model brain similarity. Our results provide strong evidence for the involvement
of VVS in location perception (especially RP prediction) from a computational
perspective.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [214] [A Survey of Deep Learning for Complex Speech Spectrograms](https://arxiv.org/abs/2505.08694)
*Yuying Xie,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: 本文综述了深度学习在复杂谱图处理中的最新技术，包括网络架构、训练策略和应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在语音信号处理中的复杂谱图分析方面取得了显著进展，本文旨在为研究者和从业者提供全面的技术概述。

Method: 介绍了复杂谱图及其特征，探讨了复数神经网络的关键组件和架构，并讨论了针对复杂谱图处理的训练策略和损失函数。

Result: 深度学习在相位恢复、语音增强和语音分离等应用中取得了显著进展。

Conclusion: 本文为语音信号处理和复数神经网络领域的研究者提供了有价值的资源。

Abstract: Recent advancements in deep learning have significantly impacted the field of
speech signal processing, particularly in the analysis and manipulation of
complex spectrograms. This survey provides a comprehensive overview of the
state-of-the-art techniques leveraging deep neural networks for processing
complex spectrograms, which encapsulate both magnitude and phase information.
We begin by introducing complex spectrograms and their associated features for
various speech processing tasks. Next, we explore the key components and
architectures of complex-valued neural networks, which are specifically
designed to handle complex-valued data and have been applied for complex
spectrogram processing. We then discuss various training strategies and loss
functions tailored for training neural networks to process and model complex
spectrograms. The survey further examines key applications, including phase
retrieval, speech enhancement, and speech separation, where deep learning has
achieved significant progress by leveraging complex spectrograms or their
derived feature representations. Additionally, we examine the intersection of
complex spectrograms with generative models. This survey aims to serve as a
valuable resource for researchers and practitioners in the field of speech
signal processing and complex-valued neural networks.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [215] [Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability](https://arxiv.org/abs/2505.07896)
*Douglas Jiang,Zilin Dai,Luxuan Zhang,Qiyi Yu,Haoqi Sun,Feng Tian*

Main category: q-bio.GN

TL;DR: 提出了一种利用NCBI Gene数据库的基因注释和大型语言模型生成生物上下文细胞嵌入的新框架，用于单细胞RNA测序数据的分析。


<details>
  <summary>Details</summary>
Motivation: 解决单细胞水平测序数据中细胞身份和功能理解的挑战。

Method: 通过表达水平排序基因，获取NCBI Gene描述，使用LLMs（如OpenAI和BioBERT）将其转化为向量嵌入，并通过表达加权平均生成细胞嵌入。

Result: 生成紧凑且语义丰富的细胞嵌入，支持下游应用如细胞类型聚类和轨迹推断。

Conclusion: 该多模态策略结合生物数据和语言模型，提高了单细胞数据分析的效率和可解释性。

Abstract: Understanding cell identity and function through single-cell level sequencing
data remains a key challenge in computational biology. We present a novel
framework that leverages gene-specific textual annotations from the NCBI Gene
database to generate biologically contextualized cell embeddings. For each cell
in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by
expression level, retrieve their NCBI Gene descriptions, and transform these
descriptions into vector embedding representations using large language models
(LLMs). The models used include OpenAI text-embedding-ada-002,
text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as
domain-specific models BioBERT and SciBERT. Embeddings are computed via an
expression-weighted average across the top N most highly expressed genes in
each cell, providing a compact, semantically rich representation. This
multimodal strategy bridges structured biological data with state-of-the-art
language modeling, enabling more interpretable downstream applications such as
cell-type clustering, cell vulnerability dissection, and trajectory inference.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [216] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/abs/2505.08175)
*Zachary Novack,Zach Evans,Zack Zukowski,Josiah Taylor,CJ Carr,Julian Parker,Adnan Al-Sinan,Gian Marco Iodice,Julian McAuley,Taylor Berg-Kirkpatrick,Jordi Pons*

Main category: cs.SD

TL;DR: 提出了一种名为ARC的对抗性加速算法，用于提升文本到音频系统的推理速度，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频系统推理速度慢，延迟高，限制了其在创意应用中的实用性。

Method: 采用对抗性相对对比（ARC）后训练方法，结合相对对抗性公式和对比鉴别器目标，优化Stable Audio Open模型。

Result: 在H100上生成约12秒44.1kHz立体声音频仅需约75毫秒，移动设备上约7秒，成为目前最快的文本到音频模型。

Conclusion: ARC后训练方法显著提升了推理速度，适用于高性能和边缘设备，推动了文本到音频技术的实际应用。

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [217] [A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization](https://arxiv.org/abs/2505.08681)
*Xiaoliang He,Kangjie Dong,Jingkai Cao,Shuai Yu,Wei Li,Yi Yu*

Main category: cs.SD

TL;DR: 提出了一种基于Mamba的网络SpectMamba，用于半监督歌唱旋律提取，解决了现有方法在计算效率、频率监督和标注数据不足方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱旋律提取方法存在计算效率低、频率监督忽略音符基础以及标注数据不足的问题。

Method: 引入视觉Mamba实现线性计算复杂度，提出音符-f0解码器模拟音乐表演，并设计置信二元正则化模块利用未标注数据。

Result: 在多个公开数据集上验证了方法的有效性。

Conclusion: SpectMamba通过改进计算效率、音符建模和数据利用，显著提升了歌唱旋律提取的性能。

Abstract: Singing melody extraction (SME) is a key task in the field of music
information retrieval. However, existing methods are facing several
limitations: firstly, prior models use transformers to capture the contextual
dependencies, which requires quadratic computation resulting in low efficiency
in the inference stage. Secondly, prior works typically rely on
frequencysupervised methods to estimate the fundamental frequency (f0), which
ignores that the musical performance is actually based on notes. Thirdly,
transformers typically require large amounts of labeled data to achieve optimal
performances, but the SME task lacks of sufficient annotated data. To address
these issues, in this paper, we propose a mamba-based network, called
SpectMamba, for semi-supervised singing melody extraction using confidence
binary regularization. In particular, we begin by introducing vision mamba to
achieve computational linear complexity. Then, we propose a novel note-f0
decoder that allows the model to better mimic the musical performance. Further,
to alleviate the scarcity of the labeled data, we introduce a confidence binary
regularization (CBR) module to leverage the unlabeled data by maximizing the
probability of the correct classes. The proposed method is evaluated on several
public datasets and the conducted experiments demonstrate the effectiveness of
our proposed method.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [218] [Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations](https://arxiv.org/abs/2505.08195)
*Jinming Hu,Hassan Nawaz,Yuting Rui,Lijie Chi,Arif Ullah,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: Aitomia是一个由AI驱动的平台，旨在辅助原子和量子化学模拟，通过聊天机器人和AI代理帮助专家和非专家完成模拟设置、运行、监控和分析，并生成文本和图形结果。


<details>
  <summary>Details</summary>
Motivation: 降低原子模拟的门槛，加速相关领域的研究和开发。

Method: 利用微调的开源大语言模型（LLMs）、基于规则的代理和检索增强生成（RAG）系统。

Result: Aitomia已部分公开，并计划集成到Aitomistic Hub和XACS在线计算服务中。

Conclusion: Aitomia有望简化原子模拟流程，推动相关领域的研究进展。

Abstract: We have developed Aitomia - a platform powered by AI to assist in performing
AI-driven atomistic and quantum chemical (QC) simulations. This intelligent
assistant platform is equipped with chatbots and AI agents to help experts and
guide non-experts in setting up and running the atomistic simulations,
monitoring their computation status, analyzing the simulation results, and
summarizing them for the user in text and graphical forms. We achieve these
goals by exploiting fine-tuned open-source large language models (LLMs),
rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia
leverages the versatility of our MLatom ecosystem for AI-enhanced computational
chemistry. This intelligent assistant is going to be integrated into the
Aitomistic Hub and XACS online computing services, with some functionality
already publicly available as described at http://mlatom.com/aitomia. Aitomia
is expected to lower the barrier to performing atomistic simulations,
accelerating research and development in the relevant fields.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [219] [Non-contact Vital Signs Detection in Dynamic Environments](https://arxiv.org/abs/2505.08366)
*Shuai Sun,Chong-Xi Liang,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang*

Main category: eess.SP

TL;DR: 提出了一种新的DC偏移校准方法和HADCM解调算法，用于复杂环境下的毫米波雷达生命体征检测，显著提升了解调性能。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中时变DC偏移和相位不平衡会严重影响解调性能，需要一种更鲁棒的方法。

Method: 通过估计信号峰谷的时变DC偏移，结合差分形式和希尔伯特变换提取生命体征信息。

Result: 仿真和实验表明，该方法在低信噪比下仍保持鲁棒性能，比现有技术更准确且能有效抑制噪声干扰。

Conclusion: 提出的方法在复杂环境下显著提升了解调性能，适用于毫米波雷达生命体征检测。

Abstract: Accurate phase demodulation is critical for vital sign detection using
millimeter-wave radar. However, in complex environments, time-varying DC
offsets and phase imbalances can severely degrade demodulation performance. To
address this, we propose a novel DC offset calibration method alongside a
Hilbert and Differential Cross-Multiply (HADCM) demodulation algorithm. The
approach estimates time-varying DC offsets from neighboring signal peaks and
valleys, then employs both differential forms and Hilbert transforms of the I/Q
channel signals to extract vital sign information. Simulation and experimental
results demonstrate that the proposed method maintains robust performance under
low signal-to-noise ratios. Compared to existing demodulation techniques, it
offers more accurate signal recovery in challenging scenarios and effectively
suppresses noise interference.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [220] [Big Data and the Computational Social Science of Entrepreneurship and Innovation](https://arxiv.org/abs/2505.08706)
*Ningzi Li,Shiyang Lai,James Evans*

Main category: econ.GN

TL;DR: 论文探讨了利用大规模数据和机器学习方法研究创业与创新的机遇与挑战，提出了两种利用新型数据的方法，并强调结合大数据与大模型推动理论发展。


<details>
  <summary>Details</summary>
Motivation: 随着大规模社交数据的爆炸式增长和机器学习方法的演进，创业与创新研究面临新的机遇与挑战，需要解决如何利用这些数据识别技术及商业新颖性等问题。

Method: 提出两种方法：1）结合机器学习模型与大规模数据构建精确测量工具；2）利用大数据驱动的AI模型生成技术和商业的“数字双胞胎”，用于虚拟实验。

Result: 通过这两种方法，可以构建系统级的创新与创业观测工具，并为创新与创业过程及政策提供虚拟实验环境。

Conclusion: 论文主张将大数据与大模型结合，以推动创业与创新领域的理论发展与测试。

Abstract: As large-scale social data explode and machine-learning methods evolve,
scholars of entrepreneurship and innovation face new research opportunities but
also unique challenges. This chapter discusses the difficulties of leveraging
large-scale data to identify technological and commercial novelty, document new
venture origins, and forecast competition between new technologies and
commercial forms. It suggests how scholars can take advantage of new text,
network, image, audio, and video data in two distinct ways that advance
innovation and entrepreneurship research. First, machine-learning models,
combined with large-scale data, enable the construction of precision
measurements that function as system-level observatories of innovation and
entrepreneurship across human societies. Second, new artificial intelligence
models fueled by big data generate 'digital doubles' of technology and
business, forming laboratories for virtual experimentation about innovation and
entrepreneurship processes and policies. The chapter argues for the advancement
of theory development and testing in entrepreneurship and innovation by
coupling big data with big models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [221] [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
*Licheng Zhang,Bach Le,Naveed Akhtar,Siew-Kei Lam,Tuan Ngo*

Main category: cs.LG

TL;DR: 本文首次系统综述了大型语言模型（LLMs）与计算机辅助设计（CAD）的结合，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着现代设计复杂度的增加，LLMs在优化CAD流程方面具有巨大潜力，但目前缺乏相关综述研究。

Method: 文章首先概述了CAD的工业意义和LLMs的基础，随后分类探讨了LLMs在CAD中的六大应用领域。

Result: 提出了LLMs在CAD中的六类关键应用，并指出了未来研究方向。

Conclusion: LLMs与CAD的结合为技术创新提供了广阔空间，将塑造CAD技术的未来。

Abstract: Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy

</details>


### [222] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/abs/2505.07045)
*Junjie Yu,John S. Schreck,David John Gagne,Keith W. Oleson,Jie Li,Yongtu Liang,Qi Liao,Mingfei Sun,David O. Topping,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一种结合强化学习（RL）与城市气候模型的框架，评估了RL在HVAC控制中的效果及其对室内和城市气候的影响，发现效果因背景气候而异，并强调了跨城市学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索RL在HVAC控制中的效果及其对气候的影响，尤其是在不同背景气候下的表现和策略的可转移性。

Method: 结合RL与城市气候模型，评估不同气候条件下RL策略的效果及其对室内和城市气候的影响。

Result: RL策略的效果因城市背景气候而异，热气候城市表现更好，且策略可转移性与气温变化相关。

Conclusion: 需在不同气候条件下全面评估RL策略，跨城市学习有助于RL在HVAC控制中的部署。

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [223] [Blockbuster, Part 1: Block-level AI Operator Fusion](https://arxiv.org/abs/2505.07829)
*Ofer Dekel*

Main category: cs.LG

TL;DR: Blockbuster是一个用于AI算子融合的框架，兼容多处理器架构，通过块程序和数据移动建模实现高效融合。


<details>
  <summary>Details</summary>
Motivation: 解决AI程序中算子融合的挑战，特别是在多处理器架构中优化数据移动和计算效率。

Method: 采用基于图的块程序表示和两阶段融合算法（候选选择与规则融合），直接建模内存层级间的数据移动。

Result: 成功重新发现Flash Attention内核，并实现复杂算子（如LayerNorm与矩阵乘法）的高效融合。

Conclusion: Blockbuster的融合算法通过直接建模数据移动，显著提升了AI程序的性能，适用于大规模应用。

Abstract: Blockbuster is a framework for AI operator fusion in inference programs. The
Blockbuster framework is compatible with any multiprocessor architecture that
has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI
accelerator chips. It includes a graph-based representation for AI workloads,
called a block program, which explicitly models how blocks of data move between
the memory tiers. It also includes an operator fusion procedure, which is made
up of a candidate selection algorithm and a fusion algorithm that fuses each
individual candidate - this two-algorithm structure makes Blockbuster
especially suitable for large AI programs. The current paper focuses on the
fusion algorithm, which is a rule-based technique. While the literature is full
of previous rule-based fusion algorithms, what sets our algorithm apart is its
direct modeling of data movement between memory tiers, resulting in uniquely
powerful fusion results. As a first sanity check, we demonstrate how our
algorithm automatically rediscovers the well-known Flash Attention kernel.
Then, we demonstrate the real power of our approach by fusing LayerNorm with
matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing
three matrix multiplications, a Hadamard product, a reduction, and a few
elementwise operations into a single mega-kernel.

</details>


### [224] [A General Approach of Automated Environment Design for Learning the Optimal Power Flow](https://arxiv.org/abs/2505.07832)
*Thomas Wolgast,Astrid Nieße*

Main category: cs.LG

TL;DR: 本文提出了一种利用多目标优化自动设计强化学习（RL）环境的方法，并在最优潮流（OPF）问题上验证了其优于手动设计的效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何设计RL环境以最大化训练性能，尤其是在OPF问题中，目前尚无明确答案。

Method: 采用多目标优化方法，结合超参数优化（HPO）框架，自动设计RL环境。

Result: 在五个OPF基准问题上，自动设计方法表现优于手动设计，并揭示了环境设计中的关键因素。

Conclusion: 这是首个通用的自动化RL环境设计方法，但也需警惕对特定RL算法的过拟合风险。

Abstract: Reinforcement learning (RL) algorithms are increasingly used to solve the
optimal power flow (OPF) problem. Yet, the question of how to design RL
environments to maximize training performance remains unanswered, both for the
OPF and the general case. We propose a general approach for automated RL
environment design by utilizing multi-objective optimization. For that, we use
the hyperparameter optimization (HPO) framework, which allows the reuse of
existing HPO algorithms and methods. On five OPF benchmark problems, we
demonstrate that our automated design approach consistently outperforms a
manually created baseline environment design. Further, we use statistical
analyses to determine which environment design decisions are especially
important for performance, resulting in multiple novel insights on how RL-OPF
environments should be designed. Finally, we discuss the risk of overfitting
the environment to the utilized RL algorithm. To the best of our knowledge,
this is the first general approach for automated RL environment design.

</details>


### [225] [Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks](https://arxiv.org/abs/2505.07895)
*Jiafan Li,Jiaqi Zhu,Liang Chang,Yilin Li,Miaomiao Li,Yang Wang,Hongan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为HGNN-IMA的新模型，用于多模态异构网络（MMHNs）中的节点分类，通过跨模态注意力机制实现自适应多模态融合，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法（早期或晚期融合）存在模态特性丢失或跨模态指导不足的问题，需要一种更有效的方法来处理多模态异构网络中的节点分类。

Method: HGNN-IMA模型结合了跨模态注意力机制和异构图变换器，通过嵌套的跨模态注意力实现自适应多模态融合，并引入模态对齐和注意力损失来优化模型。

Result: 实验证明HGNN-IMA在节点分类任务中表现优越，为处理多模态数据提供了创新视角。

Conclusion: HGNN-IMA通过跨模态注意力机制和模态对齐，有效解决了多模态异构网络中的节点分类问题，为相关领域提供了新思路。

Abstract: Nowadays, numerous online platforms can be described as multi-modal
heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's
product review networks. Accurately categorizing nodes within these networks is
crucial for analyzing the corresponding entities, which requires effective
representation learning on nodes. However, existing multi-modal fusion methods
often adopt either early fusion strategies which may lose the unique
characteristics of individual modalities, or late fusion approaches overlooking
the cross-modal guidance in GNN-based information propagation. In this paper,
we propose a novel model for node classification in MMHNs, named Heterogeneous
Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node
representations by capturing the mutual influence of multiple modalities during
the information propagation process, within the framework of heterogeneous
graph transformer. Specifically, a nested inter-modal attention mechanism is
integrated into the inter-node attention to achieve adaptive multi-modal
fusion, and modality alignment is also taken into account to encourage the
propagation among nodes with consistent similarities across all modalities.
Moreover, an attention loss is augmented to mitigate the impact of missing
modalities. Extensive experiments validate the superiority of the model in the
node classification task, providing an innovative view to handle multi-modal
data, especially when accompanied with network structures.

</details>


### [226] [Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting](https://arxiv.org/abs/2505.07901)
*Minh-Duc Nguyen,Hyung-Jeong Yang,Soo-Hyung Kim,Ji-Eun Shin,Seung-Won Kim*

Main category: cs.LG

TL;DR: 论文提出了一种基于潜在行为扩散模型的新方法，用于生成与对话伙伴行为一致的面部反应，提升交互模拟的自然性和效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成多样且上下文相关的面部反应时的挑战。

Method: 结合上下文感知自编码器和扩散条件生成器，自编码器压缩输入特征，扩散生成器在潜在空间生成非自回归的面部反应。

Result: 实验表明该方法在二元反应合成任务中优于现有方法。

Conclusion: 潜在行为扩散模型能有效生成多样且上下文相关的面部反应。

Abstract: The dyadic reaction generation task involves synthesizing responsive facial
reactions that align closely with the behaviors of a conversational partner,
enhancing the naturalness and effectiveness of human-like interaction
simulations. This paper introduces a novel approach, the Latent Behavior
Diffusion Model, comprising a context-aware autoencoder and a diffusion-based
conditional generator that addresses the challenge of generating diverse and
contextually relevant facial reactions from input speaker behaviors. The
autoencoder compresses high-dimensional input features, capturing dynamic
patterns in listener reactions while condensing complex input data into a
concise latent representation, facilitating more expressive and contextually
appropriate reaction synthesis. The diffusion-based conditional generator
operates on the latent space generated by the autoencoder to predict realistic
facial reactions in a non-autoregressive manner. This approach allows for
generating diverse facial reactions that reflect subtle variations in
conversational cues and emotional states. Experimental results demonstrate the
effectiveness of our approach in achieving superior performance in dyadic
reaction synthesis tasks compared to existing methods.

</details>


### [227] [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
*Karahan Sarıtaş,Çağatay Yıldız*

Main category: cs.LG

TL;DR: 本文重新审视了自注意力机制实现核主成分分析（KPCA）的近期观点，发现其与实证数据不符，指出三点关键不一致性。


<details>
  <summary>Details</summary>
Motivation: 验证自注意力机制是否如Teo等人（2024）所声称的那样实现了KPCA，并分析其合理性。

Method: 通过比较学习到的自注意力值向量与KPCA理论预测的对应关系，评估相似性指标（如余弦相似度和CKA），并分析重构损失和Gram矩阵特征值统计。

Result: 实证数据显示自注意力值与KPCA预测无显著对应（相似性指标低），重构损失差异大，且特征值统计无法复现。

Conclusion: 自注意力的KPCA解释缺乏实证支持。

Abstract: In this reproduction study, we revisit recent claims that self-attention
implements kernel principal component analysis (KPCA) (Teo et al., 2024),
positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix
of the keys, and (ii) that self-attention projects queries onto the principal
component axes of the key matrix $K$ in a feature space. Our analysis reveals
three critical inconsistencies: (1) No alignment exists between learned
self-attention value vectors and what is proposed in the KPCA perspective, with
average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA
(Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating
negligible correspondence; (2) Reported decreases in reconstruction loss
$J_\text{proj}$, arguably justifying the claim that the self-attention
minimizes the projection error of KPCA, are misinterpreted, as the quantities
involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix
eigenvalue statistics, introduced to justify that $V$ captures the eigenvector
of the gram matrix, are irreproducible without undocumented
implementation-specific adjustments. Across 10 transformer architectures, we
conclude that the KPCA interpretation of self-attention lacks empirical
support.

</details>


### [228] [Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization](https://arxiv.org/abs/2505.07910)
*Alexander Hinterleitner,Thomas Bartz-Beielstein*

Main category: cs.LG

TL;DR: 论文提出了一种新的XAI一致性概念，并将其纳入超参数调优目标，通过多目标优化框架平衡预测性能和解释稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前超参数调优和神经网络架构优化中，解释性常被忽视，研究旨在填补这一空白。

Method: 引入XAI一致性指标，结合SPOT工具箱，采用加权聚合和基于期望的策略进行多目标优化。

Result: 发现架构配置空间中存在三个区域：性能差且解释性低、性能强但解释性弱、以及平衡两者的折中区域。

Conclusion: 研究为未来探索模型在平衡性能和XAI一致性时的鲁棒性奠定了基础。

Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI),
explainability is rarely considered during hyperparameter tuning or neural
architecture optimization, where the focus remains primarily on minimizing
predictive loss. In this work, we introduce the novel concept of XAI
consistency, defined as the agreement among different feature attribution
methods, and propose new metrics to quantify it. For the first time, we
integrate XAI consistency directly into the hyperparameter tuning objective,
creating a multi-objective optimization framework that balances predictive
performance with explanation robustness. Implemented within the Sequential
Parameter Optimization Toolbox (SPOT), our approach uses both weighted
aggregation and desirability-based strategies to guide model selection. Through
our proposed framework and supporting tools, we explore the impact of
incorporating XAI consistency into the optimization process. This enables us to
characterize distinct regions in the architecture configuration space: one
region with poor performance and comparatively low interpretability, another
with strong predictive performance but weak interpretability due to low
\gls{xai} consistency, and a trade-off region that balances both objectives by
offering high interpretability alongside competitive performance. Beyond
introducing this novel approach, our research provides a foundation for future
investigations into whether models from the trade-off zone-balancing
performance loss and XAI consistency-exhibit greater robustness by avoiding
overfitting to training performance, thereby leading to more reliable
predictions on out-of-distribution data.

</details>


### [229] [Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review](https://arxiv.org/abs/2505.07911)
*Chengmin Zhou,Ville Kyrki,Pasi Fränti,Laura Ruotsalainen*

Main category: cs.LG

TL;DR: 本文综述了贝叶斯推理与强化学习（RL）在智能体决策中的结合，总结了贝叶斯方法在数据效率、泛化性、可解释性和安全性方面的优势，并系统分析了其应用和最新进展。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理在智能体决策中具有数据效率、泛化性、可解释性和安全性等优势，但目前缺乏对其与强化学习结合的全面综述，本文旨在填补这一空白。

Method: 本文讨论了五种主题：1）适用于智能体决策的贝叶斯方法；2）贝叶斯方法与基于模型的RL、无模型RL和逆向RL的经典结合；3）贝叶斯方法与RL的最新结合；4）方法对比分析；5）六种复杂RL问题的深入讨论。

Result: 总结了贝叶斯方法在RL各阶段的应用，为智能体决策提供了更优策略。

Conclusion: 贝叶斯推理与RL的结合为智能体决策提供了显著优势，未来研究可进一步探索其潜力。

Abstract: Bayesian inference has many advantages in decision making of agents (e.g.
robotics/simulative agent) over a regular data-driven black-box neural network:
Data-efficiency, generalization, interpretability, and safety where these
advantages benefit directly/indirectly from the uncertainty quantification of
Bayesian inference. However, there are few comprehensive reviews to summarize
the progress of Bayesian inference on reinforcement learning (RL) for decision
making to give researchers a systematic understanding. This paper focuses on
combining Bayesian inference with RL that nowadays is an important approach in
agent decision making. To be exact, this paper discusses the following five
topics: 1) Bayesian methods that have potential for agent decision making.
First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and
Bayesian conjugate models) are discussed followed by variational inference,
Bayesian optimization, Bayesian deep learning, Bayesian active learning,
Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian
learning. 2) Classical combinations of Bayesian methods with model-based RL
(with approximation methods), model-free RL, and inverse RL. 3) Latest
combinations of potential Bayesian methods with RL. 4) Analytical comparisons
of methods that combine Bayesian methods with RL with respect to
data-efficiency, generalization, interpretability, and safety. 5) In-depth
discussions in six complex problem variants of RL, including unknown reward,
partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and
hierarchical RL problems and the summary of how Bayesian methods work in the
data collection, data processing and policy learning stages of RL to pave the
way for better agent decision-making strategies.

</details>


### [230] [Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids](https://arxiv.org/abs/2505.08082)
*Yuting Cai,Shaohuai Liu,Chao Tian,Le Xie*

Main category: cs.LG

TL;DR: 提出一种基于Fréchet距离的新指标，用于评估智能电网中生成模型的数据质量。


<details>
  <summary>Details</summary>
Motivation: 传统欧氏距离指标无法有效评估合成数据集的质量差异，需要一种从分布角度评估的方法。

Method: 基于Fréchet距离，在学习的特征空间中估计两个数据集之间的距离。

Result: 实证结果表明，该指标在不同时间尺度和模型中表现优越，提升了智能电网数据驱动决策的可靠性。

Conclusion: 提出的方法为生成模型数据质量评估提供了更有效的工具，有助于智能电网的优化运行。

Abstract: Generative artificial intelligence (AI) models in smart grids have advanced
significantly in recent years due to their ability to generate large amounts of
synthetic data, which would otherwise be difficult to obtain in the real world
due to confidentiality constraints. A key challenge in utilizing such synthetic
data is how to assess the data quality produced from such generative models.
Traditional Euclidean distance-based metrics only reflect pair-wise relations
between two individual samples, and could fail in evaluating quality
differences between groups of synthetic datasets. In this work, we propose a
novel metric based on the Fr\'{e}chet Distance (FD) estimated between two
datasets in a learned feature space. The proposed method evaluates the quality
of generation from a distributional perspective. Empirical results demonstrate
the superiority of the proposed metric across timescales and models, enhancing
the reliability of data-driven decision-making in smart grid operations.

</details>


### [231] [Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning](https://arxiv.org/abs/2505.07921)
*Qi Xu,Junyang Zhu,Dongdong Zhou,Hao Chen,Yang Liu,Jiangrong Shen,Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于脉冲神经网络（SNNs）的小样本学习框架，结合自特征提取和跨特征对比模块，提升性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管SNNs在稀疏动态数据处理上高效，但在复杂时空特征提取和跨类比较上仍有不足，需提升其在小样本学习中的表现和效率。

Method: 提出FSL-SNN框架，结合自特征提取模块和跨特征对比模块，使用时序高效训练损失和InfoNCE损失优化。

Result: 在N-Omniglot数据集上分类性能显著提升，在CUB和miniImageNet静态数据集上性能接近ANN且能耗低。

Conclusion: FSL-SNN框架有效提升了SNNs在小样本学习中的性能和效率，同时保持低能耗。

Abstract: Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.

</details>


### [232] [Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness](https://arxiv.org/abs/2505.07985)
*Héber H. Arcolezi,Mina Alishahi,Adda-Akram Bendoukha,Nesrine Kaaniche*

Main category: cs.LG

TL;DR: 论文研究了匿名化技术对机器学习公平性的影响，发现其可能显著降低群体公平性，但提升个体公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习依赖的训练数据常包含敏感信息，匿名化技术虽保护隐私，但对公平性的影响尚不明确。

Method: 系统评估匿名化技术（如k-匿名、l-多样性和t-接近性）对个体和群体公平性的影响。

Result: 匿名化可能使群体公平性指标下降四个数量级，但相似性个体公平性指标因输入同质性增强而改善。

Conclusion: 研究揭示了隐私、公平性和实用性之间的权衡，为负责任AI开发提供了指导。

Abstract: Machine learning (ML) algorithms are heavily based on the availability of
training data, which, depending on the domain, often includes sensitive
information about data providers. This raises critical privacy concerns.
Anonymization techniques have emerged as a practical solution to address these
issues by generalizing features or suppressing data to make it more difficult
to accurately identify individuals. Although recent studies have shown that
privacy-enhancing technologies can influence ML predictions across different
subgroups, thus affecting fair decision-making, the specific effects of
anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and
$t$-closeness, on ML fairness remain largely unexplored. In this work, we
systematically audit the impact of anonymization techniques on ML fairness,
evaluating both individual and group fairness. Our quantitative study reveals
that anonymization can degrade group fairness metrics by up to four orders of
magnitude. Conversely, similarity-based individual fairness metrics tend to
improve under stronger anonymization, largely as a result of increased input
homogeneity. By analyzing varying levels of anonymization across diverse
privacy settings and data distributions, this study provides critical insights
into the trade-offs between privacy, fairness, and utility, offering actionable
guidelines for responsible AI development. Our code is publicly available at:
https://github.com/hharcolezi/anonymity-impact-fairness.

</details>


### [233] [Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities](https://arxiv.org/abs/2505.08283)
*Jueqing Lu,Yuanyuan Qi,Xiaohao Yang,Shujie Zhou,Lan Du*

Main category: cs.LG

TL;DR: 提出了一种基于解耦原型的多模态学习输出头，动态适应缺失模态场景，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有模态可用，但现实中常缺失模态，需解决性能下降问题。

Method: 引入缺失感知的类原型输出头，动态适应不同缺失场景，并与现有提示方法兼容。

Result: 实验表明，该方法在多种缺失率和场景下显著提升性能。

Conclusion: 解耦原型输出头有效解决了多模态学习中的缺失模态问题。

Abstract: Multimodal learning enhances deep learning models by enabling them to
perceive and understand information from multiple data modalities, such as
visual and textual inputs. However, most existing approaches assume the
availability of all modalities, an assumption that often fails in real-world
applications. Recent works have introduced learnable missing-case-aware prompts
to mitigate performance degradation caused by missing modalities while reducing
the need for extensive model fine-tuning. Building upon the effectiveness of
missing-case-aware handling for missing modalities, we propose a novel
decoupled prototype-based output head, which leverages missing-case-aware
class-wise prototypes tailored for each individual modality. This approach
dynamically adapts to different missing modality scenarios and can be
seamlessly integrated with existing prompt-based methods. Extensive experiments
demonstrate that our proposed output head significantly improves performance
across a wide range of missing-modality scenarios and varying missing rates.

</details>


### [234] [Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments](https://arxiv.org/abs/2505.08299)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了一种针对Mamba模型的无结构化剪枝框架，减少70%参数同时保持95%性能。


<details>
  <summary>Details</summary>
Motivation: Mamba模型参数过多，难以在资源受限环境中部署。

Method: 结合梯度感知的幅度剪枝、迭代剪枝计划和全局剪枝策略。

Result: 在多个基准测试中实现高效性提升，性能损失极小。

Conclusion: 揭示了Mamba架构的冗余性和鲁棒性，拓宽了其应用范围。

Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged
as powerful alternatives to Transformers for sequence modeling, offering
linear-time complexity and competitive performance across diverse tasks.
However, their large parameter counts pose significant challenges for
deployment in resource-constrained environments. We propose a novel
unstructured pruning framework tailored for Mamba models that achieves up to
70\% parameter reduction while retaining over 95\% of the original performance.
Our approach integrates three key innovations: (1) a gradient-aware magnitude
pruning technique that combines weight magnitude and gradient information to
identify less critical parameters, (2) an iterative pruning schedule that
gradually increases sparsity to maintain model stability, and (3) a global
pruning strategy that optimizes parameter allocation across the entire model.
Through extensive experiments on WikiText-103, Long Range Arena, and ETT
time-series benchmarks, we demonstrate significant efficiency gains with
minimal performance degradation. Our analysis of pruning effects on Mamba's
components reveals critical insights into the architecture's redundancy and
robustness, enabling practical deployment in resource-constrained settings
while broadening Mamba's applicability.

</details>


### [235] [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
*Dong Shu,Xuansheng Wu,Haiyan Zhao,Mengnan Du,Ninghao Liu*

Main category: cs.LG

TL;DR: GradSAE通过结合输出梯度信息，识别稀疏自编码器中具有高因果影响的潜在特征，以改进模型解释和操控。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器分析仅依赖输入激活，忽略了潜在特征对模型输出的因果影响，导致解释和操控效果有限。

Method: 提出GradSAE方法，利用输出梯度信息筛选高因果影响的潜在特征。

Result: 验证了激活潜在特征对模型输出的贡献不均，且高因果影响的特征更有效。

Conclusion: GradSAE通过梯度信息提升了稀疏自编码器在模型解释和操控中的效果。

Abstract: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.

</details>


### [236] [GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning](https://arxiv.org/abs/2505.08528)
*Minsu Kim,Seong-Hyeon Hwang,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 论文提出GradMix，一种基于梯度的选择性数据增强方法，用于缓解持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，如何在获取新知识的同时保持旧知识是一个重要挑战。现有方法通常使用经验回放技术，但随机混合样本可能损害旧任务知识。

Method: GradMix通过基于类别的选择性混合策略，仅混合有益类别的样本对，以减少灾难性遗忘。

Result: 实验表明，GradMix在多个真实数据集上优于基线方法，显著减少了旧知识的遗忘。

Conclusion: GradMix是一种有效的持续学习数据增强方法，能够显著缓解灾难性遗忘问题。

Abstract: In the context of continual learning, acquiring new knowledge while
maintaining previous knowledge presents a significant challenge. Existing
methods often use experience replay techniques that store a small portion of
previous task data for training. In experience replay approaches, data
augmentation has emerged as a promising strategy to further improve the model
performance by mixing limited previous task data with sufficient current task
data. However, we theoretically and empirically analyze that training with
mixed samples from random sample pairs may harm the knowledge of previous tasks
and cause greater catastrophic forgetting. We then propose GradMix, a robust
data augmentation method specifically designed for mitigating catastrophic
forgetting in class-incremental learning. GradMix performs gradient-based
selective mixup using a class-based criterion that mixes only samples from
helpful class pairs and not from detrimental class pairs for reducing
catastrophic forgetting. Our experiments on various real datasets show that
GradMix outperforms data augmentation baselines in accuracy by minimizing the
forgetting of previous knowledge.

</details>


### [237] [High-order Regularization for Machine Learning and Learning-based Control](https://arxiv.org/abs/2505.08129)
*Xinghua Liu,Ming Cao*

Main category: cs.LG

TL;DR: 论文提出了一种新的高阶正则化（HR）方法，用于机器学习中的正则化过程，确保近似算法的可证明收敛性，并连接正则化与可解释学习。


<details>
  <summary>Details</summary>
Motivation: 正则化在神经网络训练中广泛应用，但缺乏与可解释学习的理论联系。HR方法旨在填补这一空白，并提供理论支持。

Method: 提出高阶正则化（HR）方法，将其视为逆映射的近似，并推导出误差上下界。HR方法适用于任何映射矩阵的神经网络。

Result: HR方法在经典控制问题中验证了其性能，显著提升了神经网络的泛化能力。

Conclusion: HR方法不仅理论上有创新，还通过案例研究展示了其实际应用价值，推动了可解释学习的发展。

Abstract: The paper proposes a novel regularization procedure for machine learning. The
proposed high-order regularization (HR) provides new insight into
regularization, which is widely used to train a neural network that can be
utilized to approximate the action-value function in general reinforcement
learning problems. The proposed HR method ensures the provable convergence of
the approximation algorithm, which makes the much-needed connection between
regularization and explainable learning using neural networks. The proposed HR
method theoretically demonstrates that regularization can be regarded as an
approximation in terms of inverse mapping with explicitly calculable
approximation error, and the $L_2$ regularization is a lower-order case of the
proposed method. We provide lower and upper bounds for the error of the
proposed HR solution, which helps build a reliable model. We also find that
regularization with the proposed HR can be regarded as a contraction. We prove
that the generalizability of neural networks can be maximized with a proper
regularization matrix, and the proposed HR is applicable for neural networks
with any mapping matrix. With the theoretical explanation of the extreme
learning machine for neural network training and the proposed high-order
regularization, one can better interpret the output of the neural network, thus
leading to explainable learning. We present a case study based on regularized
extreme learning neural networks to demonstrate the application of the proposed
HR and give the corresponding incremental HR solution. We verify the
performance of the proposed HR method by solving a classic control problem in
reinforcement learning. The result demonstrates the superior performance of the
method with significant enhancement in the generalizability of the neural
network.

</details>


### [238] [Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning](https://arxiv.org/abs/2505.08138)
*Brennon Brimhall,Philip Mathew,Neil Fendley,Yinzhi Cao,Matthew Green*

Main category: cs.LG

TL;DR: 论文提出了一种名为“计算性遗忘”的强形式化定义，用于衡量机器遗忘方法的有效性，并证明当前方法无法满足该定义。


<details>
  <summary>Details</summary>
Motivation: 研究机器遗忘方法的有效性，发现现有方法无法完全消除遗忘数据集的影响，提出更严格的定义以评估其性能。

Method: 通过区分算法（如成员推断分数和Kullback-Leibler散度）比较镜像模型与遗忘模型，提出计算性遗忘的定义。

Result: 证明当前机器遗忘方法无法满足计算性遗忘定义，且基于差分隐私的方法虽可行但会导致效用崩溃。

Conclusion: 计算性遗忘为机器遗忘提供了理论框架，但现有方法仍需改进，未来需解决多个开放问题。

Abstract: Machine unlearning methods take a model trained on a dataset and a forget
set, then attempt to produce a model as if it had only been trained on the
examples not in the forget set. We empirically show that an adversary is able
to distinguish between a mirror model (a control model produced by retraining
without the data to forget) and a model produced by an unlearning method across
representative unlearning methods from the literature. We build distinguishing
algorithms based on evaluation scores in the literature (i.e. membership
inference scores) and Kullback-Leibler divergence.
  We propose a strong formal definition for machine unlearning called
computational unlearning. Computational unlearning is defined as the inability
for an adversary to distinguish between a mirror model and a model produced by
an unlearning method. If the adversary cannot guess better than random (except
with negligible probability), then we say that an unlearning method achieves
computational unlearning.
  Our computational unlearning definition provides theoretical structure to
prove unlearning feasibility results. For example, our computational unlearning
definition immediately implies that there are no deterministic computational
unlearning methods for entropic learning algorithms. We also explore the
relationship between differential privacy (DP)-based unlearning methods and
computational unlearning, showing that DP-based approaches can satisfy
computational unlearning at the cost of an extreme utility collapse. These
results demonstrate that current methodology in the literature fundamentally
falls short of achieving computational unlearning. We conclude by identifying
several open questions for future work.

</details>


### [239] [Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model](https://arxiv.org/abs/2505.08158)
*Xiannan Huang,Shuhan Qiu*

Main category: cs.LG

TL;DR: 提出了一种轻量级的共形预测方法，用于时间序列预测中的不确定性量化，无需重新训练即可提供有效覆盖和更短的置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度点预测模型的置信区间建模方法存在成本高、未能充分利用深度模型表示能力或缺乏理论保证的问题。

Method: 利用预训练点预测模型提取的特征拟合残差预测器，并通过自适应覆盖控制机制构建置信区间。

Result: 在12个数据集上的实验表明，该方法能提供更紧的置信区间并保持期望的覆盖率。

Conclusion: 该方法在理论上证明了渐近覆盖收敛性，且实际应用中表现优异。

Abstract: Time series forecasting is critical for many applications, where deep
learning-based point prediction models have demonstrated strong performance.
However, in practical scenarios, there is also a need to quantify predictive
uncertainty through online confidence intervals. Existing confidence interval
modeling approaches building upon these deep point prediction models suffer
from key limitations: they either require costly retraining, fail to fully
leverage the representational strengths of deep models, or lack theoretical
guarantees. To address these gaps, we propose a lightweight conformal
prediction method that provides valid coverage and shorter interval lengths
without retraining. Our approach leverages features extracted from pre-trained
point prediction models to fit a residual predictor and construct confidence
intervals, further enhanced by an adaptive coverage control mechanism.
Theoretically, we prove that our method achieves asymptotic coverage
convergence, with error bounds dependent on the feature quality of the
underlying point prediction model. Experiments on 12 datasets demonstrate that
our method delivers tighter confidence intervals while maintaining desired
coverage rates. Code, model and dataset in
\href{https://github.com/xiannanhuang/FFDCI}{Github}

</details>


### [240] [Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL](https://arxiv.org/abs/2505.08179)
*Zhikun Tao,Gang Xiong,He Fang,Zhen Shen,Yunjun Han,Qing-Shan Jia*

Main category: cs.LG

TL;DR: FASP框架通过H-J可达性分析和悲观估计方法，解决了离线安全强化学习中长期安全和样本效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法仅关注短期安全，忽视长期保护，且对分布外数据表现不佳。

Method: 结合H-J可达性分析生成安全标签，使用CVAE和悲观估计方法优化策略。

Result: FASP在DSRL基准测试中表现优异，尤其在安全性上超越现有算法。

Conclusion: FASP为离线安全强化学习提供了长期安全保证和高效样本利用的解决方案。

Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying
policies from pre-collected datasets, offers a promising avenue for deploying
RL in safety-critical real-world domains such as robotics. However, the
majority of existing approaches emphasize only short-term safety, neglecting
long-horizon considerations. Consequently, they may violate safety constraints
and fail to ensure sustained protection during online deployment. Moreover, the
learned policies often struggle to handle states and actions that are not
present or out-of-distribution(OOD) from the offline dataset, and exhibit
limited sample efficiency. To address these challenges, we propose a novel
framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based
Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis
to generate reliable safety labels, which serve as supervisory signals for
training both a conditional variational autoencoder (CVAE) and a safety
classifier. This approach not only ensures high sampling efficiency but also
provides rigorous long-horizon safety guarantees. Furthermore, we utilize
pessimistic estimation methods to estimate the Q-value of reward and cost,
which mitigates the extrapolation errors induces by OOD actions, and penalize
unsafe actions to enabled the agent to proactively avoid high-risk behaviors.
Moreover, we theoretically prove the validity of this pessimistic estimation.
Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm
achieves competitive performance across multiple experimental tasks,
particularly outperforming state-of-the-art algorithms in terms of safety.

</details>


### [241] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种双系统自适应决策框架（DSADF），结合强化学习（RL）和视觉语言模型（VLM），以提升在动态环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在动态环境中泛化能力不足，而现有方法缺乏RL与基础模型的无缝协作，导致决策不合理和效率瓶颈。

Method: 受Kahneman的双系统理论启发，DSADF整合了快速直觉决策的RL代理（System 1）和深度推理的VLM（System 2）。

Result: 在Crafter和Housekeep游戏环境中的实验表明，DSADF显著提升了已知和未知任务的决策能力。

Conclusion: DSADF通过结合RL和VLM的优势，实现了高效且自适应的决策，为复杂环境中的智能决策提供了新思路。

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [242] [LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/abs/2505.08265)
*Hang Gao,Wenxuan Huang,Fengge Wu,Junsuo Zhao,Changwen Zheng,Huaping Liu*

Main category: cs.LG

TL;DR: 论文探讨了利用大语言模型（LLMs）作为特征增强器优化图神经网络（GNNs）节点表示的方法，并通过干预分析揭示了其内部机制，最终设计了一个优化模块。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs作为GNNs特征增强器的潜在性质尚未深入探索，需要更系统的分析。

Method: 构建可控因果关系的合成图数据集，采用干预分析方法研究LLMs和GNNs的内部机制，并设计优化模块。

Result: 实验验证了优化模块在多数据集和模型中的有效性。

Conclusion: 研究揭示了LLMs与GNNs结合的深层逻辑，并提供了实用的优化工具。

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [243] [A Practical Introduction to Deep Reinforcement Learning](https://arxiv.org/abs/2505.08295)
*Yinghan Sun,Hongxi Wang,Hua Chen,Wei Zhang*

Main category: cs.LG

TL;DR: 这篇教程旨在为初学者提供深度强化学习（DRL）的简明实用介绍，重点讲解近端策略优化（PPO）算法，并通过广义策略迭代（GPI）框架统一视角。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（DRL）在多个领域表现出色，但算法多样性和理论复杂性对初学者构成挑战。本文旨在降低学习门槛。

Method: 以PPO算法为核心，通过GPI框架统一各类算法，强调直观解释和实际工程技巧，而非冗长理论证明。

Result: 提供了一种高效且易于理解的学习路径，帮助读者从基础概念快速过渡到高级DRL算法的实现。

Conclusion: 本文是初学者快速掌握DRL的实用指南，特别适合希望快速上手PPO算法的读者。

Abstract: Deep reinforcement learning (DRL) has emerged as a powerful framework for
solving sequential decision-making problems, achieving remarkable success in a
wide range of applications, including game AI, autonomous driving, biomedicine,
and large language models. However, the diversity of algorithms and the
complexity of theoretical foundations often pose significant challenges for
beginners seeking to enter the field. This tutorial aims to provide a concise,
intuitive, and practical introduction to DRL, with a particular focus on the
Proximal Policy Optimization (PPO) algorithm, which is one of the most widely
used and effective DRL methods. To facilitate learning, we organize all
algorithms under the Generalized Policy Iteration (GPI) framework, offering
readers a unified and systematic perspective. Instead of lengthy theoretical
proofs, we emphasize intuitive explanations, illustrative examples, and
practical engineering techniques. This work serves as an efficient and
accessible guide, helping readers rapidly progress from basic concepts to the
implementation of advanced DRL algorithms.

</details>


### [244] [FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing](https://arxiv.org/abs/2505.08325)
*Haodong Zhao,Peng Peng,Chiyu Chen,Linqing Huang,Gongshen Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedRS的真实联邦遥感数据集，并构建了包含10种基线算法的FedRS-Bench基准测试，以解决现有联邦学习在遥感领域缺乏真实数据集和标准化评估的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感图像数据分布广泛且分散，集中训练模型面临数据共享和隐私问题。联邦学习（FL）提供了一种解决方案，但现有研究缺乏真实的数据集和基准测试。

Method: 提出FedRS数据集，包含8个数据集和135个客户端，模拟真实场景中的标签分布不均、数据量不平衡和领域异质性。基于FedRS，实现了10种基线FL算法和评估指标。

Result: 实验表明，联邦学习能显著提升模型性能，同时揭示了不同方法在客户端异质性和可用性条件下的性能权衡。

Conclusion: FedRS-Bench为大规模、真实的联邦遥感研究提供了标准化测试平台，有望加速该领域的发展。

Abstract: Remote sensing (RS) images are usually produced at an unprecedented scale,
yet they are geographically and institutionally distributed, making centralized
model training challenging due to data-sharing restrictions and privacy
concerns. Federated learning (FL) offers a solution by enabling collaborative
model training across decentralized RS data sources without exposing raw data.
However, there lacks a realistic federated dataset and benchmark in RS. Prior
works typically rely on manually partitioned single dataset, which fail to
capture the heterogeneity and scale of real-world RS data, and often use
inconsistent experimental setups, hindering fair comparison. To address this
gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists
of eight datasets that cover various sensors and resolutions and builds 135
clients, which is representative of realistic operational scenarios. Data for
each client come from the same source, exhibiting authentic federated
properties such as skewed label distributions, imbalanced client data volumes,
and domain heterogeneity across clients. These characteristics reflect
practical challenges in federated RS and support evaluation of FL methods at
scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation
metrics to construct the comprehensive FedRS-Bench. The experimental results
demonstrate that FL can consistently improve model performance over training on
isolated data silos, while revealing performance trade-offs of different
methods under varying client heterogeneity and availability conditions. We hope
FedRS-Bench will accelerate research on large-scale, realistic FL in RS by
providing a standardized, rich testbed and facilitating fair comparisons across
future works. The source codes and dataset are available at
https://fedrs-bench.github.io/.

</details>


### [245] [Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer](https://arxiv.org/abs/2505.08327)
*Zhenrong Liu,Janne M. J. Huttunen,Mikko Honkala*

Main category: cs.LG

TL;DR: 论文探讨了在持续学习（CL）中如何通过模型压缩技术（如剪枝和知识蒸馏）解决预训练模型的高计算成本问题，提出了两种针对类增量学习（CIL）的高效框架。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在持续学习中表现优异，但其高计算成本限制了实际应用，尤其是在需要低延迟或高能效的场景。

Method: 提出了两种框架：基于剪枝的框架（包括预剪枝和后剪枝策略）和基于知识蒸馏的框架（采用教师-学生架构）。

Result: 实验表明，两种框架在多个CIL基准测试中实现了精度与推理复杂度的更好平衡，优于基线方法。

Conclusion: 论文分析了两种框架在精度和效率上的权衡，为不同场景下的选择提供了参考。

Abstract: Continual learning (CL) aims to train models that can learn a sequence of
tasks without forgetting previously acquired knowledge. A core challenge in CL
is balancing stability -- preserving performance on old tasks -- and plasticity
-- adapting to new ones. Recently, large pre-trained models have been widely
adopted in CL for their ability to support both, offering strong generalization
for new tasks and resilience against forgetting. However, their high
computational cost at inference time limits their practicality in real-world
applications, especially those requiring low latency or energy efficiency. To
address this issue, we explore model compression techniques, including pruning
and knowledge distillation (KD), and propose two efficient frameworks tailored
for class-incremental learning (CIL), a challenging CL setting where task
identities are unavailable during inference. The pruning-based framework
includes pre- and post-pruning strategies that apply compression at different
training stages. The KD-based framework adopts a teacher-student architecture,
where a large pre-trained teacher transfers downstream-relevant knowledge to a
compact student. Extensive experiments on multiple CIL benchmarks demonstrate
that the proposed frameworks achieve a better trade-off between accuracy and
inference complexity, consistently outperforming strong baselines. We further
analyze the trade-offs between the two frameworks in terms of accuracy and
efficiency, offering insights into their use across different scenarios.

</details>


### [246] [SHAP-based Explanations are Sensitive to Feature Representation](https://arxiv.org/abs/2505.08345)
*Hyunseung Hwang,Andrew Bell,Joao Fonseca,Venetia Pliatsika,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 该论文研究了数据工程选择对局部特征解释的影响，发现常见的数据处理技术（如年龄直方图或种族编码）可以操纵SHAP等方法的特征重要性，甚至可能被利用来掩盖歧视问题。


<details>
  <summary>Details</summary>
Motivation: 探索数据工程选择如何影响局部特征解释，填补了现有研究中缺乏系统性分析的空白。

Method: 通过实验展示常见数据工程技术（如年龄直方图、种族编码）对SHAP等解释方法的影响。

Result: 发现数据工程技术可以操纵特征重要性，甚至被恶意利用来掩盖问题（如歧视）。

Conclusion: 这是首个研究表明标准数据工程技术可能误导解释器，强调了在解释性分析中谨慎处理数据的重要性。

Abstract: Local feature-based explanations are a key component of the XAI toolkit.
These explanations compute feature importance values relative to an
``interpretable'' feature representation. In tabular data, feature values
themselves are often considered interpretable. This paper examines the impact
of data engineering choices on local feature-based explanations. We demonstrate
that simple, common data engineering techniques, such as representing age with
a histogram or encoding race in a specific way, can manipulate feature
importance as determined by popular methods like SHAP. Notably, the sensitivity
of explanations to feature representation can be exploited by adversaries to
obscure issues like discrimination. While the intuition behind these results is
straightforward, their systematic exploration has been lacking. Previous work
has focused on adversarial attacks on feature-based explainers by biasing data
or manipulating models. To the best of our knowledge, this is the first study
demonstrating that explainers can be misled by standard, seemingly innocuous
data engineering techniques.

</details>


### [247] [ConDiSim: Conditional Diffusion Models for Simulation Based Inference](https://arxiv.org/abs/2505.08403)
*Mayank Nautiyal,Andreas Hellander,Prashant Singh*

Main category: cs.LG

TL;DR: ConDiSim是一种基于条件扩散模型的模拟推理方法，用于处理复杂系统中难以计算的似然问题，通过扩散模型近似后验分布，表现高效且稳定。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中似然难以计算的问题，提供一种高效且稳定的后验分布近似方法。

Method: 利用去噪扩散概率模型，包括添加高斯噪声的前向过程和学习去噪的反向过程，并结合观测数据进行条件化。

Result: 在十个基准问题和两个实际测试问题中表现出高效的后验近似能力，计算效率高且训练稳定。

Conclusion: ConDiSim为模拟推理提供了一个稳健且可扩展的框架，特别适用于需要快速推理的参数推断工作流。

Abstract: We present a conditional diffusion model - ConDiSim, for simulation-based
inference of complex systems with intractable likelihoods. ConDiSim leverages
denoising diffusion probabilistic models to approximate posterior
distributions, consisting of a forward process that adds Gaussian noise to
parameters, and a reverse process learning to denoise, conditioned on observed
data. This approach effectively captures complex dependencies and
multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark
problems and two real-world test problems, where it demonstrates effective
posterior approximation accuracy while maintaining computational efficiency and
stability in model training. ConDiSim offers a robust and extensible framework
for simulation-based inference, particularly suitable for parameter inference
workflows requiring fast inference methods.

</details>


### [248] [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
*Adel Ammar,Anis Koubaa,Omer Nacar,Wadii Boulila*

Main category: cs.LG

TL;DR: 论文分析了检索增强生成（RAG）系统中超参数对速度和性能的影响，揭示了速度与准确性的权衡，并展示了优化配置如何实现近乎完美的检索精度。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在生成内容时可能出现的幻觉或依赖过时知识的问题，通过RAG结合外部搜索提升性能。

Method: 评估不同向量存储（Chroma和Faiss）、分块策略、交叉编码器重排序和温度参数对RAG系统的影响，使用六项指标衡量性能。

Result: Chroma查询速度更快，Faiss检索精度更高；固定长度分块表现最佳；重排序提升质量但增加运行时。优化配置实现99%的上下文精度。

Conclusion: RAG系统通过合理配置超参数可以在速度和准确性之间取得平衡，适用于对检索质量要求高的应用场景（如医疗决策支持）。

Abstract: Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.

</details>


### [249] [An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling](https://arxiv.org/abs/2505.08487)
*Chetra Mang,Axel TahmasebiMoradi,David Danan,Mouadh Yagoubi*

Main category: cs.LG

TL;DR: 提出了一种自适应采样算法（ASADG），用于生成更代表性的输入数据，以解决物理模型数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 物理模型的数值求解计算成本高，且数据不平衡会导致响应流形表示不准确，影响模型预测效果。

Method: 通过迭代添加输入数据，每次在满足阈值时添加单纯形复形的重心作为新数据点。

Result: ASADG算法比LHS方法能生成更具代表性的输入数据，提高了响应流形的表示能力。

Conclusion: ASADG算法在生成输入数据方面优于LHS方法，适用于构建更准确的物理模型替代模型。

Abstract: Physical models classically involved Partial Differential equations (PDE) and
depending of their underlying complexity and the level of accuracy required,
and known to be computationally expensive to numerically solve them. Thus, an
idea would be to create a surrogate model relying on data generated by such
solver. However, training such a model on an imbalanced data have been shown to
be a very difficult task. Indeed, if the distribution of input leads to a poor
response manifold representation, the model may not learn well and
consequently, it may not predict the outcome with acceptable accuracy. In this
work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)
involving a physical model. As the initial input data may not accurately
represent the response manifold in higher dimension, this algorithm iteratively
adds input data into it. At each step the barycenter of each simplicial
complex, that the manifold is discretized into, is added as new input data, if
a certain threshold is satisfied. We demonstrate the efficiency of the data
sampling algorithm in comparison with LHS method for generating more
representative input data. To do so, we focus on the construction of a harmonic
transport problem metamodel by generating data through a classical solver. By
using such algorithm, it is possible to generate the same number of input data
as LHS while providing a better representation of the response manifold.

</details>


### [250] [Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain](https://arxiv.org/abs/2505.08516)
*Hyowon Wi,Jeongwhan Choi,Noseong Park*

Main category: cs.LG

TL;DR: 论文提出了一种名为AGF的新方法，将自注意力解释为图信号处理中的图滤波器，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制作为低通滤波器，限制了频率信息的有效利用，设计较为简化。

Method: 提出AGF方法，将自注意力视为学习有向图的图滤波器，复杂度为线性。

Result: AGF在多项任务中达到最先进性能，包括长距离序列和时间序列分类。

Conclusion: AGF通过图信号处理视角改进自注意力机制，显著提升了模型表现。

Abstract: Transformers have demonstrated remarkable performance across diverse domains.
The key component of Transformers is self-attention, which learns the
relationship between any two tokens in the input sequence. Recent studies have
revealed that the self-attention can be understood as a normalized adjacency
matrix of a graph. Notably, from the perspective of graph signal processing
(GSP), the self-attention can be equivalently defined as a simple graph filter,
applying GSP using the value vector as the signal. However, the self-attention
is a graph filter defined with only the first order of the polynomial matrix,
and acts as a low-pass filter preventing the effective leverage of various
frequency information. Consequently, existing self-attention mechanisms are
designed in a rather simplified manner. Therefore, we propose a novel method,
called \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph
\underline{\textbf{F}}ilter (AGF), interpreting the self-attention as learning
the graph filter in the singular value domain from the perspective of graph
signal processing for directed graphs with the linear complexity w.r.t. the
input length $n$, i.e., $\mathcal{O}(nd^2)$. In our experiments, we demonstrate
that AGF achieves state-of-the-art performance on various tasks, including Long
Range Arena benchmark and time series classification.

</details>


### [251] [ExEBench: Benchmarking Foundation Models on Extreme Earth Events](https://arxiv.org/abs/2505.08529)
*Shan Zhao,Zhitong Xiong,Jie Zhao,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: ExEBench是一个针对极端事件的基准数据集，旨在评估基础模型在灾害管理中的泛化能力，并推动新机器学习方法的发展。


<details>
  <summary>Details</summary>
Motivation: 极端事件对人类和生态系统构成重大风险，而现有机器学习模型在处理极端值时存在偏差，需要可靠的工具来评估和改进。

Method: 通过构建包含七类极端事件的全球数据集（ExEBench），并设计多种机器学习任务来测试基础模型的性能。

Result: ExEBench提供了多样化的数据和任务，支持极端事件的检测、监测和预测，并促进对极端事件相互作用的研究。

Conclusion: ExEBench为评估和改进机器学习模型在极端事件管理中的应用提供了重要平台，有助于应对气候变化带来的挑战。

Abstract: Our planet is facing increasingly frequent extreme events, which pose major
risks to human lives and ecosystems. Recent advances in machine learning (ML),
especially with foundation models (FMs) trained on extensive datasets, excel in
extracting features and show promise in disaster management. Nevertheless,
these models often inherit biases from training data, challenging their
performance over extreme values. To explore the reliability of FM in the
context of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme
\textbf{E}arth Benchmark), a collection of seven extreme event categories
across floods, wildfires, storms, tropical cyclones, extreme precipitation,
heatwaves, and cold waves. The dataset features global coverage, varying data
volumes, and diverse data sources with different spatial, temporal, and
spectral characteristics. To broaden the real-world impact of FMs, we include
multiple challenging ML tasks that are closely aligned with operational needs
in extreme events detection, monitoring, and forecasting. ExEBench aims to (1)
assess FM generalizability across diverse, high-impact tasks and domains, (2)
promote the development of novel ML methods that benefit disaster management,
and (3) offer a platform for analyzing the interactions and cascading effects
of extreme events to advance our understanding of Earth system, especially
under the climate change expected in the decades to come. The dataset and code
are public https://github.com/zhaoshan2/EarthExtreme-Bench.

</details>


### [252] [AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.08687)
*Hangwei Zhang,Zhimu Huang,Yan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Chebyshev1KANs架构AC-PKAN，通过引入小波激活MLP和注意力机制，解决了原方法的秩崩溃问题，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 原Chebyshev1KANs存在计算和内存消耗高、秩崩溃问题，限制了其表达能力。

Method: 结合小波激活MLP和内部注意力机制，并引入残差梯度注意力（RGA）动态调整损失权重。

Result: AC-PKAN在多个任务中优于或匹配现有最佳模型，如PINNsFormer。

Conclusion: AC-PKAN显著提升了KANs的表达能力，适用于复杂工程问题。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving
partial differential equations (PDEs). Yet their original formulation is
computationally and memory intensive, motivating the introduction of Chebyshev
Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed
the vanilla KANs architecture, our rigorous theoretical analysis reveals that
they still suffer from rank collapse, ultimately limiting their expressive
capacity. To overcome these limitations, we enhance Chebyshev1KANs by
integrating wavelet-activated MLPs with learnable parameters and an internal
attention mechanism. We prove that this design preserves a full-rank Jacobian
and is capable of approximating solutions to PDEs of arbitrary order.
Furthermore, to alleviate the loss instability and imbalance introduced by the
Chebyshev polynomial basis, we externally incorporate a Residual Gradient
Attention (RGA) mechanism that dynamically re-weights individual loss terms
according to their gradient norms and residual magnitudes. By jointly
leveraging internal and external attention, we present AC-PKAN, a novel
architecture that constitutes an enhancement to weakly supervised
Physics-Informed Neural Networks (PINNs) and extends the expressive power of
KANs. Experimental results from nine benchmark tasks across three domains show
that AC-PKAN consistently outperforms or matches state-of-the-art models such
as PINNsFormer, establishing it as a highly effective tool for solving complex
real-world engineering problems in zero-data or data-sparse regimes. The code
will be made publicly available upon acceptance.

</details>


### [253] [PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts](https://arxiv.org/abs/2505.08719)
*Yang Su,Na Yan,Yansha Deng,Robert Schober*

Main category: cs.LG

TL;DR: PWC-MoE框架通过动态路由敏感和非敏感令牌到本地和远程专家，平衡隐私、性能和带宽限制。


<details>
  <summary>Details</summary>
Motivation: 解决云服务器上大语言模型（LLM）的隐私和带宽问题，以及本地小语言模型（SLM）性能不足的问题。

Method: 采用稀疏隐私感知门控网络动态路由令牌，引入负载均衡机制和带宽自适应令牌卸载方案。

Result: 实验表明PWC-MoE在带宽受限环境下有效保护隐私并保持高性能。

Conclusion: PWC-MoE为隐私敏感和带宽受限场景提供了实用解决方案。

Abstract: Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.

</details>


### [254] [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
*Fangyuan Yu*

Main category: cs.LG

TL;DR: 论文提出信息瓶颈语言建模（IBLM）目标，通过压缩内部表征提升泛化能力，并设计GAPT算法动态切换记忆与压缩阶段，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索数据扩展和表征压缩对泛化能力的理论影响，并模拟生物学习与睡眠交替机制。

Method: 引入IBLM目标，设计GAPT算法动态调整训练阶段，结合交叉熵和矩阵熵优化表征。

Result: GAPT在GPT-2预训练中降低矩阵熵50%，提升交叉熵4.8%，OOD泛化提升35%，减少灾难性遗忘干扰97%。

Conclusion: IBLM和GAPT有效平衡记忆与压缩，显著提升模型泛化能力，模拟生物学习机制。

Abstract: We prove theoretically that generalization improves not only through data
scaling but also by compressing internal representations. To operationalize
this insight, we introduce the Information Bottleneck Language Modeling (IBLM)
objective, which reframes language modeling as a constrained optimization
problem: minimizing representation entropy subject to optimal prediction
performance. Empirically, we observe an emergent memorization-compression cycle
during LLM pretraining, evidenced by oscillation positive/negative gradient
alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of
representation entropy. This pattern closely mirrors the predictive-compressive
trade-off prescribed by IBLM and also parallels the biological alternation
between awake learning and sleep consolidation. Motivated by this observation,
we propose Gated Phase Transition (GAPT), a training algorithm that adaptively
switches between memorization and compression phases. When applied to GPT-2
pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves
cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining
task on arithmetic multiplication. In a setting designed to simulate
catastrophic forgetting, GAPT reduces interference by compressing and
separating representations, achieving a 97% improvement in separation -
paralleling the functional role of sleep consolidation.

</details>


### [255] [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
*Shanda Li,Tanya Marwah,Junhong Shen,Weiwei Sun,Andrej Risteski,Yiming Yang,Ameet Talwalkar*

Main category: cs.LG

TL;DR: CodePDE利用大型语言模型（LLM）生成PDE求解器，无需任务特定调优，通过推理时算法和扩展策略实现高性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器依赖专家知识且计算成本高，神经网络求解器需要大量数据且缺乏可解释性。

Method: 将PDE求解视为代码生成任务，引入CodePDE框架，利用LLM的推理、调试、自优化和测试时扩展能力。

Result: CodePDE在多个代表性PDE问题上实现超人类性能，并分析了生成求解器的准确性、效率和数值方案选择。

Conclusion: LLM在PDE求解中展现出潜力，但也存在局限性，为未来模型设计和开发提供了新视角。

Abstract: Partial differential equations (PDEs) are fundamental to modeling physical
systems, yet solving them remains a complex challenge. Traditional numerical
solvers rely on expert knowledge to implement and are computationally
expensive, while neural-network-based solvers require large training datasets
and often lack interpretability. In this work, we frame PDE solving as a code
generation task and introduce CodePDE, the first inference framework for
generating PDE solvers using large language models (LLMs). Leveraging advanced
inference-time algorithms and scaling strategies, CodePDE unlocks critical
capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and
test-time scaling -- all without task-specific tuning. CodePDE achieves
superhuman performance across a range of representative PDE problems. We also
present a systematic empirical analysis of LLM generated solvers, analyzing
their accuracy, efficiency, and numerical scheme choices. Our findings
highlight the promise and the current limitations of LLMs in PDE solving,
offering a new perspective on solver design and opportunities for future model
development. Our code is available at https://github.com/LithiumDA/CodePDE.

</details>
