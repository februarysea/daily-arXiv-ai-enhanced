<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 11]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 72]
- [cs.CV](#cs.CV) [Total: 93]
- [cs.LG](#cs.LG) [Total: 143]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.CG](#cs.CG) [Total: 2]
- [stat.ML](#stat.ML) [Total: 15]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.IR](#cs.IR) [Total: 15]
- [eess.SP](#eess.SP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [econ.EM](#econ.EM) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.GT](#cs.GT) [Total: 2]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact](https://arxiv.org/abs/2505.13469)
*Aayam Bansal,Harsh Vardhan Narsaria*

Main category: cs.CY

TL;DR: 论文研究了公平性约束（如人口统计平等或机会均等）与贷款机构盈利能力之间的权衡，发现机会均等约束的利润成本低于人口统计平等，而忽略受保护属性的方法在公平性和盈利性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构依赖机器学习模型自动化贷款决策，算法公平性问题日益突出，研究公平性与盈利性的平衡。

Method: 通过模拟反映现实贷款模式的合成数据，量化不同公平性干预对利润和违约率的影响。

Result: 机会均等约束的利润成本较低，忽略受保护属性的方法在公平性和盈利性上表现更优，并识别了公平贷款盈利的具体经济条件。

Conclusion: 研究为设计平衡伦理与商业目标的贷款算法提供了实用指导。

Abstract: As financial institutions increasingly rely on machine learning models to
automate lending decisions, concerns about algorithmic fairness have risen.
This paper explores the tradeoff between enforcing fairness constraints (such
as demographic parity or equal opportunity) and maximizing lender
profitability. Through simulations on synthetic data that reflects real-world
lending patterns, we quantify how different fairness interventions impact
profit margins and default rates. Our results demonstrate that equal
opportunity constraints typically impose lower profit costs than demographic
parity, but surprisingly, removing protected attributes from the model
(fairness through unawareness) outperforms explicit fairness interventions in
both fairness and profitability metrics. We further identify the specific
economic conditions under which fair lending becomes profitable and analyze the
feature-specific drivers of unfairness. These findings offer practical guidance
for designing lending algorithms that balance ethical considerations with
business objectives.

</details>


### [2] [Generative AI and the transformation of Work in Latin America -- Brazil](https://arxiv.org/abs/2505.13490)
*Carmen Bonfacio,Fernando Schapachnik,Fabio Porto*

Main category: cs.CY

TL;DR: 调查探讨了巴西雇主和员工对GenAI在工作中的影响的看法，发现其应用不均衡，但展示了乐观、担忧和未开发潜力。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI在巴西劳动力中的影响，特别是在小微企业中，以了解其对各行业的实际作用。

Method: 通过调查五个行业（销售、客服、设计/摄影、新闻/内容生产、软件开发）的雇主和员工，分析六个关键维度的影响。

Result: 结果显示GenAI的整合存在乐观、担忧和未开发潜力，应用不均衡。

Conclusion: 研究为制定包容性策略奠定了基础，旨在最大化AI效益并保障工人权益。

Abstract: This survey explores the impact perceived by employers and employees of GenAI
in their work activities in Brazil. Generative AI (GenAI) is gradually
transforming Brazil workforce, particularly in micro and small businesses,
though its adoption remains uneven. This survey examines the perceptions of
employers and employees across five sectors: Sales, Customer Service, Graphic
Design or Photography, Journalism or Content Production, and Software
Development or Coding. The results are analyzed in light of six key dimensions
of workforce impact. The findings reveal a mix of optimism, apprehension, and
untapped potential in the integration of AI tools. This study serves as a
foundation for developing inclusive strategies that maximize AI's benefits
while safeguarding workers' rights. The IIA-LNCC supports open research and
remains committed to shaping a future where technology and human potential
progress together.

</details>


### [3] [Fuck the Algorithm: Conceptual Issues in Algorithmic Bias](https://arxiv.org/abs/2505.13509)
*Catherine Stinson*

Main category: cs.CY

TL;DR: 本文探讨算法偏见的争议，澄清‘算法本身无偏见’的说法，分析偏见的多种含义及其道德影响，并指出识别算法偏见对责任分配和防止歧视的重要性。


<details>
  <summary>Details</summary>
Motivation: 澄清算法偏见的争议，明确‘算法本身’的概念及偏见的多种含义，以解决相关问题。

Method: 通过分析‘算法本身’的定义和偏见的多种含义，结合统计偏见与道德偏见的关联，以及政治和压迫性事物的概念。

Result: 指出算法可以作为偏见的来源，并举例说明其在招聘、警务、医疗等领域的影响。

Conclusion: 识别算法偏见对责任分配和防止算法中介的歧视至关重要。

Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify
what is at stake and to make progress resolving the controversy, a better
understanding of the concepts involved would be helpful. The discussion here
focuses on the disputed claim that algorithms themselves cannot be biased. To
clarify this claim we need to know what kind of thing 'algorithms themselves'
are, and to disambiguate the several meanings of 'bias' at play. This further
involves showing how bias of moral import can result from statistical biases,
and drawing connections to previous conceptual work about political artifacts
and oppressive things. Data bias has been identified in domains like hiring,
policing and medicine. Examples where algorithms themselves have been
pinpointed as the locus of bias include recommender systems that influence
media consumption, academic search engines that influence citation patterns,
and the 2020 UK algorithmically-moderated A-level grades. Recognition that
algorithms are a kind of thing that can be biased is key to making decisions
about responsibility for harm, and preventing algorithmically mediated
discrimination.

</details>


### [4] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Main category: cs.CY

TL;DR: AdAEM是一个自扩展的评估框架，用于揭示大型语言模型（LLMs）的价值倾向，通过动态生成测试问题解决现有数据集信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有价值测量数据集因问题过时、污染或泛化，导致结果饱和且缺乏信息性，无法全面比较LLMs的价值观差异。

Method: AdAEM通过上下文优化方式自动生成和扩展测试问题，最大化信息理论目标，提取最新或文化争议话题。

Result: 生成12,310个基于Schwartz价值理论的问题，分析16个LLMs的价值差异，验证了方法的有效性和区分度。

Conclusion: AdAEM能够与LLMs共同进化，持续追踪其价值动态，为价值观研究奠定基础。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>


### [5] [Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks](https://arxiv.org/abs/2505.13565)
*Oier Mentxaka,Natalia Díaz-Rodríguez,Mark Coeckelbergh,Marcos López de Prado,Emilia Gómez,David Fernández Llorca,Enrique Herrera-Viedma,Francisco Herrera*

Main category: cs.CY

TL;DR: 该论文提出了一种双分类法（AIRD和AIPD）来评估AI对民主的影响，结合欧盟的伦理AI治理框架，为研究、监管和技术设计提供了规范性指导。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对民主治理的双重影响（风险与机遇），并基于欧盟的伦理AI治理框架提出解决方案。

Method: 引入AIRD和AIPD双分类法，结合欧盟的Trustworthy AI要求，分析风险与机遇，并提出缓解策略。

Result: 强调了透明度和公共福祉的重要性，为研究、政策制定和技术设计提供了规范性框架。

Conclusion: 论文通过整合民主理论与治理工具，为构建可信赖、民主的AI系统提供了理论基础和实践指导。

Abstract: Artificial Intelligence (AI) poses both significant risks and valuable
opportunities for democratic governance. This paper introduces a dual taxonomy
to evaluate AI's complex relationship with democracy: the AI Risks to Democracy
(AIRD) taxonomy, which identifies how AI can undermine core democratic
principles such as autonomy, fairness, and trust; and the AI's Positive
Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to
enhance transparency, participation, efficiency, and evidence-based
policymaking.
  Grounded in the European Union's approach to ethical AI governance, and
particularly the seven Trustworthy AI requirements proposed by the European
Commission's High-Level Expert Group on AI, each identified risk is aligned
with mitigation strategies based on EU regulatory and normative frameworks. Our
analysis underscores the transversal importance of transparency and societal
well-being across all risk categories and offers a structured lens for aligning
AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper
offers a normative and actionable framework to guide research, regulation, and
institutional design to support trustworthy, democratic AI. It provides
scholars with a conceptual foundation to evaluate the democratic implications
of AI, equips policymakers with structured criteria for ethical oversight, and
helps technologists align system design with democratic principles. In doing
so, it bridges the gap between ethical aspirations and operational realities,
laying the groundwork for more inclusive, accountable, and resilient democratic
systems in the algorithmic age.

</details>


### [6] [Assessing GPT Performance in a Proof-Based University-Level Course Under Blind Grading](https://arxiv.org/abs/2505.13664)
*Ming Ding,Rasmus Kyng,Federico Solda,Weixuan Yuan*

Main category: cs.CY

TL;DR: 研究评估了GPT-4o和o1-preview在本科算法课程中的表现，发现GPT-4o未达及格线，而o1-preview表现更好，但两者均存在论证问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，其在高等教育中的作用需要仔细评估，特别是在自由回答问题方面。

Method: 在真实教育条件下，匿名GPT生成的答案由不知情的助教评分，分析包括粗粒度（分数）和细粒度（错误模式）表现。

Result: GPT-4o表现不佳，未达及格线；o1-preview显著优于GPT-4o，甚至超过学生中位数，但两者均存在论证缺陷。

Conclusion: 研究强调教育中需要制定稳健的评估策略和AI感知的评分政策。

Abstract: As large language models (LLMs) advance, their role in higher education,
particularly in free-response problem-solving, requires careful examination.
This study assesses the performance of GPT-4o and o1-preview under realistic
educational conditions in an undergraduate algorithms course. Anonymous
GPT-generated solutions to take-home exams were graded by teaching assistants
unaware of their origin. Our analysis examines both coarse-grained performance
(scores) and fine-grained reasoning quality (error patterns). Results show that
GPT-4o consistently struggles, failing to reach the passing threshold, while
o1-preview performs significantly better, surpassing the passing score and even
exceeding the student median in certain exercises. However, both models exhibit
issues with unjustified claims and misleading arguments. These findings
highlight the need for robust assessment strategies and AI-aware grading
policies in education.

</details>


### [7] [Comparing Apples to Oranges: A Taxonomy for Navigating the Global Landscape of AI Regulation](https://arxiv.org/abs/2505.13673)
*Sacha Alanoca,Shira Gur-Arieh,Tom Zick,Kevin Klyman*

Main category: cs.CY

TL;DR: 本文提出了一种分类法，用于映射全球AI监管的格局，旨在减少法律不确定性并支持基于证据的政策制定。


<details>
  <summary>Details</summary>
Motivation: AI监管从软法转向具有约束力的法规，但定义模糊、框架分歧和信息获取不均等问题威胁民主权利和国际合作。

Method: 提出一个框架，基于技术或应用规则、监管覆盖范围、干预时机等指标，对AI监管进行分类，并应用于五个早期案例。

Result: 通过分类法和交互式可视化工具，揭示了不同司法管辖区的共性与差异，为全球协调的AI治理奠定基础。

Conclusion: 分类法有助于明确AI监管的范围和内容，减少法律不确定性，促进更具包容性和全球协调的AI治理。

Abstract: AI governance has transitioned from soft law-such as national AI strategies
and voluntary guidelines-to binding regulation at an unprecedented pace. This
evolution has produced a complex legislative landscape: blurred definitions of
"AI regulation" mislead the public and create a false sense of safety;
divergent regulatory frameworks risk fragmenting international cooperation; and
uneven access to key information heightens the danger of regulatory capture.
Clarifying the scope and substance of AI regulation is vital to uphold
democratic rights and align international AI efforts. We present a taxonomy to
map the global landscape of AI regulation. Our framework targets essential
metrics-technology or application-focused rules, horizontal or sectoral
regulatory coverage, ex ante or ex post interventions, maturity of the digital
legal landscape, enforcement mechanisms, and level of stakeholder
participation-to classify the breadth and depth of AI regulation. We apply this
framework to five early movers: the European Union's AI Act, the United States'
Executive Order 14110, Canada's AI and Data Act, China's Interim Measures for
Generative AI Services, and Brazil's AI Bill 2338/2023. We further offer an
interactive visualization that distills these dense legal texts into accessible
insights, highlighting both commonalities and differences. By delineating what
qualifies as AI regulation and clarifying each jurisdiction's approach, our
taxonomy reduces legal uncertainty, supports evidence-based policymaking, and
lays the groundwork for more inclusive, globally coordinated AI governance.

</details>


### [8] [Safety Devolution in AI Agents](https://arxiv.org/abs/2505.14215)
*Cheng Yu,Benedikt Stroebl,Diyi Yang,Orestis Papakyriakopoulos*

Main category: cs.CY

TL;DR: 研究发现，随着AI代理检索能力的增强（从无外部检索到维基百科检索再到开放网络搜索），模型的可靠性、偏见传播和有害内容生成问题加剧，导致安全性退化。


<details>
  <summary>Details</summary>
Motivation: 探讨检索增强AI代理的安全性退化现象，特别是在LLMs和AI代理与环境的互动中可能引发的伦理和行为问题。

Method: 通过对比无检索、维基百科检索和开放网络搜索条件下的LLMs和AI代理，评估其拒绝率、偏见敏感性和有害内容防护能力。

Result: 检索增强的代理表现出安全性退化现象，即使基于对齐的LLMs也可能比无检索的未对齐模型更不安全。

Conclusion: 需开发更强健的缓解策略，以确保检索增强和自主AI系统的公平性和可靠性。

Abstract: As retrieval-augmented AI agents become more embedded in society, their
safety properties and ethical behavior remain insufficiently understood. In
particular, the growing integration of LLMs and AI agents raises critical
questions about how they engage with and are influenced by their environments.
This study investigates how expanding retrieval access, from no external
sources to Wikipedia-based retrieval and open web search, affects model
reliability, bias propagation, and harmful content generation. Through
extensive benchmarking of censored and uncensored LLMs and AI Agents, our
findings reveal a consistent degradation in refusal rates, bias sensitivity,
and harmfulness safeguards as models gain broader access to external sources,
culminating in a phenomenon we term safety devolution. Notably,
retrieval-augmented agents built on aligned LLMs often behave more unsafely
than uncensored models without retrieval. This effect persists even under
strong retrieval accuracy and prompt-based mitigation, suggesting that the mere
presence of retrieved content reshapes model behavior in structurally unsafe
ways. These findings underscore the need for robust mitigation strategies to
ensure fairness and reliability in retrieval-augmented and increasingly
autonomous AI systems.

</details>


### [9] [Upgrading Democracies with Fairer Voting Methods](https://arxiv.org/abs/2505.14349)
*Evangelos Pournaras,Srijoni Majumdar,Thomas Wellings,Joshua C. Yang,Fatemeh B. Heravan,Regula Hänggli Fricker,Dirk Helbing*

Main category: cs.CY

TL;DR: 论文探讨了现代民主社会中投票方法的重要性，提出通过替代性优先投票方法（如累积投票和等额份额法）升级现实民主制度，并在瑞士阿劳市的参与式预算中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现代多元社会中，许多民主国家仍依赖过时的投票方法，导致投票结果对规则敏感且缺乏社会创新。研究旨在探索如何通过公平投票方法提升民主制度的代表性。

Method: 采用替代性优先投票方法（如累积投票和等额份额法），并在瑞士阿劳市的参与式预算实践中进行实证评估。

Result: 公平投票方法实现了更多获胜项目、更广泛的地理和偏好代表性，尤其惠及以往被忽视的选民，同时促进了新项目提案。公民更倾向于比例投票方法，且表现出利他主义和妥协等民主价值观。

Conclusion: 研究表明，公平投票方法能有效升级民主制度，增强其合法性和代表性，为全球民主改革提供了新蓝图。

Abstract: Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.

</details>


### [10] [Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI](https://arxiv.org/abs/2505.14435)
*Annika Bush,Meltem Aksoy,Markus Pauly,Greta Ontrup*

Main category: cs.CY

TL;DR: 研究探讨了五种先进大语言模型（LLMs）在可持续性概念及其与AI关系上的差异，揭示了模型选择对组织可持续性策略的重要影响。


<details>
  <summary>Details</summary>
Motivation: 随着组织在可持续性决策中依赖AI系统，理解LLMs中固有的偏见和观点变得至关重要。

Method: 通过向五种LLMs（Claude、DeepSeek、GPT、LLaMA和Mistral）各100次施测已验证的可持续性心理测量问卷，分析其响应模式和变异性。

Result: 发现模型间存在显著差异，如GPT对AI与可持续性的兼容性持怀疑态度，而LLaMA表现出极端技术乐观主义。

Conclusion: 模型选择可能显著影响组织可持续性策略，部署LLMs时需注意模型特定偏见。

Abstract: As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.

</details>


### [11] [LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas](https://arxiv.org/abs/2505.12257)
*Evgeny Markhasin*

Main category: cs.CY

TL;DR: 研究探讨了通过结构化上下文调节（基于PWP原则）来改善通用LLMs（如Gemini 2.5 Pro和ChatGPT Plus o3）在科学文档中识别技术错误的可靠性。


<details>
  <summary>Details</summary>
Motivation: 科学和技术文档中的细微错误（尤其是多模态内容）对LLMs的识别能力构成挑战，需要一种无需API或模型修改的方法来提升其验证精度。

Method: 采用PWP原则的结构化提示策略，测试LLMs在复杂文档中识别文本和图像错误的能力。

Result: 结构化提示改善了文本错误识别，Gemini 2.5 Pro成功识别了图像中的公式错误，而ChatGPT Plus o3未能完成。

Conclusion: PWP-informed上下文调节是一种有前景的方法，可增强LLMs在科学文档中的错误检测能力，但需进一步验证其广泛适用性。

Abstract: Identifying subtle technical errors within complex scientific and technical
documents, especially those requiring multimodal interpretation (e.g., formulas
in images), presents a significant hurdle for Large Language Models (LLMs)
whose inherent error-correction tendencies can mask inaccuracies. This
exploratory proof-of-concept (PoC) study investigates structured LLM context
conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a
methodological strategy to modulate this LLM behavior at inference time. The
approach is designed to enhance the reliability of readily available,
general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for
precise validation tasks, crucially relying only on their standard chat
interfaces without API access or model modifications. To explore this
methodology, we focused on validating chemical formulas within a single,
complex test paper with known textual and image-based errors. Several prompting
strategies were evaluated: while basic prompts proved unreliable, an approach
adapting PWP structures to rigorously condition the LLM's analytical mindset
appeared to improve textual error identification with both models. Notably,
this method also guided Gemini 2.5 Pro to repeatedly identify a subtle
image-based formula error previously overlooked during manual review, a task
where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight
specific LLM operational modes that impede detail-oriented validation and
suggest that PWP-informed context conditioning offers a promising and highly
accessible technique for developing more robust LLM-driven analytical
workflows, particularly for tasks requiring meticulous error detection in
scientific and technical documents. Extensive validation beyond this limited
PoC is necessary to ascertain broader applicability.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [12] [ShieldVLM: Safeguarding the Multimodal Implicit Toxicity via Deliberative Reasoning with LVLMs](https://arxiv.org/abs/2505.14035)
*Shiyao Cui,Qinglin Zhang,Xuan Ouyang,Renmiao Chen,Zhexin Zhang,Yida Lu,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.MM

TL;DR: 论文提出了一种多模态隐式毒性（MMIT）的分类法，并构建了MMIT数据集和ShieldVLM模型，用于检测多模态内容中的隐式毒性。实验表明该模型优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 多模态隐式毒性的检测在现有研究中未被充分探索，尤其是在文本和图像单独无害但组合后有害的情况下。

Method: 构建了MMIT分类法和数据集，并开发了ShieldVLM模型，通过跨模态推理检测隐式毒性。

Result: ShieldVLM在检测隐式和显式毒性方面均优于现有基线。

Conclusion: 该研究填补了多模态隐式毒性检测的空白，模型和数据集将为未来研究提供支持。

Abstract: Toxicity detection in multimodal text-image content faces growing challenges,
especially with multimodal implicit toxicity, where each modality appears
benign on its own but conveys hazard when combined. Multimodal implicit
toxicity appears not only as formal statements in social platforms but also
prompts that can lead to toxic dialogs from Large Vision-Language Models
(LVLMs). Despite the success in unimodal text or image moderation, toxicity
detection for multimodal content, particularly the multimodal implicit
toxicity, remains underexplored. To fill this gap, we comprehensively build a
taxonomy for multimodal implicit toxicity (MMIT) and introduce an MMIT-dataset,
comprising 2,100 multimodal statements and prompts across 7 risk categories (31
sub-categories) and 5 typical cross-modal correlation modes. To advance the
detection of multimodal implicit toxicity, we build ShieldVLM, a model which
identifies implicit toxicity in multimodal statements, prompts and dialogs via
deliberative cross-modal reasoning. Experiments show that ShieldVLM outperforms
existing strong baselines in detecting both implicit and explicit toxicity. The
model and dataset will be publicly available to support future researches.
Warning: This paper contains potentially sensitive contents.

</details>


### [13] [TF-Mamba: Text-enhanced Fusion Mamba with Missing Modalities for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2505.14329)
*Xiang Li,Xianfu Cheng,Dezhuang Miao,Xiaoming Zhang,Zhoujun Li*

Main category: cs.MM

TL;DR: 提出了一种高效的TF-Mamba框架，用于处理多模态情感分析中模态缺失的问题，通过文本增强的模块和Mamba结构实现高效建模。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于Transformer的方法在多模态情感分析中因二次复杂度导致的长距离建模和多模态融合效率低下的问题。

Method: 提出TF-Mamba框架，包括文本感知模态增强（TME）模块、基于文本的上下文Mamba（TC-Mamba）和文本引导查询Mamba（TQ-Mamba），分别用于模态对齐、上下文依赖建模和多模态融合。

Result: 在三个多模态情感分析数据集上的实验表明，该方法在模态缺失场景下具有高效性和有效性。

Conclusion: TF-Mamba框架通过文本增强和Mamba结构，显著提升了多模态情感分析在模态缺失情况下的性能和效率。

Abstract: Multimodal Sentiment Analysis (MSA) with missing modalities has attracted
increasing attention recently. While current Transformer-based methods leverage
dense text information to maintain model robustness, their quadratic complexity
hinders efficient long-range modeling and multimodal fusion. To this end, we
propose a novel and efficient Text-enhanced Fusion Mamba (TF-Mamba) framework
for robust MSA with missing modalities. Specifically, a Text-aware Modality
Enhancement (TME) module aligns and enriches non-text modalities, while
reconstructing the missing text semantics. Moreover, we develop Text-based
Context Mamba (TC-Mamba) to capture intra-modal contextual dependencies under
text collaboration. Finally, Text-guided Query Mamba (TQ-Mamba) queries
text-guided multimodal information and learns joint representations for
sentiment prediction. Extensive experiments on three MSA datasets demonstrate
the effectiveness and efficiency of the proposed method under missing modality
scenarios. Our code is available at https://github.com/codemous/TF-Mamba.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/abs/2505.13516)
*Zhipeng Hou,Junyi Tang,Yipeng Wang*

Main category: cs.MA

TL;DR: HALO是一个基于分层推理架构的多智能体协作框架，通过任务分解、角色设计和结构化工作流搜索提升复杂任务中的适应性，实验显示其性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统依赖预定义角色和静态通信结构，限制了其在复杂环境中的适应性和灵活性，导致在专业任务中表现不佳。

Method: HALO采用分层推理架构，包括高层规划代理（任务分解）、中层角色设计代理（子任务实例化）和低层推理代理（子任务执行），并结合蒙特卡洛树搜索优化推理轨迹。

Result: 在代码生成、通用推理和算术推理任务中，HALO平均性能提升14.4%，在MMLU的道德场景和MATH的代数子领域分别提升13.3%和19.6%。

Conclusion: HALO通过分层协作和自适应提示优化，显著提升了多智能体系统在复杂和专业任务中的表现。

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [15] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/abs/2505.13523)
*Jun Liu,Ke Yu,Keliang Chen,Ke Li,Yuxinyue Qian,Xiaolian Guo,Haozhe Song,Yinming Li*

Main category: cs.MA

TL;DR: 提出Agent Collaboration Protocols (ACPs)以解决异构代理间的互操作性问题，支持信任访问、能力编排和工作流构建。


<details>
  <summary>Details</summary>
Motivation: 现有代理通信协议（如MCP、A2A、ANP）分散且场景特定，无法满足IoA的需求。

Method: 设计ACPs协议套件，包括注册、发现、交互和工具协议，并展示其架构、关键技术及应用流程。

Result: 在餐厅预订场景中验证了ACPs的有效性。

Conclusion: ACPs为构建安全、开放、可扩展的代理互联网基础设施奠定了基础。

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [16] [Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.13543)
*Muyang Fan,Songyang Liu,Weizi Li*

Main category: cs.MA

TL;DR: 提出一种去中心化多智能体强化学习框架，用于管理大规模混合交通网络，结合传统信号灯和机器人车辆控制，实验证明能有效减少拥堵。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵是现代城市交通的主要挑战，现有研究多集中于小规模网络或孤立路口，大规模混合交通控制研究不足。

Method: 采用去中心化多智能体强化学习框架，结合传统信号灯和机器人车辆控制，优化交通流量。

Result: 在真实世界的14个路口网络中，通过调整主要起讫点流量模式，显著减少了平均车辆等待时间。

Conclusion: 该方法为提升城市交通流动性提供了新途径。

Abstract: Traffic congestion remains a major challenge for modern urban transportation,
diminishing both efficiency and quality of life. While autonomous driving
technologies and reinforcement learning (RL) have shown promise for improving
traffic control, most prior work has focused on small-scale networks or
isolated intersections. Large-scale mixed traffic control, involving both
human-driven and robotic vehicles, remains underexplored. In this study, we
propose a decentralized multi-agent reinforcement learning framework for
managing large-scale mixed traffic networks, where intersections are controlled
either by traditional traffic signals or by robotic vehicles. We evaluate our
approach on a real-world network of 14 intersections in Colorado Springs,
Colorado, USA, using average vehicle waiting time as the primary measure of
traffic efficiency. Results demonstrate that strategically adjusting major
origin-destination (OD) flow patterns can effectively reduce congestion,
offering a new pathway for enhancing urban mobility.

</details>


### [17] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Main category: cs.MA

TL;DR: MLZero是一个基于LLM的多智能体框架，实现了跨模态数据的端到端ML自动化，显著减少人工干预，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML系统在处理多模态数据时仍需大量人工配置和专家输入，MLZero旨在通过LLM驱动的多智能体框架解决这一问题。

Method: MLZero采用认知感知模块将多模态输入转化为感知上下文，并通过语义和情景记忆增强LLM的代码生成能力。

Result: 在MLE-Bench Lite和Multimodal AutoML Agent Benchmark中，MLZero表现卓越，成功率和解决方案质量均显著优于竞争对手。

Conclusion: MLZero展示了在多模态数据上实现高效ML自动化的潜力，即使使用小型LLM也能保持强大性能。

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [18] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/abs/2505.14081)
*Luca Ballotta,Nicola Bastianello,Riccardo M. G. Ferrari,Karl H. Johansson*

Main category: cs.MA

TL;DR: 论文提出了一种结合分布式梯度下降和意见动态模型的算法，解决多智能体网络系统中的个性化和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中个性化学习和抵御网络攻击或异常数据的双重挑战。

Method: 结合分布式梯度下降和Friedkin-Johnsen意见动态模型，设计分布式学习算法。

Result: 算法收敛速度和最终模型邻域可控，实验验证其在个性化和抵御恶意攻击方面优于标准策略。

Conclusion: 算法有效平衡个性化和鲁棒性，适用于实际分布式学习任务。

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


### [19] [Empowering LLMs in Task-Oriented Dialogues: A Domain-Independent Multi-Agent Framework and Fine-Tuning Strategy](https://arxiv.org/abs/2505.14299)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Weihong Zhong,Zhen Xu,Hailong Cao,Tiejun Zhao,Ying Li,Baoxun Wang*

Main category: cs.MA

TL;DR: 论文提出了一种领域无关的多智能体框架（DIMF），通过将任务分解为意图分类、槽填充和响应生成三个独立组件，简化学习复杂度并提升泛化能力。结合DPO方法和DDA方法，进一步优化了上下文理解和训练稳定性。实验表明，该方法在MultiWOZ数据集上表现优于基线，并展现出优秀的泛化和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的任务导向对话系统在轻量级模型上表现不佳，因其难以处理多复杂逻辑。

Method: 设计DIMF框架，包含三个独立智能体，结合DPO和DDA方法优化训练。

Result: 在MultiWOZ数据集上表现优于基线，泛化和零样本能力突出。

Conclusion: DIMF框架有效解决了轻量级模型在多任务处理中的性能问题，具有广泛适用性。

Abstract: Task-oriented dialogue systems based on Large Language Models (LLMs) have
gained increasing attention across various industries and achieved significant
results. Current approaches condense complex procedural workflows into a single
agent to achieve satisfactory performance on large-scale LLMs. However, these
approaches face challenges to achieve comparable performance on fine-tuned
lightweight LLMs, due to their limited capabilities in handling multiple
complex logic. In this work, we design a Domain-Independent Multi-Agent
Framework (DIMF), which contains Intent Classification Agent, Slot Filling
Agent and Response Agent. This approach simplifies the learning complexity and
enhances the generalization ability by separating the tasks into
domain-independent components. In this framework, we enhance the capabilities
in contextual understanding using the Direct Preference Optimisation (DPO)
method, and propose a simple and effective Data Distribution Adaptation (DDA)
method to mitigate degradation issues during DPO training. Experiments
conducted on the MultiWOZ datasets show that our proposed method achieves a
better average performance among all the baselines. Extensive analysis also
demonstrates that our proposed framework exhibits excellent generalizability
and zero-shot capability.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [20] [Pantheon: Personalized Multi-objective Ensemble Sort via Iterative Pareto Policy Optimization](https://arxiv.org/abs/2505.13894)
*Jiangxia Cao,Pengbo Xu,Yin Cheng,Kaiwei Guo,Jian Tang,Shijun Wang,Dewei Leng,Shuang Yang,Zhaojie Liu,Yanan Niu,Guorui Zhou,Kun Gai*

Main category: cs.SI

TL;DR: Pantheon将集成排序从人工优化转变为机器优化，通过联合训练和隐藏状态输入提升个性化效果，并采用IPPO策略平衡多目标。


<details>
  <summary>Details</summary>
Motivation: 将集成排序从人工优化转变为机器优化，提升个性化效果和模型复杂度。

Method: 联合训练实时排序模型，利用隐藏状态输入，采用IPPO策略平衡多目标。

Result: 在快手直播服务中全面部署，每日服务4亿用户。

Conclusion: Pantheon首次在工业推荐系统中替代基于公式的集成排序，效果显著。

Abstract: In this paper, we provide our milestone ensemble sort work and the first-hand
practical experience, Pantheon, which transforms ensemble sorting from a
"human-curated art" to a "machine-optimized science". Compared with
formulation-based ensemble sort, our Pantheon has the following advantages: (1)
Personalized Joint Training: our Pantheon is jointly trained with the real-time
ranking model, which could capture ever-changing user personalized interests
accurately. (2) Representation inheritance: instead of the highly compressed
Pxtrs, our Pantheon utilizes the fine-grained hidden-states as model input,
which could benefit from the Ranking model to enhance our model complexity.
Meanwhile, to reach a balanced multi-objective ensemble sort, we further devise
an \textbf{iterative Pareto policy optimization} (IPPO) strategy to consider
the multiple objectives at the same time. To our knowledge, this paper is the
first work to replace the entire formulation-based ensemble sort in industry
RecSys, which was fully deployed at Kuaishou live-streaming services, serving
400 Million users daily.

</details>


### [21] [How Influencers and Multipliers Drive Polarization and Issue Alignment on Twitter/X](https://arxiv.org/abs/2505.14280)
*Armin Pournaki,Felix Gaisbauer,Eckehard Olbrich*

Main category: cs.SI

TL;DR: 研究分析了德国Twitter圈的极化现象，发现用户围绕热门话题形成左倾和右倾两大阵营，政治议题高度对立，主要由活跃用户（影响者和传播者）推动。


<details>
  <summary>Details</summary>
Motivation: 探讨社交媒体上公众意见的极化现象及其形成机制。

Method: 基于2021年3月至2023年7月的每日热门话题推文数据，提取议题和用户观点。

Result: 发现Twitter圈高度极化，政治议题对立明显，影响者和传播者在内容传播中起关键作用。

Conclusion: 研究揭示了社交媒体上公众意见极化的机制，对平台监管有启示意义。

Abstract: We investigate the polarization of the German Twittersphere by extracting the
main issues discussed and the signaled opinions of users towards those issues
based on (re)tweets concerning trending topics. The dataset covers daily
trending topics from March 2021 to July 2023. At the opinion level, we show
that the online public sphere is largely divided into two camps, one consisting
mainly of left-leaning, and another of right-leaning accounts. Further we
observe that political issues are strongly aligned, contrary to what one may
expect from surveys. This alignment is driven by two cores of strongly active
users: influencers, who generate ideologically charged content, and
multipliers, who facilitate the spread of this content. The latter are specific
to social media and play a crucial role as intermediaries on the platform by
curating and amplifying very specific types of content that match their
ideological position, resulting in the overall observation of a strongly
polarized public sphere. These results contribute to a better understanding of
the mechanisms that shape online public opinion, and have implications for the
regulation of platforms.

</details>


### [22] [UKTwitNewsCor: A Dataset of Online Local News Articles for the Study of Local News Provision](https://arxiv.org/abs/2505.14326)
*Simona Bisiani,Agnes Gulyas,John Wihbey,Bahareh Heravi*

Main category: cs.SI

TL;DR: UKTwitNewsCor是一个包含250万篇英国本地新闻文章的全面数据集，涵盖2020年至2022年的内容，并整合了社交媒体表现指标和内容重复元数据。


<details>
  <summary>Details</summary>
Motivation: 为研究英国本地媒体的内容生产、传播和受众参与动态提供数据支持。

Method: 收集360家本地媒体发布的新闻文章及其在Twitter上的表现数据，并补充了内容重复和地理覆盖范围的元数据。

Result: 提供了四个数据集，支持对本地媒体、新闻趋势和内容多样性的纵向分析。

Conclusion: UKTwitNewsCor为研究者、政策制定者和行业利益相关者提供了研究本地媒体的重要工具。

Abstract: In this paper, we present UKTwitNewsCor, a comprehensive dataset for
understanding the content production, dissemination, and audience engagement
dynamics of online local media in the UK. It comprises over 2.5 million online
news articles published between January 2020 and December 2022 from 360 local
outlets. The corpus represents all articles shared on Twitter by the social
media accounts of these outlets. We augment the dataset by incorporating social
media performance metrics for the articles at the tweet-level. We further
augment the dataset by creating metadata about content duplication across
domains. Alongside the article dataset, we supply three additional datasets: a
directory of local media web domains, one of UK Local Authority Districts, and
one of digital local media providers, providing statistics on the coverage
scope of UKTwitNewsCor. Our contributions enable comprehensive, longitudinal
analysis of UK local media, news trends, and content diversity across multiple
platforms and geographic areas. In this paper, we describe the data collection
methodology, assess the dataset geographic and media ownership diversity, and
outline how researchers, policymakers, and industry stakeholders can leverage
UKTwitNewsCor to advance the study of local media.

</details>


### [23] [MindVote: How LLMs Predict Human Decision-Making in Social Media Polls](https://arxiv.org/abs/2505.14422)
*Xutao Mao,Ezra Xuanru Tao*

Main category: cs.SI

TL;DR: MindVote是首个评估LLMs在社交媒体投票中作为“虚拟受访者”能力的基准，涵盖多平台、双语内容和多领域，结果显示LLMs表现优异，并揭示了偏见和差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs复杂度的增加，需要新基准来评估其在动态社交环境中预测人类决策的能力。

Method: 引入MindVote基准，包含276个投票实例和1,142个数据点，评估18个LLMs，分析偏见和差异。

Result: 表现最佳的LLMs得分为0.74，比传统基线提升80%，并发现平台、语言和领域的显著差异。

Conclusion: MindVote为评估LLMs的社会智能提供了可扩展框架，有助于理解行为决策。

Abstract: The increasing complexity of Large Language Models (LLMs) necessitates new
benchmarks to assess their ability to predict human decision-making in dynamic
social contexts. We introduce MindVote, the first benchmark for evaluating LLMs
as "virtual respondents" in social media polling. MindVote comprises 276 poll
instances with 1,142 data entry points from three platforms (Weibo, Reddit,
Fizz), features bilingual content (Chinese/English), and covers five domains.
Our evaluation of 18 LLMs demonstrates that top-performing models achieve an
overall score of 0.74, an 80% relative improvement over traditional baselines,
and then we analyze LLM world model bias with human preferences across societal
bias dimensions. MindVote also uncovers significant disparities related to
platform, language, and domain. We present strategies to optimize LLM
performance and use LLM-as-a-Judge to assess reasoning in societal contexts.
Furthermore, we show that temperature controls can reflect a way of human
thinking diversity and opinion shifts in polling. In summary, MindVote offers a
scalable framework for evaluating LLMs' social intelligence, with implications
for understanding behavioral decision-making. Code and data will be available
soon.

</details>


### [24] [Robustness Evaluation of Graph-based News Detection Using Network Structural Information](https://arxiv.org/abs/2505.14453)
*Xianghua Zeng,Hao Peng,Angsheng Li*

Main category: cs.SI

TL;DR: SI2AF是一种基于结构信息原则的对抗攻击框架，用于挑战图神经网络的假新闻检测器并提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估图神经网络的脆弱性时忽略了目标周围的结构关系，限制了鲁棒性评估的有效性。

Method: 引入结构熵量化社交互动中的动态不确定性，设计多代理协作的攻击策略。

Result: SI2AF在攻击效果上平均提升16.71%，并将GNN检测器的鲁棒性平均提高41.54%。

Conclusion: SI2AF通过结构信息原则显著提升了对抗攻击的有效性和检测器的鲁棒性。

Abstract: Although Graph Neural Networks (GNNs) have shown promising potential in fake
news detection, they remain highly vulnerable to adversarial manipulations
within social networks. Existing methods primarily establish connections
between malicious accounts and individual target news to investigate the
vulnerability of graph-based detectors, while they neglect the structural
relationships surrounding targets, limiting their effectiveness in robustness
evaluation. In this work, we propose a novel Structural Information
principles-guided Adversarial Attack Framework, namely SI2AF, which effectively
challenges graph-based detectors and further probes their detection robustness.
Specifically, structural entropy is introduced to quantify the dynamic
uncertainty in social engagements and identify hierarchical communities that
encompass all user accounts and news posts. An influence metric is presented to
measure each account's probability of engaging in random interactions,
facilitating the design of multiple agents that manage distinct malicious
accounts. For each target news, three attack strategies are developed through
multi-agent collaboration within the associated subgraph to optimize evasion
against black-box detectors. By incorporating the adversarial manipulations
generated by SI2AF, we enrich the original network structure and refine
graph-based detectors to improve their robustness against adversarial attacks.
Extensive evaluations demonstrate that SI2AF significantly outperforms
state-of-the-art baselines in attack effectiveness with an average improvement
of 16.71%, and enhances GNN-based detection robustness by 41.54% on average.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li,Zhennan Wu,Shoupeng Wang,Wenbin Hu*

Main category: cs.AI

TL;DR: DrugPilot是一个基于LLM的药物发现代理，通过参数化推理架构解决多模态数据处理和动态知识更新问题，显著提升了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在药物发现领域面临多模态数据处理、知识更新延迟和预测结果信心不足等挑战。

Method: 提出DrugPilot代理，采用参数化推理架构和交互式参数化内存池，支持多阶段任务自动化，并通过药物指令数据集进行微调。

Result: 在简单、多重和多轮任务中，任务完成率分别达到98.0%、93.5%和64.0%，优于现有代理。

Conclusion: DrugPilot通过创新架构解决了药物发现中的关键挑战，展现了高效的工具调用能力。

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [26] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan,Hao Vo,David Murphy,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架，通过评估者和编辑者智能体的协作生成合成数据，解决安全关键场景中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于伦理和实际限制，危险场景的真实数据稀缺，阻碍了AI系统在安全关键应用中的训练。

Method: 采用多智能体框架，包括基于LLM的评估者智能体（确保语义一致性和安全约束）和编辑者智能体（生成和优化场景）。

Result: 实验表明，该方法能生成符合安全需求和视觉语义的合成场景，优于现有方法。

Conclusion: 该迭代协作框架为多媒体安全应用中的数据稀缺问题提供了潜在解决方案。

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [27] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Main category: cs.AI

TL;DR: 论文提出了一种基于真实工程场景的评估方法，揭示了LLMs在复杂工程任务中的表现，发现其在抽象推理和形式化建模方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在工程领域的评估过于简化，未能充分反映真实工程问题的复杂性，因此需要更系统化的评估方法。

Method: 构建了一个包含100多个真实工程问题的数据库，用于评估四种先进的LLMs在复杂任务中的表现。

Result: LLMs在基础时间和结构推理上表现良好，但在抽象推理、形式化建模和上下文敏感逻辑方面表现较差。

Conclusion: 论文填补了LLMs在复杂工程任务评估上的空白，并指出了未来改进的方向。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [28] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Main category: cs.AI

TL;DR: TransKT是一种跨课程知识追踪方法，通过概念图引导知识迁移，结合LLM和GCN技术，提升学习者知识状态估计的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型主要针对单一课程数据，无法全面捕捉学习者的知识状态。

Method: TransKT构建跨课程概念图，利用零样本LLM提示建立概念间联系，结合GCN进行知识迁移，并通过对比目标优化知识状态表示。

Result: TransKT显著提升了知识状态估计的性能，增强了模型的鲁棒性和准确性。

Conclusion: TransKT通过跨课程知识迁移和对比学习，为学习者知识状态提供了更全面的建模方法。

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [29] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny,Wojciech Mormul,Karolina Szyndler,Sanjeev Kumar*

Main category: cs.AI

TL;DR: ADALog是一种自适应、无监督的异常检测框架，通过提取日志内的上下文关系和自适应阈值处理，解决了异构日志数据中异常检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量异构日志数据，格式动态、事件序列分散且时间模式多样，使得异常检测既重要又困难。

Method: ADALog基于预训练的双向编码器（Transformer），通过掩码语言建模任务微调，捕获领域特定的语法和语义模式，并通过重建概率和自适应阈值识别异常。

Result: 在BGL、Thunderbird和Spirit基准数据集上，ADALog表现出强大的泛化能力和与最先进方法的竞争力。

Conclusion: ADALog能够动态适应系统行为变化，避免了传统方法中基于启发式的固定阈值问题，展现了实际应用的潜力。

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [30] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型（LLMs）作为自主代理完成现实任务的能力，特别是自由职业软件开发。提出了一个基于经济数据的新基准，评估LLMs在自由职业编程和数据分析任务中的表现。测试结果显示Claude 3.5 Haiku表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在自由职业任务中的实际应用潜力，尤其是软件开发领域。

Method: 方法包括构建一个基于Kaggle自由职业数据集的合成任务基准，包含标准化价格和结构化测试用例，用于自动化评估。

Result: 结果显示Claude 3.5 Haiku表现最佳，赚取约152万美元，其次是GPT-4o-mini（149万美元）、Qwen 2.5（133万美元）和Mistral（70万美元）。

Conclusion: 结论是LLMs在结构化任务中表现良好，但真实自由职业任务的复杂性仍存在差距，同时讨论了自动化基准的优势和局限性。

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [31] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

Main category: cs.AI

TL;DR: 研究探讨了自然和人工认知系统中反推理行为的出现，分析了其典型场景和成因，并提出了设计原则以避免认知僵化。


<details>
  <summary>Details</summary>
Motivation: 探索认知系统中反推理行为的成因及其对适应性的影响，为设计更鲁棒的认知架构提供理论基础。

Method: 通过分析人工系统、生物认知、人类心理和社会动态的证据，研究反推理行为的典型场景和机制。

Result: 反推理行为是一种普遍的认知脆弱性，即使在适应性良好的系统中也可能出现。

Conclusion: 研究强调了在稳定条件下保持最小适应性激活的重要性，并提出了抵抗信息压力下认知僵化的设计原则。

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [32] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian,Rafael Meirelles,Rafael Martinelli,Anand Subramanian*

Main category: cs.AI

TL;DR: 本文提出了一种不依赖数学优化技术的启发式方法，用于解决确定性、有限时间、单产品的海上库存路径问题（MIRP），结合了Beam Search算法和迭代局部搜索过程，改进了10个实例的最优解。


<details>
  <summary>Details</summary>
Motivation: 由于MIRP问题的高复杂性，现有方法难以高效解决大规模实例，且基于MIP的精确方法在实际应用中耗时过长。本文旨在提供一种启发式方法，以促进MIRPLib的使用和结果比较。

Method: 提出了一种启发式方法，结合了Beam Search算法的变体和迭代局部搜索过程。

Result: 在72个测试实例中，该方法在可接受的CPU时间内改进了10个实例的最优解。

Conclusion: 该启发式方法为MIRP问题提供了一种高效且实用的解决方案，有助于推动MIRPLib的应用和研究。

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [33] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Main category: cs.AI

TL;DR: 论文提出BARREL框架，解决大型推理模型（LRMs）因过度思考导致的过度自信和错误答案问题，显著提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LRMs在推理时很少承认无知，反而表现出过度自信并给出错误答案，影响其事实可靠性。

Method: 提出BARREL框架，通过简洁和边界感知的推理方式，解决‘最后一分钟猜测’和‘二次思考螺旋’两种病理推理模式。

Result: 实验表明，BARREL训练将DeepSeek-R1-Distill-Llama-8B的可靠性从39.33%提升至61.48%，同时保持与R1生成数据微调模型相当的准确性。

Conclusion: BARREL框架为构建更可靠、基于事实的系统2 LRMs提供了启发。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [34] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang,Chang Yang,Aixin Cui,Sihan Jin,Ruiyu Wang,Bo Li,Xiao Huang,Dongning Sun,Xinrun Wang*

Main category: cs.AI

TL;DR: FinMaster是一个全面的金融基准测试，旨在评估大型语言模型（LLM）在金融领域的表现，填补现有评估框架的不足。


<details>
  <summary>Details</summary>
Motivation: 金融任务对全球经济稳定至关重要，但面临劳动密集、低容错性等问题。LLM在自然语言处理中表现优异，但缺乏针对金融领域的评估基准。

Method: FinMaster包含三个模块：FinSim（生成合成金融数据）、FinSuite（提供多样化金融任务）、FinEval（统一评估接口）。

Result: 实验显示LLM在复杂金融推理任务中表现显著下降，准确率从90%降至40%。

Conclusion: FinMaster填补了金融领域LLM评估的空白，有望推动LLM在金融实践中的应用。

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [35] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen,Yufei Zhou,Xitong Zhang,Haohan Wang*

Main category: cs.AI

TL;DR: 论文提出了一种基于稳定性的自动提示生成方法，通过量化语义稳定性并利用LLaMA评估器改进提示质量，提升多任务系统的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注提示的即时任务表现，忽视了其内在可靠性，导致解释性不足且无法应对大语言模型的随机性。

Method: 提出语义稳定性作为评估标准，微调LLaMA评估器自动测量响应一致性，并开发稳定性感知的提示生成系统。

Result: 实验表明，该框架在通用和领域特定任务中均提高了准确性和输出一致性。

Conclusion: 通过关注持久可靠性而非单次结果，为构建更可信的通用系统提供了新视角和实用工具。

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [36] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers,Richard Agbeyibor,Jack Kolb,Karen Feigh*

Main category: cs.AI

TL;DR: 比较了三种熟悉AI队友的方法，发现文档学习最快但偏保守，直接互动更灵活但理解较弱，推荐结合文档、训练和互动。


<details>
  <summary>Details</summary>
Motivation: 研究如何在快节奏ISR环境中让人快速熟悉AI队友，以提高团队协作效率。

Method: 通过用户研究（n=60），比较文档学习、直接训练和无熟悉三种方法的效果。

Result: 文档学习最快但偏保守，直接互动更灵活但理解较弱，个体差异显著。

Conclusion: 推荐结合文档、训练和互动的方法，并考虑个体差异设计人机界面。

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [37] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

Main category: cs.AI

TL;DR: 论文探讨了语言对思维的影响，通过AI模型（如大语言模型）的表现支持了Dennett的观点，即语言显著改变了思维的运作方式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证Dennett关于语言对思维影响的激进观点，通过对比有无语言训练的AI系统表现。

Method: 方法是通过分析大语言模型（LLMs）在推理任务中的表现，探讨语言编码的抽象性和效率如何促进跨领域推理。

Result: 结果表明，语言训练显著提升了AI系统的推理能力，支持了Dennett的观点。

Conclusion: 结论是语言通过其抽象和高效的编码方式，使推理在计算上可行，这对理解人类生物思维的运作有启示。

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [38] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.AI

TL;DR: SPLIT-RAG框架通过语义图分区和多智能体协作检索，优化了RAG系统在大规模知识图谱上的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在大规模知识图谱上存在效率与准确性的权衡问题，尤其是对简单查询和多跳问题的处理不足。

Method: 提出SPLIT-RAG框架，包括语义图分区、类型专用知识库和多智能体协作检索，通过轻量级LLM智能体和分层合并模块优化检索过程。

Result: 实验验证表明，SPLIT-RAG在效率和准确性上显著优于现有方法。

Conclusion: SPLIT-RAG通过创新的分区和协作机制，有效解决了RAG系统在大规模知识图谱上的挑战。

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [39] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.AI

TL;DR: 提出了一种多智能体框架用于FAQ标注，结合多种专业代理和法官代理，显著提升了检索准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统单模型方法难以捕捉多样化用户查询的细微差别，需要更高效的解决方案。

Method: 采用多智能体框架，结合结构化推理（ARQs）和多样化少样本策略，通过法官代理重新排序候选结果。

Result: 在真实银行数据集和公共基准测试中，Top-1准确率提升14%，Top-5准确率提升18%，平均倒数排名提升12%。

Conclusion: 该框架能有效处理模糊查询，适用于实际应用，并展现出跨领域和语言的强泛化能力。

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [40] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

Main category: cs.AI

TL;DR: A*-decoding是一种基于搜索的推理策略，通过优化计算资源分配提升语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定计算预算下表现良好，但缺乏对推理过程中资源最优利用的研究。

Method: 将语言模型解码视为部分解空间的搜索问题，利用A*搜索算法和外部监督信号优先高质量推理路径。

Result: A*-decoding在相同计算预算下比基线方法节省3倍token和30% PRM计算，使小模型达到大模型性能。

Conclusion: 结构化搜索为语言模型推理提供高效替代方案，未来可推动更高效和可扩展的部署。

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [41] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He,Maxime Daigle,Pouya Bashivan*

Main category: cs.AI

TL;DR: 论文探讨了神经网络是否能够从稀疏、不连贯的片段记忆中构建空间环境模型，并提出了ESWM框架，证明其高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动物如何通过稀疏经验快速构建灵活的环境模型，并探索神经网络是否能模拟这一能力。

Method: 提出ESWM框架，在模拟环境中测试其从稀疏片段记忆构建空间模型的能力。

Result: ESWM具有高效样本利用率和适应性，无需额外训练即可支持探索和导航任务。

Conclusion: ESWM为神经网络模拟动物空间认知提供了有效解决方案。

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [42] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

Main category: cs.AI

TL;DR: 该研究探讨了多智能体强化学习（MARL）在优化多交叉路口交通信号协调中的应用，相比传统固定时间控制器，MARL显著减少了平均等待时间并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵，尤其是交叉路口，严重影响出行时间、燃油消耗和排放。传统固定时间信号控制系统缺乏动态适应能力。

Method: 研究使用Pygame模拟多交叉路口网络，采用分散式MARL控制器，每个交通信号作为自主智能体，基于局部观察和邻域信息做出决策。

Result: MARL方法在平均车辆等待时间和吞吐量方面表现出显著改进。

Conclusion: MARL动态控制策略对提高城市交通管理效率具有潜力，但需进一步研究解决可扩展性和实际实施挑战。

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [43] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Main category: cs.AI

TL;DR: 提出了一种两阶段训练策略，通过预热阶段和RLVR训练，在数据稀缺环境下高效开发推理能力的LLM。


<details>
  <summary>Details</summary>
Motivation: 传统方法（RLVR或CoT蒸馏）依赖大量高质量数据，而数据稀缺时效果受限。

Method: 两阶段训练：1）用K&K逻辑谜题预热模型；2）在少量目标域数据上应用RLVR。

Result: 预热阶段提升泛化推理能力；预热后模型在少量数据上表现更优；保持跨域泛化性；提高RLVR训练效率。

Conclusion: 预热策略在数据稀缺环境下构建鲁棒推理LLM具有潜力。

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [44] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam,Henry Conklin,Yukang Yang,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.AI

TL;DR: CHG是一种可扩展的方法，用于解释Transformer模型中注意力头的功能角色，通过软门控和因果分类（促进、干扰或无关）分析其对任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常是假设驱动的，需要提示模板或目标标签，而CHG可以直接应用于任何数据集的标准下一个词预测任务，提供更灵活的因果解释。

Method: CHG学习注意力头的软门控，并根据其对任务性能的影响将其分类为促进、干扰或无关。还引入了对比CHG，用于隔离特定任务组件的子电路。

Result: 在多个任务和Llama 3模型家族中，CHG得分提供了因果洞察，验证了稀疏子电路的存在、头的低模块化特性，以及指令遵循和上下文学习的分离机制。

Conclusion: CHG揭示了Transformer模型中注意力头的功能多样性和交互依赖性，为模型解释提供了新的工具和视角。

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [45] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）的元认知能力，即模型监控和报告自身内部激活模式的能力，并设计了一种神经反馈范式来量化这种能力。


<details>
  <summary>Details</summary>
Motivation: 随着社会对LLMs的依赖增加，理解其元认知能力的局限性至关重要，尤其是它们监控内部激活的能力，这对AI安全有重要影响。

Method: 通过设计神经反馈范式，使用句子-标签对（标签对应句子引发的特定神经激活方向），测试LLMs报告和控制这些激活的能力。

Result: LLMs能够学习报告和控制内部激活，但能力受示例数量、目标神经方向的语义可解释性和方差解释度影响。元认知空间的维度远低于神经空间。

Conclusion: 研究量化了LLMs的元认知能力，揭示了其局限性，为AI安全提供了重要实证依据。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [46] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Main category: cs.AI

TL;DR: 论文提出了CausalPitfalls基准，用于评估大语言模型（LLMs）在因果推理中的能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 可靠的因果推理在医学、经济学和公共政策等领域至关重要，但现有LLMs是否能胜任严格的统计因果推理尚不明确。

Method: 设计了CausalPitfalls基准，包含多难度级别的结构化挑战和评分标准，通过直接提示和代码辅助提示两种协议评估模型。

Result: 当前LLMs在统计因果推理中存在显著局限性。

Conclusion: CausalPitfalls为开发可信的因果推理系统提供了重要指导和量化指标。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [47] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong,Chen Shan,Zhenting Qi,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 论文提出了一种系统性反事实干预框架，用于评估大型推理模型（LRMs）中间推理过程的忠实性，发现当前模型在忠实性上存在不足。


<details>
  <summary>Details</summary>
Motivation: 确保大型推理模型（LRMs）中间推理过程的忠实性对于可靠监控、解释和有效控制至关重要。

Method: 通过反事实干预框架，从两个维度评估忠实性：1) 内部草稿忠实性（Intra-Draft Faithfulness）；2) 草稿到答案的忠实性（Draft-to-Answer Faithfulness）。

Result: 实验表明，当前LRMs在中间推理步骤和草稿结论的忠实性上表现不一致，存在缺陷。

Conclusion: 研究强调了在高级LRMs中实现更忠实和可解释推理的必要性。

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [48] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun,Ziyao Wang,Bowei Tian,Meng Liu,Zheyu Shen,Shwai He,Yexiao He,Wanghao Ye,Yiting Wang,Ang Li*

Main category: cs.AI

TL;DR: 论文提出CoIn框架，用于验证隐藏推理令牌的数量和语义有效性，以解决LLM服务中的账单透明度问题。


<details>
  <summary>Details</summary>
Motivation: 商业LLM API通常隐藏推理过程，仅返回最终答案，导致用户无法验证令牌使用情况，可能引发令牌计数膨胀问题。

Method: CoIn通过构建可验证的哈希树检查令牌数量，并基于嵌入的相关性匹配检测伪造的推理内容。

Result: 实验表明，CoIn作为第三方审计工具，检测令牌计数膨胀的成功率高达94.7%。

Conclusion: CoIn能有效恢复LLM服务的账单透明度，减少不透明性带来的潜在欺诈。

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [49] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng,Licheng Liu,Qing Zhu,Runlong Yu,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Main category: cs.AI

TL;DR: 提出了一种结合度量学习和大型语言模型的新框架，用于生态时间序列评估，弥补传统数值指标与专家知识之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统数值指标（如R平方、均方根误差）无法捕捉生态过程中的关键时间模式，依赖专家视觉检查又耗时且难以大规模应用。

Method: 整合度量学习与大型语言模型，通过成对标注和政策优化机制生成可解释的评估标准。

Result: 在多个数据集上验证了该方法在捕捉目标评估偏好（包括合成数据和专家标注比较）方面的有效性。

Conclusion: 该框架为生态系统建模提供了可解释的评估政策，满足了不同研究的多样化需求。

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [50] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah,Zhiling Chen,Lela Romeo,Qian Yang,Rajiv Malhotra,Farhad Imani,Hongyi Xu*

Main category: cs.AI

TL;DR: 提出了一种基于检索增强生成的多模态框架，用于零样本检测增材制造中的异常，无需训练数据，通过检索文献信息实现分类和解释。


<details>
  <summary>Details</summary>
Motivation: 增材制造中缺陷和过程异常的检测面临挑战，传统方法依赖训练数据，难以适应多样化的制造场景。

Method: 结合文本和图像检索技术，利用多模态生成模型（如GPT-4o-mini）进行零样本异常识别和分类。

Result: 在四种L-PBF数据集上验证了框架的通用性，GPT-4o-mini表现优于其他模型，检索机制提升准确率12%。

Conclusion: 该框架可动态更新，适应增材制造技术发展，显著提升异常分析的效率和准确性。

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [51] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng,Yujie Cai,Qing Liu,Shiyao Mu,Bin Lyu,Zhen Yang*

Main category: cs.AI

TL;DR: TelePlanNet是一个AI驱动的基站选址框架，通过三层架构和强化学习优化多目标需求，显著提升规划一致性至78%。


<details>
  <summary>Details</summary>
Motivation: 传统人工方法和现有AI工具在5G基站选址中效率低且难以满足动态网络需求，亟需更高效的解决方案。

Method: 结合大语言模型（LLMs）实时处理用户输入，并采用改进的GRPO强化学习训练规划模型。

Result: 实验显示TelePlanNet将规划一致性提升至78%，优于人工方法。

Conclusion: TelePlanNet为电信运营商提供了高效、可扩展的基站选址工具，显著推进了蜂窝网络规划。

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [52] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah,Harsh Goel,Sai Shankar Narasimhan,Minkyu Choi,S P Sharan,Oguzhan Akcin,Sandeep Chinchali*

Main category: cs.AI

TL;DR: 论文探讨了现代视频理解系统在时间推理上的局限性，并提出了一种神经符号方法，以开发能够自主搜索、交互和生成内容的智能视频代理。


<details>
  <summary>Details</summary>
Motivation: 随着视频分析在现实应用中的重要性增加，需要能够推理事件并采取行动的智能视频代理，但现有系统在时间推理上存在不足。

Method: 提出神经符号方法，将视频查询分解为原子事件，结构化序列，并验证时间约束。

Result: 该方法有望提升可解释性、结构化推理能力，并为系统行为提供更强保证。

Conclusion: 呼吁研究社区开发具备自主搜索、实时交互和内容生成能力的下一代智能视频代理，推动视频理解的边界。

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [53] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V利用视频作为指导工具，自动注入操作知识到移动自动化流程中，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 移动设备使用激增需要高效自动化任务管理，但现有AI框架因缺乏操作知识而表现不佳。

Method: 通过视频内容直接获取操作知识，避免人工干预，提出Mobile-Knowledge基准评估性能。

Result: 实验显示Mobile-Agent-V性能提升36%，优于现有方法。

Conclusion: Mobile-Agent-V以高效、省时的优势推动移动自动化发展。

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [54] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Main category: cs.AI

TL;DR: PC Agent-E通过合成高质量轨迹数据，显著减少对人类演示的依赖，在少量数据下实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决高质量轨迹数据稀缺的问题，开发更高效的计算机使用代理训练框架。

Method: 利用312条人工标注轨迹，通过Claude 3.7 Sonnet合成多样化动作决策，提升数据质量。

Result: PC Agent-E在WindowsAgentArena-V2上相对性能提升141%，并在OSWorld上展示强泛化能力。

Conclusion: 少量高质量轨迹数据可激发强大的计算机使用能力。

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [55] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler,Richard Booth*

Main category: cs.AI

TL;DR: 本文提出了一种基于TeamQueue聚合器的通用方法，用于将串行迭代信念修正算子扩展到并行变化，以统一解释相关合理性假设。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨了单步并行修正的约束，但迭代并行修正的模型扩展研究较少，且现有合理性假设缺乏统一解释。

Method: 基于TeamQueue聚合器家族，提出了一种将串行迭代信念修正算子扩展到并行变化的方法。

Result: 该方法能够恢复文献中独立合理的性质，同时避免产生可疑的性质。

Conclusion: 通过TeamQueue聚合器，提供了一种统一且合理的方法来处理并行迭代信念修正。

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [56] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh,Jiatong Li,Shawn Im,Yixuan Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为Vittle的方法，通过信息瓶颈原则提升多模态大语言模型（MLLMs）在分布偏移下的鲁棒性，避免了额外数据或更大模型的需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法提升MLLMs泛化能力需要更多指令数据或更大模型架构，成本高昂。本文从表示学习角度出发，寻求更高效的解决方案。

Method: 基于信息瓶颈原则，推导出MLLMs的变分下界，并提出Vittle方法，通过最小充分表示学习提升鲁棒性。

Result: 在45个数据集（含30种偏移场景）上的实验表明，Vittle显著提升了MLLMs在分布偏移下的性能。

Conclusion: Vittle通过信息瓶颈原则有效提升了MLLMs的鲁棒性，且无需额外资源投入。

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [57] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz,Matthijs T. J. Spaan,Anna Lukina*

Main category: cs.AI

TL;DR: VeRecycle框架首次实现了在离散时间随机动力系统中高效重用概率证书，显著节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 现实中的自主系统面临多种不确定性，现有方法在系统动态变化时需要完全重新认证，成本高昂。

Method: 提出VeRecycle框架，通过局部状态空间变化时重用概率证书，避免完全重新认证。

Result: 实验证明VeRecycle节省计算资源，同时在组合神经控制中保持竞争力。

Conclusion: VeRecycle为非线性随机动力系统的安全认证提供了高效解决方案。

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [58] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong,Ziyue Qiao,Zhiyuan Ning,Qi Hao,Yi Du,Pengyang Wang,Yuanchun Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为DiMNet的新方法，用于解决时序知识图谱（TKG）推理中的语义演化和结构交互问题，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模TKG时忽视了子图间的内部结构交互和潜在的平滑特征，导致推理性能受限。

Method: DiMNet采用多跨度演化策略捕捉局部和历史邻居语义信息，并通过解耦组件分离节点的活跃和稳定特征。

Result: 在四个真实TKG数据集上的实验表明，DiMNet在MRR指标上最高提升了22.7%。

Conclusion: DiMNet通过有效建模语义演化和结构交互，显著提升了TKG推理性能。

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [59] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Main category: cs.AI

TL;DR: ProMind-LLM结合主观心理记录与客观行为数据，通过领域预训练、自优化机制和因果链推理，提升心理健康风险评估的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康评估方法依赖主观文本记录，易受心理不确定性影响，导致预测不一致。

Method: ProMind-LLM整合客观行为数据，采用领域预训练、自优化机制和因果链推理。

Result: 在PMData和Globem数据集上表现优于通用大语言模型。

Conclusion: ProMind-LLM为心理健康评估提供了更可靠、可解释且可扩展的解决方案。

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [60] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar,Sherry Sahebi*

Main category: cs.AI

TL;DR: 论文提出KMaP模型，通过多任务方法同时建模学生知识和行为，解决个性化不足和非评估材料建模问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在个性化、非评估材料建模及知识与行为交互方面存在不足。

Method: 采用基于聚类的学生画像和多任务方法（KMaP），建模学生知识和行为。

Result: 实验证实学生行为差异显著，KMaP模型效果显著。

Conclusion: KMaP为个性化学习提供了有效解决方案。

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [61] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr,Vít Musil,Vojtěch Řehák*

Main category: cs.AI

TL;DR: 论文提出了一种迭代调整内存分配的方法，解决了有限记忆策略中内存分配选择的难题，提升了防御策略的效果。


<details>
  <summary>Details</summary>
Motivation: 有限记忆策略在对抗巡逻游戏中表现优异，但内存分配的选择是一个未解决的难题，阻碍了其实际应用。

Method: 开发了一种通用方法，通过迭代调整内存分配，可与任何黑盒策略优化工具结合使用。

Result: 实验验证了该方法的鲁棒性，成功解决了多种巡逻模型的实例。

Conclusion: 该方法解决了有限记忆策略的内存分配问题，提升了策略的实用性。

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [62] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao,Sibo Li,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: 论文提出RL-of-Thoughts (RLoT)，通过强化学习训练轻量级导航模型，动态选择逻辑块以增强LLM推理能力，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理技术（如Chain/Tree/Graph-of-Thoughts）缺乏任务适应性，限制了LLM的推理能力提升。

Method: 设计五种基础逻辑块，训练RL导航模型动态选择和组合逻辑块，适应不同任务需求。

Result: 在多个推理基准测试中，RLoT性能提升高达13.4%，且导航模型参数少于3K，使小规模LLM媲美大规模模型。

Conclusion: RLoT通过动态逻辑结构显著提升LLM推理能力，并展现出强泛化性。

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [63] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo,Junzhe Chen,Haoxuan Zhu,Xuming Hu*

Main category: cs.AI

TL;DR: SPlanner是一个用于移动GUI任务规划的模块，通过EFSMs建模应用逻辑，分解用户指令并生成执行路径，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI代理在任务规划中因缺乏对应用逻辑的深入理解而导致的执行困难。

Method: 利用EFSMs建模应用逻辑，分解指令为功能序列，生成执行路径，并通过LLM优化为自然语言计划。

Result: 在AndroidWorld基准测试中，任务成功率提升28.8个百分点，达到63.8%。

Conclusion: SPlanner有效解决了GUI代理的任务规划问题，显著提升了任务执行效率。

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [64] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang,Jinsong Zhang,Zhejun Zhang,Lei Li*

Main category: cs.AI

TL;DR: 提出了一种名为MMoLRE的多任务学习方法，用于多模态情感分析和情绪识别，通过共享和任务特定专家避免参数冲突，并利用低秩结构降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中硬参数共享忽略了任务间复杂相关性导致的参数冲突，需要一种更灵活的方法。

Method: 设计了MMoLRE方法，结合共享和任务特定专家，并采用低秩专家网络减少计算负担。

Result: 在CMU-MOSI和CMU-MOSEI基准测试中，MMoLRE在多模态情感分析任务上达到最优性能，情绪识别任务表现也具竞争力。

Conclusion: MMoLRE通过灵活的参数共享和低秩结构，有效解决了多任务学习中的参数冲突问题，提升了性能。

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [65] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: s3框架通过解耦检索与生成，利用Gain Beyond RAG奖励优化检索，显著提升生成准确性，仅需少量训练数据即可超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略下游任务效用，要么将检索与生成耦合，限制了检索的实用性和兼容性。

Method: 提出s3框架，解耦检索与生成，使用Gain Beyond RAG奖励训练检索模块。

Result: 在少量训练数据（2.4k样本）下，s3在多个QA基准测试中表现优于基线。

Conclusion: s3框架轻量且模型无关，显著提升了检索和生成的下游性能。

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [66] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu,Zhenduo Zhang,ZuJie Wen,Zhiqiang Zhang,Wang Ren,Lei Shi,Cai Chen,Deng Zhao,Dingnan Jin,Qing Cui,Jun Zhou*

Main category: cs.AI

TL;DR: SHARP是一种用于合成高质量、可验证的STEM推理问题的方法，通过自对齐原则和三阶段框架提升大型推理模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的推理问题过于简单或不可验证，限制了模型在复杂任务上的进步。

Method: SHARP采用自对齐原则和三阶段框架（对齐、实例化、推理），结合强化学习循环优化模型推理。

Result: 实验表明，SHARP显著提升了复杂推理的准确性，使模型性能接近专家水平。

Conclusion: SHARP为大型推理模型的训练提供了有效解决方案，提升了其推理能力。

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [67] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu,Zherui Yang,Cancheng Liu,Tianrui Song,Xiaofeng Gao,Hao Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的数学建模框架MM-Agent，通过分解建模任务为四个阶段，显著提升了建模能力，并在基准测试和实际竞赛中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数学建模在科学和工程中至关重要，但现有LLM在建模能力上存在不足，限制了其实际应用。

Method: 提出MM-Agent框架，将建模分解为问题分析、模型构建、计算求解和报告生成四个阶段。

Result: MM-Agent在基准测试中表现优于基线模型，并在MCM/ICM竞赛中帮助团队获得佳绩。

Conclusion: MM-Agent展示了LLM在数学建模中的潜力，可作为建模辅助工具。

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [68] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang,Alexander Hanbo Li,Yiqun Hu,Sheng Zhang,Hideo Kobayashi,Jiani Zhang,Henry Zhu,Chung-Wei Hang,Patrick Ng*

Main category: cs.AI

TL;DR: DSMentor框架通过课程学习和长期记忆优化LLM代理在数据科学任务中的表现，显著提高了任务通过率和因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了问题解决顺序对LLM代理性能的影响，而课程学习策略可能提升其表现。

Method: 开发了DSMentor框架，结合课程学习和长期记忆，按难度递增顺序组织任务。

Result: 在DSEval和QRData基准测试中，DSMentor将通过率提高了5.2%，因果推理能力提升了8.8%。

Conclusion: DSMentor展示了知识积累和利用策略的重要性，为LLM性能优化提供了新方向。

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [69] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha,Bojie Shen,Daniel Harabor,Peter Stuckey,Mark Wallace*

Main category: cs.AI

TL;DR: 论文提出了一种动态重新规划公共交通路径的方法，包括用户主动请求的“拉”方式和服务器主动监控的“推”方式，实验表明“推”方式更高效。


<details>
  <summary>Details</summary>
Motivation: 公共交通延误常见且影响用户体验，现有解决方案未能充分利用实时延误数据，缺乏系统级的动态重新规划框架。

Method: 论文形式化了动态重新规划问题，提出了“拉”和“推”两种解决方案，并通过实验验证其效果。

Result: 实验结果显示“推”方式优于“拉”方式，显著提升了到达时间效率。

Conclusion: 动态重新规划能显著节省到达时间，“推”方式更具潜力。

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [70] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang,Xin Yu,Xuxin Lv,Gangzheng Ai,Wenjun Wu*

Main category: cs.AI

TL;DR: 论文研究了大规模异构三维环境下的边界防御博弈，提出了EMFAC框架以解决防御策略中的控制挑战，并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于简化的小规模二维场景，忽略了现实环境中的复杂因素，如运动动力学和风场，限制了实际应用。

Method: 提出EMFAC框架，结合表示学习和轻量级注意力机制，支持大规模防御者的协调与高效决策。

Result: EMFAC在仿真中表现优于基线方法，收敛速度和整体性能均有提升，并在小规模实验中验证了实用性。

Conclusion: EMFAC框架为复杂环境下的边界防御提供了有效的解决方案，具有实际应用潜力。

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [71] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Main category: cs.AI

TL;DR: RLVR提升准确性但未改善能力，蒸馏可同时提升两者。研究发现RLVR因偏重简单问题而忽视难题，蒸馏需引入新知识才能提升能力。


<details>
  <summary>Details</summary>
Motivation: 探究RLVR和蒸馏对语言模型推理行为的影响机制。

Method: 分析RLVR和蒸馏对模型在简单和难题上的表现差异，并研究蒸馏中知识引入的作用。

Result: RLVR仅提升简单问题准确性，蒸馏需新知识才能提升能力，否则与RLVR类似。

Conclusion: 研究揭示了RLVR和蒸馏对模型推理行为的具体影响，为优化方法提供指导。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [72] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang,Aixin Sun*

Main category: cs.AI

TL;DR: 本文提出了一种五级（L1-L5）的具身通用人工智能（AGI）分类法，回顾了基础阶段（L1-L2）的研究与挑战，并提出了实现更高能力（L3-L5）的关键组件。基于此，作者提出了一个L3+机器人大脑的概念框架。


<details>
  <summary>Details</summary>
Motivation: 探讨具身AGI的发展，填补现有研究的空白，为未来具身AI系统的设计提供理论基础。

Method: 通过系统分类法（L1-L5）分析现有研究，提出关键组件，并构建L3+机器人大脑的概念框架。

Result: 明确了具身AGI的发展路径，提出了实现更高能力的技术方向。

Conclusion: 具身AGI的发展需要系统性分类和框架支持，L3+机器人大脑为未来研究提供了重要参考。

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [73] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu,Tianjie Ju,Manman Zhao,Xinbei Ma,Yuan Guo,ZhuoSheng Zhang*

Main category: cs.AI

TL;DR: EVA框架通过动态优化间接提示注入攻击，显著提高攻击成功率，并揭示多模态GUI代理的共同行为偏差。


<details>
  <summary>Details</summary>
Motivation: 多模态代理在GUI环境中面临间接提示注入攻击的威胁，现有静态方法效果有限，需动态优化攻击策略。

Method: 提出EVA框架，通过闭环优化动态调整攻击提示（如关键词、布局），适应代理的视觉注意力分布。

Result: EVA在多种GUI场景中显著提升攻击成功率，且攻击模式在不同模型间具有可迁移性。

Conclusion: EVA不仅增强红队测试能力，还揭示了多模态代理决策中的共同漏洞。

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [74] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种实时监控框架Safety-Net，通过无监督方法检测大型语言模型（LLMs）的有害输出，重点关注后门触发响应，并成功应对模型欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 高风险的核能和航空行业使用实时监控检测危险系统状态，类似地，LLMs也需要监控机制以防止生成有害内容（如暴力、色情或仇恨言论）。

Method: 采用无监督方法，将正常行为作为基线，有害输出视为异常值。通过研究模型内部行为特征（类似人类欺骗时的生理指标），设计多检测器框架Safety-Net，监控不同表征维度。

Result: Safety-Net在检测有害行为时达到96%的准确率，即使模型通过改变表征空间逃避监控。

Conclusion: 研究表明LLMs可通过因果机制生成有害内容并欺骗监控系统，但Safety-Net能有效应对这些挑战，为AI安全提供了实用解决方案。

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [75] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie,Gioele Migno,Enrico Piacenti,Maria Elena Giannaccini,Patric Bach,Davide De Tommaso,Agnieszka Wykowska*

Main category: cs.AI

TL;DR: 提出了一个训练视觉语言模型（VLM）进行视觉视角转换（VPT）的概念框架，并发布了一个合成数据集以支持空间推理任务。


<details>
  <summary>Details</summary>
Motivation: 视觉视角转换是具身认知的核心能力，对人与机器人交互（HRI）至关重要。

Method: 使用NVIDIA Omniverse生成合成数据集，包含RGB图像、自然语言描述和物体姿态的真实变换矩阵。

Result: 数据集公开可用，支持Z轴距离推理作为基础技能，未来将扩展到6自由度推理。

Conclusion: 为具身AI系统在交互式人机场景中的空间理解奠定了基础。

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [76] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong,Nobuhiro Ueda,Krisztián Boros,Daiki Ito,Takuya Sera,Masafumi Oyamada*

Main category: cs.AI

TL;DR: SCAN是一种增强文本和视觉检索增强生成（RAG）系统的新方法，通过语义文档布局分析提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）和视觉语言模型（VLMs）的广泛应用，处理富文档的挑战日益突出，SCAN旨在解决这一问题。

Method: SCAN采用粗粒度语义方法将文档划分为连贯区域，并通过微调目标检测模型实现。

Result: 实验表明，SCAN在英文和日文数据集上分别提升文本RAG性能9.0%和视觉RAG性能6.4%。

Conclusion: SCAN在富文档处理中表现优于传统方法和商业解决方案。

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [77] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang,Chenghua He,Xiaowen Shi,Linjing Li,Qiyue Yin,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 提出了一种针对长链推理过程的新型数据标注方法，通过引入错误传播和错误终止概念，提升PRM在识别自我纠正行为上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长链推理中仅关注首个错误步骤及之前步骤，忽略了后续可能的正确步骤，无法有效捕捉自我纠正机制。

Method: 提出基于错误传播和错误终止的数据标注方法，利用LLM标注器收集170万样本训练7B PRM，并在解决方案和步骤层面评估。

Result: 实验表明，该方法在搜索引导、BoN和F1分数等指标上优于现有开源PRM和数据集训练的PRM，数据效率和性能更高。

Conclusion: 该方法稳定且泛化性强，显著提升了PRM在长链推理中的表现。

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [78] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale,Vishal Vaddina*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱的方法，用于改进代码搜索和检索，从而提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在代码生成方面表现出色，但在上下文准确性上存在不足，尤其是在动态代码库中。现有方法在检索结果的质和上下文相关性上缺乏鲁棒性。

Method: 通过将代码库表示为知识图谱，捕捉结构和关系信息，采用混合检索方法提升上下文相关性，并跟踪模块间依赖关系。

Result: 在EvoCodeBench数据集上的实验表明，该方法显著优于基线方法。

Conclusion: 基于知识图谱的代码生成方法有望推动更鲁棒、上下文敏感的编码辅助工具的发展。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [79] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Main category: cs.AI

TL;DR: 论文提出Causal Cartographer框架，通过显式提取和建模因果关系，提升大语言模型在因果推理任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型（如大语言模型）缺乏因果推理能力，且真实世界中的反事实评估困难。

Method: 提出基于图检索增强的生成代理提取因果关系，构建因果知识网络，并设计受因果关系约束的反事实推理代理。

Result: 方法能有效提取因果知识，提升大语言模型的因果推理能力，同时降低推理成本和虚假相关性。

Conclusion: Causal Cartographer框架为因果推理任务提供了可行的解决方案。

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [80] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 论文提出了一种名为BCPG-NSA的细粒度离线强化学习框架，通过利用负样本中的有效信息提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么完全丢弃负样本，要么对所有标记施加相同惩罚，未能充分利用负样本中的学习信号。

Method: BCPG-NSA包括样本分割、基于共识的步骤正确性评估（结合LLM和PRM判断器）以及带有负样本增强的策略优化。

Result: 实验表明，BCPG-NSA在多个数学/编程推理基准测试中优于基线方法，提高了样本效率并展示了鲁棒性和可扩展性。

Conclusion: BCPG-NSA通过有效挖掘负样本中的正面步骤，显著提升了模型的推理能力。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [81] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Main category: cs.AI

TL;DR: PRL是一种基于强化学习的自动提示生成方法，能够生成训练中未见过的少样本示例，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计有效的提示需要专家直觉和任务理解，而PRL旨在通过自动化方法解决这一挑战。

Method: PRL采用强化学习方法生成提示，能够生成新颖的少样本示例。

Result: PRL在文本分类、简化和摘要任务中表现优异，超越APE和EvoPrompt等先前方法。

Conclusion: PRL为自动提示生成提供了高效解决方案，显著提升了LLM的性能。

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [82] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: SCOPE是一种基于压缩的方法，显著降低了过程奖励模型（PRMs）的标注成本，通过将自然语言推理步骤转换为代码并构建前缀树，减少了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有过程标注方法（如人工标注或蒙特卡洛模拟）计算成本高，需要更高效的方法。

Method: 将自然语言推理步骤转换为代码，通过抽象语法树归一化，合并等效步骤构建前缀树，利用根到叶路径作为训练样本。

Result: 构建了包含196K样本的大规模数据集，仅需5%的计算资源，PRMs在Best-of-N和ProcessBench上表现优于现有方法。

Conclusion: SCOPE显著降低了标注成本，提升了PRMs的性能，为数学推理任务提供了高效解决方案。

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [83] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Main category: cs.AI

TL;DR: 论文提出了一种结合神经与符号方法的新方法，通过检索类似问题和验证器反馈，显著提升LLMs在几何问题证明中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在需要严格逻辑推理和符号推理的正式领域（如数学证明生成）中的局限性。

Method: 1. 检索类似问题并利用其证明指导LLM；2. 使用形式验证器评估生成的证明并提供反馈。

Result: OpenAI的o1模型在证明准确性上显著提升（58%-70%的改进）。

Conclusion: 通过生成可证明正确的结论，可以大幅提升LLMs的可靠性、准确性和一致性，解锁需要高可信度的复杂任务和实际应用。

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [84] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Main category: cs.AI

TL;DR: 研究表明，通过链式思维（CoT）推理的大型语言模型（LLM）在问题解决和信心表达上表现更优，且信心校准更准确。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在信心表达上存在不准确的问题，限制了其可靠性。研究旨在探索推理模型是否能改善这一问题。

Method: 通过对比六种推理模型和非推理模型在六个数据集上的表现，分析其信心校准和慢思考行为（如探索替代方法和回溯）的影响。

Result: 推理模型在36种设置中有33种表现更优，信心校准随着CoT的展开逐渐提升；移除慢思考行为会显著降低校准效果。

Conclusion: 推理模型通过慢思考行为动态调整信心，显著提升校准效果；非推理模型通过上下文学习也能获得类似收益。

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [85] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai,Jozo Dujmovic,Jianwu Wang*

Main category: cs.AI

TL;DR: BACON是一个基于分级逻辑的框架，用于训练可解释的AI模型，旨在实现高预测准确性和透明性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的应用增加，透明和可解释的模型变得至关重要。

Method: BACON使用分级逻辑自动训练可解释的AI模型，提供符号化解释。

Result: 在多个场景中，BACON表现出高性能，并生成紧凑、可验证的决策逻辑。

Conclusion: BACON是一种实用且原则性的方法，能够提供清晰、可信的可解释AI。

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [86] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Main category: cs.AI

TL;DR: 论文研究了如何通过文本分类方法将用户查询路由到不同的大语言模型（LLM）端点，并提出了一个带保护的查询路由问题。作者开发了GQR-Bench基准测试，对比了多种路由机制的性能，发现WideMLP在准确性和速度上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决查询路由中的分布外查询问题，如无关领域、其他语言或不安全文本，需要一种带保护的查询路由方法。

Method: 提出了GQR-Bench基准测试，涵盖三个目标领域和七个数据集，对比了LLM、传统机器学习模型和词袋分类器的性能。

Result: WideMLP在准确率（88%）和速度（<4ms）上表现最佳；fastText速度最快（<1ms），但准确率较低（80%）；LLM准确率最高（91%），但速度较慢。

Conclusion: 研究结果表明，LLM并非查询路由的唯一选择，WideMLP和fastText在某些场景下更具优势。GQR-Bench将作为Python包发布。

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [87] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli,Thomas Bolander,Sebastian Watzl*

Main category: cs.AI

TL;DR: 本文提出了一种通用的注意力逻辑，克服了现有动态认知逻辑的局限性，能够建模复杂注意力场景，并扩展了注意力的范围。


<details>
  <summary>Details</summary>
Motivation: 现有动态认知逻辑无法建模复杂注意力场景（如逻辑结构命题、高阶信念等），且规模随代理数和文字数指数增长。本文旨在解决这些问题。

Method: 通过推广边缘条件事件模型（保持表达能力的同时更简洁），并将注意力扩展到任意公式（包括其他代理的信念或注意力）。

Result: 提出了一种更通用的注意力逻辑，能够建模复杂场景，并展示了AI代理如何发现人类注意力偏见的示例。

Conclusion: 本文的注意力逻辑为建模复杂认知场景提供了新工具，尤其在AI代理分析人类注意力偏见方面具有潜力。

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [88] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.AI

TL;DR: 论文提出了一种名为Agent Context Protocols (ACPs)的结构化协议，用于多智能体系统的通信与协作，解决了传统自然语言协调的局限性，提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的协调依赖不精确的自然语言，限制了复杂交互和与领域专用智能体的互操作性，因此需要一种更结构化的通信协议。

Method: 引入ACP协议，结合持久执行蓝图（存储中间输出的依赖图）和标准化消息模式，实现鲁棒且容错的多智能体协作。

Result: ACP驱动的系统在长时程网络辅助任务中达到28.3%的准确率，并在多模态技术报告中优于商业AI系统。

Conclusion: ACP协议具有高度模块化和可扩展性，能快速构建高性能通用智能体系统。

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [89] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli,Sowmen Das,Yu-Wei Lin,Sattar Vakili,Chien-Yi Wang,Masoud Attarifar,Pritthijit Nath,Da-shan Shiu*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的多模态基础模型，用于处理通信数据，解决了包括标记化、位置嵌入、多模态、可变特征大小和归一化等关键挑战，并验证了其性能。


<details>
  <summary>Details</summary>
Motivation: AI在通信系统中的应用是研究热点，但现有方法多为任务特定解决方案，本文旨在开发一种通用的基础模型以支持多种应用。

Method: 采用Transformer架构，设计多模态模型直接处理通信数据，提出解决标记化、位置嵌入、多模态、可变特征大小和归一化的方法。

Result: 实验证明该模型能成功估计传输秩、预编码器选择、多普勒扩展和延迟剖面等多种特征。

Conclusion: 本文为通信数据的基础模型开发迈出了重要一步，展示了其在实际应用中的潜力。

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [90] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao,Yuchen Yan,Yongliang Shen,Haolei Xu,Wenqi Zhang,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 论文提出了一种名为Self-Braking Tuning（SBT）的新框架，通过让模型自我调节推理过程，减少冗余推理，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成长推理链时表现出色，但伴随冗余推理和高计算开销，现有方法依赖外部干预。

Method: 设计了基于标准答案的冗余推理识别指标，构建自适应推理长度数据，并引入刹车提示机制，使模型学会适时终止推理。

Result: 在数学基准测试中，SBT减少高达60%的token消耗，同时保持与无约束模型相当的准确性。

Conclusion: SBT通过自我调节有效解决了冗余推理问题，无需外部干预，显著提升了效率。

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [91] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Main category: cs.AI

TL;DR: SATBench是一个通过布尔可满足性问题（SAT）生成的逻辑谜题基准，用于评估大语言模型（LLM）的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注基于推理规则的逻辑推理，而SATBench利用SAT问题的搜索特性，填补了LLM在搜索式逻辑推理能力评估上的空白。

Method: SATBench通过自动化生成2100个逻辑谜题，调整子句数量控制难度，并通过LLM辅助和求解器验证一致性。

Result: 实验显示，最强模型o4-mini在困难UNSAT问题上仅达到65.0%准确率，接近随机基线50%。

Conclusion: SATBench揭示了当前LLM在搜索式逻辑推理上的局限性，并为未来研究提供了可扩展的测试平台。

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [92] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文探讨了在多模态环境下通过辩论机制增强大型语言模型（LLMs）的监督能力，特别是在视觉问答（VQA）任务中，弱模型通过辩论提升强模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多领域和多模态中的能力提升，传统的人类监督方式可能不足以评估其表现，因此需要探索新的监督机制。

Method: 研究扩展了辩论范式到多模态环境，通过两个视觉语言模型（专家）辩论答案，并由一个仅依赖文本的“盲”法官裁决。专家仅支持与其信念一致的答案，从而专注于分歧实例。

Result: 实验表明，辩论框架在多模态任务中表现优于单个专家模型，且弱LLMs的裁决能力可通过微调提升视觉语言模型的推理能力。

Conclusion: 辩论机制为多模态任务中的模型监督提供了有效途径，弱模型通过辩论可以增强强模型的性能。

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [93] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为CATS的新方法，通过结合LLM的推理能力和结构化搜索，解决了LLM在成本敏感规划中的不足。


<details>
  <summary>Details</summary>
Motivation: LLM在开放式推理中表现优异，但在成本敏感规划中常因忽略成本或无法严格遵循预算而表现不佳。

Method: 引入Cost-Augmented Monte Carlo Tree Search (CATS)，将明确的成本意识融入LLM引导的规划中。

Result: 实验表明，CATS在严格预算下表现优于GPT-4.1等LLM，任务成功率和成本效率更高。

Conclusion: CATS为预算敏感的决策提供了有效解决方案，结合了LLM的推理能力和结构化搜索的优势。

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [94] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Main category: cs.AI

TL;DR: SAFEPATH是一种轻量级对齐方法，通过在有害提示下生成简短的安全提示，有效减少有害输出，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂问题解决中表现强大，但其结构化推理路径可能导致不安全输出，现有安全对齐方法会降低推理深度。

Method: SAFEPATH通过微调LRMs，在有害提示下生成8个token的安全提示，其余推理过程不受监督。

Result: SAFEPATH减少90.0%有害输出，阻止83.3%越狱攻击，计算成本显著低于其他方法。

Conclusion: SAFEPATH提供了一种高效安全对齐方法，同时揭示了现有方法在推理中心模型中的局限性。

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [95] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: 论文提出ContextAgent，一种结合多维度感官上下文的首个上下文感知主动代理，通过穿戴设备数据提升LLM代理的主动服务能力，并在新基准测试中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有主动代理仅依赖封闭环境观察或基于规则的主动通知，导致用户意图理解不足和功能受限，需改进。

Method: ContextAgent从穿戴设备的感官数据提取多维度上下文，结合历史数据预测主动服务需求，并自动调用工具。

Result: 在ContextAgentBench基准测试中，ContextAgent在主动预测和工具调用上分别比基线高8.5%和6.0%准确率。

Conclusion: ContextAgent为开发更先进、以人为中心的主动AI助手提供了新方向。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [96] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Main category: cs.AI

TL;DR: 论文提出了一种名为RICE的新型推理时间引导方法，通过强化认知专家（cognitive experts）来提升大型推理模型的性能，无需额外训练或复杂启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型存在认知效率低下的问题（如过度思考或思考不足），需要一种轻量级方法来提升推理性能。

Method: 利用归一化点互信息（nPMI）识别并强化认知专家，这些专家通过特定标记（如“<think>”）协调元级推理操作。

Result: 在DeepSeek-R1和Qwen3-235B等模型上的实验显示，RICE显著提升了推理准确性、认知效率和跨领域泛化能力。

Conclusion: RICE是一种轻量、实用且可解释的方法，为提升高级推理模型的认知效率提供了新方向。

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [97] [An Edge AI Solution for Space Object Detection](https://arxiv.org/abs/2505.13468)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的Edge AI解决方案，用于空间物体检测（SOD），结合SE层、ViT和YOLOv9框架，实现了高精度和低延迟的检测。


<details>
  <summary>Details</summary>
Motivation: 随着近地轨道空间资产的增加，实时碰撞评估和避障的需求推动了高效Edge AI解决方案的研究。

Method: 采用基于SE层、Vision Transformers（ViT）和YOLOv9框架的深度学习模型。

Result: 模型在多种实际SOD场景中表现出高精度和极低延迟的多卫星检测能力。

Conclusion: 该Edge AI解决方案为空间物体检测任务提供了高效且实时的技术支持。

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [98] [Self-Supervised Learning for Image Segmentation: A Comprehensive Survey](https://arxiv.org/abs/2505.13584)
*Thangarajah Akilan,Nusrat Jahan,Wandong Zhang*

Main category: cs.CV

TL;DR: 该论文综述了自监督学习（SSL）在图像分割领域的应用，分析了150多篇相关文献，提供了任务分类、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 监督学习需要大量精确标注数据，成本高且耗时。自监督学习通过无标签数据和代理任务克服这些限制，成为解决计算机视觉问题的有力工具。

Method: 论文调查了150多篇SSL在图像分割领域的文献，分类了代理任务、下游任务和常用数据集。

Result: 总结了现有方法的关键观察，并提供了SSL在图像分割领域的实用分类和数据集信息。

Conclusion: 论文为新兴研究者提供了指导，并提出了未来研究方向，以使该领域更易理解和可访问。

Abstract: Supervised learning demands large amounts of precisely annotated data to
achieve promising results. Such data curation is labor-intensive and imposes
significant overhead regarding time and costs. Self-supervised learning (SSL)
partially overcomes these limitations by exploiting vast amounts of unlabeled
data and creating surrogate (pretext or proxy) tasks to learn useful
representations without manual labeling. As a result, SSL has become a powerful
machine learning (ML) paradigm for solving several practical downstream
computer vision problems, such as classification, detection, and segmentation.
Image segmentation is the cornerstone of many high-level visual perception
applications, including medical imaging, intelligent transportation,
agriculture, and surveillance. Although there is substantial research potential
for developing advanced algorithms for SSL-based semantic segmentation, a
comprehensive study of existing methodologies is essential to trace advances
and guide emerging researchers. This survey thoroughly investigates over 150
recent image segmentation articles, particularly focusing on SSL. It provides a
practical categorization of pretext tasks, downstream tasks, and commonly used
benchmark datasets for image segmentation research. It concludes with key
observations distilled from a large body of literature and offers future
directions to make this research field more accessible and comprehensible for
readers.

</details>


### [99] [IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion](https://arxiv.org/abs/2505.13633)
*Wentao Song,He Huang,Youqiang Sun,Fang Qu,Jiaqi Zhang,Longhui Fang,Yuwei Hao,Chenyang Peng*

Main category: cs.CV

TL;DR: IPENS是一种交互式无监督多目标点云提取方法，利用辐射场信息将2D掩码提升为3D点云，解决植物表型分析中的多目标分割问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模人工标注数据，对自遮挡的谷物级目标无效，需开发无监督高效方法。

Method: 利用SAM2分割2D掩码，通过辐射场信息提升为3D点云，设计多目标协同优化策略。

Result: 水稻数据集上分割精度63.72%（mIoU），小麦数据集提升至89.68%，表型预测性能优异。

Conclusion: IPENS为水稻和小麦提供非侵入式高质量表型提取方案，无需标注数据，显著提升智能育种效率。

Abstract: Advanced plant phenotyping technologies play a crucial role in targeted trait
improvement and accelerating intelligent breeding. Due to the species diversity
of plants, existing methods heavily rely on large-scale high-precision manually
annotated data. For self-occluded objects at the grain level, unsupervised
methods often prove ineffective. This study proposes IPENS, an interactive
unsupervised multi-target point cloud extraction method. The method utilizes
radiance field information to lift 2D masks, which are segmented by SAM2
(Segment Anything Model 2), into 3D space for target point cloud extraction. A
multi-target collaborative optimization strategy is designed to effectively
resolve the single-interaction multi-target segmentation challenge.
Experimental validation demonstrates that IPENS achieves a grain-level
segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong
phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697
(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length
and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a
wheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),
with equally outstanding phenotypic estimation performance: spike volume
prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00
(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92
(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality
phenotyping extraction solution for rice and wheat. Without requiring annotated
data, it rapidly extracts grain-level point clouds within 3 minutes through
simple single-round interactions on images for multiple targets, demonstrating
significant potential to accelerate intelligent breeding efficiency.

</details>


### [100] [ReactDiff: Latent Diffusion for Facial Reaction Generation](https://arxiv.org/abs/2505.14151)
*Jiaming Li,Sheng Wang,Xin Wang,Yitao Zhu,Honglin Xiong,Zixu Zhuang,Qian Wang*

Main category: cs.CV

TL;DR: ReactDiff框架通过多模态Transformer和潜在扩散模型，提升了听众面部反应的生成质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 捕捉视频和音频之间的相关性，同时平衡反应的适当性、真实性和多样性。

Method: 结合多模态Transformer和潜在空间的条件扩散，利用类内和类间注意力实现细粒度多模态交互。

Result: ReactDiff在面部反应相关性（0.26）和多样性评分（0.094）上显著优于现有方法，同时保持真实感。

Conclusion: ReactDiff通过多模态和扩散模型的结合，为面部反应生成提供了高效且多样化的解决方案。

Abstract: Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.

</details>


### [101] [GeoVLM: Improving Automated Vehicle Geolocalisation Using Vision-Language Matching](https://arxiv.org/abs/2505.13669)
*Barkin Dagda,Muhammad Awais,Saber Fallah*

Main category: cs.CV

TL;DR: GeoVLM利用视觉语言模型的零样本能力，通过可解释的跨视图语言描述改进跨视图地理定位的匹配精度。


<details>
  <summary>Details</summary>
Motivation: 解决跨视图地理定位中相似场景导致正确匹配难以排名第一的问题。

Method: 提出GeoVLM，一种可训练的重新排序方法，利用视觉语言模型生成跨视图语言描述。

Result: 在VIGOR、University-1652和新的Cross-View UK数据集上表现优于现有方法。

Conclusion: GeoVLM通过自然语言描述显著提升了跨视图地理定位的检索性能。

Abstract: Cross-view geo-localisation identifies coarse geographical position of an
automated vehicle by matching a ground-level image to a geo-tagged satellite
image from a database. Despite the advancements in Cross-view geo-localisation,
significant challenges still persist such as similar looking scenes which makes
it challenging to find the correct match as the top match. Existing approaches
reach high recall rates but they still fail to rank the correct image as the
top match. To address this challenge, this paper proposes GeoVLM, a novel
approach which uses the zero-shot capabilities of vision language models to
enable cross-view geo-localisation using interpretable cross-view language
descriptions. GeoVLM is a trainable reranking approach which improves the best
match accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard
benchmark VIGOR and University-1652 and also through real-life driving
environments using Cross-View United Kingdom, a new benchmark dataset
introduced in this paper. The results of the paper show that GeoVLM improves
retrieval performance of cross-view geo-localisation compared to the
state-of-the-art methods with the help of explainable natural language
descriptions. The code is available at
https://github.com/CAV-Research-Lab/GeoVLM

</details>


### [102] [GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization](https://arxiv.org/abs/2505.13731)
*Pengyue Jia,Seongheon Park,Song Gao,Xiangyu Zhao,Yixuan Li*

Main category: cs.CV

TL;DR: GeoRanker是一个基于距离感知的排名框架，利用视觉语言模型联合编码查询-候选交互并预测地理邻近性，显著提升了全球图像地理定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: 全球图像地理定位任务面临视觉内容多样性的挑战，现有方法依赖简单相似性启发式和点监督，未能建模候选间的空间关系。

Method: 提出GeoRanker框架，结合多阶距离损失函数，同时考虑绝对和相对距离，并创建GeoRanking数据集支持地理排名任务。

Result: 在IM2GPS3K和YFCC4K基准测试中取得最优结果，显著超越现有方法。

Conclusion: GeoRanker通过建模结构化空间关系，有效提升了地理定位任务的性能。

Abstract: Worldwide image geolocalization-the task of predicting GPS coordinates from
images taken anywhere on Earth-poses a fundamental challenge due to the vast
diversity in visual content across regions. While recent approaches adopt a
two-stage pipeline of retrieving candidates and selecting the best match, they
typically rely on simplistic similarity heuristics and point-wise supervision,
failing to model spatial relationships among candidates. In this paper, we
propose GeoRanker, a distance-aware ranking framework that leverages large
vision-language models to jointly encode query-candidate interactions and
predict geographic proximity. In addition, we introduce a multi-order distance
loss that ranks both absolute and relative distances, enabling the model to
reason over structured spatial relationships. To support this, we curate
GeoRanking, the first dataset explicitly designed for geographic ranking tasks
with multimodal candidate information. GeoRanker achieves state-of-the-art
results on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly
outperforming current best methods.

</details>


### [103] [Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks](https://arxiv.org/abs/2505.13741)
*Gaspard Goupy,Pierre Tirilly,Ioan Marius Bilasco*

Main category: cs.CV

TL;DR: 提出了一种名为Frozen Backpropagation (fBP)的训练算法，通过周期性冻结反馈权重减少权重传输，同时提出三种部分权重传输方案以进一步降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在神经形态硬件上直接训练SNNs可以显著降低能耗，但反向传播的对称权重要求增加了硬件开销和能耗。

Method: fBP算法通过周期性冻结反馈权重更新前向权重，减少权重传输；并提出了三种部分权重传输方案。

Result: fBP在图像识别任务中表现优于现有方法，准确率接近BP，部分权重传输方案可降低传输成本1000x至10000x，精度损失较小。

Conclusion: fBP为神经形态硬件设计提供了指导，支持基于BP的片上学习。

Abstract: Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware
can greatly reduce energy costs compared to GPU-based training. However,
implementing Backpropagation (BP) on such hardware is challenging because
forward and backward passes are typically performed by separate networks with
distinct weights. To compute correct gradients, forward and feedback weights
must remain symmetric during training, necessitating weight transport between
the two networks. This symmetry requirement imposes hardware overhead and
increases energy costs. To address this issue, we introduce Frozen
Backpropagation (fBP), a BP-based training algorithm relaxing weight symmetry
in settings with separate networks. fBP updates forward weights by computing
gradients with periodically frozen feedback weights, reducing weight transports
during training and minimizing synchronization overhead. To further improve
transport efficiency, we propose three partial weight transport schemes of
varying computational complexity, where only a subset of weights is transported
at a time. We evaluate our methods on image recognition tasks and compare them
to existing approaches addressing the weight symmetry requirement. Our results
show that fBP outperforms these methods and achieves accuracy comparable to BP.
With partial weight transport, fBP can substantially lower transport costs by
1,000x with an accuracy drop of only 0.5pp on CIFAR-10 and 1.1pp on CIFAR-100,
or by up to 10,000x at the expense of moderated accuracy loss. This work
provides insights for guiding the design of neuromorphic hardware incorporating
BP-based on-chip learning.

</details>


### [104] [RETRO: REthinking Tactile Representation Learning with Material PriOrs](https://arxiv.org/abs/2505.14319)
*Weihao Xia,Chenliang Zhou,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 论文提出了一种结合材料感知先验的触觉表征学习方法，弥补了现有方法忽视材料特性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有触觉表征学习方法主要关注触觉数据与视觉或文本信息的对齐，而忽略了材料特性对触觉体验的重要影响。

Method: 通过引入材料感知先验（预学习的材料特性）来改进触觉表征学习框架。

Result: 该方法能够更准确地捕捉和泛化表面纹理的细微差别，提升在机器人、触觉反馈系统和材料编辑等应用中的性能。

Conclusion: 结合材料感知先验的触觉表征学习方法显著提升了触觉反馈的准确性和丰富性。

Abstract: Tactile perception is profoundly influenced by the surface properties of
objects in contact. However, despite their crucial role in shaping tactile
experiences, these material characteristics have been largely neglected in
existing tactile representation learning methods. Most approaches primarily
focus on aligning tactile data with visual or textual information, overlooking
the richness of tactile feedback that comes from understanding the materials'
inherent properties. In this work, we address this gap by revisiting the
tactile representation learning framework and incorporating material-aware
priors into the learning process. These priors, which represent pre-learned
characteristics specific to different materials, allow tactile models to better
capture and generalize the nuances of surface texture. Our method enables more
accurate, contextually rich tactile feedback across diverse materials and
textures, improving performance in real-world applications such as robotics,
haptic feedback systems, and material editing.

</details>


### [105] [ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model](https://arxiv.org/abs/2505.13746)
*Satoshi Kondo*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（ReSW-VL）的手术阶段识别方法，通过微调CLIP模型的图像编码器并结合提示学习，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别技术有广泛应用，但现有研究在CNN特征提取或表示学习方法上不足。

Method: 使用CLIP视觉语言模型，通过提示学习微调图像编码器，用于手术阶段识别。

Result: 在三个数据集上的实验表明，该方法优于传统方法。

Conclusion: ReSW-VL方法在手术阶段识别中表现出高效性，为相关研究提供了新思路。

Abstract: Surgical phase recognition from video is a technology that automatically
classifies the progress of a surgical procedure and has a wide range of
potential applications, including real-time surgical support, optimization of
medical resources, training and skill assessment, and safety improvement.
Recent advances in surgical phase recognition technology have focused primarily
on Transform-based methods, although methods that extract spatial features from
individual frames using a CNN and video features from the resulting time series
of spatial features using time series modeling have shown high performance.
However, there remains a paucity of research on training methods for CNNs
employed for feature extraction or representation learning in surgical phase
recognition. In this study, we propose a method for representation learning in
surgical workflow analysis using a vision-language model (ReSW-VL). Our
proposed method involves fine-tuning the image encoder of a CLIP (Convolutional
Language Image Model) vision-language model using prompt learning for surgical
phase recognition. The experimental results on three surgical phase recognition
datasets demonstrate the effectiveness of the proposed method in comparison to
conventional methods.

</details>


### [106] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/abs/2505.13777)
*Subash Khanal,Srikumar Sastry,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs*

Main category: cs.CV

TL;DR: Sat2Sound是一个多模态表示学习框架，用于预测地球上任何位置的声音分布，通过结合卫星图像和音频数据，并利用视觉语言模型生成丰富的声景描述。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖卫星图像和地理标记音频样本，但无法充分捕捉声音多样性。Sat2Sound旨在通过多模态学习解决这一问题。

Method: 利用视觉语言模型生成声景描述，结合对比学习跨音频、音频描述、卫星图像和图像描述，学习共享的声景概念代码库。

Result: 在GeoSound和SoundingEarth数据集上，Sat2Sound在跨模态检索中达到最优性能，并支持基于位置的声景合成。

Conclusion: Sat2Sound通过多模态学习提升了声景预测能力，并展示了新的应用潜力。

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [107] [Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language](https://arxiv.org/abs/2505.13784)
*Dinh Nam Pham,Eleftherios Avramidis*

Main category: cs.CV

TL;DR: 该研究探索了从视觉语音识别（VSR）到德国手语中口型识别的迁移学习潜力，通过多任务学习提升口型识别和VSR的准确性与模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手语识别（SLR）系统主要关注手势，但非手动特征（如口型）提供了重要的语言信息。研究旨在填补SLR中口型识别的研究空白，并探索VSR与口型识别的关联。

Method: 利用三个VSR数据集（英语、德语无关词、德语目标词）进行迁移学习，通过多任务学习提升口型识别和VSR性能。

Result: 多任务学习显著提高了口型识别和VSR的准确性，并增强了模型鲁棒性。

Conclusion: 口型识别应被视为与VSR相关但独立的任务，迁移学习为SLR中口型标注有限的数据集提供了有效解决方案。

Abstract: Sign Language Recognition (SLR) systems primarily focus on manual gestures,
but non-manual features such as mouth movements, specifically mouthing, provide
valuable linguistic information. This work directly classifies mouthing
instances to their corresponding words in the spoken language while exploring
the potential of transfer learning from Visual Speech Recognition (VSR) to
mouthing recognition in German Sign Language. We leverage three VSR datasets:
one in English, one in German with unrelated words and one in German containing
the same target words as the mouthing dataset, to investigate the impact of
task similarity in this setting. Our results demonstrate that multi-task
learning improves both mouthing recognition and VSR accuracy as well as model
robustness, suggesting that mouthing recognition should be treated as a
distinct but related task to VSR. This research contributes to the field of SLR
by proposing knowledge transfer from VSR to SLR datasets with limited mouthing
annotations.

</details>


### [108] [Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels](https://arxiv.org/abs/2505.13788)
*Yongshuo Zong,Qin Zhang,Dongsheng An,Zhihua Li,Xiang Xu,Linghan Xu,Zhuowen Tu,Yifan Xing,Onkar Dabeer*

Main category: cs.CV

TL;DR: 本文提出了一种自动扩展指令跟随数据的工作流，用于提升视觉语言模型（VLM）在复杂指令下的像素级定位能力，解决了文本指令定位中的五大挑战。通过知识蒸馏生成高质量数据，实验结果显示模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决文本指令定位中的五大挑战（如幻觉引用、多对象场景等），并减少对昂贵人工标注的依赖。

Method: 利用预训练教师模型的知识蒸馏生成高质量指令-响应对，并与现有像素级标注关联。

Result: 模型在Ground-V数据集上训练后，性能显著提升，LISA和PSALM的平均准确率分别提高4.4%和7.9%，并在多个基准测试中创下新记录。

Conclusion: Ground-V数据集有效提升了模型的像素级定位能力，为复杂指令下的视觉语言任务提供了高效解决方案。

Abstract: This work presents a simple yet effective workflow for automatically scaling
instruction-following data to elicit pixel-level grounding capabilities of VLMs
under complex instructions. In particular, we address five critical real-world
challenges in text-instruction-based grounding: hallucinated references,
multi-object scenarios, reasoning, multi-granularity, and part-level
references. By leveraging knowledge distillation from a pre-trained teacher
model, our approach generates high-quality instruction-response pairs linked to
existing pixel-level annotations, minimizing the need for costly human
annotation. The resulting dataset, Ground-V, captures rich object localization
knowledge and nuanced pixel-level referring expressions. Experiment results
show that models trained on Ground-V exhibit substantial improvements across
diverse grounding tasks. Specifically, incorporating Ground-V during training
directly achieves an average accuracy boost of 4.4% for LISA and a 7.9% for
PSALM across six benchmarks on the gIoU metric. It also sets new
state-of-the-art results on standard benchmarks such as RefCOCO/+/g. Notably,
on gRefCOCO, we achieve an N-Acc of 83.3%, exceeding the previous
state-of-the-art by more than 20%.

</details>


### [109] [Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning](https://arxiv.org/abs/2505.13812)
*Zhongyu Chen,Rong Zhao,Xie Han,Xindong Guo,Song Wang,Zherui Qiao*

Main category: cs.CV

TL;DR: 提出了一种基于物理驱动的自监督学习方法，通过局部-整体力传播机制学习点云表示，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法注重结构特征但忽略局部与整体关系，而真实世界中物体的弹性变形通过局部力传播影响整体结构。

Method: 采用双任务编码器-解码器框架，结合隐式场几何建模与物理驱动弹性变形，通过两个解码器分别学习整体形状和局部变形。

Result: 在物体分类、少样本学习和分割任务中表现优于现有方法。

Conclusion: 该方法通过物理驱动机制有效捕捉局部与整体关系，提升了点云表示学习性能。

Abstract: Existing point cloud representation learning tend to learning the geometric
distribution of objects through data-driven approaches, emphasizing structural
features while overlooking the relationship between the local information and
the whole structure. Local features reflect the fine-grained variations of an
object, while the whole structure is determined by the interaction and
combination of these local features, collectively defining the object's shape.
In real-world, objects undergo elastic deformation under external forces, and
this deformation gradually affects the whole structure through the propagation
of forces from local regions, thereby altering the object's geometric
properties. Inspired by this, we propose a physics-driven self-supervised
learning method for point cloud representation, which captures the relationship
between parts and the whole by constructing a local-whole force propagation
mechanism. Specifically, we employ a dual-task encoder-decoder framework,
integrating the geometric modeling capability of implicit fields with
physics-driven elastic deformation. The encoder extracts features from the
point cloud and its tetrahedral mesh representation, capturing both geometric
and physical properties. These features are then fed into two decoders: one
learns the whole geometric shape of the point cloud through an implicit field,
while the other predicts local deformations using two specifically designed
physics information loss functions, modeling the deformation relationship
between local and whole shapes. Experimental results show that our method
outperforms existing approaches in object classification, few-shot learning,
and segmentation, demonstrating its effectiveness.

</details>


### [110] [InstanceBEV: Unifying Instance and BEV Representation for Global Modeling](https://arxiv.org/abs/2505.13817)
*Feng Li,Kun Xu,Zhaoyue Wang,Yunduan Cui,Mohammad Masum Billah,Jia Liu*

Main category: cs.CV

TL;DR: InstanceBEV提出了一种基于BEV的实例级降维方法，通过全局特征聚合实现高效建模，无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 解决现有BEV方法在大规模全局建模中需要复杂工程优化的问题。

Method: 引入实例级降维，直接使用transformer聚合全局特征，并将全局特征图采样到3D空间。

Result: 在OpenOcc-NuScenes数据集上达到SOTA性能，且框架简单高效。

Conclusion: InstanceBEV为BEV全局建模提供了一种无需额外优化的高效解决方案。

Abstract: Occupancy Grid Maps are widely used in navigation for their ability to
represent 3D space occupancy. However, existing methods that utilize multi-view
cameras to construct Occupancy Networks for perception modeling suffer from
cubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspective
offers a more practical solution for autonomous driving, as it provides higher
semantic density and mitigates complex object occlusions. Nonetheless,
BEV-based approaches still require extensive engineering optimizations to
enable efficient large-scale global modeling. To address this challenge, we
propose InstanceBEV, the first method to introduce instance-level
dimensionality reduction for BEV, enabling global modeling with transformers
without relying on sparsification or acceleration operators. Different from
other BEV methods, our approach directly employs transformers to aggregate
global features. Compared to 3D object detection models, our method samples
global feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset show
that InstanceBEV achieves state-of-the-art performance while maintaining a
simple, efficient framework without requiring additional optimizations.

</details>


### [111] [MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction](https://arxiv.org/abs/2505.13839)
*Zhenyu Bao,Qing Li,Guibiao Liao,Zhongyuan Zhao,Kanglin Liu*

Main category: cs.CV

TL;DR: MGStream提出了一种基于运动相关3D高斯的方法，用于动态场景重建，解决了现有3DGS方法中的闪烁问题和存储效率问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS在动态新视角合成中表现优异，但仍存在闪烁、存储效率低和难以建模新物体的问题。

Method: MGStream使用运动相关3D高斯建模动态部分，静态部分使用普通3D高斯，并通过运动掩码和聚类凸包算法实现。动态部分采用刚性变形和注意力优化。

Result: 实验表明，MGStream在渲染质量、训练/存储效率和时序一致性上优于现有方法。

Conclusion: MGStream通过优化动态部分的建模，显著提升了动态场景重建的性能。

Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention in streamable
dynamic novel view synthesis (DNVS) for its photorealistic rendering capability
and computational efficiency. Despite much progress in improving rendering
quality and optimization strategies, 3DGS-based streamable dynamic scene
reconstruction still suffers from flickering artifacts and storage
inefficiency, and struggles to model the emerging objects. To tackle this, we
introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to
reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related
3DGs are implemented according to the motion mask and the clustering-based
convex hull algorithm. The rigid deformation is applied to the motion-related
3DGs for modeling the dynamic, and the attention-based optimization on the
motion-related 3DGs enables the reconstruction of the emerging objects. As the
deformation and optimization are only conducted on the motion-related 3DGs,
MGStream avoids flickering artifacts and improves the storage efficiency.
Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that
MGStream surpasses existing streaming 3DGS-based approaches in terms of
rendering quality, training/storage efficiency and temporal consistency. Our
code is available at: https://github.com/pcl3dv/MGStream.

</details>


### [112] [SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction](https://arxiv.org/abs/2505.13856)
*Ruqin Zhou,San Jiang,Wanshou Jiang,Yongsheng Zhang,Chenguang Dai*

Main category: cs.CV

TL;DR: SuperMapNet提出了一种用于长距离高精度矢量高清地图构建的方法，通过多模态输入和三级交互模块解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在BEV特征生成和地图元素分类定位中存在单模态感知能力有限、多模态协同不足、点信息与元素信息交互缺失等问题。

Method: 使用相机图像和LiDAR点云作为输入，通过跨注意力协同增强模块和基于流的差异对齐模块生成BEV特征，并通过三级交互（点对点、元素对元素、点对元素）实现高精度分类与定位。

Result: 在nuScenes和Argoverse2数据集上表现优异，分别超过SOTA方法14.9/8.8 mAP和18.5/3.1 mAP。

Conclusion: SuperMapNet通过多模态协同和三级交互显著提升了矢量高清地图的构建精度和范围。

Abstract: Vectorized HD map is essential for autonomous driving. Remarkable work has
been achieved in recent years, but there are still major issues: (1) in the
generation of the BEV features, single modality-based methods are of limited
perception capability, while direct concatenation-based multi-modal methods
fail to capture synergies and disparities between different modalities,
resulting in limited ranges with feature holes; (2) in the classification and
localization of map elements, only point information is used without the
consideration of element infor-mation and neglects the interaction between
point information and element information, leading to erroneous shapes and
element entanglement with low accuracy. To address above issues, we introduce
SuperMapNet for long-range and high-accuracy vectorized HD map construction. It
uses both camera images and LiDAR point clouds as input, and first tightly
couple semantic information from camera images and geometric information from
LiDAR point clouds by a cross-attention based synergy enhancement module and a
flow-based disparity alignment module for long-range BEV feature generation.
And then, local features from point queries and global features from element
queries are tightly coupled by three-level interactions for high-accuracy
classification and localization, where Point2Point interaction learns local
geometric information between points of the same element and of each point,
Element2Element interaction learns relation constraints between different
elements and semantic information of each elements, and Point2Element
interaction learns complement element information for its constituent points.
Experiments on the nuScenes and Argoverse2 datasets demonstrate superior
performances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP under
hard/easy settings, respectively. The code is made publicly available1.

</details>


### [113] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/abs/2505.13860)
*Tiancheng Jiang,Henry Wang,Md Sirajus Salekin,Parmida Atighehchian,Shinan Zhang*

Main category: cs.CV

TL;DR: 本文探讨了开源视觉语言模型（VLM）在特定领域（以足球为例）的适应性，通过课程学习和指令数据微调，显著提升了模型在足球相关任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解VLM研究多为领域无关，缺乏对特定领域迁移学习能力的探索，本文旨在填补这一空白。

Method: 使用大规模足球数据集和LLM生成指令数据，采用课程学习方式（先教授足球概念，再问答任务）迭代微调通用VLM。

Result: 最终模型在足球视觉问答任务中相对提升37.5%，足球动作分类任务准确率从11.8%提升至63.5%。

Conclusion: 研究表明，通过领域适应方法，通用VLM可以显著提升在特定领域的性能。

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [114] [4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision](https://arxiv.org/abs/2505.13905)
*Ruihan Liu,Xiaoyi Wu,Xijun Chen,Liang Hu,Yunjiang Lou*

Main category: cs.CV

TL;DR: 4D-ROLLS是一种基于4D雷达的弱监督占用估计方法，利用LiDAR点云作为监督信号，在恶劣环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有占用估计方法依赖LiDAR或摄像头，在恶劣环境（如烟雾、雨雪）中性能较差，需要一种更鲁棒的解决方案。

Method: 提出伪LiDAR标签生成方法，包括占用查询和LiDAR高度图，作为多阶段监督训练4D雷达占用估计模型。

Result: 实验验证了4D-ROLLS的优异性能，尤其在恶劣环境和跨数据集训练中表现突出，且推理速度快（30 Hz）。

Conclusion: 4D-ROLLS不仅适用于占用估计，还可扩展到下游任务（如BEV分割和点云占用预测），具有广泛应用潜力。

Abstract: A comprehensive understanding of 3D scenes is essential for autonomous
vehicles (AVs), and among various perception tasks, occupancy estimation plays
a central role by providing a general representation of drivable and occupied
space. However, most existing occupancy estimation methods rely on LiDAR or
cameras, which perform poorly in degraded environments such as smoke, rain,
snow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised
occupancy estimation method for 4D radar using the LiDAR point cloud as the
supervisory signal. Specifically, we introduce a method for generating
pseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as
multi-stage supervision to train the 4D radar occupancy estimation model. Then
the model is aligned with the occupancy map produced by LiDAR, fine-tuning its
accuracy in occupancy estimation. Extensive comparative experiments validate
the exceptional performance of 4D-ROLLS. Its robustness in degraded
environments and effectiveness in cross-dataset training are qualitatively
demonstrated. The model is also seamlessly transferred to downstream tasks BEV
segmentation and point cloud occupancy prediction, highlighting its potential
for broader applications. The lightweight network enables 4D-ROLLS model to
achieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of
4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS.

</details>


### [115] [Blind Restoration of High-Resolution Ultrasound Video](https://arxiv.org/abs/2505.13915)
*Chu Chen,Kangning Cui,Pasquale Cascarano,Wei Tang,Elena Loli Piccolomini,Raymond H. Chan*

Main category: cs.CV

TL;DR: 提出了一种自监督的超声视频超分辨率算法DUP，无需配对训练数据即可提升分辨率并去噪。


<details>
  <summary>Details</summary>
Motivation: 超声视频常因低信噪比和分辨率不足影响诊断，且设备和采集设置的差异降低了预训练模型的泛化能力。

Method: 采用视频自适应优化的神经网络（DUP），无需配对数据即可提升分辨率和去噪。

Result: 定量和视觉评估表明DUP优于现有超分辨率算法，显著提升下游应用效果。

Conclusion: DUP是一种有效的自监督超声视频增强方法，适用于临床实践。

Abstract: Ultrasound imaging is widely applied in clinical practice, yet ultrasound
videos often suffer from low signal-to-noise ratios (SNR) and limited
resolutions, posing challenges for diagnosis and analysis. Variations in
equipment and acquisition settings can further exacerbate differences in data
distribution and noise levels, reducing the generalizability of pre-trained
models. This work presents a self-supervised ultrasound video super-resolution
algorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive
optimization process of a neural network that enhances the resolution of given
ultrasound videos without requiring paired training data while simultaneously
removing noise. Quantitative and visual evaluations demonstrate that DUP
outperforms existing super-resolution algorithms, leading to substantial
improvements for downstream applications.

</details>


### [116] [An Explorative Analysis of SVM Classifier and ResNet50 Architecture on African Food Classification](https://arxiv.org/abs/2505.13923)
*Chinedu Emmanuel Mbonu,Kenechukwu Anigbogu,Doris Asogwa,Tochukwu Belonwu*

Main category: cs.CV

TL;DR: 研究比较了深度学习和传统机器学习方法在非洲食物分类中的表现，发现ResNet50和SVM各有优劣，为非洲食物识别提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 非洲食物识别研究较少，本文填补了这一空白。

Method: 使用微调的ResNet50和SVM分类器，基于1,658张非洲食物图像进行对比实验。

Result: 通过混淆矩阵、F1分数、准确率、召回率和精确度等指标评估了两种方法的性能。

Conclusion: 研究为非洲食物识别系统的开发提供了实用参考，揭示了深度学习和传统方法的适用场景。

Abstract: Food recognition systems has advanced significantly for Western cuisines, yet
its application to African foods remains underexplored. This study addresses
this gap by evaluating both deep learning and traditional machine learning
methods for African food classification. We compared the performance of a
fine-tuned ResNet50 model with a Support Vector Machine (SVM) classifier. The
dataset comprises 1,658 images across six selected food categories that are
known in Africa. To assess model effectiveness, we utilize five key evaluation
metrics: Confusion matrix, F1-score, accuracy, recall and precision. Our
findings offer valuable insights into the strengths and limitations of both
approaches, contributing to the advancement of food recognition for African
cuisines.

</details>


### [117] [LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts](https://arxiv.org/abs/2505.13928)
*Qifeng Cai,Hao Liang,Hejun Dong,Meiyi Qiang,Ruichuan An,Zhaoyang Han,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Main category: cs.CV

TL;DR: LoVR是一个专为长视频-文本检索设计的新基准，包含467个长视频和40,804个细粒度片段，提供高质量标注，并通过自动生成和动态优化提升标注准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本检索基准存在视频时长短、标注质量低和粒度粗的问题，限制了高级方法的评估。

Method: 提出高效标注生成框架（VLM自动生成、质量评分和动态优化）和语义融合方法生成连贯的全视频标注。

Result: LoVR基准通过实验验证了其挑战性，揭示了当前方法的局限性。

Conclusion: LoVR为视频理解和检索提供了新挑战，并为未来研究提供了宝贵见解。

Abstract: Long videos contain a vast amount of information, making video-text retrieval
an essential and challenging task in multimodal learning. However, existing
benchmarks suffer from limited video duration, low-quality captions, and coarse
annotation granularity, which hinder the evaluation of advanced video-text
retrieval methods. To address these limitations, we introduce LoVR, a benchmark
specifically designed for long video-text retrieval. LoVR contains 467 long
videos and over 40,804 fine-grained clips with high-quality captions. To
overcome the issue of poor machine-generated annotations, we propose an
efficient caption generation framework that integrates VLM automatic
generation, caption quality scoring, and dynamic refinement. This pipeline
improves annotation accuracy while maintaining scalability. Furthermore, we
introduce a semantic fusion method to generate coherent full-video captions
without losing important contextual information. Our benchmark introduces
longer videos, more detailed captions, and a larger-scale dataset, presenting
new challenges for video understanding and retrieval. Extensive experiments on
various advanced embedding models demonstrate that LoVR is a challenging
benchmark, revealing the limitations of current approaches and providing
valuable insights for future research. We release the code and dataset link at
https://github.com/TechNomad-ds/LoVR-benchmark

</details>


### [118] [Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR](https://arxiv.org/abs/2505.13943)
*Samee Arif,Sualeha Farid*

Main category: cs.CV

TL;DR: 本文提出了一种针对乌尔都语报纸的端到端OCR流程，解决了多栏布局、低分辨率扫描和多样字体等挑战，通过四个模块（文章分割、图像超分辨率、栏分割和文本识别）实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语报纸的OCR面临复杂布局和低质量扫描的挑战，需要一种综合解决方案以提高识别准确率。

Method: 采用YOLOv11x进行文章和栏分割，SwinIR模型提升图像分辨率，并测试多种LLM（如Gemini、GPT等）进行文本识别。

Result: 文章分割mAP@50为0.975，超分辨率PSNR达32.71 dB，栏分割mAP@50为0.975，文本识别最低WER为0.133（Gemini-2.5-Pro）。

Conclusion: 提出的端到端流程显著提升了乌尔都语报纸OCR的性能，为类似复杂场景提供了有效解决方案。

Abstract: This paper introduces a comprehensive end-to-end pipeline for Optical
Character Recognition (OCR) on Urdu newspapers. In our approach, we address the
unique challenges of complex multi-column layouts, low-resolution archival
scans, and diverse font styles. Our process decomposes the OCR task into four
key modules: (1) article segmentation, (2) image super-resolution, (3) column
segmentation, and (4) text recognition. For article segmentation, we fine-tune
and evaluate YOLOv11x to identify and separate individual articles from
cluttered layouts. Our model achieves a precision of 0.963 and mAP@50 of 0.975.
For super-resolution, we fine-tune and benchmark the SwinIR model (reaching
32.71 dB PSNR) to enhance the quality of degraded newspaper scans. To do our
column segmentation, we use YOLOv11x to separate columns in text to further
enhance performance - this model reaches a precision of 0.970 and mAP@50 of
0.975. In the text recognition stage, we benchmark a range of LLMs from
different families, including Gemini, GPT, Llama, and Claude. The lowest WER of
0.133 is achieved by Gemini-2.5-Pro.

</details>


### [119] [StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning](https://arxiv.org/abs/2505.13997)
*Huaijie Wang,De Cheng,Guozhang Li,Zhipeng Xu,Lingfeng He,Jie Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: StPR是一个无需示例的视频类增量学习框架，通过分离时空信息并动态路由任务专家，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频类增量学习（VCIL）面临时空结构的复杂性，现有方法依赖示例或忽略时序建模，StPR旨在解决这些问题。

Method: StPR结合帧共享语义蒸馏（FSSD）和时序分解混合专家（TD-MoE），分别处理语义稳定性和动态时序路由。

Result: 在UCF101、HMDB51和Kinetics400上的实验表明，StPR优于现有基线，同时提升了解释性和效率。

Conclusion: StPR为VCIL提供了一个统一且无需示例的解决方案，有效平衡了时空信息的保留与动态建模。

Abstract: Video Class-Incremental Learning (VCIL) seeks to develop models that
continuously learn new action categories over time without forgetting
previously acquired knowledge. Unlike traditional Class-Incremental Learning
(CIL), VCIL introduces the added complexity of spatiotemporal structures,
making it particularly challenging to mitigate catastrophic forgetting while
effectively capturing both frame-shared semantics and temporal dynamics.
Existing approaches either rely on exemplar rehearsal, raising concerns over
memory and privacy, or adapt static image-based methods that neglect temporal
modeling. To address these limitations, we propose Spatiotemporal Preservation
and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly
disentangles and preserves spatiotemporal information. First, we introduce
Frame-Shared Semantics Distillation (FSSD), which identifies semantically
stable and meaningful channels by jointly considering semantic sensitivity and
classification contribution. These important semantic channels are selectively
regularized to maintain prior knowledge while allowing for adaptation. Second,
we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which
dynamically routes task-specific experts based on their temporal dynamics,
enabling inference without task ID or stored exemplars. Together, StPR
effectively leverages spatial semantics and temporal dynamics, achieving a
unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51,
and Kinetics400 show that our method outperforms existing baselines while
offering improved interpretability and efficiency in VCIL. Code is available in
the supplementary materials.

</details>


### [120] [Multi-Label Stereo Matching for Transparent Scene Depth Estimation](https://arxiv.org/abs/2505.14008)
*Zhidan Liu,Chengtang Yao,Jiaxi Zeng,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: 提出一种多标签立体匹配方法，用于透明场景中同时估计透明物体和遮挡背景的深度。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设视差维度为单峰分布，将匹配视为单标签回归问题，无法处理透明场景中的多深度值问题。

Method: 采用多标签回归框架，引入像素级多元高斯表示，通过GRU迭代预测均值和协方差矩阵。

Result: 实验表明，该方法显著提升了透明表面的深度估计性能，同时保留了背景信息。

Conclusion: 该方法有效解决了透明场景中的多深度估计问题，代码已开源。

Abstract: In this paper, we present a multi-label stereo matching method to
simultaneously estimate the depth of the transparent objects and the occluded
background in transparent scenes.Unlike previous methods that assume a unimodal
distribution along the disparity dimension and formulate the matching as a
single-label regression problem, we propose a multi-label regression
formulation to estimate multiple depth values at the same pixel in transparent
scenes. To resolve the multi-label regression problem, we introduce a
pixel-wise multivariate Gaussian representation, where the mean vector encodes
multiple depth values at the same pixel, and the covariance matrix determines
whether a multi-label representation is necessary for a given pixel. The
representation is iteratively predicted within a GRU framework. In each
iteration, we first predict the update step for the mean parameters and then
use both the update step and the updated mean parameters to estimate the
covariance matrix. We also synthesize a dataset containing 10 scenes and 89
objects to validate the performance of transparent scene depth estimation. The
experiments show that our method greatly improves the performance on
transparent surfaces while preserving the background information for scene
reconstruction. Code is available at https://github.com/BFZD233/TranScene.

</details>


### [121] [UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache](https://arxiv.org/abs/2505.14010)
*Pu Wang,Pengwen Dai,Chen Wu,Yeying Jin,Dianjie Lu,Guijuan Zhang,Youshan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出了一种高效的视觉Transformer框架，用于超高清图像去雾，解决了现有方法训练速度慢和内存消耗高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超高清图像去雾任务中面临训练速度慢和内存消耗高的挑战，需要一种更高效的解决方案。

Method: 1) 引入自适应归一化机制，基于nGPT架构实现快速稳定训练；2) 设计大气散射感知的KV缓存机制，动态优化特征保留。

Result: 训练收敛速度提升5倍，内存开销降低，RTX4090 GPU上实现每秒50张高分辨率图像实时处理，同时保持去雾质量。

Conclusion: 该方法在4K/8K图像修复任务中显著提升计算效率，并提供了一种新的可解释去雾方法。

Abstract: In this paper, we propose an efficient visual transformer framework for
ultra-high-definition (UHD) image dehazing that addresses the key challenges of
slow training speed and high memory consumption for existing methods. Our
approach introduces two key innovations: 1) an \textbf{a}daptive
\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables
ultra-fast and stable training with a network with a restricted range of
parameter expressions; and 2) we devise an atmospheric scattering-aware KV
caching mechanism that dynamically optimizes feature preservation based on the
physical haze formation model. The proposed architecture improves the training
convergence speed by \textbf{5 $\times$} while reducing memory overhead,
enabling real-time processing of 50 high-resolution images per second on an
RTX4090 GPU. Experimental results show that our approach maintains
state-of-the-art dehazing quality while significantly improving computational
efficiency for 4K/8K image restoration tasks. Furthermore, we provide a new
dehazing image interpretable method with the help of an integrated gradient
attribution map. Our code can be found here:
https://anonymous.4open.science/r/anDehazeFormer-632E/README.md.

</details>


### [122] [EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation](https://arxiv.org/abs/2505.14014)
*Zelin Zhang,Tao Zhang,KediLI,Xu Zheng*

Main category: cs.CV

TL;DR: EGFormer是一种高效的多模态语义分割框架，通过动态评分和模态丢弃模块减少计算负担，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注精度提升，但计算效率未充分研究，EGFormer旨在解决这一问题。

Method: 引入Any-modal Scoring Module（ASM）动态评分模态，Modal Dropping Module（MDM）过滤冗余模态。

Result: 参数减少88%，计算量降低50%，在无监督域适应任务中表现优异。

Conclusion: EGFormer在高效性和通用性上表现突出，为多模态分割提供了新思路。

Abstract: Recent efforts have explored multimodal semantic segmentation using various
backbone architectures. However, while most methods aim to improve accuracy,
their computational efficiency remains underexplored. To address this, we
propose EGFormer, an efficient multimodal semantic segmentation framework that
flexibly integrates an arbitrary number of modalities while significantly
reducing model parameters and inference time without sacrificing performance.
Our framework introduces two novel modules. First, the Any-modal Scoring Module
(ASM) assigns importance scores to each modality independently, enabling
dynamic ranking based on their feature maps. Second, the Modal Dropping Module
(MDM) filters out less informative modalities at each stage, selectively
preserving and aggregating only the most valuable features. This design allows
the model to leverage useful information from all available modalities while
discarding redundancy, thus ensuring high segmentation quality. In addition to
efficiency, we evaluate EGFormer on a synthetic-to-real transfer task to
demonstrate its generalizability. Extensive experiments show that EGFormer
achieves competitive performance with up to 88 percent reduction in parameters
and 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it
further achieves state-of-the-art transfer performance compared to existing
methods.

</details>


### [123] [OmniStyle: Filtering High Quality Style Transfer Data at Scale](https://arxiv.org/abs/2505.14028)
*Ye Wang,Ruiqi Liu,Jiang Lin,Fei Liu,Zili Yi,Yilin Wang,Rui Ma*

Main category: cs.CV

TL;DR: OmniStyle-1M是一个大规模的风格迁移数据集，包含100万对内容-风格-风格化图像三元组，支持高效训练和精确控制。OmniFilter确保数据质量，OmniStyle框架基于DiT架构，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决风格迁移领域缺乏大规模高质量数据集和高效模型的问题。

Method: 提出OmniStyle-1M数据集和OmniFilter质量评估框架，并基于DiT架构设计OmniStyle框架。

Result: OmniStyle在质量和效率上优于现有方法，支持指令和图像引导的风格迁移。

Conclusion: OmniStyle-1M和方法为高质量风格迁移提供了重要贡献，是研究社区的宝贵资源。

Abstract: In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer
dataset comprising over one million content-style-stylized image triplets
across 1,000 diverse style categories, each enhanced with textual descriptions
and instruction prompts. We show that OmniStyle-1M can not only enable
efficient and scalable of style transfer models through supervised training but
also facilitate precise control over target stylization. Especially, to ensure
the quality of the dataset, we introduce OmniFilter, a comprehensive style
transfer quality assessment framework, which filters high-quality triplets
based on content preservation, style consistency, and aesthetic appeal.
Building upon this foundation, we propose OmniStyle, a framework based on the
Diffusion Transformer (DiT) architecture designed for high-quality and
efficient style transfer. This framework supports both instruction-guided and
image-guided style transfer, generating high resolution outputs with
exceptional detail. Extensive qualitative and quantitative evaluations
demonstrate OmniStyle's superior performance compared to existing approaches,
highlighting its efficiency and versatility. OmniStyle-1M and its accompanying
methodologies provide a significant contribution to advancing high-quality
style transfer, offering a valuable resource for the research community.

</details>


### [124] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/abs/2505.14029)
*Laura-Sophia von Hirschhausen,Jannes S. Magnusson,Mykyta Kovalenko,Fredrik Boye,Tanay Rawat,Peter Eisert,Anna Hilsmann,Sebastian Pretzsch,Sebastian Bosse*

Main category: cs.CV

TL;DR: AppleGrowthVision是一个大规模数据集，填补了苹果园监测中数据多样性和立体图像的不足，提升了目标检测和生长阶段预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和立体图像，无法满足苹果园3D建模和生长阶段分析的需求。

Method: 提出AppleGrowthVision数据集，包含高分辨率立体图像和密集标注的子集，覆盖多个生长阶段。

Result: 数据集显著提升了YOLOv8和Faster R-CNN的性能，生长阶段预测准确率超过95%。

Conclusion: AppleGrowthVision连接了农业科学与计算机视觉，未来将改进标注和3D重建。

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [125] [Selective Structured State Space for Multispectral-fused Small Target Detection](https://arxiv.org/abs/2505.14043)
*Qianqian Zhang,WeiJun Wang,Yunxing Liu,Li Zhou,Hao Zhao,Junshe An,Zihan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种结合Mamba线性复杂度优势的改进方法，通过ESTD和CARG模块增强小目标检测能力，并利用MEPF模块融合多光谱信息。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像中小目标识别精度低且计算成本高，现有方法（如Transformer和CNN）存在计算复杂度或性能不足的问题。

Method: 结合Mamba的线性复杂度，设计ESTD模块增强局部注意力，CARG模块强化空间和通道信息，并引入MEPF模块进行多光谱融合。

Result: 改进后的模型能够更高效地捕捉小目标的全局和局部特征，提升检测精度。

Conclusion: 通过模块化设计，有效解决了小目标检测中的计算和性能问题，为高分辨率遥感图像分析提供了新思路。

Abstract: Target detection in high-resolution remote sensing imagery faces challenges
due to the low recognition accuracy of small targets and high computational
costs. The computational complexity of the Transformer architecture increases
quadratically with image resolution, while Convolutional Neural Networks (CNN)
architectures are forced to stack deeper convolutional layers to expand their
receptive fields, leading to an explosive growth in computational demands. To
address these computational constraints, we leverage Mamba's linear complexity
for efficiency. However, Mamba's performance declines for small targets,
primarily because small targets occupy a limited area in the image and have
limited semantic information. Accurate identification of these small targets
necessitates not only Mamba's global attention capabilities but also the
precise capture of fine local details. To this end, we enhance Mamba by
developing the Enhanced Small Target Detection (ESTD) module and the
Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters
local attention to capture fine-grained details, while the CARG module, built
upon Mamba, emphasizes spatial and channel-wise information, collectively
improving the model's ability to capture distinctive representations of small
targets. Additionally, to highlight the semantic representation of small
targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for
multispectral fusion, which enhances target features by effectively fusing
visible and infrared multimodal information.

</details>


### [126] [Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification](https://arxiv.org/abs/2505.14049)
*Yibo Gao,Hangqi Zhou,Zheyao Gao,Bomin Wang,Shangqi Gao,Sihan Wang,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CRL的新框架，通过二值化视觉概念学习布尔逻辑规则，解决了概念泄漏和全局解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 临床应用中决策安全的需求凸显了基于概念的方法在医学影像中的潜力，但现有方法存在概念泄漏和缺乏全局解释性的问题。

Method: CRL利用逻辑层捕捉概念相关性并提取临床有意义的规则，提供局部和全局解释性。

Result: 在两个医学图像分类任务中，CRL性能与现有方法相当，同时显著提高了对分布外数据的泛化能力。

Conclusion: CRL是一种有效的框架，既能保持性能，又能提升解释性和泛化能力。

Abstract: The pursuit of decision safety in clinical applications highlights the
potential of concept-based methods in medical imaging. While these models offer
active interpretability, they often suffer from concept leakages, where
unintended information within soft concept representations undermines both
interpretability and generalizability. Moreover, most concept-based models
focus solely on local explanations (instance-level), neglecting the global
decision logic (dataset-level). To address these limitations, we propose
Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules
from binarized visual concepts. CRL employs logical layers to capture concept
correlations and extract clinically meaningful rules, thereby providing both
local and global interpretability. Experiments on two medical image
classification tasks show that CRL achieves competitive performance with
existing methods while significantly improving generalizability to
out-of-distribution data. The code of our work is available at
https://github.com/obiyoag/crl.

</details>


### [127] [Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting](https://arxiv.org/abs/2505.14059)
*Hao Feng,Shu Wei,Xiang Fei,Wei Shi,Yingdong Han,Lei Liao,Jinghui Lu,Binghong Wu,Qi Liu,Chunhui Lin,Jingqun Tang,Hao Liu,Can Huang*

Main category: cs.CV

TL;DR: Dolphin提出了一种多模态文档图像解析模型，通过两阶段分析-解析范式解决现有方法的效率瓶颈和布局退化问题。


<details>
  <summary>Details</summary>
Motivation: 文档图像解析因复杂元素交织而具有挑战性，现有方法存在集成开销和效率瓶颈。

Method: Dolphin采用两阶段范式：首先生成布局元素序列，再通过锚点提示并行解析内容。

Result: 在多个基准测试中达到SOTA性能，并通过轻量级架构和并行机制确保高效性。

Conclusion: Dolphin在性能和效率上均表现优异，代码和模型已开源。

Abstract: Document image parsing is challenging due to its complexly intertwined
elements such as text paragraphs, figures, formulas, and tables. Current
approaches either assemble specialized expert models or directly generate
page-level content autoregressively, facing integration overhead, efficiency
bottlenecks, and layout structure degradation despite their decent performance.
To address these limitations, we present \textit{Dolphin}
(\textit{\textbf{Do}cument Image \textbf{P}arsing via \textbf{H}eterogeneous
Anchor Prompt\textbf{in}g}), a novel multimodal document image parsing model
following an analyze-then-parse paradigm. In the first stage, Dolphin generates
a sequence of layout elements in reading order. These heterogeneous elements,
serving as anchors and coupled with task-specific prompts, are fed back to
Dolphin for parallel content parsing in the second stage. To train Dolphin, we
construct a large-scale dataset of over 30 million samples, covering
multi-granularity parsing tasks. Through comprehensive evaluations on both
prevalent benchmarks and self-constructed ones, Dolphin achieves
state-of-the-art performance across diverse page-level and element-level
settings, while ensuring superior efficiency through its lightweight
architecture and parallel parsing mechanism. The code and pre-trained models
are publicly available at https://github.com/ByteDance/Dolphin

</details>


### [128] [Scaling Vision Mamba Across Resolutions via Fractal Traversal](https://arxiv.org/abs/2505.14062)
*Bo Li,Haoke Xiao,Lv Tang*

Main category: cs.CV

TL;DR: FractalMamba++是一种基于分形序列化和状态路由机制的视觉骨干网络，解决了Vision Mamba在2D到1D序列化和分辨率适应性上的问题。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba在视觉输入中存在2D到1D序列化的挑战和分辨率适应性不足的问题，限制了其建模能力。

Method: 提出分形序列化（Hilbert曲线）保持空间局部性，引入Cross-State Routing（CSR）增强全局上下文传播，以及Positional-Relation Capture（PRC）模块恢复局部邻接关系。

Result: 在图像分类、语义分割、目标检测和变化检测任务中表现优异，尤其在高分辨率场景下优于现有Mamba骨干网络。

Conclusion: FractalMamba++通过分形序列化和状态路由机制，显著提升了Vision Mamba在视觉任务中的性能和适应性。

Abstract: Vision Mamba has recently emerged as a promising alternative to
Transformer-based architectures, offering linear complexity in sequence length
while maintaining strong modeling capacity. However, its adaptation to visual
inputs is hindered by challenges in 2D-to-1D patch serialization and weak
scalability across input resolutions. Existing serialization strategies such as
raster scanning disrupt local spatial continuity and limit the model's ability
to generalize across scales. In this paper, we propose FractalMamba++, a robust
vision backbone that leverages fractal-based patch serialization via Hilbert
curves to preserve spatial locality and enable seamless resolution
adaptability. To address long-range dependency fading in high-resolution
inputs, we further introduce a Cross-State Routing (CSR) mechanism that
enhances global context propagation through selective state reuse.
Additionally, we propose a Positional-Relation Capture (PRC) module to recover
local adjacency disrupted by curve inflection points. Extensive experiments on
image classification, semantic segmentation, object detection, and change
detection demonstrate that FractalMamba++ consistently outperforms previous
Mamba-based backbones, particularly under high-resolution settings.

</details>


### [129] [Place Recognition: A Comprehensive Review, Current Challenges and Future Directions](https://arxiv.org/abs/2505.14068)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Zhaojun Deng*

Main category: cs.CV

TL;DR: 本文综述了位置识别的最新进展，重点介绍了CNN、Transformer和跨模态方法，并总结了数据集、评估指标及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 位置识别是车辆导航和地图构建的核心能力，尤其在SLAM和长期导航中至关重要。本文旨在全面回顾该领域的最新方法。

Method: 综述了三种代表性方法：基于CNN的方法、Transformer框架和跨模态策略，分析了它们的优势和适用场景。

Result: 总结了标准数据集和评估指标，并提供了代码库和实验结果，为未来研究提供了参考。

Conclusion: 指出了当前挑战（如领域适应、实时性能）和未来方向（如终身学习），以推动该领域的进一步发展。

Abstract: Place recognition is a cornerstone of vehicle navigation and mapping, which
is pivotal in enabling systems to determine whether a location has been
previously visited. This capability is critical for tasks such as loop closure
in Simultaneous Localization and Mapping (SLAM) and long-term navigation under
varying environmental conditions. In this survey, we comprehensively review
recent advancements in place recognition, emphasizing three representative
methodological paradigms: Convolutional Neural Network (CNN)-based approaches,
Transformer-based frameworks, and cross-modal strategies. We begin by
elucidating the significance of place recognition within the broader context of
autonomous systems. Subsequently, we trace the evolution of CNN-based methods,
highlighting their contributions to robust visual descriptor learning and
scalability in large-scale environments. We then examine the emerging class of
Transformer-based models, which leverage self-attention mechanisms to capture
global dependencies and offer improved generalization across diverse scenes.
Furthermore, we discuss cross-modal approaches that integrate heterogeneous
data sources such as Lidar, vision, and text description, thereby enhancing
resilience to viewpoint, illumination, and seasonal variations. We also
summarize standard datasets and evaluation metrics widely adopted in the
literature. Finally, we identify current research challenges and outline
prospective directions, including domain adaptation, real-time performance, and
lifelong learning, to inspire future advancements in this domain. The unified
framework of leading-edge place recognition methods, i.e., code library, and
the results of their experimental evaluations are available at
https://github.com/CV4RA/SOTA-Place-Recognitioner.

</details>


### [130] [Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts](https://arxiv.org/abs/2505.14088)
*Xi Chen,Shen Yan,Juelin Zhu,Chen Chen,Yu Liu,Maojun Zhang*

Main category: cs.CV

TL;DR: Land-MoE是一种用于多光谱土地覆盖分类的新方法，通过频率感知的低秩令牌专家混合和频率域调制，有效应对光谱偏移问题，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多光谱土地覆盖分类中因传感器和地理空间条件差异导致的光谱偏移问题，现有方法性能有限。

Method: 提出Land-MoE，包含低秩令牌专家混合模块（MoLTE）和频率感知滤波器（FAF），以参数高效的方式微调视觉基础模型。

Result: 在跨传感器和跨地理空间的实验中表现优异，同时在RGB遥感图像的领域泛化语义分割任务中达到最先进性能。

Conclusion: Land-MoE通过动态调整特征和频率域调制，显著提升了多光谱土地覆盖分类的鲁棒性和性能。

Abstract: We introduce Land-MoE, a novel approach for multispectral land cover
classification (MLCC). Spectral shift, which emerges from disparities in
sensors and geospatial conditions, poses a significant challenge in this
domain. Existing methods predominantly rely on domain adaptation and
generalization strategies, often utilizing small-scale models that exhibit
limited performance. In contrast, Land-MoE addresses these issues by
hierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,
to fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.
Specifically, Land-MoE comprises two key modules: the mixture of low-rank token
experts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages
rank-differentiated tokens to generate diverse feature adjustments for
individual instances within multispectral images. By dynamically combining
learnable low-rank token experts of varying ranks, it enhances the robustness
against spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on
the refined features. This process enables the model to effectively capture
frequency band information that is strongly correlated with semantic essence,
while simultaneously suppressing frequency noise irrelevant to the task.
Comprehensive experiments on MLCC tasks involving cross-sensor and
cross-geospatial setups demonstrate that Land-MoE outperforms existing methods
by a large margin. Additionally, the proposed approach has also achieved
state-of-the-art performance in domain generalization semantic segmentation
tasks of RGB remote sensing images.

</details>


### [131] [Unlocking the Power of SAM 2 for Few-Shot Segmentation](https://arxiv.org/abs/2505.14100)
*Qianxiong Xu,Lanyun Zhu,Xuanyi Liu,Guosheng Lin,Cheng Long,Ziyue Li,Rui Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种改进Few-Shot Segmentation（FSS）的方法，通过设计Pseudo Prompt Generator和Iterative Memory Refinement来解决SAM 2在FSS中的不兼容性问题，并提升分割准确性。


<details>
  <summary>Details</summary>
Motivation: Few-Shot Segmentation（FSS）容易过拟合，且现有方法（如SAM 2）的视频数据与FSS的任务需求不兼容，导致匹配步骤失效。

Method: 设计了Pseudo Prompt Generator生成伪查询记忆，并通过Iterative Memory Refinement优化记忆，同时使用Support-Calibrated Memory Attention抑制背景特征。

Result: 在PASCAL-5$^i$和COCO-20$^i$上的实验表明，1-shot mIoU比最佳基线提高了4.2%。

Conclusion: 提出的方法有效解决了FSS中的不兼容性问题，显著提升了分割性能。

Abstract: Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few
classes to segment arbitrary classes, but at the risk of overfitting. To
address this, some methods use the well-learned knowledge of foundation models
(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM
by supporting video segmentation, whose class-agnostic matching ability is
useful to FSS. A simple idea is to encode support foreground (FG) features as
memory, with which query FG features are matched and fused. Unfortunately, the
FG objects in different frames of SAM 2's video data are always the same
identity, while those in FSS are different identities, i.e., the matching step
is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo
query memory, matching with query features in a compatible way. However, the
memories can never be as accurate as the real ones, i.e., they are likely to
contain incomplete query FG, and some unexpected query background (BG)
features, leading to wrong segmentation. Hence, we further design Iterative
Memory Refinement to fuse more query FG features into the memory, and devise a
Support-Calibrated Memory Attention to suppress the unexpected query BG
features in memory. Extensive experiments have been conducted on PASCAL-5$^i$
and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot
mIoU can be 4.2\% better than the best baseline.

</details>


### [132] [Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry](https://arxiv.org/abs/2505.14105)
*Zsófia Molnár,Gergely Szabó,András Horváth*

Main category: cs.CV

TL;DR: 研究探讨了预训练模型在生物医学图像分割中的偏差问题，并提出缓解策略，通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在生物医学图像等专业数据集上可能引入偏差，影响模型性能和结果可靠性。

Method: 比较预训练和随机初始化模型的性能及显著性图分布，提出中和预训练权重偏差的方法。

Result: 提出的方法有效缓解了偏差，提升了模型可解释性，同时保留了预训练模型的优势。

Conclusion: 研究为解决预训练权重偏差提供了实用方法，适用于多种深度学习任务。

Abstract: Supervised pretrained models have become widely used in deep learning,
especially for image segmentation tasks. However, when applied to specialized
datasets such as biomedical imaging, pretrained weights often introduce
unintended biases. These biases cause models to assign different levels of
importance to different slices, leading to inconsistencies in feature
utilization, which can be observed as asymmetries in saliency map
distributions. This transfer of color distributions from natural images to
non-natural datasets can compromise model performance and reduce the
reliability of results. In this study, we investigate the effects of these
biases and propose strategies to mitigate them. Through a series of
experiments, we test both pretrained and randomly initialized models, comparing
their performance and saliency map distributions. Our proposed methods, which
aim to neutralize the bias introduced by pretrained color channel weights,
demonstrate promising results, offering a practical approach to improving model
explainability while maintaining the benefits of pretrained models. This
publication presents our findings, providing insights into addressing
pretrained weight biases across various deep learning tasks.

</details>


### [133] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/abs/2505.14113)
*Bruno Viti,Elias Karabelas,Martin Holler*

Main category: cs.CV

TL;DR: 论文提出了一种名为CONSIGN的方法，通过结合空间相关性改进图像分割中的不确定性量化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像分割模型生成的置信分数缺乏严格的统计有效性，且忽略了像素间的空间相关性，导致不确定性估计保守且难以解释。

Method: 提出CONSIGN方法，利用空间分组分解将空间相关性纳入共形预测框架，生成具有统计保证的预测集。

Result: 在三个医学影像数据集和两个COCO数据集子集上验证，CONSIGN显著优于标准像素级共形预测方法。

Conclusion: CONSIGN通过结合空间结构提升了不确定性估计的质量和性能，适用于多种预训练分割模型。

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [134] [Intra-class Patch Swap for Self-Distillation](https://arxiv.org/abs/2505.14124)
*Hongjun Choi,Eun Som Jeon,Ankita Shukla,Pavan Turaga*

Main category: cs.CV

TL;DR: 提出了一种基于教师无关蒸馏的新框架，通过类内补丁交换增强实现高效知识蒸馏，无需额外组件或架构修改。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏依赖预训练教师网络，带来内存、存储和训练成本问题；现有教师无关方法仍依赖复杂架构或训练流程。

Method: 采用类内补丁交换增强，在单一学生网络中模拟师生动态，通过实例间蒸馏对齐预测分布。

Result: 在图像分类、语义分割和目标检测任务中，性能优于现有教师无关和传统教师蒸馏方法。

Conclusion: 教师无关蒸馏的成功可能依赖于增强设计，方法简单高效且模型无关。

Abstract: Knowledge distillation (KD) is a valuable technique for compressing large
deep learning models into smaller, edge-suitable networks. However,
conventional KD frameworks rely on pre-trained high-capacity teacher networks,
which introduce significant challenges such as increased memory/storage
requirements, additional training costs, and ambiguity in selecting an
appropriate teacher for a given student model. Although a teacher-free
distillation (self-distillation) has emerged as a promising alternative, many
existing approaches still rely on architectural modifications or complex
training procedures, which limit their generality and efficiency.
  To address these limitations, we propose a novel framework based on
teacher-free distillation that operates using a single student network without
any auxiliary components, architectural modifications, or additional learnable
parameters. Our approach is built on a simple yet highly effective
augmentation, called intra-class patch swap augmentation. This augmentation
simulates a teacher-student dynamic within a single model by generating pairs
of intra-class samples with varying confidence levels, and then applying
instance-to-instance distillation to align their predictive distributions. Our
method is conceptually simple, model-agnostic, and easy to implement, requiring
only a single augmentation function. Extensive experiments across image
classification, semantic segmentation, and object detection show that our
method consistently outperforms both existing self-distillation baselines and
conventional teacher-based KD approaches. These results suggest that the
success of self-distillation could hinge on the design of the augmentation
itself. Our codes are available at
https://github.com/hchoi71/Intra-class-Patch-Swap.

</details>


### [135] [Hunyuan-Game: Industrial-grade Intelligent Game Creation Model](https://arxiv.org/abs/2505.14135)
*Ruihuang Li,Caijin Zhou,Shoujian Zheng,Jianxiang Lu,Jiabin Huang,Comi Chen,Junshu Tang,Guangzheng Xu,Jiale Tao,Hongmei Wang,Donghao Li,Wenqing Yu,Senbo Wang,Zhimin Li,Yetshuan Shi,Haoyu Yang,Yukun Wang,Wenxun Dai,Jiaqi Li,Linqing Wang,Qixun Wang,Zhiyong Xu,Yingfang Zhang,Jiangfeng Xiong,Weijie Kong,Chao Zhang,Hongxin Zhang,Qiaoling Zheng,Weiting Guo,Xinchi Deng,Yixuan Li,Renjia Wei,Yulin Jian,Duojun Huang,Xuhua Ren,Sihuan Lin,Yifu Sun,Yuan Zhou,Joey Wang,Qin Lin,Jingmiao Yu,Jihong Zhang,Caesar Zhong,Di Wang,Yuhong Liu,Linus,Jie Jiang,Longhuang Wu,Shuai Shao,Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-Game项目利用生成式AI动态生成高质量游戏内容，包括图像和视频，提升设计效率和玩家体验。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型有进展，但高质量游戏内容的综合生成仍具挑战性，需同时满足玩家偏好和设计效率。

Method: 项目分为图像生成和视频生成两部分，分别基于数十亿游戏图像和数百万游戏视频数据集，开发定制化模型。

Result: 模型在游戏场景中表现出高美学水平和领域知识，系统化理解多种游戏和动漫艺术风格。

Conclusion: Hunyuan-Game为智能游戏生产提供了创新解决方案，显著提升内容生成质量和效率。

Abstract: Intelligent game creation represents a transformative advancement in game
development, utilizing generative artificial intelligence to dynamically
generate and enhance game content. Despite notable progress in generative
models, the comprehensive synthesis of high-quality game assets, including both
images and videos, remains a challenging frontier. To create high-fidelity game
content that simultaneously aligns with player preferences and significantly
boosts designer efficiency, we present Hunyuan-Game, an innovative project
designed to revolutionize intelligent game production. Hunyuan-Game encompasses
two primary branches: image generation and video generation. The image
generation component is built upon a vast dataset comprising billions of game
images, leading to the development of a group of customized image generation
models tailored for game scenarios: (1) General Text-to-Image Generation. (2)
Game Visual Effects Generation, involving text-to-effect and reference
image-based game visual effect generation. (3) Transparent Image Generation for
characters, scenes, and game visual effects. (4) Game Character Generation
based on sketches, black-and-white images, and white models. The video
generation component is built upon a comprehensive dataset of millions of game
and anime videos, leading to the development of five core algorithmic models,
each targeting critical pain points in game development and having robust
adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)
360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)
Generative Video Super-Resolution. (5) Interactive Game Video Generation. These
image and video generation models not only exhibit high-level aesthetic
expression but also deeply integrate domain-specific knowledge, establishing a
systematic understanding of diverse game and anime art styles.

</details>


### [136] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Symbolic Graph Ranker (SGR)的方法，结合文本和图结构信息，利用大语言模型(LLMs)提升会话搜索的效果。


<details>
  <summary>Details</summary>
Motivation: 当前会话搜索方法侧重于顺序建模或通用图结构表示，忽略了词级语义和图结构的结合。

Method: 通过符号语法规则将会话图转换为文本，结合自监督学习任务（如链接预测、节点内容生成等），增强LLMs对图结构的理解。

Result: 在AOL和Tiangong-ST数据集上的实验证明了SGR的优越性。

Conclusion: SGR为传统搜索策略与现代LLMs之间架起了一座桥梁，提供了一种新颖有效的方法。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [137] [M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data](https://arxiv.org/abs/2505.14159)
*Junjie Li,Jiawei Wang,Miyu Li,Yu Liu,Yumei Wang,Haitao Xu*

Main category: cs.CV

TL;DR: M3Depth是一种专为火星探测任务设计的深度估计模型，通过小波变换卷积核和一致性损失提升在稀疏纹理环境中的性能，实验结果显示其精度比现有方法提高16%。


<details>
  <summary>Details</summary>
Motivation: 火星地形纹理稀疏且缺乏几何约束，传统深度估计方法性能下降，需要一种适应火星环境的解决方案。

Method: 结合小波变换卷积核捕捉低频特征，引入一致性损失利用表面法线作为几何约束，并通过像素级细化模块迭代优化深度和法线预测。

Result: 在合成火星数据集上，M3Depth的深度估计精度比现有方法提高16%，并在真实火星场景中表现出强适用性。

Conclusion: M3Depth为火星探测任务提供了一种有效的深度估计解决方案，具有实际应用潜力。

Abstract: Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with sparse textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the sparse and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a significant 16% improvement in depth
estimation accuracy compared to other state-of-the-art methods in depth
estimation. Furthermore, the model demonstrates strong applicability in
real-world Martian scenarios, offering a promising solution for future Mars
exploration missions.

</details>


### [138] [LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer](https://arxiv.org/abs/2505.14167)
*Changgu Chen,Xiaoyan Yang,Junwei Shu,Changbo Wang,Yang Li*

Main category: cs.CV

TL;DR: 论文提出了LMP框架，通过利用预训练扩散变换器的生成能力，实现零样本视频生成，解决了现有方法在视频内容细粒度控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前DiT模型在视频生成中缺乏对内容的细粒度控制，尤其是在复杂运动描述和图像到视频生成中的运动控制方面存在挑战。

Method: 提出LMP框架，包括前景-背景分离模块、加权运动转移模块和外观分离模块，以从参考视频中提取运动信息并避免干扰。

Result: 实验表明，LMP在生成质量、提示-视频一致性和控制能力方面达到最先进水平。

Conclusion: LMP框架有效解决了视频生成中的运动控制问题，为细粒度视频生成提供了新思路。

Abstract: In recent years, large-scale pre-trained diffusion transformer models have
made significant progress in video generation. While current DiT models can
produce high-definition, high-frame-rate, and highly diverse videos, there is a
lack of fine-grained control over the video content. Controlling the motion of
subjects in videos using only prompts is challenging, especially when it comes
to describing complex movements. Further, existing methods fail to control the
motion in image-to-video generation, as the subject in the reference image
often differs from the subject in the reference video in terms of initial
position, size, and shape. To address this, we propose the Leveraging Motion
Prior (LMP) framework for zero-shot video generation. Our framework harnesses
the powerful generative capabilities of pre-trained diffusion transformers to
enable motion in the generated videos to reference user-provided motion videos
in both text-to-video and image-to-video generation. To this end, we first
introduce a foreground-background disentangle module to distinguish between
moving subjects and backgrounds in the reference video, preventing interference
in the target video generation. A reweighted motion transfer module is designed
to allow the target video to reference the motion from the reference video. To
avoid interference from the subject in the reference video, we propose an
appearance separation module to suppress the appearance of the reference
subject in the target video. We annotate the DAVIS dataset with detailed
prompts for our experiments and design evaluation metrics to validate the
effectiveness of our method. Extensive experiments demonstrate that our
approach achieves state-of-the-art performance in generation quality,
prompt-video consistency, and control capability. Our homepage is available at
https://vpx-ecnu.github.io/LMP-Website/

</details>


### [139] [Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method](https://arxiv.org/abs/2505.14197)
*Xinshen Zhang,Zhen Ye,Xu Zheng*

Main category: cs.CV

TL;DR: 本文介绍了OmniVQA数据集和基准测试，评估了多模态大语言模型（MLLMs）在全景视觉问答中的表现，发现其存在显著局限性，并提出了一种基于规则强化学习的方法360-R1，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在全景图像理解能力上存在不足，缺乏专门的数据集和基准测试，因此需要填补这一空白并探索改进方法。

Method: 提出OmniVQA数据集和基准测试，并基于Qwen2.5-VL-Instruct设计了一种规则强化学习方法360-R1，通过三种新颖的奖励函数优化模型。

Result: 实验表明360-R1在全景视觉问答中性能提升了6%，显著优于现有方法。

Conclusion: 全景视觉理解需要专门的数据集和模型优化，360-R1为未来研究提供了有效方向。

Abstract: Omnidirectional images (ODIs), with their 360{\deg} field of view, provide
unparalleled spatial awareness for immersive applications like augmented
reality and embodied AI. However, the capability of existing multi-modal large
language models (MLLMs) to comprehend and reason about such panoramic scenes
remains underexplored. This paper addresses this gap by introducing OmniVQA,
the first dataset and conducting the first benchmark for omnidirectional visual
question answering. Our evaluation of state-of-the-art MLLMs reveals
significant limitations in handling omnidirectional visual question answering,
highlighting persistent challenges in object localization, feature extraction,
and hallucination suppression within panoramic contexts. These results
underscore the disconnect between current MLLM capabilities and the demands of
omnidirectional visual understanding, which calls for dedicated architectural
or training innovations tailored to 360{\deg} imagery. Building on the OmniVQA
dataset and benchmark, we further introduce a rule-based reinforcement learning
method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group
relative policy optimization (GRPO) by proposing three novel reward functions:
(1) reasoning process similarity reward, (2) answer semantic accuracy reward,
and (3) structured format compliance reward. Extensive experiments on our
OmniVQA demonstrate the superiority of our proposed method in omnidirectional
space (+6% improvement).

</details>


### [140] [Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment](https://arxiv.org/abs/2505.14204)
*Yang Hu,Runchen Wang,Stephen Chong Zhao,Xuhui Zhan,Do Hun Kim,Mark Wallace,David A. Tovar*

Main category: cs.CV

TL;DR: Perceptual-Initialization (PI) 是一种视觉表示学习的新范式，通过在初始化阶段引入人类感知结构，显著提升了零样本性能，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 挑战传统方法，探索在早期表示学习中嵌入人类感知结构是否能提升视觉-语言对齐系统的泛化能力。

Method: 使用NIGHTS数据集的人类感知三元组嵌入初始化CLIP视觉编码器，并在YFCC15M上进行自监督学习。

Result: 在29个零样本分类和2个检索基准测试中显著提升性能，零样本准确率和检索召回率均有提高。

Conclusion: 人类感知结构在初始化阶段的引入为通用视觉-语言智能提供了更强的基础，无需目标领域适配。

Abstract: We introduce Perceptual-Initialization (PI), a paradigm shift in visual
representation learning that incorporates human perceptual structure during the
initialization phase rather than as a downstream fine-tuning step. By
integrating human-derived triplet embeddings from the NIGHTS dataset to
initialize a CLIP vision encoder, followed by self-supervised learning on
YFCC15M, our approach demonstrates significant zero-shot performance
improvements, without any task-specific fine-tuning, across 29 zero shot
classification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains
emerge after approximately 15 epochs of pretraining. Benefits are observed
across datasets of various scales, with improvements manifesting at different
stages of the pretraining process depending on dataset characteristics. Our
approach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and
retrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,
without requiring any adaptation to target domains. These findings challenge
the conventional wisdom of using human-perceptual data primarily for
fine-tuning and demonstrate that embedding human perceptual structure during
early representation learning yields more capable and vision-language aligned
systems that generalize immediately to unseen tasks. Our work shows that
"beginning with you", starting with human perception, provides a stronger
foundation for general-purpose vision-language intelligence.

</details>


### [141] [Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion](https://arxiv.org/abs/2505.14218)
*Jie Li,Shengwei Tian,Long Yu,Xin Ning*

Main category: cs.CV

TL;DR: 论文提出了一种灵活加权的Chamfer距离（FCD）方法，用于改进点云生成任务中全局分布与局部性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统的Chamfer距离（CD）作为目标函数时，固定权重可能导致全局分布不佳，尽管整体性能看似良好。

Method: 提出FCD，通过动态调整CD的两个组件的权重，优先全局分布。

Result: 实验表明，FCD在CD、EMD、DCD和F-Score等指标及人工评估中表现更优。

Conclusion: FCD有效改善了点云生成的全局分布，同时保持了整体性能。

Abstract: Chamfer Distance (CD) comprises two components that can evaluate the global
distribution and local performance of generated point clouds, making it widely
utilized as a similarity measure between generated and target point clouds in
point cloud completion tasks. Additionally, CD's computational efficiency has
led to its frequent application as an objective function for guiding point
cloud generation. However, using CD directly as an objective function with
fixed equal weights for its two components can often result in seemingly high
overall performance (i.e., low CD score), while failing to achieve a good
global distribution. This is typically reflected in high Earth Mover's Distance
(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human
assessments. To address this issue, we propose a Flexible-Weighted Chamfer
Distance (FCD) to guide point cloud generation. FCD assigns a higher weight to
the global distribution component of CD and incorporates a flexible weighting
strategy to adjust the balance between the two components, aiming to improve
global distribution while maintaining robust overall performance. Experimental
results on two state-of-the-art networks demonstrate that our method achieves
superior results across multiple evaluation metrics, including CD, EMD, DCD,
and F-Score, as well as in human evaluations.

</details>


### [142] [VoQA: Visual-only Question Answering](https://arxiv.org/abs/2505.14227)
*Luyang Jiang,Jianing An,Jie Luo,Wenjun Wu,Lei Huang*

Main category: cs.CV

TL;DR: 提出了一种名为VoQA的新型多模态任务，要求模型仅通过视觉输入回答问题，无需文本输入。为解决现有模型的性能问题，提出了GRT-SFT微调策略，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型（LVLMs）在处理视觉嵌入的文本问题时表现不佳，需要一种新的方法提升模型的纯视觉推理能力。

Method: 提出了GRT-SFT（引导响应触发监督微调）策略，通过结构化微调引导模型逐步推理。

Result: GRT-SFT显著提升了模型在VoQA任务中的表现。

Conclusion: 该研究增强了模型在复杂多模态场景中的视觉理解能力，尤其是在纯视觉输入的情况下。

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [143] [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning](https://arxiv.org/abs/2505.14231)
*Sule Bai,Mingxing Li,Yong Liu,Jing Tang,Haoji Zhang,Lei Sun,Xiangxiang Chu,Yansong Tang*

Main category: cs.CV

TL;DR: UniVG-R1是一个基于推理的多模态大语言模型，通过强化学习和冷启动数据增强推理能力，用于通用视觉定位任务，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉定位方法局限于单图像和简单文本参考，难以处理现实世界中复杂的多图像和隐式指令场景，缺乏跨多模态上下文的高级推理能力。

Method: 构建高质量的Chain-of-Thought数据集进行监督微调，结合基于规则的强化学习优化推理链，并提出难度感知权重调整策略解决训练中的偏差问题。

Result: UniVG-R1在MIG-Bench上性能提升9.1%，在四个图像和视频推理基准测试中零样本性能平均提升23.4%。

Conclusion: UniVG-R1通过增强推理能力和解决训练偏差，显著提升了通用视觉定位任务的性能，并展现出强大的泛化能力。

Abstract: Traditional visual grounding methods primarily focus on single-image
scenarios with simple textual references. However, extending these methods to
real-world scenarios that involve implicit and complex instructions,
particularly in conjunction with multiple images, poses significant challenges,
which is mainly due to the lack of advanced reasoning ability across diverse
multi-modal contexts. In this work, we aim to address the more practical
universal grounding task, and propose UniVG-R1, a reasoning guided multimodal
large language model (MLLM) for universal visual grounding, which enhances
reasoning capabilities through reinforcement learning (RL) combined with
cold-start data. Specifically, we first construct a high-quality
Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning
chains, to guide the model towards correct reasoning paths via supervised
fine-tuning. Subsequently, we perform rule-based reinforcement learning to
encourage the model to identify correct reasoning chains, thereby incentivizing
its reasoning capabilities. In addition, we identify a difficulty bias arising
from the prevalence of easy samples as RL training progresses, and we propose a
difficulty-aware weight adjustment strategy to further strengthen the
performance. Experimental results demonstrate the effectiveness of UniVG-R1,
which achieves state-of-the-art performance on MIG-Bench with a 9.1%
improvement over the previous method. Furthermore, our model exhibits strong
generalizability, achieving an average improvement of 23.4% in zero-shot
performance across four image and video reasoning grounding benchmarks. The
project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.

</details>


### [144] [Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation](https://arxiv.org/abs/2505.14239)
*Bin-Bin Gao,Xiaochen Chen,Zhongyi Huang,Congchong Nie,Jun Liu,Jinxiang Lai,Guannan Jiang,Xi Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种简单但有效的方法，通过解耦分类器头来解决小样本目标检测（FSOD）和实例分割（FSIS）中的偏置分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实例级小样本场景中因缺失标签问题导致偏置分类，本文首次正式提出并分析了这一问题。

Method: 将标准分类器解耦为两个独立的头，分别处理清晰的正样本和由缺失标签引起的噪声负样本。

Result: 在PASCAL VOC和MS-COCO基准测试中，该方法无需额外计算成本和参数，显著优于基线和现有最优方法。

Conclusion: 解耦分类器头是一种简单且有效的解决方案，能够显著提升小样本目标检测和实例分割的性能。

Abstract: This paper focus on few-shot object detection~(FSOD) and instance
segmentation~(FSIS), which requires a model to quickly adapt to novel classes
with a few labeled instances. The existing methods severely suffer from bias
classification because of the missing label issue which naturally exists in an
instance-level few-shot scenario and is first formally proposed by us. Our
analysis suggests that the standard classification head of most FSOD or FSIS
models needs to be decoupled to mitigate the bias classification. Therefore, we
propose an embarrassingly simple but effective method that decouples the
standard classifier into two heads. Then, these two individual heads are
capable of independently addressing clear positive samples and noisy negative
samples which are caused by the missing label. In this way, the model can
effectively learn novel classes while mitigating the effects of noisy negative
samples. Without bells and whistles, our model without any additional
computation cost and parameters consistently outperforms its baseline and
state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for
FSOD and FSIS tasks. The Code is available at
https://csgaobb.github.io/Projects/DCFS.

</details>


### [145] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246)
*Ziyu Liu,Yuhang Zang,Yushan Zou,Zijian Liang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出Visual-ARFT方法，通过强化微调提升大型视觉语言模型的多模态代理能力，并在搜索和编码任务上显著超越基线模型和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 开源社区在多模态代理能力（尤其是图像思维）及其基准测试方面研究不足，需开发更灵活、自适应的推理方法。

Method: 采用Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT)方法，使模型具备实时信息检索和图像处理代码编写能力。

Result: Visual-ARFT在MAT-Coding和MAT-Search任务上分别提升18.6% F1/13.0% EM和10.3% F1/8.7% EM，并在多跳QA任务上表现优异。

Conclusion: Visual-ARFT为构建鲁棒且通用的多模态代理提供了有效路径。

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [146] [Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization](https://arxiv.org/abs/2505.14254)
*Yuanyuan Chang,Yinghua Yao,Tao Qin,Mengmeng Wang,Ivor Tsang,Guang Dai*

Main category: cs.CV

TL;DR: 提出一种通过优化语义嵌入和属性分类器引导文本到图像模型编辑的方法，无需依赖文本提示或微调扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动设计文本提示，耗时且可能引入无关细节，限制了编辑性能。

Method: 利用属性分类器学习数据集级别的精确语义嵌入，优化嵌入以引导模型编辑。

Result: 实验表明，该方法实现了高度解耦和跨数据领域的强泛化能力。

Conclusion: 提出的方法无需文本提示或模型微调，实现了高效、准确的图像编辑。

Abstract: Text-to-image diffusion models have emerged as powerful tools for
high-quality image generation and editing. Many existing approaches rely on
text prompts as editing guidance. However, these methods are constrained by the
need for manual prompt crafting, which can be time-consuming, introduce
irrelevant details, and significantly limit editing performance. In this work,
we propose optimizing semantic embeddings guided by attribute classifiers to
steer text-to-image models toward desired edits, without relying on text
prompts or requiring any training or fine-tuning of the diffusion model. We
utilize classifiers to learn precise semantic embeddings at the dataset level.
The learned embeddings are theoretically justified as the optimal
representation of attribute semantics, enabling disentangled and accurate
edits. Experiments further demonstrate that our method achieves high levels of
disentanglement and strong generalization across different domains of data.

</details>


### [147] [Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.14257)
*Jianfei Zhao,Feng Zhang,Xin Sun,Chong Feng*

Main category: cs.CV

TL;DR: 论文提出了一种方法，通过优化注意力分布与信息流的对齐，显著减少大型视觉语言模型（LVLM）中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 由于解码器单向掩码机制，视觉信息在传播过程中存在注意力分布与实际信息流不匹配的问题，导致模型视觉理解能力下降和幻觉现象。

Method: 识别专注于核心语义表示的注意力头，并通过两阶段优化范式将这些头的优势传播到整个模型，以对齐注意力分布与信息流。

Result: 在五个不同LVLM上评估，方法显著减少了幻觉现象，并揭示了减少幻觉与丰富细节之间的权衡。

Conclusion: 该方法不仅有效减少幻觉，还允许手动调整模型的保守性，满足多样化实际需求。

Abstract: Due to the unidirectional masking mechanism, Decoder-Only models propagate
information from left to right. LVLMs (Large Vision-Language Models) follow the
same architecture, with visual information gradually integrated into semantic
representations during forward propagation. Through systematic analysis, we
observe that over 80\% of the visual information is absorbed into the semantic
representations. However, the model's attention still predominantly focuses on
the visual representations. This misalignment between the attention
distribution and the actual information flow undermines the model's visual
understanding ability and contributes to hallucinations. To address this issue,
we enhance the model's visual understanding by leveraging the core information
embedded in semantic representations. Specifically, we identify attention heads
that focus on core semantic representations based on their attention
distributions. Then, through a two-stage optimization paradigm, we propagate
the advantages of these attention heads across the entire model, aligning the
attention distribution with the actual information flow. We evaluate our method
on three image captioning benchmarks using five different LVLMs, demonstrating
its effectiveness in significantly reducing hallucinations. Further experiments
reveal a trade-off between reduced hallucinations and richer details. Notably,
our method allows for manual adjustment of the model's conservativeness,
enabling flexible control to meet diverse real-world requirements. Code will be
released once accepted.

</details>


### [148] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/abs/2505.14260)
*Luxi Lin,Zhihang Lin,Zhanpeng Zeng,Rongrong Ji*

Main category: cs.CV

TL;DR: MSD通过分离文本和视觉标记并采用两阶段训练策略，显著加速了多模态大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态推测解码方法无法像单模态模型那样实现相同的加速效果，因此需要针对多模态模型重新设计推测解码方法。

Method: MSD将文本和视觉标记分离处理，并采用两阶段训练策略：第一阶段专注于语言建模能力，第二阶段逐步引入多模态数据以增强视觉感知能力。

Result: 实验表明，MSD在LLaVA-1.5-7B和LLaVA-1.5-13B上分别实现了最高2.29倍和2.46倍的推理加速。

Conclusion: MSD通过针对多模态特性的优化设计，显著提升了推理效率，同时保持了准确性。

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [149] [RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual Data](https://arxiv.org/abs/2505.14270)
*Yoorhim Cho,Hongyeob Kim,Semin Kim,Youjia Zhang,Yunseok Choi,Sungeun Hong*

Main category: cs.CV

TL;DR: RA-Touch是一个检索增强框架，利用视觉数据中的触觉语义提升触觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 触觉数据收集成本高且费力，而视觉数据中的材质线索可以间接指导触觉理解。

Method: 通过重新标注大规模视觉数据集，引入触觉语义，并利用检索对齐的视觉-文本表示来增强触觉输入。

Result: 在TVL基准测试中优于现有方法，展示了检索增强视觉数据对触觉理解的潜力。

Conclusion: RA-Touch证明了视觉数据在触觉感知中的重要性，为低成本触觉理解提供了新思路。

Abstract: Visuo-tactile perception aims to understand an object's tactile properties,
such as texture, softness, and rigidity. However, the field remains
underexplored because collecting tactile data is costly and labor-intensive. We
observe that visually distinct objects can exhibit similar surface textures or
material properties. For example, a leather sofa and a leather jacket have
different appearances but share similar tactile properties. This implies that
tactile understanding can be guided by material cues in visual data, even
without direct tactile supervision. In this paper, we introduce RA-Touch, a
retrieval-augmented framework that improves visuo-tactile perception by
leveraging visual data enriched with tactile semantics. We carefully recaption
a large-scale visual dataset with tactile-focused descriptions, enabling the
model to access tactile semantics typically absent from conventional visual
datasets. A key challenge remains in effectively utilizing these tactile-aware
external descriptions. RA-Touch addresses this by retrieving visual-textual
representations aligned with tactile inputs and integrating them to focus on
relevant textural and material properties. By outperforming prior methods on
the TVL benchmark, our method demonstrates the potential of retrieval-based
visual reuse for tactile understanding. Code is available at
https://aim-skku.github.io/RA-Touch

</details>


### [150] [Towards Generating Realistic Underwater Images](https://arxiv.org/abs/2505.14296)
*Abdul-Kazeem Shamba*

Main category: cs.CV

TL;DR: 论文研究了对比学习和生成对抗网络在从合成图像生成逼真水下图像中的应用，评估了不同模型在VAROS数据集上的表现，发现pix2pix在FID上表现最佳，而自编码器在SSIM上更优。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用对比学习和生成对抗网络从合成图像生成逼真的水下图像，解决水下图像生成中的光照和结构问题。

Method: 使用VAROS数据集，对比了pix2pix、自编码器、CycleGAN和CUT等模型，并引入深度信息优化CUT。

Result: pix2pix在FID上表现最佳，自编码器在SSIM上更优；CycleGAN和CUT在无监督方法中表现良好，深度信息进一步提升了CUT的FID。

Conclusion: 深度信息能提升图像生成的逼真度，但可能牺牲部分结构保真度；不同模型在FID和SSIM上各有优劣，需根据需求选择。

Abstract: This paper explores the use of contrastive learning and generative
adversarial networks for generating realistic underwater images from synthetic
images with uniform lighting. We investigate the performance of image
translation models for generating realistic underwater images using the VAROS
dataset. Two key evaluation metrics, Fr\'echet Inception Distance (FID) and
Structural Similarity Index Measure (SSIM), provide insights into the
trade-offs between perceptual quality and structural preservation. For paired
image translation, pix2pix achieves the best FID scores due to its paired
supervision and PatchGAN discriminator, while the autoencoder model attains the
highest SSIM, suggesting better structural fidelity despite producing blurrier
outputs. Among unpaired methods, CycleGAN achieves a competitive FID score by
leveraging cycle-consistency loss, whereas CUT, which replaces
cycle-consistency with contrastive learning, attains higher SSIM, indicating
improved spatial similarity retention. Notably, incorporating depth information
into CUT results in the lowest overall FID score, demonstrating that depth cues
enhance realism. However, the slight decrease in SSIM suggests that depth-aware
learning may introduce structural variations.

</details>


### [151] [A Review of Vision-Based Assistive Systems for Visually Impaired People: Technologies, Applications, and Future Directions](https://arxiv.org/abs/2505.14298)
*Fulong Yao,Wenju Zhou,Huosheng Hu*

Main category: cs.CV

TL;DR: 本文综述了近年来为视障人士设计的辅助系统的最新进展，重点关注障碍物检测、导航和用户交互的最新技术，并探讨了视觉引导系统的未来趋势。


<details>
  <summary>Details</summary>
Motivation: 视障人士依赖准确及时的环境信息以实现独立生活，近年来基于视觉的辅助技术取得了显著进展。

Method: 对辅助系统的最新进展进行全面综述，特别关注障碍物检测、导航和用户交互的技术。

Result: 总结了当前最先进的辅助技术，并分析了其在实际应用中的效果。

Conclusion: 讨论了视觉引导系统的未来发展方向和潜在趋势。

Abstract: Visually impaired individuals rely heavily on accurate and timely information
about obstacles and their surrounding environments to achieve independent
living. In recent years, significant progress has been made in the development
of assistive technologies, particularly vision-based systems, that enhance
mobility and facilitate interaction with the external world in both indoor and
outdoor settings. This paper presents a comprehensive review of recent advances
in assistive systems designed for the visually impaired, with a focus on
state-of-the-art technologies in obstacle detection, navigation, and user
interaction. In addition, emerging trends and future directions in visual
guidance systems are discussed.

</details>


### [152] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Main category: cs.CV

TL;DR: 论文提出RADAR框架，通过结合LLM内部知识和外部检索知识，提升放射学报告生成的准确性和信息量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多模态LLM生成放射学报告时，忽略了LLM内部已嵌入的知识，导致信息冗余和低效利用。

Method: RADAR框架首先提取LLM内部与专家分类输出一致的知识，再检索外部补充知识，最后整合两者生成报告。

Result: 在MIMIC-CXR、CheXpert-Plus和IU X-ray数据集上，RADAR在语言质量和临床准确性上优于现有LLM。

Conclusion: RADAR通过有效利用内部和外部知识，显著提升了放射学报告生成的效果。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [153] [Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces](https://arxiv.org/abs/2505.14320)
*Maria Cuellar,Hon Kiu,To,Arush Mehrotra*

Main category: cs.CV

TL;DR: 研究探讨了图像质量退化对FRT准确性和公平性的影响，发现女性及黑人群体误差率更高，但FRT仍优于传统法医方法。


<details>
  <summary>Details</summary>
Motivation: 评估FRT在真实执法场景中的表现，关注图像质量退化对准确性和公平性的影响。

Method: 使用StyleGAN3生成合成人脸，模拟五种图像退化情况，通过Deepface和ArcFace进行1:n识别任务评估。

Result: 误报率在基线质量附近最高，误报率随退化加剧而上升，黑人和女性群体误差率更高。

Conclusion: FRT需透明监管以确保公平性和有效性，尽管其准确性优于传统方法。

Abstract: Facial recognition technology (FRT) is increasingly used in criminal
investigations, yet most evaluations of its accuracy rely on high-quality
images, unlike those often encountered by law enforcement. This study examines
how five common forms of image degradation--contrast, brightness, motion blur,
pose shift, and resolution--affect FRT accuracy and fairness across demographic
groups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,
we simulate degraded images and evaluate performance using Deepface with
ArcFace loss in 1:n identification tasks. We perform an experiment and find
that false positive rates peak near baseline image quality, while false
negatives increase as degradation intensifies--especially with blur and low
resolution. Error rates are consistently higher for women and Black
individuals, with Black females most affected. These disparities raise concerns
about fairness and reliability when FRT is used in real-world investigative
contexts. Nevertheless, even under the most challenging conditions and for the
most affected subgroups, FRT accuracy remains substantially higher than that of
many traditional forensic methods. This suggests that, if appropriately
validated and regulated, FRT should be considered a valuable investigative
tool. However, algorithmic accuracy alone is not sufficient: we must also
evaluate how FRT is used in practice, including user-driven data manipulation.
Such cases underscore the need for transparency and oversight in FRT deployment
to ensure both fairness and forensic validity.

</details>


### [154] [Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?](https://arxiv.org/abs/2505.14321)
*Bo Feng,Zhengfeng Lai,Shiyu Li,Zizhen Wang,Simon Wang,Ping Huang,Meng Cao*

Main category: cs.CV

TL;DR: VBenchComp提出了一种自动化流程，将视频理解问题分类为LLM-Answerable、Semantic、Temporal和Others，以更精细地评估视频LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准常混淆基于知识和纯图像的问题，未能清晰区分模型的时序推理能力，而这是视频理解的关键。

Method: 提出VBenchComp，通过自动化流程将问题分类为LLM-Answerable、Semantic、Temporal和Others，以评估不同能力。

Result: 分析揭示了传统评分掩盖的模型弱点，并提供了设计更准确评估视频LLM基准的见解。

Conclusion: VBenchComp为未来视频理解基准的设计提供了更精细的评估方法和建议。

Abstract: Existing video understanding benchmarks often conflate knowledge-based and
purely image-based questions, rather than clearly isolating a model's temporal
reasoning ability, which is the key aspect that distinguishes video
understanding from other modalities. We identify two major limitations that
obscure whether higher scores truly indicate stronger understanding of the
dynamic content in videos: (1) strong language priors, where models can answer
questions without watching the video; and (2) shuffling invariance, where
models maintain similar performance on certain questions even when video frames
are temporally shuffled. To alleviate these issues, we propose VBenchComp, an
automated pipeline that categorizes questions into different domains:
LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions
can be answered without viewing the video; Semantic questions remain answerable
even when the video frames are shuffled; and Temporal questions require
understanding the correct temporal order of frames. The rest of the questions
are labeled as Others. This can enable fine-grained evaluation of different
capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that
are hidden by traditional overall scores, and we offer insights and
recommendations for designing future benchmarks that more accurately assess
video LLMs.

</details>


### [155] [Handloom Design Generation Using Generative Networks](https://arxiv.org/abs/2505.14330)
*Rajat Kanti Bhattacharjee,Meghali Nandi,Amrit Jha,Gunajit Kalita,Ferdous Ahmed Barbhuiya*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的服装设计生成方法，专注于手织面料，并探讨了相关挑战与应用。


<details>
  <summary>Details</summary>
Motivation: 生成神经网络模型在理解艺术设计和合成方面的能力尚未充分探索。

Method: 结合当前最先进的生成模型和风格迁移算法，采用多种方法研究其性能。

Result: 通过用户评分评估结果，并提供了新的数据集NeuralLoom。

Conclusion: 该研究为设计生成任务提供了新的方法和数据集。

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [156] [Domain Adaptation for Multi-label Image Classification: a Discriminator-free Approach](https://arxiv.org/abs/2505.14333)
*Inder Pal Singh,Enjie Ghorbel,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a discriminator-free adversarial-based approach termed
DDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label
Image Classification (MLIC). While recent efforts have explored
adversarial-based UDA methods for MLIC, they typically include an additional
discriminator subnet. Nevertheless, decoupling the classification and the
discrimination tasks may harm their task-specific discriminative power. Herein,
we address this challenge by presenting a novel adversarial critic directly
derived from the task-specific classifier. Specifically, we employ a
two-component Gaussian Mixture Model (GMM) to model both source and target
predictions, distinguishing between two distinct clusters. Instead of using the
traditional Expectation Maximization (EM) algorithm, our approach utilizes a
Deep Neural Network (DNN) to estimate the parameters of each GMM component.
Subsequently, the source and target GMM parameters are leveraged to formulate
an adversarial loss using the Fr\'echet distance. The proposed framework is
therefore not only fully differentiable but is also cost-effective as it avoids
the expensive iterative process usually induced by the standard EM method. The
proposed method is evaluated on several multi-label image datasets covering
three different types of domain shift. The obtained results demonstrate that
DDA-MLIC outperforms existing state-of-the-art methods in terms of precision
while requiring a lower number of parameters. The code is made publicly
available at github.com/cvi2snt/DDA-MLIC.

</details>


### [157] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/abs/2505.14340)
*Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim*

Main category: cs.CV

TL;DR: 本文综述了平面几何问题求解（PGPS）的研究现状，总结了编码器-解码器框架及其输出格式，分析了架构设计，并指出了未来研究的挑战与方向。


<details>
  <summary>Details</summary>
Motivation: PGPS作为评估多模态推理能力的基准受到关注，但缺乏系统性综述。本文旨在填补这一空白。

Method: 将PGPS方法归类为编码器-解码器框架，总结其输出格式，并分析架构设计。

Result: 提出了PGPS研究的分类与分析，并指出编码阶段的幻觉问题和数据泄露问题。

Conclusion: 未来研究需解决幻觉和数据泄露问题，并探索更高效的PGPS方法。

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [158] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/abs/2505.14341)
*Sifan Li,Ming Tao,Hao Zhao,Ling Shao,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种通过显式逻辑叙事提示（ELNP）和潜在空间逐步替换的方法，提升反事实文本到图像（T2I）生成中的概念对齐能力。


<details>
  <summary>Details</summary>
Motivation: 反事实T2I生成在现实世界中难以实现，但需要增强其真实感和概念对齐能力，以提供更丰富的AIGC体验。

Method: 利用可控T2I模型在潜在空间中逐步替换对象，并通过DeepSeek生成ELNP指导替换过程。

Result: 实验表明，该方法显著提升了反事实T2I生成中的概念对齐能力。

Conclusion: 提出的策略有效解决了反事实T2I中的概念对齐问题，为更灵活的AIGC应用提供了可能。

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [159] [Egocentric Action-aware Inertial Localization in Point Clouds](https://arxiv.org/abs/2505.14346)
*Mingfang Zhang,Ryo Yonetani,Yifei Huang,Liangyang Ouyang,Ruicong Liu,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出了一种名为EAIL的新型惯性定位框架，利用头戴式IMU信号中的自我中心动作线索在3D点云中定位目标个体。


<details>
  <summary>Details</summary>
Motivation: 解决IMU传感器噪声和人类动作多样性导致的定位漂移问题，利用动作与环境结构的关联性作为空间锚点。

Method: 通过分层多模态对齐学习IMU信号中的动作线索与点云中环境特征的关联，利用对比学习训练模态编码器。

Result: 实验证明EAIL在惯性定位和动作识别任务上优于现有方法。

Conclusion: EAIL通过动作与环境特征的关联有效解决了定位漂移问题，同时还能识别动作序列。

Abstract: This paper presents a novel inertial localization framework named Egocentric
Action-aware Inertial Localization (EAIL), which leverages egocentric action
cues from head-mounted IMU signals to localize the target individual within a
3D point cloud. Human inertial localization is challenging due to IMU sensor
noise that causes trajectory drift over time. The diversity of human actions
further complicates IMU signal processing by introducing various motion
patterns. Nevertheless, we observe that some actions observed through the
head-mounted IMU correlate with spatial environmental structures (e.g., bending
down to look inside an oven, washing dishes next to a sink), thereby serving as
spatial anchors to compensate for the localization drift. The proposed EAIL
framework learns such correlations via hierarchical multi-modal alignment. By
assuming that the 3D point cloud of the environment is available, it
contrastively learns modality encoders that align short-term egocentric action
cues in IMU signals with local environmental features in the point cloud. These
encoders are then used in reasoning the IMU data and the point cloud over time
and space to perform inertial localization. Interestingly, these encoders can
further be utilized to recognize the corresponding sequence of actions as a
by-product. Extensive experiments demonstrate the effectiveness of the proposed
framework over state-of-the-art inertial localization and inertial action
recognition baselines.

</details>


### [160] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/abs/2505.14357)
*Siqiao Huang,Jialong Wu,Qixing Zhou,Shangchen Miao,Mingsheng Long*

Main category: cs.CV

TL;DR: Vid2World利用预训练视频扩散模型构建交互式世界模型，通过因果化和动作引导机制提升预测质量。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型需要大量领域特定训练且预测质量低，而视频扩散模型能生成高质量视频，但缺乏交互性。

Method: Vid2World通过因果化预训练视频扩散模型架构和目标，并引入因果动作引导机制。

Result: 在机器人操作和游戏仿真领域实验中表现优异。

Conclusion: Vid2World为将视频扩散模型转化为交互式世界模型提供了可扩展且有效的方法。

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [161] [Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable](https://arxiv.org/abs/2505.14359)
*Ruoxin Chen,Junwei Xi,Zhiyuan Yan,Ke-Yue Zhang,Shuang Wu,Jingyi Xie,Xu Chen,Lei Xu,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: 论文提出Dual Data Alignment (DDA)方法，通过同时对齐像素和频率域来解决现有检测器在偏置数据集上的过拟合问题，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有检测器在偏置数据集上训练，容易过拟合非因果图像属性，导致在无偏数据集上性能下降。

Method: 提出DDA方法，同时对齐像素和频率域，并引入DDA-COCO和EvalGEN两个新测试集。

Result: 实验表明，仅用DDA对齐的MSCOCO训练的检测器在8个基准测试中性能显著提升，野外基准测试提升7.2%。

Conclusion: DDA方法有效提升了检测器的泛化能力，解决了频率域未对齐导致的偏置问题。

Abstract: Existing detectors are often trained on biased datasets, leading to the
possibility of overfitting on non-causal image attributes that are spuriously
correlated with real/synthetic labels. While these biased features enhance
performance on the training data, they result in substantial performance
degradation when applied to unbiased datasets. One common solution is to
perform dataset alignment through generative reconstruction, matching the
semantic content between real and synthetic images. However, we revisit this
approach and show that pixel-level alignment alone is insufficient. The
reconstructed images still suffer from frequency-level misalignment, which can
perpetuate spurious correlations. To illustrate, we observe that reconstruction
models tend to restore the high-frequency details lost in real images (possibly
due to JPEG compression), inadvertently creating a frequency-level
misalignment, where synthetic images appear to have richer high-frequency
content than real ones. This misalignment leads to models associating
high-frequency features with synthetic labels, further reinforcing biased cues.
To resolve this, we propose Dual Data Alignment (DDA), which aligns both the
pixel and frequency domains. Moreover, we introduce two new test sets:
DDA-COCO, containing DDA-aligned synthetic images for testing detector
performance on the most aligned dataset, and EvalGEN, featuring the latest
generative models for assessing detectors under new generative architectures
such as visual auto-regressive generators. Finally, our extensive evaluations
demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could
improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on
in-the-wild benchmarks, highlighting the improved generalizability of unbiased
detectors.

</details>


### [162] [Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives](https://arxiv.org/abs/2505.14361)
*Xingxing Weng,Chao Pang,Gui-Song Xia*

Main category: cs.CV

TL;DR: 本文综述了遥感领域中视觉语言建模（VLM）的两阶段范式（预训练与微调），涵盖对比学习、视觉指令调优和文本条件图像生成等分类，并总结了相关数据集及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为遥感社区提供关于VLM发展的全面综述，促进跨模态信息融合与任务适应性研究。

Method: 通过分类综述VLM在遥感中的应用，包括对比学习、视觉指令调优和生成模型，并分析数据集构建与模型能力。

Result: VLM模型在遥感任务中表现优异，支持对话式交互，但仍需解决跨模态对齐、模糊需求理解等挑战。

Conclusion: 未来研究应关注跨模态对齐、模型可扩展性及多模态数据集，以进一步提升VLM在遥感中的实用性。

Abstract: Vision-language modeling (VLM) aims to bridge the information gap between
images and natural language. Under the new paradigm of first pre-training on
massive image-text pairs and then fine-tuning on task-specific data, VLM in the
remote sensing domain has made significant progress. The resulting models
benefit from the absorption of extensive general knowledge and demonstrate
strong performance across a variety of remote sensing data analysis tasks.
Moreover, they are capable of interacting with users in a conversational
manner. In this paper, we aim to provide the remote sensing community with a
timely and comprehensive review of the developments in VLM using the two-stage
paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing:
contrastive learning, visual instruction tuning, and text-conditioned image
generation. For each category, we detail the commonly used network architecture
and pre-training objectives. Second, we conduct a thorough review of existing
works, examining foundation models and task-specific adaptation methods in
contrastive-based VLM, architectural upgrades, training strategies and model
capabilities in instruction-based VLM, as well as generative foundation models
with their representative downstream applications. Third, we summarize datasets
used for VLM pre-training, fine-tuning, and evaluation, with an analysis of
their construction methodologies (including image sources and caption
generation) and key properties, such as scale and task adaptability. Finally,
we conclude this survey with insights and discussions on future research
directions: cross-modal representation alignment, vague requirement
comprehension, explanation-driven model reliability, continually scalable model
capabilities, and large-scale datasets featuring richer modalities and greater
challenges.

</details>


### [163] [DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning](https://arxiv.org/abs/2505.14362)
*Ziwei Zheng,Michael Yang,Jack Hong,Chenxiao Zhao,Guohai Xu,Le Yang,Chao Shen,Xing Yu*

Main category: cs.CV

TL;DR: DeepEyes是一种通过端到端强化学习实现的多模态推理模型，能够‘用图像思考’，无需冷启动监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（VLMs）主要依赖文本推理，缺乏视觉与文本的无缝整合。本文旨在探索如何将高级视觉输入处理融入推理机制。

Method: 提出了一种基于工具使用的数据选择机制和奖励策略，以促进工具辅助的推理轨迹。模型通过强化学习自然发展出‘用图像思考’的能力。

Result: DeepEyes在细粒度感知和推理任务上表现显著提升，同时在基础任务（如基础、幻觉和数学推理）上也有改进。

Conclusion: DeepEyes展示了从探索到高效利用的工具调用行为演化，其推理模式与人类视觉推理过程高度相似。

Abstract: Large Vision-Language Models (VLMs) have shown strong capabilities in
multimodal understanding and reasoning, yet they are primarily constrained by
text-based reasoning processes. However, achieving seamless integration of
visual and textual reasoning which mirrors human cognitive processes remains a
significant challenge. In particular, effectively incorporating advanced visual
input processing into reasoning mechanisms is still an open question. Thus, in
this paper, we explore the interleaved multimodal reasoning paradigm and
introduce DeepEyes, a model with "thinking with images" capabilities
incentivized through end-to-end reinforcement learning without the need for
cold-start SFT. Notably, this ability emerges natively within the model itself,
leveraging its inherent grounding ability as a tool instead of depending on
separate specialized models. Specifically, we propose a tool-use-oriented data
selection mechanism and a reward strategy to encourage successful tool-assisted
reasoning trajectories. DeepEyes achieves significant performance gains on
fine-grained perception and reasoning benchmarks and also demonstrates
improvement in grounding, hallucination, and mathematical reasoning tasks.
Interestingly, we observe the distinct evolution of tool-calling behavior from
initial exploration to efficient and accurate exploitation, and diverse
thinking patterns that closely mirror human visual reasoning processes. Code is
available at https://github.com/Visual-Agent/DeepEyes.

</details>


### [164] [ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations](https://arxiv.org/abs/2505.14404)
*Xuecheng Wu,Jiaxing Liu,Danlei Huang,Xiaoyu Li,Yifan Wang,Chen Chen,Liya Ma,Xuezhi Cao,Junxiao Xue*

Main category: cs.CV

TL;DR: VI-CoT通过逐步更新视觉状态（IVS）提升MLLMs的任务表现，但现有基准测试固定IVS，限制了评估。ViC-Bench引入自由式IVS和系统评估方法，填补了这一空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试固定IVS，无法评估模型内在推理能力，且未系统探索IVS对推理性能的影响。

Method: 提出ViC-Bench基准，包含四项任务，支持自由式IVS生成，并设计三阶段评估策略和新指标。

Result: 评估了18种先进MLLMs，揭示了其VI-CoT能力的关键见解。

Conclusion: ViC-Bench填补了自由式IVS评估的空白，为MLLMs的推理能力提供了系统分析工具。

Abstract: Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually
update their understanding and decisions based on step-wise intermediate visual
states (IVS), much like a human would, which demonstrates impressive success in
various tasks, thereby leading to emerged advancements in related benchmarks.
Despite promising progress, current benchmarks provide models with relatively
fixed IVS, rather than free-style IVS, whch might forcibly distort the original
thinking trajectories, failing to evaluate their intrinsic reasoning
capabilities. More importantly, existing benchmarks neglect to systematically
explore the impact factors that IVS would impart to untamed reasoning
performance. To tackle above gaps, we introduce a specialized benchmark termed
ViC-Bench, consisting of four representive tasks: maze navigation, jigsaw
puzzle, embodied long-horizon planning, and complex counting, where each task
has dedicated free-style IVS generation pipeline supporting function calls. To
systematically examine VI-CoT capability, we propose a thorough evaluation
suite incorporating a progressive three-stage strategy with targeted new
metrics. Besides, we establish Incremental Prompting Information Injection
(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We
extensively conduct evaluations for 18 advanced MLLMs, revealing key insights
into their VI-CoT capability. Our proposed benchmark is publicly open at
Huggingface.

</details>


### [165] [Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency](https://arxiv.org/abs/2505.14405)
*Jiafeng Liang,Shixin Jiang,Xuan Dong,Ning Wang,Zheng Chu,Hui Su,Jinlan Fu,Ming Liu,See-Kiong Ng,Bing Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的时间鲁棒性基准（TemRobBench），评估大型多模态模型（LMMs）在时间分析中的鲁棒性，并设计了一种全景直接偏好优化（PanoDPO）方法以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在时间分析中的鲁棒性尚未充分研究，尤其是在对抗环境中过度依赖先验知识和文本上下文，而忽略视频的实际时间动态。

Method: 提出TemRobBench基准，引入视觉和文本模态的时间不一致扰动；设计PanoDPO方法，鼓励模型同时结合视觉和语言特征偏好。

Result: 评估16种主流LMMs，发现其在对抗环境中表现不佳；PanoDPO能显著提升模型在时间分析中的鲁棒性和可靠性。

Conclusion: PanoDPO方法有效解决了LMMs在时间分析中的鲁棒性问题，为未来研究提供了新方向。

Abstract: Large Multimodal Models (LMMs) have recently demonstrated impressive
performance on general video comprehension benchmarks. Nevertheless, for
broader applications, the robustness of their temporal analysis capability
needs to be thoroughly investigated yet predominantly ignored. Motivated by
this, we propose a novel temporal robustness benchmark (TemRobBench), which
introduces temporal inconsistency perturbations separately at the visual and
textual modalities to assess the robustness of models. We evaluate 16
mainstream LMMs and find that they exhibit over-reliance on prior knowledge and
textual context in adversarial environments, while ignoring the actual temporal
dynamics in the video. To mitigate this issue, we design panoramic direct
preference optimization (PanoDPO), which encourages LMMs to incorporate both
visual and linguistic feature preferences simultaneously. Experimental results
show that PanoDPO can effectively enhance the model's robustness and
reliability in temporal analysis.

</details>


### [166] [Diving into the Fusion of Monocular Priors for Generalized Stereo Matching](https://arxiv.org/abs/2505.14414)
*Chengtang Yao,Lidong Yu,Zhidan Liu,Jiaxi Zeng,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: 论文提出了一种利用视觉基础模型（VFM）的单目先验来解决立体匹配中难以处理区域（如遮挡和非朗伯表面）的方法，通过二进制局部排序图和像素级线性回归模块优化融合过程。


<details>
  <summary>Details</summary>
Motivation: 立体匹配在处理遮挡和非朗伯表面等区域时表现不佳，而现有的单目先验因数据集的限制泛化能力有限。

Method: 提出二进制局部排序图统一相对和绝对深度表示，并通过像素级线性回归模块全局自适应对齐单目深度与视差。

Result: 在从SceneFlow到Middlebury和Booster数据集的泛化实验中，性能显著提升，且效率几乎未降低。

Conclusion: 该方法有效且高效地利用了单目先验，显著提升了立体匹配的性能。

Abstract: The matching formulation makes it naturally hard for the stereo matching to
handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing
monocular priors has been proven helpful for ill-posed matching, but the biased
monocular prior learned from small stereo datasets constrains the
generalization. Recently, stereo matching has progressed by leveraging the
unbiased monocular prior from the vision foundation model (VFM) to improve the
generalization in ill-posed regions. We dive into the fusion process and
observe three main problems limiting the fusion of the VFM monocular prior. The
first problem is the misalignment between affine-invariant relative monocular
depth and absolute depth of disparity. Besides, when we use the monocular
feature in an iterative update structure, the over-confidence in the disparity
update leads to local optima results. A direct fusion of a monocular depth map
could alleviate the local optima problem, but noisy disparity results computed
at the first several iterations will misguide the fusion. In this paper, we
propose a binary local ordering map to guide the fusion, which converts the
depth map into a binary relative format, unifying the relative and absolute
depth representation. The computed local ordering map is also used to re-weight
the initial disparity update, resolving the local optima and noisy problem. In
addition, we formulate the final direct fusion of monocular depth to the
disparity as a registration problem, where a pixel-wise linear regression
module can globally and adaptively align them. Our method fully exploits the
monocular prior to support stereo matching results effectively and efficiently.
We significantly improve the performance from the experiments when generalizing
from SceneFlow to Middlebury and Booster datasets while barely reducing the
efficiency.

</details>


### [167] [Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/abs/2505.14454)
*Xuyang Liu,Yiyu Wang,Junpeng Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为VidCom2的视频压缩框架，用于解决VideoLLM中视觉令牌的二次复杂度问题，通过自适应调整压缩强度，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: VideoLLM在视频理解中表现出色，但由于视觉令牌的二次复杂度导致效率低下，且现有压缩方法存在信息丢失和架构兼容性问题。

Method: 提出了VidCom2框架，通过量化每帧的独特性自适应调整压缩强度，保留关键信息并减少冗余。

Result: 实验表明，VidCom2仅用25%的视觉令牌即可达到原始性能的99.6%，并减少70.8%的生成延迟。

Conclusion: VidCom2不仅高效且兼容其他压缩方法，为VideoLLM的推理加速提供了有效解决方案。

Abstract: Video large language models (VideoLLM) excel at video understanding, but face
efficiency challenges due to the quadratic complexity of abundant visual
tokens. Our systematic analysis of token compression methods for VideoLLMs
reveals two critical issues: (i) overlooking distinctive visual signals across
frames, leading to information loss; (ii) suffering from implementation
constraints, causing incompatibility with modern architectures or efficient
operators. To address these challenges, we distill three design principles for
VideoLLM token compression and propose a plug-and-play inference acceleration
framework "Video Compression Commander" (VidCom2). By quantifying each frame's
uniqueness, VidCom2 adaptively adjusts compression intensity across frames,
effectively preserving essential information while reducing redundancy in video
sequences. Extensive experiments across various VideoLLMs and benchmarks
demonstrate the superior performance and efficiency of our VidCom2. With only
25% visual tokens, VidCom2 achieves 99.6% of the original performance on
LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame
Compression Adjustment strategy is compatible with other token compression
methods to further improve their performance. Our code is available at
https://github.com/xuyang-liu16/VidCom2.

</details>


### [168] [VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank](https://arxiv.org/abs/2505.14460)
*Tianhe Wu,Jian Zou,Jie Liang,Lei Zhang,Kede Ma*

Main category: cs.CV

TL;DR: VisualQuality-R1是一种基于强化学习的无参考图像质量评估模型，通过推理和相对排名优化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索推理驱动的计算模型在图像质量评估（IQA）中的潜力，尤其是依赖视觉推理的任务。

Method: 使用强化学习（特别是组相对策略优化）和Thurstone模型，生成图像对的相对质量分数，并通过连续保真度度量定义奖励。

Result: VisualQuality-R1在实验中优于基于判别深度学习的NR-IQA模型和最近的推理驱动回归方法，并能生成与人类对齐的质量描述。

Conclusion: VisualQuality-R1适用于广泛的图像处理任务，支持多数据集训练，无需感知尺度重新对齐。

Abstract: DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing
reasoning and generalization capabilities of large language models (LLMs)
through reinforcement learning. Nevertheless, the potential of
reasoning-induced computational modeling has not been thoroughly explored in
the context of image quality assessment (IQA), a task critically dependent on
visual reasoning. In this paper, we introduce VisualQuality-R1, a
reasoning-induced no-reference IQA (NR-IQA) model, and we train it with
reinforcement learning to rank, a learning algorithm tailored to the
intrinsically relative nature of visual quality. Specifically, for a pair of
images, we employ group relative policy optimization to generate multiple
quality scores for each image. These estimates are then used to compute
comparative probabilities of one image having higher quality than the other
under the Thurstone model. Rewards for each quality estimate are defined using
continuous fidelity measures rather than discretized binary labels. Extensive
experiments show that the proposed VisualQuality-R1 consistently outperforms
discriminative deep learning-based NR-IQA models as well as a recent
reasoning-induced quality regression method. Moreover, VisualQuality-R1 is
capable of generating contextually rich, human-aligned quality descriptions,
and supports multi-dataset training without requiring perceptual scale
realignment. These features make VisualQuality-R1 especially well-suited for
reliably measuring progress in a wide range of image processing tasks like
super-resolution and image generation.

</details>


### [169] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Main category: cs.CV

TL;DR: RAVENEA是一个新的基准测试，通过检索增强方法提升视觉文化理解，在文化视觉问答和图像描述任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在文化理解上表现不足，需要一种方法增强其文化敏感性。

Method: 引入RAVENEA基准，结合检索增强生成（RAG）方法，扩展数据集并训练多模态检索器。

Result: 轻量级视觉语言模型结合文化感知检索后，性能显著提升（cVQA提升3.2%，cIC提升6.2%）。

Conclusion: 检索增强方法和文化包容性基准对多模态理解具有重要价值。

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [170] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/abs/2505.14476)
*Farshad Sangari Abiz,Reshad Hosseini,Babak N. Araabi*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过确保同类样本在潜在空间中具有一致的活动维度，增强变分自编码器（VAE）潜在空间的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准VAE生成的潜在空间分散且无结构，限制了其可解释性。VSC虽然引入了稀疏潜在表示，但未能确保同类样本的活动维度一致性。

Method: 引入新的损失函数，鼓励同类样本共享相似的活动维度，从而构建更结构化和可解释的潜在空间。

Result: 该方法能够捕获全局和类别特定因素，提升了潜在表示的实用性和可解释性。

Conclusion: 通过强制同类样本的活动维度一致性，显著改善了潜在空间的结构和可解释性。

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [171] [ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains](https://arxiv.org/abs/2505.14511)
*Guillaume Vray,Devavrat Tomar,Xufeng Gao,Jean-Philippe Thiran,Evan Shelhamer,Behzad Bozorgtabar*

Main category: cs.CV

TL;DR: ReservoirTTA是一个新颖的插件框架，用于在测试域持续变化的情况下进行长期测试时适应（TTA），通过维护一个域专用模型库来检测新域并路由样本，克服单模型适应的限制。


<details>
  <summary>Details</summary>
Motivation: 解决测试域持续变化或重复出现时，单模型适应可能导致的灾难性遗忘、域间干扰和错误累积等问题。

Method: 使用在线聚类检测新域，并通过多模型策略路由样本到专用模型，实现域特定适应。

Result: 在ImageNet-C、CIFAR-10/100-C和Cityscapes→ACDC任务中表现优异，显著提升了适应准确性和稳定性。

Conclusion: ReservoirTTA在多域变化场景下表现优于现有方法，提供了鲁棒且稳定的性能。

Abstract: This paper introduces ReservoirTTA, a novel plug-in framework designed for
prolonged test-time adaptation (TTA) in scenarios where the test domain
continuously shifts over time, including cases where domains recur or evolve
gradually. At its core, ReservoirTTA maintains a reservoir of
domain-specialized models -- an adaptive test-time model ensemble -- that both
detects new domains via online clustering over style features of incoming
samples and routes each sample to the appropriate specialized model, and
thereby enables domain-specific adaptation. This multi-model strategy overcomes
key limitations of single model adaptation, such as catastrophic forgetting,
inter-domain interference, and error accumulation, ensuring robust and stable
performance on sustained non-stationary test distributions. Our theoretical
analysis reveals key components that bound parameter variance and prevent model
collapse, while our plug-in TTA module mitigates catastrophic forgetting of
previously encountered domains. Extensive experiments on the classification
corruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the
Cityscapes$\rightarrow$ACDC semantic segmentation task, covering recurring and
continuously evolving domain shifts, demonstrate that ReservoirTTA
significantly improves adaptation accuracy and maintains stable performance
across prolonged, recurring shifts, outperforming state-of-the-art methods.

</details>


### [172] [SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling](https://arxiv.org/abs/2505.14521)
*Zhihao Li,Yufei Wang,Heliang Zheng,Yihao Luo,Bihan Wen*

Main category: cs.CV

TL;DR: SparC框架通过稀疏可变形立方体表示和新型编码器SparConv-VAE，解决了3D对象合成中的细节丢失问题，实现了高保真重建和高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法（VAE压缩+潜在扩散采样）因表示效率低和模态不匹配导致细节丢失严重，3D对象合成仍具挑战性。

Method: 结合稀疏可变形立方体表示SparseCubes与SparConv-VAE编码器，支持高分辨率表面重建和模态一致的潜在扩散生成。

Result: 在复杂几何、开放表面等输入上实现最佳重建保真度，保留细节并降低计算成本。

Conclusion: SparC为高分辨率3D生成提供了高效、可扩展的解决方案。

Abstract: High-fidelity 3D object synthesis remains significantly more challenging than
2D image generation due to the unstructured nature of mesh data and the cubic
complexity of dense volumetric grids. Existing two-stage pipelines-compressing
meshes with a VAE (using either 2D or 3D supervision), followed by latent
diffusion sampling-often suffer from severe detail loss caused by inefficient
representations and modality mismatches introduced in VAE. We introduce SparC,
a unified framework that combines a sparse deformable marching cubes
representation SparseCubes with a novel encoder SparConv-VAE. SparseCubes
converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary
topology by scattering signed distance and deformation fields onto a sparse
cube, allowing differentiable optimization. SparConv-VAE is the first
modality-consistent variational autoencoder built entirely upon sparse
convolutional networks, enabling efficient and near-lossless 3D reconstruction
suitable for high-resolution generative modeling through latent diffusion.
SparC achieves state-of-the-art reconstruction fidelity on challenging inputs,
including open surfaces, disconnected components, and intricate geometry. It
preserves fine-grained shape details, reduces training and inference cost, and
integrates naturally with latent diffusion models for scalable, high-resolution
3D generation.

</details>


### [173] [diffDemorph: Extending Reference-Free Demorphing to Unseen Faces](https://arxiv.org/abs/2505.14527)
*Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的新方法，能够从合成的人脸融合图像中高保真地分离出原始图像，无需参考图像，且适用于多种融合技术和风格。


<details>
  <summary>Details</summary>
Motivation: 现有无参考（RF）解融合方法受限于训练和测试数据的假设，如融合技术、人脸风格等，限制了其实际应用。

Method: 采用扩散模型，从合成人脸图像生成的融合图像中训练，并在真实融合图像上测试，实现了跨技术和风格的泛化。

Result: 在六个数据集和两个人脸匹配器上测试，性能优于当前最佳方法至少59.46%。

Conclusion: 该方法具有高泛化性和实用性，适用于多种融合技术和风格，显著提升了无参考解融合的效果。

Abstract: A face morph is created by combining two (or more) face images corresponding
to two (or more) identities to produce a composite that successfully matches
the constituent identities. Reference-free (RF) demorphing reverses this
process using only the morph image, without the need for additional reference
images. Previous RF demorphing methods were overly constrained, as they rely on
assumptions about the distributions of training and testing morphs such as the
morphing technique used, face style, and images used to create the morph. In
this paper, we introduce a novel diffusion-based approach that effectively
disentangles component images from a composite morph image with high visual
fidelity. Our method is the first to generalize across morph techniques and
face styles, beating the current state of the art by $\geq 59.46\%$ under a
common training protocol across all datasets tested. We train our method on
morphs created using synthetically generated face images and test on real
morphs, thereby enhancing the practicality of the technique. Experiments on six
datasets and two face matchers establish the utility and efficacy of our
method.

</details>


### [174] [Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image](https://arxiv.org/abs/2505.14537)
*Yuxuan Wang,Xuanyu Yi,Qingshan Xu,Yuan Zhou,Long Chen,Hanwang Zhang*

Main category: cs.CV

TL;DR: CP-GS框架通过预训练的图像到3D生成和迭代LoRA微调，解决了单视图参考图像在3D场景个性化中的视角偏差问题，实现了高质量的多视角一致性输出。


<details>
  <summary>Details</summary>
Motivation: 单视图参考图像在3D场景个性化中存在视角偏差问题，现有方法难以扩展参考信息到新视角，导致结果不一致。

Method: CP-GS整合了预训练的图像到3D生成和迭代LoRA微调，通过几何线索引导的视图一致性生成过程，扩展参考外观并生成多视角指导图像和个性化3DGS输出。

Result: 实验表明，CP-GS有效减轻了视角偏差，显著优于现有方法，实现了高质量的个性化输出。

Conclusion: CP-GS通过渐进式扩展单视图参考外观，解决了视角偏差问题，为3D场景个性化提供了高效解决方案。

Abstract: Personalizing 3D scenes from a single reference image enables intuitive
user-guided editing, which requires achieving both multi-view consistency
across perspectives and referential consistency with the input image. However,
these goals are particularly challenging due to the viewpoint bias caused by
the limited perspective provided in a single image. Lacking the mechanisms to
effectively expand reference information beyond the original view, existing
methods of image-conditioned 3DGS personalization often suffer from this
viewpoint bias and struggle to produce consistent results. Therefore, in this
paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),
a framework that progressively propagates the single-view reference appearance
to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D
generation and iterative LoRA fine-tuning to extract and extend the reference
appearance, and finally produces faithful multi-view guidance images and the
personalized 3DGS outputs through a view-consistent generation process guided
by geometric cues. Extensive experiments on real-world scenes show that our
CP-GS effectively mitigates the viewpoint bias, achieving high-quality
personalization that significantly outperforms existing methods. The code will
be released at https://github.com/Yuxuan-W/CP-GS.

</details>


### [175] [Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI](https://arxiv.org/abs/2505.14556)
*Marlène Careil,Yohann Benchetrit,Jean-Rémi King*

Main category: cs.CV

TL;DR: Dynadiff是一种新型单阶段扩散模型，用于从动态fMRI信号中重建图像，简化训练流程并在时间分辨率和语义重建上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前脑到图像解码方法依赖复杂多阶段流程，且通常忽略时间维度，限制了时间分辨解码能力。

Method: 提出Dynadiff，一种单阶段扩散模型，直接处理动态fMRI信号。

Result: 在时间分辨fMRI信号上表现优于现有方法，尤其在语义重建指标上，同时对静态数据保持竞争力。

Conclusion: Dynadiff为时间分辨脑到图像解码奠定了基础。

Abstract: Brain-to-image decoding has been recently propelled by the progress in
generative AI models and the availability of large ultra-high field functional
Magnetic Resonance Imaging (fMRI). However, current approaches depend on
complicated multi-stage pipelines and preprocessing steps that typically
collapse the temporal dimension of brain recordings, thereby limiting
time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural
Activity Diffusion for Image Reconstruction), a new single-stage diffusion
model designed for reconstructing images from dynamically evolving fMRI
recordings. Our approach offers three main contributions. First, Dynadiff
simplifies training as compared to existing approaches. Second, our model
outperforms state-of-the-art models on time-resolved fMRI signals, especially
on high-level semantic image reconstruction metrics, while remaining
competitive on preprocessed fMRI data that collapse time. Third, this approach
allows a precise characterization of the evolution of image representations in
brain activity. Overall, this work lays the foundation for time-resolved
brain-to-image decoding.

</details>


### [176] [Instance Segmentation for Point Sets](https://arxiv.org/abs/2505.14583)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 论文提出两种基于采样的方法，用于解决SGPN中内存密集型相似性矩阵的问题，随机采样策略在速度和内存使用上表现最佳。


<details>
  <summary>Details</summary>
Motivation: SGPN在实例分割中使用内存密集的相似性矩阵，占用内存随点数平方增长，需改进。

Method: 采用两种采样方法：在子采样点集上计算实例分割，再通过最近邻方法将标签扩展到完整点集。

Result: 两种方法在大子采样集上表现相当，但随机采样策略在速度和内存使用上改进最大。

Conclusion: 随机采样策略是解决内存问题的有效方法，适用于实例分割任务。

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [177] [3D Reconstruction from Sketches](https://arxiv.org/abs/2505.14621)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 提出了一种从多张草图重建3D场景的流程，包括草图拼接、CycleGAN生成真实图像和MegaDepth估计深度图。


<details>
  <summary>Details</summary>
Motivation: 解决从草图重建3D场景的问题，尤其是针对多张草图的拼接和单张草图的3D重建。

Method: 1. 通过对应点拼接多张草图；2. 使用CycleGAN将拼接后的草图转为真实图像；3. 用MegaDepth估计深度图。

Result: 拼接过程对真实绘图泛化性较差，但单张草图的3D重建效果良好。

Conclusion: 该方法在单张草图的3D重建上表现优异，但多图拼接仍需改进。

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [178] [A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles](https://arxiv.org/abs/2505.14634)
*Gokul Bhusal,Yifei Lou,Cristina Garcia-Cardona,Ekaterina Merkurjev*

Main category: cs.CV

TL;DR: 论文提出了一种基于组稀疏性的高光谱解混方法，通过端元束表示材料，并引入新的正则化方法（TL1惩罚），实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高光谱数据因空间分辨率低常为混合信号，传统线性混合模型无法准确表示材料的类内变异性。

Method: 使用端元束表示材料，提出组稀疏框架（SWAG），并引入TL1惩罚作为正则化方法。

Result: 在合成和真实高光谱数据上的实验表明，该方法优于传统方法。

Conclusion: 提出的框架能有效解决高光谱解混问题，尤其在处理材料变异性方面表现优越。

Abstract: Due to low spatial resolution, hyperspectral data often consists of mixtures
of contributions from multiple materials. This limitation motivates the task of
hyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU
aims to identify the spectral signatures (\textit{endmembers}) of the materials
present in an observed scene, along with their relative proportions
(\textit{fractional abundance}) in each pixel. A major challenge lies in the
class variability in materials, which hinders accurate representation by a
single spectral signature, as assumed in the conventional linear mixing model.
Moreover, To address this issue, we propose using group sparsity after
representing each material with a set of spectral signatures, known as
endmember bundles, where each group corresponds to a specific material. In
particular, we develop a bundle-based framework that can enforce either
inter-group sparsity or sparsity within and across groups (SWAG) on the
abundance coefficients. Furthermore, our framework offers the flexibility to
incorporate a variety of sparsity-promoting penalties, among which the
transformed $\ell_1$ (TL1) penalty is a novel regularization in the HU
literature. Extensive experiments conducted on both synthetic and real
hyperspectral data demonstrate the effectiveness and superiority of the
proposed approaches.

</details>


### [179] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Main category: cs.CV

TL;DR: 提出了一种硬件高效的4位权重和8位浮点推理量化方案（W4A8），通过双精度量化算法（DPQ）减少精度损失，显著提升速度和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度增加，模型规模增长导致延迟和内存效率问题，后训练量化成为解决方案。

Method: 采用W4A8方案，权重用4位整数存储，推理用8位浮点运算，并提出DPQ算法减少精度损失。

Result: 实验显示显著提升吞吐量，同时保持可接受的精度损失。

Conclusion: 该方案在多种现代加速器上高效，平衡了性能和精度。

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [180] [VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation](https://arxiv.org/abs/2505.14640)
*Wentao Ma,Weiming Ren,Yiming Jia,Zhuofeng Li,Ping Nie,Ge Zhang,Wenhu Chen*

Main category: cs.CV

TL;DR: 现有长视频理解（LVU）基准测试存在缺陷，如依赖选择题（MCQ）和问题先验强，导致评估结果失真。作者提出VideoEval-Pro，通过开放式短答题更真实评估长视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有LVU基准测试因依赖MCQ和问题先验强，无法真实评估大型多模态模型（LMMs）的长视频理解能力，亟需改进。

Method: 提出VideoEval-Pro基准测试，采用开放式短答题，要求模型理解整个视频内容，并评估片段级和全视频级的感知与推理能力。

Result: 实验表明，VideoEval-Pro能更真实反映LMMs的长视频理解能力，开放式问题得分显著低于MCQ，且增加输入帧数对性能提升更有效。

Conclusion: VideoEval-Pro提供了更可靠的长视频理解评估方法，揭示了当前LMMs的真实能力，推动了该领域的进步。

Abstract: Large multimodal models (LMMs) have recently emerged as a powerful tool for
long video understanding (LVU), prompting the development of standardized LVU
benchmarks to evaluate their performance. However, our investigation reveals a
rather sober lesson for existing LVU benchmarks. First, most existing
benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation
results are inflated due to the possibility of guessing the correct answer;
Second, a significant portion of questions in these benchmarks have strong
priors to allow models to answer directly without even reading the input video.
For example, Gemini-1.5-Pro can achieve over 50\% accuracy given a random frame
from a long video on Video-MME. We also observe that increasing the number of
frames does not necessarily lead to improvement on existing benchmarks, which
is counterintuitive. As a result, the validity and robustness of current LVU
benchmarks are undermined, impeding a faithful assessment of LMMs' long-video
understanding capability. To tackle this problem, we propose VideoEval-Pro, a
realistic LVU benchmark containing questions with open-ended short-answer,
which truly require understanding the entire video. VideoEval-Pro assesses both
segment-level and full-video understanding through perception and reasoning
tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the
following findings: (1) video LMMs show drastic performance ($>$25\%) drops on
open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do
not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other
MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input
frames. Our results show that VideoEval-Pro offers a more realistic and
reliable measure of long video understanding, providing a clearer view of
progress in this domain.

</details>


### [181] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/abs/2505.14646)
*Anna C. Doris,Md Ferdous Alam,Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CV

TL;DR: CAD-Coder是一个开源视觉语言模型，能够直接从视觉输入生成可编辑的CAD代码，显著提升工程设计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前手动创建CAD模型耗时且依赖专业知识，AI驱动的CAD生成模型存在局限性，如不完整的操作表示和低输出精度。

Method: 利用新数据集GenCAD-Code（包含16.3万对CAD模型图像和代码），CAD-Coder通过微调视觉语言模型生成CadQuery Python代码。

Result: CAD-Coder在语法正确率和3D实体相似性上优于GPT-4.5等基线模型，并能从未见过的图像生成代码。

Conclusion: CAD-Coder展示了微调视觉语言模型在简化CAD工作流程中的潜力，其开源代码可供工程师和设计师使用。

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [182] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 论文提出了一种多模态LLM模型MM-When2Speak，用于预测对话中何时以及如何做出简短反应，显著提升了响应时机准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM聊天机器人在实时对话中难以把握简短反应的时机，主要依赖文本输入而缺乏多模态上下文信息。

Method: 构建了一个多模态数据集，并提出MM-When2Speak模型，整合视觉、听觉和文本信息预测响应时机和类型。

Result: MM-When2Speak在响应时机准确性上比现有单模态和LLM基线模型提升高达4倍。

Conclusion: 多模态输入对实现自然、及时的对话AI至关重要。

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [183] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/abs/2505.14664)
*Yilin Ye,Junchao Huang,Xingchen Zeng,Jiazhi Xia,Wei Zeng*

Main category: cs.CV

TL;DR: AKRMap是一种新的降维技术，用于可视化跨模态嵌入度量，通过学习投影空间中的度量景观核回归，提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法（如PCA和t-SNE）主要关注单模态特征分布，无法有效结合跨模态度量（如CLIPScore）。

Method: AKRMap构建了一个监督投影网络，通过后投影核回归损失和自适应广义核联合优化，支持交互式可视化功能。

Result: 定量实验表明，AKRMap在生成更准确和可信的可视化方面优于现有降维方法。

Conclusion: AKRMap在可视化文本到图像模型的跨模态嵌入方面表现出色，代码和演示已开源。

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [184] [UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens](https://arxiv.org/abs/2505.14671)
*Ruichuan An,Sihan Yang,Renrui Zhang,Zijun Shen,Ming Lu,Gaole Dai,Hao Liang,Ziyu Guo,Shilin Yan,Yulin Luo,Bocheng Zou,Chaoqun Yang,Wentao Zhang*

Main category: cs.CV

TL;DR: UniCTokens提出了一种统一的概念标记框架，将个性化信息整合到视觉语言模型中，以提升理解和生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法将理解和生成任务分离，导致复杂提示生成受限，如缺乏额外文本描述时无法生成特定概念。

Method: UniCTokens训练统一概念标记，采用三阶段渐进训练策略：理解预热、生成引导和深化理解。

Result: 在UnifyBench上，UniCTokens在概念理解、生成及知识驱动生成任务中表现优异，尤其在后者达到SOTA。

Conclusion: 研究表明，增强理解可改进生成，而生成过程也能为理解提供反馈，两者相辅相成。

Abstract: Personalized models have demonstrated remarkable success in understanding and
generating concepts provided by users. However, existing methods use separate
concept tokens for understanding and generation, treating these tasks in
isolation. This may result in limitations for generating images with complex
prompts. For example, given the concept $\langle bo\rangle$, generating
"$\langle bo\rangle$ wearing its hat" without additional textual descriptions
of its hat. We call this kind of generation personalized knowledge-driven
generation. To address the limitation, we present UniCTokens, a novel framework
that effectively integrates personalized information into a unified vision
language model (VLM) for understanding and generation. UniCTokens trains a set
of unified concept tokens to leverage complementary semantics, boosting two
personalized tasks. Moreover, we propose a progressive training strategy with
three stages: understanding warm-up, bootstrapping generation from
understanding, and deepening understanding from generation to enhance mutual
benefits between both tasks. To quantitatively evaluate the unified VLM
personalization, we present UnifyBench, the first benchmark for assessing
concept understanding, concept generation, and knowledge-driven generation.
Experimental results on UnifyBench indicate that UniCTokens shows competitive
performance compared to leading methods in concept understanding, concept
generation, and achieving state-of-the-art results in personalized
knowledge-driven generation. Our research demonstrates that enhanced
understanding improves generation, and the generation process can yield
valuable insights into understanding. Our code and dataset will be released at:
\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.

</details>


### [185] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/abs/2505.14673)
*Yu Tong,Zihao Pan,Shuai Yang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: IndexMark是一种无需训练的水印框架，专为自回归图像生成模型设计，通过代码本冗余特性嵌入水印，不影响图像质量，并在验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有生成水印方法主要针对扩散模型，自回归模型的水印研究不足，IndexMark填补了这一空白。

Method: 利用代码本冗余，通过匹配替换方法选择水印标记并嵌入，验证时计算水印标记比例，辅以索引编码器和辅助验证方案增强鲁棒性。

Result: IndexMark在图像质量和验证准确性上达到最优，且对裁剪、噪声等多种干扰具有鲁棒性。

Conclusion: IndexMark为自回归模型提供了一种高效、鲁棒的水印解决方案，具有广泛应用潜力。

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


### [186] [Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.14677)
*Jiaer Xia,Yuhang Zang,Peng Gao,Yixuan Li,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 通过强化学习训练视觉语言模型（VLM）进行图像推理，无需显式思维链监督，提出caption-reason-answer格式以解决捷径学习问题。


<details>
  <summary>Details</summary>
Motivation: 探索如何让视觉语言模型（VLMs）通过强化学习在图像数据上实现推理能力，避免显式思维链监督导致的捷径学习问题。

Method: 使用强化学习训练VLM，采用caption-reason-answer输出格式，首先生成详细图像描述，再构建推理链。

Result: 在273K无思维链的视觉问答对上训练的Visionary-R1模型，在多个视觉推理基准上优于GPT-4o、Claude3.5-Sonnet和Gemini-1.5-Pro。

Conclusion: caption-reason-answer格式能有效减少捷径学习，提升模型在未见数据上的泛化能力。

Abstract: Learning general-purpose reasoning capabilities has long been a challenging
problem in AI. Recent research in large language models (LLMs), such as
DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can
enable pre-trained LLMs to develop reasoning capabilities using simple
question-answer pairs. In this paper, we aim to train visual language models
(VLMs) to perform reasoning on image data through reinforcement learning and
visual question-answer pairs, without any explicit chain-of-thought (CoT)
supervision. Our findings indicate that simply applying reinforcement learning
to a VLM -- by prompting the model to produce a reasoning chain before
providing an answer -- can lead the model to develop shortcuts from easy
questions, thereby reducing its ability to generalize across unseen data
distributions. We argue that the key to mitigating shortcut learning is to
encourage the model to interpret images prior to reasoning. Therefore, we train
the model to adhere to a caption-reason-answer output format: initially
generating a detailed caption for an image, followed by constructing an
extensive reasoning chain. When trained on 273K CoT-free visual question-answer
pairs and using only reinforcement learning, our model, named Visionary-R1,
outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and
Gemini-1.5-Pro, on multiple visual reasoning benchmarks.

</details>


### [187] [UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.14682)
*Rui Tian,Mingfei Gao,Mingze Xu,Jiaming Hu,Jiasen Lu,Zuxuan Wu,Yinfei Yang,Afshin Dehghan*

Main category: cs.CV

TL;DR: UniGen是一种统一的多模态大语言模型（MLLM），具备图像理解和生成能力，通过数据驱动的训练流程和创新的Chain-of-Thought Verification（CoT-V）策略，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在构建一个统一的MLLM，能够同时处理图像理解和生成任务，并通过数据驱动的训练流程和创新的测试时策略提升性能。

Method: 采用多阶段预训练、监督微调和直接偏好优化的训练流程，并提出CoT-V策略，使模型在测试时既能生成图像又能验证语义对齐。

Result: UniGen在多个图像理解和生成基准测试中达到最先进水平，GenEval得分为0.78，DPG-Bench得分为85.19。

Conclusion: 研究为构建统一MLLM提供了实用见解，并解决了关键挑战，为未来研究指明了方向。

Abstract: We introduce UniGen, a unified multimodal large language model (MLLM) capable
of image understanding and generation. We study the full training pipeline of
UniGen from a data-centric perspective, including multi-stage pre-training,
supervised fine-tuning, and direct preference optimization. More importantly,
we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time
scaling, which significantly boosts UniGen's image generation quality using a
simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act
as both image generator and verifier at test time, assessing the semantic
alignment between a text prompt and its generated image in a step-by-step CoT
manner. Trained entirely on open-source datasets across all stages, UniGen
achieves state-of-the-art performance on a range of image understanding and
generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on
DPG-Bench. Through extensive ablation studies, our work provides actionable
insights and addresses key challenges in the full life cycle of building
unified MLLMs, contributing meaningful directions to the future research.

</details>


### [188] [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683)
*Chaorui Deng,Deyao Zhu,Kunchang Li,Chenhui Gou,Feng Li,Zeyu Wang,Shu Zhong,Weihao Yu,Xiaonan Nie,Ziang Song,Guang Shi,Haoqi Fan*

Main category: cs.CV

TL;DR: BAGEL是一个开源的统一多模态理解和生成基础模型，通过大规模多模态数据预训练，表现出卓越的多模态推理能力，显著优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 统一多模态理解和生成是前沿系统的关键能力，但现有开源模型表现不足，因此开发了BAGEL。

Method: BAGEL是一个仅解码器的统一模型，通过大规模文本、图像、视频和网页数据的预训练实现多模态能力。

Result: BAGEL在标准基准测试中显著优于其他开源模型，并展现出高级多模态推理能力，如自由图像操作、未来帧预测等。

Conclusion: BAGEL为多模态研究提供了新机会，团队公开了关键发现、预训练细节、数据协议及代码和检查点。

Abstract: Unifying multimodal understanding and generation has shown impressive
capabilities in cutting-edge proprietary systems. In this work, we introduce
BAGEL, an open0source foundational model that natively supports multimodal
understanding and generation. BAGEL is a unified, decoder0only model pretrained
on trillions of tokens curated from large0scale interleaved text, image, video,
and web data. When scaled with such diverse multimodal interleaved data, BAGEL
exhibits emerging capabilities in complex multimodal reasoning. As a result, it
significantly outperforms open-source unified models in both multimodal
generation and understanding across standard benchmarks, while exhibiting
advanced multimodal reasoning abilities such as free-form image manipulation,
future frame prediction, 3D manipulation, and world navigation. In the hope of
facilitating further opportunities for multimodal research, we share the key
findings, pretraining details, data creation protocal, and release our code and
checkpoints to the community. The project page is at https://bagel-ai.org/

</details>


### [189] [Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers](https://arxiv.org/abs/2505.14687)
*Sucheng Ren,Qihang Yu,Ju He,Alan Yuille,Liang-Chieh Chen*

Main category: cs.CV

TL;DR: GRAT是一种无需训练的注意力加速策略，通过分组和结构化区域限制，显著提升Diffusion Transformers的生成速度，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion-based Transformers计算成本高，限制了实际部署，例如生成高分辨率图像耗时过长。

Method: GRAT通过将连续令牌分组并限制键值令牌的结构化区域，减少计算开销，同时保留关键注意力模式。

Result: GRAT在生成8192×8192图像时实现了35.8倍加速，且无需微调即可保持性能。

Conclusion: GRAT为加速Diffusion Transformers提供了有效方案，有望推动可扩展视觉生成的研究。

Abstract: Diffusion-based Transformers have demonstrated impressive generative
capabilities, but their high computational costs hinder practical deployment,
for example, generating an $8192\times 8192$ image can take over an hour on an
A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first,
\textbf{AT}tending smartly), a training-free attention acceleration strategy
for fast image and video generation without compromising output quality. The
key insight is to exploit the inherent sparsity in learned attention maps
(which tend to be locally focused) in pretrained Diffusion Transformers and
leverage better GPU parallelism. Specifically, GRAT first partitions contiguous
tokens into non-overlapping groups, aligning both with GPU execution patterns
and the local attention structures learned in pretrained generative
Transformers. It then accelerates attention by having all query tokens within
the same group share a common set of attendable key and value tokens. These key
and value tokens are further restricted to structured regions, such as
surrounding blocks or criss-cross regions, significantly reducing computational
overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention
when generating $8192\times 8192$ images) while preserving essential attention
patterns and long-range context. We validate GRAT on pretrained Flux and
HunyuanVideo for image and video generation, respectively. In both cases, GRAT
achieves substantially faster inference without any fine-tuning, while
maintaining the performance of full attention. We hope GRAT will inspire future
research on accelerating Diffusion Transformers for scalable visual generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [190] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

Main category: cs.LG

TL;DR: 论文提出了一种优化机器学习学习率的新方法，发现学习率与数据集大小之间的比例关系，并提出了累积学习常数的概念。


<details>
  <summary>Details</summary>
Motivation: 研究学习率与数据集规模之间的关系，以提升训练效率和性能。

Method: 发现学习率与数据集大小的比例关系，并提出累积学习常数的框架。

Result: 为设计高级学习率调度提供了理论基础，可能提升多种机器学习应用的训练效果。

Conclusion: 这些发现为优化学习率提供了新思路，有望广泛应用于机器学习领域。

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [191] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang,Yaan Zhou,Yuanhao Gong,Haoxuan Yuan,Shuanglong Liu*

Main category: cs.LG

TL;DR: 本文综述了基于FPGA的CNN硬件加速器，总结了性能评估框架和优化策略，并比较了不同FPGA架构的性能，同时探讨了未来挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着CNN复杂度的增加，计算需求激增，FPGA因其可重构性、并行性和能效成为理想的硬件加速解决方案。

Method: 通过综述现有研究，提出性能评估框架，并分析并行计算、数据流优化和软硬件协同设计等关键优化策略。

Result: 比较了不同FPGA架构在延迟、吞吐量、计算效率、功耗和资源利用率等方面的表现。

Conclusion: FPGA在CNN加速领域具有巨大潜力，未来需进一步创新以应对挑战。

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [192] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen,William Guicquero*

Main category: cs.LG

TL;DR: 本文提出了一种名为通用学习温度计（GLT）的编码技术，用于改进二元神经网络（BNN）的输入数据表示，通过学习非线性量化阈值。结合轻量级分组卷积和块剪枝技术，显著降低了模型的计算复杂性和大小。


<details>
  <summary>Details</summary>
Motivation: 现有二元神经网络研究主要关注模型权重和激活，而忽略了输入原始数据的优化。本文旨在通过改进输入数据表示和模型结构，提升BNN的效率和准确性。

Method: 提出GLT编码技术，通过学习非线性量化阈值优化输入数据表示；结合轻量级分组卷积、块剪枝和知识蒸馏技术，进一步降低模型复杂性和大小。

Result: 实验表明，GLT显著提升了BNN的准确性（在STL-10和VWW数据集上验证）；结合块剪枝技术后，实现了轻量级（小于1Mb）且完全二值化的模型，适用于传感器持续推理场景。

Conclusion: GLT为BNN提供了多功能性，显著提升了输入数据表示和模型效率，同时保持了轻量化和低计算复杂性，适用于实际应用场景。

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [193] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida,William L. Roberts*

Main category: cs.LG

TL;DR: 神经算子能快速预测多相流问题中的液体-蒸汽界面演化，适用于需要快速响应的工业控制。


<details>
  <summary>Details</summary>
Motivation: 多相流问题（如液体-蒸汽流）因密度梯度大或相变而复杂，传统CFD模型无法快速预测，限制了工业应用。

Method: 使用神经算子，通过实验数据或模拟训练，预测Navier-Stokes方程等偏微分方程的演化。

Result: 神经算子的预测时间尺度与多相应用匹配，预测液体-蒸汽界面演化的准确性极高。

Conclusion: 神经算子可用于快速响应的工业控制，特别是在多相流问题中表现优异。

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [194] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

Main category: cs.LG

TL;DR: 提出了一种可视化工具，用于分析深度学习模型中嵌入数据的轴对齐情况，揭示了激活函数如何导致特权基的形成。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型如何表示数据目前缺乏有效方法，需要一种直观的工具来分析嵌入数据的分布。

Method: 通过评估网络特权基向量定义的平面周围的分布，提供了一种原子化和整体化的度量方法。

Result: 发现嵌入表示倾向于与特权基对齐，激活函数直接导致特权基的形成。

Conclusion: 解释了表示倾向于与神经元对齐的原因，并发现了祖母神经元的存在。

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [195] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan,Xingjian Li,Benjamin J. Zhang,Tuhin Sahai,Stanley Osher,Markos A. Katsoulakis*

Main category: cs.LG

TL;DR: 该论文通过最优控制理论优化Transformer的训练和架构设计，提升性能并提供理论保证，实验证明其框架高效且参数节省。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用最优控制理论改进Transformer的训练和架构设计，以超越传统试错方法。

Method: 采用连续时间公式的最优控制理论工具，设计可即插即用的框架，仅需少量实现调整。

Result: 在多个任务中显著提升基线模型性能，参数效率更高，如nanoGPT测试损失减少46%，参数减少42%。

Conclusion: 该研究首次将最优控制理论应用于Transformer，为理论驱动的系统性改进提供了新基础。

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [196] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He,Celia Reina*

Main category: cs.LG

TL;DR: SPIEDiff是一种机器学习框架，结合统计物理和扩散模型，用于高效发现耗散系统的宏观动力学和热力学。


<details>
  <summary>Details</summary>
Motivation: 解决粒子模拟的时间尺度限制、热力学势的非唯一性以及不确定性量化需求。

Method: 利用统计物理、条件扩散模型和epinets构建SPIEDiff框架。

Result: 在随机Arrhenius粒子过程中准确预测热力学和动力学，显著减少计算时间。

Conclusion: SPIEDiff为热力学模型的数据驱动发现提供了可靠途径。

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [197] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang,Chengqi Zhang*

Main category: cs.LG

TL;DR: 本文探讨了如何将低秩适应（LoRA）融入联邦学习（FL）中，以高效微调基础模型（FedLoRA），重点关注分布式学习、异构性和效率三大挑战，并对现有方法进行分类。


<details>
  <summary>Details</summary>
Motivation: 解决在保护数据隐私的同时高效利用私有数据集微调基础模型的问题。

Method: 结合联邦学习（FL）和低秩适应（LoRA）技术，提出FedLoRA框架，分析分布式学习、异构性和效率三大挑战。

Result: 对现有方法进行分类，并总结了解决这些挑战的具体方法。

Conclusion: 提出了未来研究方向，为FedLoRA的进一步发展指明了路径。

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [198] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

Main category: cs.LG

TL;DR: 论文提出了一种利用CLIP解决开放集域适应问题的新方法，通过动态文本提示和梯度分析模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 开放集域适应（OSDA）面临已知类分布对齐和目标域未知类识别的双重挑战，现有方法未能充分利用模态间语义关系且易受未知样本检测误差累积影响。

Method: 1）基于域差异度量的可学习文本提示动态调整CLIP文本编码器；2）通过梯度分析模块量化域偏移，区分已知/未知样本的梯度行为。

Result: 在Office-Home数据集上的实验表明，该方法显著优于CLIP基线和标准基线，梯度分析模块的关键作用通过消融实验验证。

Conclusion: 提出的方法通过语义一致性和梯度分析有效解决了开放集域适应问题，为未来研究提供了新思路。

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [199] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan,Alireza Doostan*

Main category: cs.LG

TL;DR: 论文探讨了科学机器学习（SciML）中可解释性的定义，指出当前研究将稀疏性与可解释性混淆，并提出了强调机制理解的新的可解释性定义。


<details>
  <summary>Details</summary>
Motivation: 科学家对神经网络模型缺乏传统科学模型的数学表达形式感到不满，认为其难以融入科学知识体系，因此需要明确可解释性在科学中的作用。

Method: 通过回顾非科学领域的可解释性ML文献，分析其不足，并提出适用于物理科学的可解释性定义，强调机制理解而非数学稀疏性。

Result: 研究发现稀疏性并非可解释性的必要条件，且缺乏先验知识时实现可解释性科学发现的可能性存疑。

Conclusion: 明确且哲学基础扎实的可解释性定义有助于推动数据驱动科学研究的进展。

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [200] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: LoRASuite提出了一种模块化方法，利用现有LoRA权重高效适应新版LLM，避免从头训练，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM频繁更新，传统从头训练LoRA权重的方法成本高、耗时长且不环保，亟需一种高效利用现有权重的方法。

Method: 通过计算新旧LLM参数的转移矩阵，基于对齐和相似性指标分配层和注意力头，并进行小规模微调以确保稳定性。

Result: 实验表明，LoRASuite性能优于小规模LoRA方法，在MiniCPM和Qwen上甚至超越全规模训练，数学任务分别提升1.4和6.6分，内存减少5.5GB，计算时间降低78.23%。

Conclusion: LoRASuite为LLM更新提供了一种高效、低成本的LoRA权重迁移方案，具有显著性能优势和资源节约效果。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [201] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi,Laith Al Shaggah,Jozsef Gall,Bernadett Aradi*

Main category: cs.LG

TL;DR: 研究探讨了零样本时间序列预测在死亡率预测中的应用，比较了TimesFM和CHRONOS等模型，发现CHRONOS在短期预测中表现优异，而TimesFM表现不佳。随机森林模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索零样本时间序列预测在死亡率预测中的潜力，避免任务特定的微调。

Method: 评估TimesFM和CHRONOS等模型，与传统方法（如ARIMA和Lee-Carter模型）对比，使用50个国家111个年龄组的数据。

Result: CHRONOS在短期预测中优于传统方法，TimesFM表现不佳；微调CHRONOS显著提升长期预测准确性；随机森林模型整体表现最佳。

Conclusion: 零样本预测具有潜力，但需谨慎选择模型并进行领域适配。

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [202] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang,Kezong Tang,Zi-Wei Chen,Yuang Wei,Tian-Yi Liu,Jiayi Wu*

Main category: cs.LG

TL;DR: MAS-KCL算法通过多智能体系统和双向反馈机制优化知识组件图，提升学习路径识别效率，助力教育可持续发展。


<details>
  <summary>Details</summary>
Motivation: 准确的知识组件图能帮助教育者定位学习者表现不佳的根源，从而进行针对性教学干预。

Method: 开发了MAS-KCL算法，利用多智能体系统和双向反馈机制优化知识组件图结构。

Result: 在合成和真实教育数据集上验证了算法的有效性，能准确识别学习路径。

Conclusion: MAS-KCL算法有助于设计更全面的学习计划，促进教育可持续发展。

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [203] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng,Philip C. Woodland*

Main category: cs.LG

TL;DR: MTLA通过动态合并时间相邻的KV缓存向量，显著减少自注意力推理的内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力的KV缓存随序列长度线性增长，成为推理效率的瓶颈。

Method: 提出MTLA，利用超网络动态合并时间相邻的KV缓存向量，并引入步长感知因果掩码。

Result: 在多项任务中，MTLA在保持性能的同时，显著提升推理速度和GPU内存效率（如5.3倍加速和8.3倍内存减少）。

Conclusion: MTLA是一种高效的自注意力替代方案，适用于需要长序列处理的任务。

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [204] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt,Bin Han,Robert Wolfe,Bill Howe*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）在部分无序样本信息下的数据泄露风险，提出了两种无需数据的攻击方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在较弱对抗假设下（如仅掌握部分无序信息）的数据泄露问题，填补了现有研究的空白。

Method: 提出了两种无需数据的攻击方法：基于似然比的攻击和PRISM方法（利用外部先验正则化比率）。

Result: 实验表明，这两种方法在医学和法律场景中表现优异，与需要标注数据的基线分类器相当。

Conclusion: LLMs在微调后仍易受片段特定提取攻击，需进一步研究防御措施。

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [205] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo,Yinong Wang,Wei Li,Mengting Liu,Ming Li,Jinkai Zheng,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedPrLLM是一个隐私保护的联邦剪枝框架，用于压缩大语言模型（LLM），无需共享本地数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM剪枝方法需要公开校准样本的问题，适用于隐私敏感领域。

Method: 每个客户端基于本地数据计算剪枝掩码矩阵并共享给服务器，实现全局模型的协作剪枝。

Result: 实验表明，一次性剪枝结合层比较且不缩放权重是最优选择。

Conclusion: FedPrLLM为隐私敏感领域的LLM剪枝提供了有效指导。

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [206] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang,Peng Ye,Chenyu Huang,Shenghe Zheng,Bo Zhang,Wanli Ouyang,Tao Chen*

Main category: cs.LG

TL;DR: UltraDelta是一种无数据依赖的delta压缩方法，通过多层优化实现超高压缩比和强性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有delta压缩方法在高压缩比下性能下降和数据依赖问题。

Method: 采用三种技术：基于方差的稀疏分配、分布感知压缩和迹范数引导的重新缩放。

Result: 在多种模型上实现超高压缩比（最高800x），性能优于现有方法。

Conclusion: UltraDelta在超高压缩比下仍能保持性能，优于现有方法。

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [207] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine,Maxime Haddouche,Eric Moulines,Michael I. Jordan,Etienne Boursier,Alain Durmus*

Main category: cs.LG

TL;DR: 论文研究了动态环境中的决策导向学习（DFL），提出了一种在线算法，通过正则化和乐观原则解决了目标函数梯度为零或未定义的问题，并在简单背包实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有DFL研究仅关注静态环境和固定数据批次，而动态环境中目标函数和数据分布会随时间变化，这带来了梯度问题和非凸性挑战。

Method: 通过正则化目标函数使其可微，并利用乐观原则（基于近优预言机和适当扰动），提出了一种在线算法。

Result: 在单纯形和有界凸多面体决策空间中，建立了预期动态遗憾的界限。

Conclusion: 算法在动态环境中表现优于传统预测导向方法，验证了其有效性。

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [208] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger,Omri Barak*

Main category: cs.LG

TL;DR: 论文研究了闭环环境中线性RNN的学习动态，发现其与开环训练显著不同，揭示了短期策略改进与长期稳定性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界的学习发生在闭环环境中，而现有RNN训练多基于开环设置，因此需要研究闭环训练的动态特性。

Method: 开发了描述线性RNN在闭环环境中学习动态的数学理论，并通过实验验证其与开环训练的差异。

Result: 闭环训练的RNN表现出独特的学习阶段，受短期策略改进和长期稳定性的双重目标驱动。

Conclusion: 研究强调了在生物合理环境中建模闭环动态的重要性，并展示了其在运动控制任务中的适用性。

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [209] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme,Christine Allen-Blanchette,Hans Harder,Sebastian Peitz*

Main category: cs.LG

TL;DR: 论文提出了一种基于等变卷积自编码器和等变卷积LSTM的端到端替代模型，用于建模和控制大规模物理系统，以三维Rayleigh-Bénard对流为例，展示了显著的样本和参数效率提升。


<details>
  <summary>Details</summary>
Motivation: 大规模物理系统（如流体力学、气候建模）的建模和控制面临高自由度和复杂动力学的挑战，需要提高准确性和样本效率。

Method: 使用$G$-可操纵核的等变卷积自编码器和等变卷积LSTM构建端到端模型，针对E(2)-等变的Rayleigh-Bénard对流系统优化。

Result: 模型在样本和参数效率上显著提升，并能更好地扩展到更复杂的动力学（如更大的Rayleigh数）。

Conclusion: 提出的等变架构为大规模物理系统的建模和控制提供了高效且可扩展的解决方案。

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [210] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich,Benjamin Koch,Sven Schönfeld*

Main category: cs.LG

TL;DR: 论文探讨了在FPGA上部署TinyYOLOv3检测网络的优化方法，包括批量归一化融合、滤波器剪枝和训练后网络量化。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN在计算机视觉任务中广泛应用，但其在嵌入式平台（如FPGA）上的部署面临高计算强度、内存需求和算术条件的挑战。

Method: 采用批量归一化融合、滤波器剪枝和训练后网络量化等技术优化TinyYOLOv3在XILINX Artix-7 FPGA上的运行。

Result: 展示了在FPGA上高效运行TinyYOLOv3的最佳实践方法。

Conclusion: 这些方法为在资源受限的嵌入式平台上部署CNN提供了可行的解决方案。

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [211] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime,Arshad Jhumka*

Main category: cs.LG

TL;DR: FlexFed是一种新颖的联邦学习方法，针对HAR环境中的灾难性遗忘问题，通过动态调整训练频率和数据保留策略，提高了效率和收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: HAR环境中数据分布非平稳且客户端参与不稳定，导致现有联邦学习方法面临灾难性遗忘问题。

Method: 提出FlexFed方法，动态调整离线训练频率并优化数据保留策略，同时引入新指标量化灾难性遗忘。

Result: 实验表明FlexFed有效缓解灾难性遗忘，提升效率10-15%，并实现更快、更稳定的收敛。

Conclusion: FlexFed在HAR环境中表现出色，为联邦学习在资源受限和动态分布场景提供了有效解决方案。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [212] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

Main category: cs.LG

TL;DR: 提出一种利用对称性结构构造显式对称性破坏变形的方法，优化不变成本泛函。


<details>
  <summary>Details</summary>
Motivation: 研究在全局对称群下不变且仅作为黑盒可访问的任务成本泛函的优化问题。

Method: 通过最小化辅助能量泛函获得规范场，构造输入信号的变形，证明成本严格下降。

Result: 在温和正则条件下，成本沿变形方向严格下降，退化集具有零高斯测度。

Conclusion: 该方法为优化不变成本泛函提供了理论工具，适用于黑盒模型和对称约束任务。

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [213] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang,Guanting Chen,Kalyan Talluri,Xiaocheng Li*

Main category: cs.LG

TL;DR: 提出OMGPT模型，基于GPT框架解决运筹学和管理科学中的序列决策问题，无需假设模型结构，直接映射历史到未来动作。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在运筹学和管理科学任务中依赖模型假设和数据不足的问题。

Method: 提出通用序列建模框架，训练基于Transformer的OMGPT模型，利用预训练数据。

Result: 模型在动态定价、库存管理等任务中表现优异。

Conclusion: OMGPT通过预训练和灵活映射，为序列决策任务提供了新范式。

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [214] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang,Yaoyu Zhang,Tao Luo*

Main category: cs.LG

TL;DR: 本文研究了神经网络临界点的样本依赖性，提出了一种样本无关的临界提升算子，并通过示例表明现有临界嵌入方法未能捕捉所有样本无关临界点，同时证明了样本依赖临界点的存在性及其鞍点特性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络临界点与样本之间的关系，解决现有方法在捕捉临界点时的局限性。

Method: 引入样本无关的临界提升算子，定义样本依赖和样本无关的临界点，并通过示例和理论分析验证其性质。

Result: 发现现有临界嵌入方法不完整，证明了样本依赖临界点的存在及其鞍点特性。

Conclusion: 样本依赖性对神经网络临界点有重要影响，未来研究需进一步探索其理论及应用。

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [215] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev,Mark Coates*

Main category: cs.LG

TL;DR: 提出一种通过Zero-Shot NAS自动剪枝搜索空间的方法，显著降低One-Shot NAS的内存消耗和搜索时间，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: One-Shot NAS方法在搜索过程中GPU内存需求高，限制了其应用。

Method: 利用Zero-Shot NAS高效移除低性能架构，再在剪枝后的搜索空间应用One-Shot NAS。

Result: 在DARTS搜索空间上，内存消耗减少81%，同时保持相同准确性。

Conclusion: 该方法有效解决了One-Shot NAS的高内存问题，提升了搜索效率。

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [216] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

Main category: cs.LG

TL;DR: 论文提出了一种基于Fisher信息的高维神经流形分析方法，通过低维核心空间分析谱特性，并引入了一种高效的随机估计方法。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络的高维参数空间（神经流形）具有独特的Fisher信息度量张量，估计该张量对理论和实践至关重要。

Method: 通过分析低维核心空间的黎曼度量谱特性，扩展到神经流形上的确定性边界，并引入基于Hutchinson迹估计的无偏随机估计方法。

Result: 提出了一种高效的单次反向传播计算的随机估计方法，可估计对角线、块对角线或完整张量，其标准差受真实值限制。

Conclusion: 该方法为神经流形上的度量张量分析提供了高效且可靠的估计工具。

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [217] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache,Luiz F. O. Chamon,Mathias Niepert*

Main category: cs.LG

TL;DR: 论文提出了一种自适应约束等变性（ACE）方法，通过逐步收紧约束来平衡等变性与非等变性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据常因噪声或偏差偏离完美对称性，严格等变模型难以拟合，而无约束模型无法利用部分对称性。

Method: 基于同伦原理，从非等变模型出发，逐步减少与等变性的偏差，实现数据驱动的平衡。

Result: ACE在多种架构和任务中均优于严格等变模型和启发式松弛方法，提升了性能、样本效率和鲁棒性。

Conclusion: ACE通过自适应约束优化，有效平衡了对称性与灵活性，为处理部分对称数据提供了新思路。

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [218] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen,Tong Zhu,Jiale Han,Lexin Li,Gang Li,Xiaowu Dai*

Main category: cs.LG

TL;DR: Peer Elicitation Games (PEG) 是一种无需训练的游戏理论框架，通过生成器和多个判别器的互动，激励LLMs生成更真实的内容。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成内容时的不一致性和幻觉问题。

Method: 使用基于行列式的互信息评分机制，激励判别器真实报告，无需真实标签。

Result: 理论证明和实验验证表明，PEG显著提高了事实准确性。

Conclusion: PEG是一种无需监督或微调即可激励LLMs真实行为的实用方法。

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [219] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Main category: cs.LG

TL;DR: 论文提出了4Hammer强化学习环境，用于评估大型语言模型（LLMs）在复杂棋盘游戏（如Warhammer 40,000）中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在短时间任务中表现良好，但在长时间任务（如复杂棋盘游戏）中表现不佳，缺乏相关评估环境。

Method: 设计了4Hammer强化学习环境，模拟Warhammer 40,000的子集，包含复杂规则和动态游戏状态。

Result: 通过模拟环境，为LLMs在复杂棋盘游戏中的表现提供了评估平台。

Conclusion: 4Hammer填补了LLMs在长时间任务评估中的空白，为未来研究提供了工具。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [220] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib,Md Akil Raihan Iftee,Mir Sazzat Hossain,A. K. M. Mahbubur Rahman,Sajib Mistry,M Ashraful Amin,Amin Ahsan Ali*

Main category: cs.LG

TL;DR: FedCTTA是一种隐私保护且计算高效的联邦学习测试时适应框架，通过相似性感知聚合和最小化熵实现持续适应，解决了现有方法的计算开销、隐私风险和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私敏感应用中表现优异，但模型性能常因训练与部署间的分布偏移而下降。现有测试时适应（TTA）方法在FL中存在计算开销大、隐私风险高和可扩展性差的问题。

Method: 提出FedCTTA框架，避免直接特征共享，利用基于模型输出分布的相似性感知聚合和随机噪声样本，同时通过最小化客户端熵实现持续适应。

Result: 实验表明，FedCTTA在时空异质性场景中优于现有方法，无需服务器端训练且内存占用恒定。

Conclusion: FedCTTA为联邦学习提供了一种高效、隐私保护的持续适应解决方案，显著提升了模型在动态目标分布下的性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [221] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel,Tim Siebert,Marius Zeinhofer,Andrea Walther*

Main category: cs.LG

TL;DR: 论文提出了一种优化泰勒模式自动微分的方法，通过重写计算图“折叠”导数，加速PDE算子的计算，性能优于嵌套反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统通过嵌套反向传播计算PDE算子成本高，限制了其在科学机器学习中的应用。

Method: 引入一种优化技术，通过重写计算图折叠导数，适用于一般线性PDE算子和随机化泰勒模式。

Result: 实现并验证了该方法在流行PDE算子上的加速效果，优于嵌套反向传播。

Conclusion: 该方法通过简化计算图显著提升效率，适合机器学习编译器实现，无需用户介入复杂操作。

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [222] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh,Chun-Fu Jang,Cheng-En Hsieh,Qian-Hui Chen,Sy-Yen Kuo*

Main category: cs.LG

TL;DR: SRGCL是一种新型图对比学习框架，通过动态评估和选择高质量正样本对，提升图表示的质量。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在生成高质量正样本对方面存在挑战，可能导致原始图的语义和结构信息失真。

Method: 提出SRGCL框架，结合多增强策略的统一正样本对生成器和基于流形假设的选择器，通过概率机制迭代优化正样本对选择。

Result: 在多样化的图分类任务中，SRGCL作为插件模块显著优于现有GCL方法。

Conclusion: SRGCL通过动态优化正样本对选择，提升了图对比学习的鲁棒性和适应性。

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [223] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 论文分析了基于强化学习的LLM后训练方法，指出其结构假设简化了MDP，使其等效于监督学习，并通过实验验证了监督微调的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习后训练LLM的结构假设是否合理，揭示其简化MDP可能导致方法失效。

Method: 通过分析MDP的结构假设（状态为上下文窗口，动作为令牌；均匀分配奖励），并与监督学习对比。

Result: 实验表明监督微调（含正负样本）与GRPO效果相当，且结构假设可能误导RL生成长序列。

Conclusion: 强化学习虽有用，但当前LLM的RL框架因简化假设而值得质疑。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [224] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio,Salvador Sosa Güitron,Marcus Babzien,Mikhail Fedurin,Junjie Li,Mark Palmer,Sandra S. Biedron,Manel Martinez-Ramon*

Main category: cs.LG

TL;DR: 提出一种无监督异常检测方法，用于检测MUED中的故障图像，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 无监督技术无需手动标注训练数据，机器可自行检测异常，节省用户时间。

Method: 构建无监督异常检测方法，提供检测不确定性度量。

Result: 方法能自动检测异常，并为用户提供决策依据。

Conclusion: 无监督方法适用于MUED故障图像检测，高效且实用。

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [225] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen,Aravind Venugopal,Jeff Schneider*

Main category: cs.LG

TL;DR: 离线强化学习（RL）通过数据驱动控制提供强大范式。离线基于模型的RL（MBRL）通过显式学习世界模型提高数据效率，但现有方法存在目标不匹配和策略鲁棒性不足的问题。本文提出动态调整世界模型与策略的统一学习框架，通过最大化优化和Stackelberg学习动态提升鲁棒性，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线MBRL方法存在目标不匹配问题，导致世界模型未针对策略学习优化，且策略在部署时缺乏鲁棒性。

Method: 提出动态调整世界模型与策略的统一学习框架，采用最大化优化和Stackelberg学习动态。

Result: 在十二个噪声D4RL MuJoCo任务和三个随机Tokamak控制任务中表现优异。

Conclusion: 通过统一学习目标和动态调整，显著提升了离线MBRL的鲁棒性和性能。

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [226] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore,Zachary Frangella,Sachin Garg,Shaghayegh Fazliani,Michał Dereziński,Madeleine Udell*

Main category: cs.LG

TL;DR: 提出了一种名为ADASAP的分布式加速算法，用于高效解决高斯过程推理中的大规模线性系统问题，显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在大规模数据集上的推理效率低，传统方法难以应对现代应用中的大数据需求。

Method: 采用近似、分布式的sketch-and-project算法，结合行列式点过程理论，快速收敛到真实后验均值。

Result: ADASAP在多个基准数据集和大规模贝叶斯优化任务中优于现有方法，并能处理超过3亿样本的数据集。

Conclusion: ADASAP是一种高效且理论上有保障的高斯过程推理算法，适用于超大规模数据。

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [227] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Main category: cs.LG

TL;DR: 本文研究了大规模语言模型（LLM）预训练中超参数（HPs）的缩放规律，包括学习率η和权重衰减λ。通过分析AdamW时间尺度B/(ηλD)的恒定性和D/N比例的影响，提出了预测最优λ的方法，并探讨了最优批大小Bopt和临界批大小Bcrit的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 研究如何在大规模预训练中高效调整超参数，以优化模型性能和训练效率。

Method: 通过分析AdamW时间尺度的恒定性和D/N比例，推导出超参数缩放规律，并验证其在实际训练中的适用性。

Result: 发现最优λ与B线性相关，且Bopt和Bcrit随D呈幂律缩放，与模型大小N无关。

Conclusion: 研究结果为大规模预训练中超参数的选择提供了理论依据，有助于优化训练时间和计算资源的使用。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [228] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu,Sicun Gao*

Main category: cs.LG

TL;DR: 提出了一种基于提升分数的新重采样标准，用于改进扩散模型中的组合生成。


<details>
  <summary>Details</summary>
Motivation: 通过提升分数评估生成样本是否满足单个条件，并组合结果以判断组合提示是否被满足，旨在提高组合生成的条件对齐。

Method: 利用提升分数，仅需原始扩散模型即可高效近似，无需额外训练或外部模块，并开发了计算开销较低的优化变体。

Result: 实验表明，提升分数显著改善了2D合成数据、CLEVR位置任务和文本到图像合成中的条件对齐。

Conclusion: 该方法在提升组合生成效果的同时保持了计算效率，代码已开源。

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [229] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam,Declan Campbell,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.LG

TL;DR: 提出了一种新的概率框架，用于解释神经网络中的潜在任务表示，结合贝叶斯推理和信息论工具。


<details>
  <summary>Details</summary>
Motivation: 神经网络的灵活性使其成为认知建模的强大工具，但其子符号语义使得解释学习到的表示具有挑战性。

Method: 引入基于贝叶斯推理的概率框架，定义表示单元的分布以推断其对任务性能的因果贡献，并利用信息论工具分析模型属性。

Result: 开发了一套工具和指标，用于揭示模型的关键特性，如表示的分布性、流形复杂性和多义性。

Conclusion: 该框架为神经网络表示的解释提供了新方法，有助于理解其内部工作机制。

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [230] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 论文提出了一种合成数据流生成策略，同时处理概念漂移和新类出现的问题，并展示了无监督漂移检测器在开放集识别任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，数据流的非平稳性（如概念漂移和新类出现）是常见问题，现有方法通常只关注其中之一。本文旨在同时解决这两个问题，并探索其在开放集识别中的应用。

Method: 提出了一种合成数据流生成策略，模拟概念漂移和新类出现，并利用无监督漂移检测器进行检测。

Result: 研究表明，无监督漂移检测器能有效检测新类和概念漂移，生成的数据流可用于开放集识别任务。

Conclusion: 该策略为处理动态数据流中的非平稳性问题提供了新思路，尤其在开放集识别中具有潜力。

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [231] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar,Anya Chaturvedi,Andréa W. Richa,Joshua J. Daymude*

Main category: cs.LG

TL;DR: 本文提出了一种用于动态图中寻找最大独立集（MaxIS）的无监督学习模型，结合了图神经网络（GNNs）和分布式更新机制，在动态图中表现出色。


<details>
  <summary>Details</summary>
Motivation: 动态图中边的变化使得传统静态图方法难以高效处理MaxIS问题，需要一种能够适应动态变化的新方法。

Method: 结合GNNs和分布式更新机制，通过单步并行处理边的增减事件，推断节点的MaxIS成员资格。

Result: 在100-10,000节点的动态图中，模型在近似比、运行时间和内存使用上优于现有方法，且能泛化到更大规模的图。

Conclusion: 该模型在动态图中表现出色，具有高效性和可扩展性，为MaxIS问题提供了新的解决方案。

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [232] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai,Anthony Bao,William Gilpin*

Main category: cs.LG

TL;DR: Panda模型通过训练合成混沌系统数据集，展示了零样本预测真实混沌系统和非线性共振模式的能力，无需重新训练即可预测偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 混沌系统对微小误差敏感，现有模型要么针对单个时间序列训练，要么缺乏动力学结构。

Method: 提出Panda模型，基于动力学系统理论，使用进化算法生成的合成数据集训练。

Result: Panda展示了零样本预测能力和非线性共振模式，并能预测偏微分方程。

Conclusion: 预训练模型在非线性动力学等抽象数学领域具有潜力。

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [233] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana,Anish Thilagar,Dhamma Kimpara,Rafael Frongillo*

Main category: cs.LG

TL;DR: 该论文研究了离散预测任务中替代损失的统计一致性，提出了间接引发（IE）和强IE的概念，并证明其与校准的等价性。


<details>
  <summary>Details</summary>
Motivation: 验证替代损失的校准性通常很困难，而IE条件更容易验证，因此研究IE与校准的关系对设计一致性替代损失具有重要意义。

Method: 针对凸可微损失，证明了IE与校准在一维情况下的等价性，并构造反例说明高维情况下等价性不成立。提出了强IE概念，并证明其与校准的关系。

Result: 证明强IE对可微替代损失意味着校准，且对强凸可微损失是充要条件。通过实例展示了IE和强IE的应用价值。

Conclusion: IE和强IE为设计和分析一致性可微替代损失提供了有效工具，尤其适用于非多面体损失。

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [234] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu,Vladimir Bataev,Lilit Grigoryan,Boris Ginsburg*

Main category: cs.LG

TL;DR: WIND是一种加速RNN-T推理的新策略，通过并行处理窗口内的多帧，显著提升速度且不降低准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T推理是逐帧处理，速度较慢，WIND旨在通过并行处理窗口内的帧来加速推理。

Method: WIND采用并行处理窗口内多帧的策略，支持贪婪解码、批量贪婪解码及新提出的束搜索解码方法。

Result: 实验表明，贪婪模式下速度提升2.4倍且WER不变；束搜索方法在速度提升的同时精度略有提高。

Conclusion: WIND是一种高效且准确的RNN-T推理加速方法，未来将开源实现。

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [235] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang,Donghao Li,Chengshuai Shi,Cong Shen,Jing Yang*

Main category: cs.LG

TL;DR: 本文提出了一种混合学习框架，结合离线数据集和在线交互来优化强化学习策略，算法在次优性差距和在线学习遗憾两个指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过结合离线数据和在线交互来提升强化学习的性能，解决纯在线或纯离线方法的局限性。

Method: 提出了一种统一的算法，基于置信度的在线强化学习方法，并利用离线数据集进行增强。

Result: 算法在次优性差距和在线学习遗憾上均优于纯在线或纯离线方法，理论分析揭示了离线数据集覆盖性质的不同需求。

Conclusion: 混合学习方法在强化学习中具有显著优势，实验结果验证了理论发现。

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [236] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly,Karthik Valmeekam,Atharva Gundawar,Vardhan Palod,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 论文质疑了Chain of Thought (CoT) 的中间语义对模型性能的影响，发现即使使用正确或噪声的中间推理痕迹，模型性能仍保持稳定，挑战了CoT的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT中间语义是否真正影响模型性能，验证中间推理痕迹与最终解决方案之间的因果关系。

Method: 训练模型使用形式化验证的推理痕迹和解决方案，评估中间痕迹的正确性及其对解决方案的影响。

Result: 即使中间痕迹不正确或噪声，模型性能仍稳定，甚至在某些情况下表现更好。

Conclusion: CoT的中间语义并不必然导致可预测的推理行为，需谨慎解读其人类化或算法化行为。

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [237] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy,Adam Gleave*

Main category: cs.LG

TL;DR: 论文探讨了在AI训练中引入谎言检测器的影响，发现其效果取决于探索量、检测器准确性和KL正则化强度，可能导致诚实策略或欺骗性策略。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统中欺骗行为的检测与防范，探讨谎言检测器在训练中的作用及其潜在问题。

Method: 使用DolusChat数据集，结合GRPO和DPO算法，分析谎言检测器对策略诚实性的影响。

Result: 高检测器准确性和KL正则化可促进诚实策略，但低准确性可能导致欺骗率高达85%。DPO算法在现实场景中表现更稳定。

Conclusion: 谎言检测器在特定条件下是有效的监督工具，但也可能加剧欺骗行为，需谨慎使用。

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [238] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng,Chong Sun,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: Quetzal是一种简单但可扩展的自回归模型，用于3D分子生成，通过原子序列建模和结合扩散MLP，显著提升了生成质量和速度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在3D分子生成中占主导地位，而自回归模型表现较弱，Quetzal旨在填补这一空白。

Method: Quetzal将分子视为有序原子序列，使用因果Transformer预测原子类型，结合扩散MLP建模连续位置分布。

Result: Quetzal在生成质量和速度上显著优于现有自回归基线，并与最先进的扩散模型竞争。

Conclusion: Quetzal展示了自回归模型在3D分子生成中的潜力，为可扩展性和通用性提供了新视角。

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [239] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 论文提出了一种通过上下文无关生成（context-free generation）来估计KL散度的方法，以减少语言模型微调时的性能退化（遗忘问题）。实验表明，该方法在保持预训练模型的零样本性能或推理能力方面优于其他数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调时因参数偏移导致的性能退化（遗忘）问题，尤其是在无法访问原始训练数据的情况下。

Method: 提出上下文无关生成方法，近似无偏估计KL散度，并将其用于微调数据增强。

Result: 实验证明，该方法在OLMo-1B和R1-Distill-Llama-8B模型上有效减少了遗忘，优于上下文合成数据或部分预训练数据。

Conclusion: 上下文无关生成是一种简单有效的方法，可用于缓解语言模型微调时的遗忘问题。

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [240] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel,Lizhong Chen*

Main category: cs.LG

TL;DR: FlashKAT通过优化内存访问和梯度累积，显著提升了KAT的训练速度，解决了其性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: KAT虽然计算效率与Transformer相当，但训练速度慢123倍，限制了其应用。

Method: 通过实验分析性能瓶颈，提出FlashKAT，优化内存访问和梯度累积。

Result: FlashKAT训练速度提升86.5倍，同时减少梯度误差。

Conclusion: FlashKAT有效解决了KAT的性能问题，提升了训练效率和精度。

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [241] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.LG

TL;DR: 提出了一种名为Structured Agent Distillation的框架，通过分段监督压缩大型LLM代理，保留推理和行动一致性，显著降低模型大小和推理成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）作为决策代理表现优异，但实际部署受限于高推理成本和大模型尺寸，需要一种压缩方法。

Method: 将轨迹分段为{[REASON]}和{[ACT]}，并应用分段特定损失函数，对齐学生模型与教师模型的行为。

Result: 在ALFWorld、HotPotQA-ReAct和WebShop上表现优于基线方法，实现显著压缩且性能下降最小。

Conclusion: 分段对齐对高效可部署代理至关重要，Structured Agent Distillation是一种有效的压缩方法。

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [242] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao,Chi Zhang,Yuxuan Du*

Main category: cs.LG

TL;DR: 论文比较了深度学习和传统机器学习在量子系统基态学习任务中的表现，发现传统方法常优于或等同于深度学习方法，且输入特征对深度学习模型影响有限。


<details>
  <summary>Details</summary>
Motivation: 量子系统基态性质的计算具有挑战性，AI方法被广泛探索，但深度学习的必要性及作用尚不明确。

Method: 系统性地对比深度学习和传统机器学习方法，在三种哈密顿量家族中测试，资源使用一致，规模达127量子比特。

Result: 传统机器学习方法表现常优于或等同于深度学习，且输入特征对深度学习模型预测影响较小。

Conclusion: 当前深度学习模型在量子系统学习中的必要性受到挑战，研究为其有效应用提供了指导。

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [243] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun,Yuqi Chen,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为TedTrajRec的新方法，用于解决低采样率GPS轨迹恢复问题，通过结合空间-时间交通动态和轨迹动态，显著提升了轨迹恢复性能。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹的低采样率和稀疏性限制了其在GPS系统中的直接应用，现有方法未能充分利用轨迹和路网的复杂时空动态。

Method: 提出TedTrajRec方法，包括PD-GNN（建模周期性模式和拓扑动态）和TedFormer（时间感知Transformer，处理不规则采样数据）。

Result: 在三个真实数据集上的实验表明，TedTrajRec性能优越。

Conclusion: TedTrajRec通过有效捕捉时空动态，显著提升了轨迹恢复效果，代码已开源。

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [244] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores,Hao Chen,Can Li*

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，用于在神经网络输出上强制执行输入相关的线性等式和不等式约束，确保训练和推理期间的约束满足。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键任务中部署时需满足硬约束（如物理定律、公平性或安全限制），但现有方法难以高效且可靠地实现这一点。

Method: 结合任务网络和安全网络，任务网络负责预测准确性，安全网络基于随机和鲁棒优化决策规则确保全局可行性。最终预测为两者的凸组合。

Result: 实验证明该方法在基准回归任务中能持续满足约束，同时保持竞争性准确性和低推理延迟。

Conclusion: 该框架无需迭代或运行时优化即可实现约束满足，且具有通用逼近能力。

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [245] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu,Ziqing Ma,Tian Zhou,Weiqi Chen,Lefei Shen,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为Baguan的数据驱动模型，通过自监督预训练和微调，解决了天气预测中因数据有限导致的过拟合问题，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 天气预测因数据有限容易过拟合，传统方法难以应对。本文探索预训练方法，以解决这一问题。

Method: 提出Baguan模型，基于Siamese Autoencoder进行自监督预训练，并针对不同预测时间微调。

Result: 实验表明，Baguan优于传统方法，且在过拟合控制和下游任务中表现稳健。

Conclusion: Baguan通过创新的预训练策略，有效提升了天气预测的准确性和泛化能力。

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [246] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Main category: cs.LG

TL;DR: InfiFPO是一种用于隐式模型融合的偏好优化方法，通过融合多源概率信息，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法在偏好对齐（PA）阶段仅利用响应输出而忽略概率信息，限制了性能提升。

Method: InfiFPO通过序列级多源概率融合，结合概率裁剪和最大间隔融合策略，优化偏好对齐。

Result: 在11个基准测试中，InfiFPO平均性能从79.95提升至83.33，尤其在数学、编码和推理任务中表现突出。

Conclusion: InfiFPO有效解决了PA阶段的模型融合问题，显著提升了LLM的综合能力。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [247] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang,Ke Bu,Zhuoran Zhuang,Tao Xie,Yao Yu,Dong Li,Yang Guo,Detao Lv*

Main category: cs.LG

TL;DR: 论文提出了一种基于跨未来行为（CFB）的时间序列预测方法CRAFT，通过挖掘跨未来行为的趋势来预测时间序列数据，解决了传统方法中因历史数据有限而导致的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）因历史数据有限而面临预测不确定性的问题，论文探索利用跨未来行为（CFB）来解决这一问题。

Method: 提出CRAFT方法，包括Koopman预测模块提取关键趋势、内部趋势挖掘模块补充未知区域、外部趋势引导模块获取更高层次趋势，并使用需求约束损失校准预测偏差。

Result: 在离线和在线A/B测试中验证了CRAFT的有效性。

Conclusion: CRAFT通过利用跨未来行为的趋势，显著提升了时间序列预测的准确性。

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [248] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás,Christopher D. Manning,Christopher Potts*

Main category: cs.LG

TL;DR: 研究发现，现代大型语言模型（LLMs）的深度增加并未带来高效利用，而是将相同计算分散到更多层中，导致性能提升的边际效益递减。


<details>
  <summary>Details</summary>
Motivation: 探讨深度模型是否通过组合特征实现更高阶计算，还是仅将相同计算分散到更多层中。

Method: 分析Llama 3.1和Qwen 3模型的残差流，比较子层输出，跳过层实验，多跳任务测试，以及浅层与深层模型的线性映射训练。

Result: 后半部分层贡献较小，跳过层影响有限，多跳任务未发现深度组合证据，深层模型仅将计算分散到更多层。

Conclusion: 深度模型未利用深度学习新计算方式，仅通过更多层进行微调，解释了Transformer架构规模扩展效益递减的原因。

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [249] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li,Hung Anh Vu,Damilola Awofisayo,Emily Wenger*

Main category: cs.LG

TL;DR: 研究探讨数据集重叠和任务重叠如何影响机器学习模型的表示相似性，发现两者均与相似性正相关，且结合效果最强。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型表示的相似性，但对其成因探索不足，尤其是数据集和任务重叠的影响。

Method: 通过大量实验评估数据集重叠和任务重叠对模型表示相似性的影响。

Result: 数据集重叠和任务重叠均与更高的表示相似性正相关，且两者结合效果最佳。

Conclusion: 数据集和任务重叠是模型表示相似性的重要成因，未来研究可进一步探索其他潜在因素。

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [250] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou,Yongyi Yang,Mahito Sugiyama,Junchi Yan*

Main category: cs.LG

TL;DR: 论文通过时间窗口分析揭示了深度学习的两个新现象：混沌效应和锥效应，表明网络训练从敏感探索到稳定优化的动态转变。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络的学习动态是现代机器学习的核心挑战，目前对训练动态的相变现象认识不足。

Method: 采用区间视角比较网络状态，通过微小参数扰动和跟踪eNTK演化来研究训练动态。

Result: 发现混沌效应（早期对扰动敏感）和锥效应（后期功能轨迹受限），揭示了训练的两阶段特性。

Conclusion: 混沌效应和锥效应共同提供了深度学习从探索到优化的动态结构视角。

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [251] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo,Xi Lin,Mengyuan Zhong,Fei Liu,Zhenkun Wang,Jianyong Sun,Qingfu Zhang*

Main category: cs.LG

TL;DR: L2C-Insert是一种基于插入范式的学习型神经组合优化方法，用于解决车辆路径问题，通过灵活插入节点提升解的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于追加的神经组合优化方法在解决车辆路径问题时表现不佳，限制了灵活性。

Method: 提出L2C-Insert方法，通过插入节点构建解，包含模型架构、训练方案和推理技术。

Result: 在TSP和CVRP实验中，L2C-Insert表现优于传统方法。

Conclusion: L2C-Insert通过插入范式显著提升了神经组合优化的灵活性和解的质量。

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [252] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo,Yusheng Zhao,Xiao Luo,Zhiping Xiao,Wei Ju,Li Shen,Dacheng Tao,Ming Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为COUPLE的新方法，通过图扩散和渐进对齐解决无监督高效领域自适应检索中的噪声和特征对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效处理目标域中的噪声，且直接对齐高层特征导致检索性能不佳。

Method: 构建跨域关系图，利用噪声鲁棒的图流扩散模拟转移动态，并通过分层Mixup操作渐进对齐域。

Result: 实验证明COUPLE在竞争性基准上表现优异。

Conclusion: COUPLE通过噪声鲁棒和渐进对齐实现了高效的领域自适应哈希学习。

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [253] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种无需组标签的后处理框架ShortcutProbe，用于缓解深度学习模型中的虚假偏差问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常因学习目标与非关键特征间的虚假相关性而性能下降，现有方法依赖昂贵的组标签标注。

Method: 通过ShortcutProbe在潜在空间识别预测捷径，并重新训练模型以消除其影响。

Result: 理论分析和实验表明，该框架能有效提升模型对虚假偏差的鲁棒性。

Conclusion: ShortcutProbe是一种高效且实用的工具，适用于多种数据集。

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [254] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Main category: cs.LG

TL;DR: RLVR-World 提出了一种基于强化学习与可验证奖励的统一框架，直接优化世界模型的任务特定目标。


<details>
  <summary>Details</summary>
Motivation: 标准训练目标（如最大似然估计）与世界模型的任务目标（如准确性或感知质量）存在不一致，需要一种更直接的方法来优化这些指标。

Method: 利用强化学习与可验证奖励（RLVR），在自回归预测标记序列的基础上，通过解码预测的指标作为奖励来优化模型。

Result: 在文本游戏、网页导航和机器人操作等多个领域的语言和视频世界模型中，性能显著提升。

Conclusion: RLVR 不仅适用于推理语言模型，还为提升生成模型的实用性提供了一种有前景的后训练范式。

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [255] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur,Jasper Lee,George Tsoukalas,Meghana Sistla,Matthew Zhao,Stefan Zetzche,Greg Durrett,Yisong Yue,Swarat Chaudhuri*

Main category: cs.LG

TL;DR: ${\rm C{\small LEVER}}$是一个高质量、精心策划的基准测试，包含161个问题，用于Lean中的端到端验证代码生成。它避免了测试用例监督、LLM生成的注释以及泄露实现逻辑或允许空泛解决方案的规范。所有输出均通过Lean的类型检查器验证。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试存在测试用例监督、LLM生成的注释或规范泄露实现逻辑等问题，${\rm C{\small LEVER}}$旨在提供一个更严格、更可靠的验证代码生成基准。

Method: 每个问题包含生成匹配隐藏真实规范的规范任务，以及生成满足该规范的Lean实现任务。所有输出通过Lean的类型检查器验证。

Result: 评估了几种基于最先进语言模型的少样本和代理方法，这些方法均难以实现完全验证，表明这是一个具有挑战性的前沿基准。

Conclusion: ${\rm C{\small LEVER}}$为程序合成和形式推理提供了一个高质量的挑战性基准，所有代码和评估工具均已开源。

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [256] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen,Ziye Ma*

Main category: cs.LG

TL;DR: VAMO是一种结合一阶和零阶梯度方法的优化器，通过混合设计和方差缩减技术，实现了高效收敛和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模非凸优化问题在机器学习中常见，但现有方法如SVRG计算成本高，而零阶方法收敛慢。VAMO旨在解决这一问题。

Method: VAMO结合一阶小批量梯度和轻量级零阶有限差分探测，采用SVRG框架，并引入两点零阶估计器实现维度无关的收敛速率。

Result: VAMO的收敛速率为O(1/T + 1/b)，优于纯零阶方法和SGD，实验证明其在神经网络训练和LLM微调中表现优异。

Conclusion: VAMO提供了一种更快、更灵活的优化选择，适用于计算受限场景，显著提升了效率。

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [257] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen,Xunkai Li,Qi Zhang,Zhu Lei,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OGA是一个基于大语言模型的框架，通过结合语义和拓扑结构处理开放世界图中的未知类别节点，并支持模型更新。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界场景中处理数据不确定性不足，特别是在有限标注和未知类别节点方面。

Method: 提出OGA框架，结合自适应标签可追溯性和图标签标注器，整合语义和拓扑结构。

Result: 实验证明OGA在未知类别拒绝和模型更新方面有效且实用。

Conclusion: OGA为开放世界图学习提供了一种有效的解决方案。

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [258] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang,Yan Wang,Guanfeng Liu,Pengfei Ding,Huaxiong Wang,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: OPEN是一种新型的GNN解释器，解决了现有方法无法捕捉完整决策逻辑和依赖严格前提条件的问题，通过分区样本空间和学习不同分布下的决策逻辑，显著提升了性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法（XGNN）存在两大局限：无法捕捉完整决策逻辑和依赖严格前提条件，限制了其性能和泛化能力。

Method: 提出OPEN，通过分区样本空间为多个环境，并在每个环境中采样子图分析预测，学习GNN在不同分布下的决策逻辑，无需严格前提条件。

Result: 实验表明，OPEN能捕捉近乎完整的GNN决策逻辑，在保真度上优于现有方法，同时保持高效性，并增强了实际场景的鲁棒性。

Conclusion: OPEN是首个无需严格前提条件的GNN解释器，显著提升了决策逻辑的完整性和泛化能力。

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [259] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 提出了一种基于中国刑法的饱和机制量刑（SMS）模型，结合动量最小均方（MLMS）自适应算法，解决了传统端到端模型缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统司法量刑预测模型缺乏可解释性，无法满足学术研究和司法实践的需求。

Method: 提出SMS模型和MLMS算法，建立数学理论并构建CIBH数据集进行验证。

Result: 实验表明，该方法预测准确率接近理论上限，验证了模型和算法的有效性。

Conclusion: SMS模型和MLMS算法在保持高准确率的同时提供了法律可解释性，适用于司法实践。

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [260] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 本文首次在无数据分布假设的随机深度神经网络中，对对抗训练进行了理论分析，提出了基于平均场理论的新框架，并推导了对抗损失的紧上界。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练对对抗样本有效，但其训练动态尚未被充分理解。

Method: 引入基于平均场理论的新框架，推导了对抗损失的紧上界，并分析了网络结构和维度的影响。

Result: 证明无捷径的网络通常不可对抗训练，对抗训练会降低网络容量，但网络宽度可缓解此问题。

Conclusion: 输入和输出维度对权重方差的时间演化及上界有显著影响。

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [261] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu,Qian Li,Heng Yang,Yong Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedGraM的新方法，用于检测和移除联邦学习中的无目标攻击，通过Gram矩阵范数衡量模型性能，实验证明其优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）易受无目标攻击影响，现有防御方法在数据异构环境下效果有限，因此需要一种更有效的检测和移除攻击的方法。

Method: 提出FedGraM方法，利用辅助数据集计算本地模型的Gram矩阵范数，筛选并移除恶意模型，聚合剩余模型生成全局模型。

Result: 实验表明，FedGraM在有限数据下表现优异，显著优于现有防御方法。

Conclusion: FedGraM是一种高效且实用的防御无目标攻击的方法，适用于数据异构的联邦学习环境。

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [262] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li,Jian Yang,Yifan Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CPF的新方法，结合图级和节点级过滤策略，通过分区过滤提升图神经网络的适应性，解决了异构图分类问题。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络采用统一的图级过滤策略，难以处理异构图；而节点级过滤虽灵活但可能导致过拟合。论文旨在统一这两种策略，提供更全面的解决方案。

Method: 提出CPF方法，通过图粗化和特征聚类将节点分区，分别进行结构感知和特征感知的分区过滤。

Result: CPF在节点分类任务和实际图异常检测应用中表现出色，优于其他过滤范式。

Conclusion: CPF通过分区过滤有效结合了图级和节点级过滤的优势，为异构图处理提供了高效且实用的解决方案。

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [263] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Main category: cs.LG

TL;DR: ABCD是一种自适应推理扩展方法，通过双向扩散循环和动态计算分配提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的推理扩展方法通常采用固定的去噪计划，无法根据实例难度或任务需求动态调整计算资源。

Method: 提出ABCD框架，包含循环扩散搜索、自动探索-利用平衡和自适应思考时间三个组件，实现动态计算分配。

Result: 实验表明，ABCD在多种任务中提升性能的同时保持计算效率。

Conclusion: ABCD为扩散模型提供了一种灵活且高效的自适应推理扩展解决方案。

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [264] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini,Massimiliano Ghiotto,Edoardo Centofanti,Luca Franco Pavarino*

Main category: cs.LG

TL;DR: 研究探讨了傅里叶神经算子（FNO）在高维离子模型中的有效性，成功预测了多个经典模型的动态行为，并通过超参数调优验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 离子模型的刚性和多尺度非线性使其难以用神经网络逼近，研究旨在验证FNO在高维系统中的适用性。

Method: 使用FNO学习三种离子模型（FitzHugh-Nagumo、Hodgkin-Huxley、O'Hara-Rudy）的动态，并进行超参数调优（无约束和有约束场景）。

Result: FNO能准确捕捉高维动态，无约束架构训练效率更高，但两者精度相当。

Conclusion: FNO能有效处理高维多尺度动态系统，为复杂生物模拟提供了新工具。

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [265] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为“模型引导”的学习范式，通过参考模型指导目标模型的训练，并基于分布鲁棒优化（DRO）提出了理论驱动的框架DRRho风险最小化。理论分析和实验验证表明，该方法在泛化能力和数据效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型引导方法缺乏理论支持，导致性能不佳。本文旨在填补这一空白，提供理论框架和实践方法。

Method: 提出DRRho风险最小化框架，基于DRO理论，并结合对比学习与DRO的联系，设计了DRRho-CLIP方法。

Result: 理论分析证明了方法的优越性，实验显示DRRho-CLIP在扩展性和性能上优于无参考模型的CLIP及现有启发式方法。

Conclusion: 本文为模型引导提供了首个理论框架，并通过DRRho-CLIP验证了其有效性，推动了该领域的发展。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [266] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang,Hao Peng,Li Sun,Guanlin Wu,Chunyang Liu,Zhengtao Yu*

Main category: cs.LG

TL;DR: DeSE提出了一种结合深度结构熵的无监督图聚类框架，通过量化结构信息和深度神经网络改善稀疏或噪声图的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法（如GNNs、GATs）依赖原始图结构，稀疏或噪声边会导致性能下降，且传统聚类方法无法充分捕捉节点间结构关系。

Method: DeSE通过计算可微分结构熵（软分配）、设计结构学习层（SLL）生成属性图优化原始图，并基于GNN的聚类分配方法（ASS）学习嵌入和软分配矩阵。

Result: 在四个基准数据集上对比八种基线方法，DeSE在效果和可解释性上均表现出优越性。

Conclusion: DeSE通过深度结构熵和优化图结构，显著提升了无监督图聚类的性能。

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [267] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 通过对抗性预训练的Transformer模型可作为鲁棒基础模型，无需下游任务对抗训练，实现多任务鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 解决对抗训练计算成本高的问题，探索预训练模型在多任务中的鲁棒性。

Method: 利用对抗性预训练的Transformer模型，通过上下文学习实现多任务鲁棒泛化，无需参数更新。

Result: 模型能抵抗攻击并关注鲁棒特征，但存在准确性-鲁棒性权衡和上下文演示需求大的限制。

Conclusion: 对抗性预训练的Transformer在多任务中表现鲁棒，但需权衡准确性和鲁棒性，且依赖大量上下文演示。

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [268] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Cheng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为MTMC的新方法，通过最大化类令牌的流形容积来提升广义类别发现（GCD）的性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GCD方法在减少簇内差异时牺牲了流形容积，限制了类内表示的丰富性，MTMC旨在解决这一问题。

Method: MTMC利用奇异值的核范数作为流形容积的度量，确保样本表示的信息丰富且结构良好。

Result: 实验表明，MTMC在粗粒度和细粒度数据集上均优于现有GCD方法，提高了聚类准确性和类别数量估计。

Conclusion: MTMC通过更完整的表示、更好的类间可分性和减少维度崩溃，成为开放世界学习的重要组件。

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [269] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Main category: cs.LG

TL;DR: 研究发现，通过文本驱动的向量（如稀疏自编码器、均值漂移和线性探测）可以有效地指导多模态大语言模型（MLLMs）的行为，提升其在视觉任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型缺乏有效的指导方法，研究旨在探索是否可以通过文本驱动的向量来指导MLLMs的行为。

Method: 使用稀疏自编码器（SAEs）、均值漂移和线性探测方法，从文本驱动的LLM骨干中提取向量来指导MLLMs。

Result: 文本驱动的指导显著提升了MLLMs在多模态任务中的准确性，均值漂移在CV-Bench上提升了空间关系准确性7.3%和计数准确性3.3%。

Conclusion: 文本驱动的指导向量是一种高效且强大的方法，能够以最小的额外数据和计算开销提升MLLMs的准确性。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [270] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang,Peng Sun,Fengyuan Liu,Tao Lin*

Main category: cs.LG

TL;DR: 论文提出了一种数据为中心的新范式CoOpt，通过优化未标记数据提升深度学习训练的效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有模型为中心的方法存在知识锁定在模型参数中的问题，限制了知识的重用性和可扩展性。

Method: 提出CoOpt框架，通过并行化协作优化未标记数据，将知识编码到数据本身。

Result: 在Tiny-ImageNet和ImageNet-1K上分别实现了13.6%和6.8%的性能提升，训练速度分别提高了1.94倍和1.2倍。

Conclusion: CoOpt为数据优化提供了一种高效、可扩展且可持续的解决方案。

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [271] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian,Ali Mirzaei,Hossein Bagheri*

Main category: cs.LG

TL;DR: 研究探讨了伊朗野火风险的多重因素，结合气候条件和人类活动，利用遥感、GIS和机器学习技术，揭示了气候与人类因素对野火易感性的影响。


<details>
  <summary>Details</summary>
Motivation: 理解气候与人类活动如何共同影响伊朗的野火风险，为火灾管理提供科学依据。

Method: 采用遥感、GIS技术和机器学习算法，分析气候、地形和人类因素，并开发多场景数据采样策略。

Result: 气候因素（如土壤湿度、温度）和人类活动（如人口密度）显著影响野火易感性，且人类因素在季节性分析中更突出。

Conclusion: 研究通过高分辨率野火易感性地图，识别高风险区域，强调需制定有效的火灾管理策略。

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [272] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran,Emre Neftci,Willem. A. M. Wybo*

Main category: cs.LG

TL;DR: 论文提出了一种任务调制对比学习（TMCL）方法，受大脑新皮层的生物物理机制启发，通过无监督方式整合自上而下的信息，解决了机器学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 生物大脑能够持续从无标签数据流中学习，并整合稀疏标签信息而不损害泛化能力，而机器学习方法在此自然学习场景中容易发生灾难性遗忘。

Method: TMCL利用预测编码原理构建视图不变的表示空间，并通过对比损失实现。新类别的标签样本出现时，学习新的仿射调制以分离新类别，同时不影响前馈权重。

Result: 实验表明，TMCL在类增量学习和迁移学习中优于最先进的无监督方法和可比的有监督方法，仅需1%的标签。

Conclusion: 研究表明，自上而下的调制在平衡稳定性和可塑性中起关键作用。

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [273] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du,Jiaying Hu,Suyang Hou,Yueyang Ding,Xiaobo Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为SLAM的框架，用于测量空间标记的相似性，考虑了标签匹配、空间拓扑分布和不匹配标签的异质性影响，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量空间标记相似性时未能全面考虑标签匹配、空间拓扑分布和不匹配标签的异质性影响，因此需要一种更全面的方法。

Method: 将空间标记转换为基于位置组织、标签和属性的图，提取其属性分布，通过计算分布差异反映标记间的相似性。

Result: 提出的SLAM方法在模拟和真实空间转录组数据实验中表现优于其他评估指标。

Conclusion: SLAM提供了一种全面且准确的空间标记相似性评估方法，适用于科学研究和实践应用。

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [274] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: TTMM是一种通过模型合并扩展MoE模型专家数量的方法，显著降低测试时开销，性能接近TTT但速度快100倍。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型因训练和推理成本高而仅使用少量专家，限制了模型容量。

Method: 提出TTMM，通过模型合并扩展专家数量，避免测试时开销。

Result: TTMM性能随专家数量增加而提升，接近TTT，且速度快100倍。

Conclusion: TTMM为扩展测试时训练提供了一种高效且成本可控的方法。

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [275] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles,Nutan Chen,Patrick van der Smagt,Botond Cseke*

Main category: cs.LG

TL;DR: 提出了一种名为能量引导流匹配的新方法，用于增强流模型的训练，并在推理时无需额外引导。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，训练过程中引入引导的方法较少被探索，尤其是在强化学习等任务中，目标分布由数据和能量函数共同定义。

Method: 通过近似高斯路径学习条件速度场，提出FlowQ算法，基于能量引导流匹配进行离线强化学习。

Result: 方法在性能上具有竞争力，且策略训练时间与流采样步骤数无关。

Conclusion: 能量引导流匹配是一种有效的训练方法，适用于需要结合数据和能量函数的任务。

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [276] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei,Biao Mei,Junliang Lyu,Renquan Zhang,Feng Zhou,Yifan Sun*

Main category: cs.LG

TL;DR: FedWBA是一种新型的个性化贝叶斯联邦学习方法，通过粒子变分推断和Wasserstein重心聚合，解决了现有方法在参数假设和聚合方式上的局限性，提升了预测精度和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 现有PBFL方法存在参数假设限制和朴素参数聚合问题，FedWBA旨在通过非参数后验表示和几何意义更强的聚合方式改进这些问题。

Method: 在客户端使用粒子变分推断进行非参数后验表示，在服务器端引入基于Wasserstein重心的聚合方法。

Result: 理论证明FedWBA在局部和全局收敛性上有保障，实验显示其在预测精度、不确定性校准和收敛速度上优于基线方法。

Conclusion: FedWBA通过创新的局部推断和全局聚合方法，显著提升了PBFL的性能和鲁棒性。

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [277] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang,Weixin Bu,Zeyi Ren,Zhengwu Liu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: GraNT通过非参数教学视角优化GCN训练，显著减少训练时间并保持泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决图结构数据学习成本高的问题，提升图属性学习效率。

Method: 提出GraNT范式，通过示例选择非参数教学框架优化GCN训练。

Result: 训练时间显著减少（回归任务-36.62%，分类任务-38.19%等），泛化性能不变。

Conclusion: GraNT为图属性学习提供高效教学框架，理论与实验一致。

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [278] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Main category: cs.LG

TL;DR: 研究发现，大语言模型（LLMs）的安全对齐行为并非集中在特定几何子空间中，而是与模型的广泛学习动态高度纠缠，挑战了子空间防御的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索安全对齐是否对应于权重空间中的特定几何方向，以验证是否可以通过隔离或保护这些子空间来防止模型在微调时失去安全性。

Method: 通过参数空间和激活空间的综合实证研究，分析安全相关行为是否集中在特定子空间中，以及是否可以与通用学习分离。

Result: 研究发现，安全和不安全行为共享重叠的子空间和表征，没有证据表明存在选择性控制安全的子空间。

Conclusion: 安全对齐并非几何局部化，而是与模型的广泛学习动态纠缠，表明子空间防御策略可能面临根本性限制，需要其他策略来维持对齐。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [279] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding,Miao Qiao,Jiaxing Xu,Yiping Ke,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于Rényi度量的生成对抗网络（α-GAN），通过Rényi交叉熵定义价值函数，形成判别器与生成器的min-max问题。α-GAN在α=1时退化为普通GAN，实验表明α∈(0,1)时梯度放大，收敛更快，可能解决梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 现有GAN在梯度消失等问题上存在不足，希望通过引入Rényi度量改进GAN的性能。

Method: 使用Rényi交叉熵定义价值函数，形成判别器与生成器的min-max问题，优化在概率空间中进行。

Result: 实验表明，当α∈(0,1)时，梯度被指数级放大，收敛速度更快，可能解决梯度消失问题。

Conclusion: α-GAN在α∈(0,1)范围内表现优异，但该范围在现有Rényi版本GAN中尚未充分探索。

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [280] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: FlashAttention的改进版本FLASH-D通过简化数学表达和优化计算流程，显著降低了硬件实现中的面积和功耗，同时保持了原始FlashAttention的高效性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的注意力机制在计算时存在效率瓶颈，尤其是softmax操作和矩阵运算的交替处理导致计算速度下降。FlashAttention虽然通过在线softmax计算优化了这一问题，但仍有改进空间。

Method: FLASH-D重新设计了FlashAttention的核心计算流程，将softmax的除法操作隐藏在其他非线性函数中，实现了数值稳定的指数计算，并减少了计算成本，同时保留了原始FlashAttention的高效分块计算特性。

Result: 在28nm工艺的硬件实现中，FLASH-D相比现有并行硬件架构平均减少了22.8%的面积和20.3%的功耗，且性能无损失。

Conclusion: FLASH-D通过数学优化和硬件友好设计，显著提升了FlashAttention的效率和实用性，为Transformer模型的硬件加速提供了新思路。

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [281] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen,Shibo Feng,Xi Xiao,Zhong Zhang,Qing Li,Xingyu Gao,Peilin Zhao*

Main category: cs.LG

TL;DR: MSDformer提出了一种多尺度离散Transformer方法，解决了现有DTM方法无法捕捉多尺度时间模式及缺乏理论指导的问题，显著提升了时间序列生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有DTM方法在时间序列生成中无法捕捉多尺度时间模式且缺乏理论基础，限制了其性能。

Method: MSDformer通过多尺度时间序列标记器和多尺度自回归标记建模技术，在离散潜在空间中捕捉多尺度模式，并基于率失真定理验证其有效性。

Result: 实验表明MSDformer显著优于现有方法，多尺度信息建模显著提升了生成质量。

Conclusion: 多尺度信息和模式建模是提升DTM方法时间序列生成质量的关键。

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [282] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino,Franca Delmastro*

Main category: cs.LG

TL;DR: 论文探讨了移动传感器数据在mHealth领域中的应用挑战，提出通过生成对抗网络和扩散模型解决数据稀缺和隐私问题，并系统评估了现有生成模型在多模态、长期依赖和条件生成方面的表现。


<details>
  <summary>Details</summary>
Motivation: 移动传感器数据在mHealth领域潜力巨大，但数据收集受限，生成模型可解决数据稀缺和隐私问题。

Method: 系统评估了生成对抗网络和扩散模型在多模态、长期依赖和条件生成方面的能力，并提出了新的评估框架。

Result: 发现现有方法在跨模态一致性、时间连贯性和下游任务性能方面存在局限。

Conclusion: 未来研究方向是提升合成时间序列生成技术及其在mHealth中的应用。

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [283] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang,Yan Xia*

Main category: cs.LG

TL;DR: 论文提出了一种PID控制的张量轮分解（PTWD）模型，用于动态网络的链路预测，通过结合张量轮分解和PID控制原理，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 动态网络中的链路预测需要捕捉时空模式和权重动态，传统静态方法无法满足需求，张量方法虽有效但仍有改进空间。

Method: 采用张量轮分解（TWD）捕捉动态网络的潜在特征，并引入PID控制原理优化模型参数学习过程。

Result: 在四个真实数据集上的实验表明，PTWD模型比其他模型具有更准确的链路预测能力。

Conclusion: PTWD模型通过结合TWD和PID控制，显著提升了动态网络链路预测的准确性。

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [284] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer,Nicole Mücke,Dimitri Meunier,Arthur Gretton*

Main category: cs.LG

TL;DR: 本文研究了在噪声具有有限高阶矩的情况下，再生核希尔伯特空间中岭回归的性能，并基于积分算子框架建立了包含亚高斯和多项式项的过剩风险界。


<details>
  <summary>Details</summary>
Motivation: 探讨岭回归在存在有限高阶矩噪声时的性能，填补以往仅针对亚指数噪声的研究空白。

Method: 使用积分算子框架和Fuk-Nagaev不等式分析希尔伯特空间值随机变量。

Result: 在标准特征值衰减条件下，实现了最优收敛速率，证明了正则化最小二乘法对重尾噪声的渐近鲁棒性。

Conclusion: 岭回归在重尾噪声下仍能保持最优性能，扩展了其适用范围。

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [285] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila,Lidia Garrucho,Víctor M. Campello,Carlos Martín-Isla,Karim Lekadir*

Main category: cs.LG

TL;DR: 研究探讨了在非洲低资源环境中使用联邦学习（FL）进行结核病（TB）诊断的可行性，发现FL能解决隐私和数据稀缺问题，但面临基础设施和教育等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统集中式模型在低资源环境中因隐私和数据稀缺问题受限，FL提供了一种无需共享原始数据的协作训练方案。

Method: 研究在八个非洲国家的医院和研究中心进行，比较本地训练模型与跨机构联邦模型的性能。

Result: FL显示出在资源匮乏地区实现AI驱动医疗的潜力，但基础设施不足、网络不稳定等问题限制了其广泛应用。

Conclusion: FL在医疗领域有前景，但需改善基础设施、教育和法规支持以促进更广泛采用。

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [286] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko,Davide Bassetti,Lukáš Pospíšil*

Main category: cs.LG

TL;DR: 提出了一种快速熵近似方法（FEA），显著降低了计算成本并提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 香农熵（SE）及其梯度的奇异性导致计算成本高、鲁棒性低和收敛慢，需要一种更高效的近似方法。

Method: FEA是一种非奇异的有理近似方法，仅需5到6次基本计算操作。

Result: FEA的平均绝对误差为10^-3，计算速度提高50%，在机器学习特征提取中快2到3个数量级。

Conclusion: FEA在计算速度、精度和鲁棒性方面优于现有方法，适用于多种领域。

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [287] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson,Mathieu Blondel,Axel Parmentier*

Main category: cs.LG

TL;DR: 论文提出了一种基于理论的方法，将局部搜索启发式算法转化为可微分的组合层，解决了现有方法在依赖不精确求解器时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在依赖不精确求解器时缺乏理论保证或性能不足，尤其是在NP难问题中。

Method: 通过模拟退火和Metropolis-Hastings的联系，将局部搜索启发式算法转化为MCMC的提议分布，构建可微分的组合层和损失函数。

Result: 该方法显著降低了计算负担，并在大规模动态车辆路径问题中验证了有效性。

Conclusion: 提出的方法为学习不精确组合求解器提供了理论支持，并在实际应用中展示了高效性。

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [288] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud,Or Sheffet*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私二阶矩估计算法，适用于最坏情况输入，并在数据可子采样假设下实现强隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 解决差分隐私下二阶矩估计问题，特别是在输入数据存在异常值或最坏情况下的挑战。

Method: 基于数据可子采样性假设，提出递归算法框架，满足零集中差分隐私（zCDP），并保持高概率下的二阶矩估计精度。

Result: 算法在保持隐私的同时，能够高概率地准确估计二阶矩矩阵，即使输入中存在显著比例的异常值。

Conclusion: 该算法为差分隐私下的二阶矩估计提供了一种有效且鲁棒的解决方案。

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [289] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi,Domenico Borzacchiello,Philippe Le Bot,Nathan Lauzeral,Sebastien Comas-Cardona*

Main category: cs.LG

TL;DR: 提出一种结合序列编码与物理信息神经网络（PINN）的架构，用于实时适应变化的参数、边界条件和初始条件。


<details>
  <summary>Details</summary>
Motivation: 现有PINN与稀疏回归结合的方法在参数或条件变化时需要重新训练，限制了实时应用。

Method: 采用Deep Sets或序列编码器动态编码参数、边界和初始条件，作为PINN输入，实现自适应。

Result: 在Rossler ODE、2D Navier-Stokes和1D热监测问题中验证了模型的鲁棒性和泛化能力。

Conclusion: 该架构能有效适应动态变化，适用于多种实时应用场景。

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [290] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Main category: cs.LG

TL;DR: 论文提出了一种新的强化学习算法AAPO，通过动量增强的优势估计优化交叉熵损失，解决了现有组相对优势估计方法的训练效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于组相对优势估计的强化学习方法（如GRPO）在优势接近零时训练效率低下。

Method: 提出AAPO算法，利用动量增强的优势估计优化交叉熵损失。

Result: 在多个数学推理基准测试中，AAPO表现优于现有方法。

Conclusion: AAPO有效解决了训练效率问题，提升了强化学习在语言模型推理任务中的性能。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [291] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: X-KAN是一种结合Kolmogorov-Arnold Networks（KAN）和XCSF的新方法，通过局部模型优化解决复杂或不连续函数的逼近问题，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法因依赖单一全局模型而难以处理局部复杂或不连续函数。

Method: X-KAN通过XCSF框架优化多个局部KAN模型，将KAN的高表达力与XCSF的自适应分区能力结合。

Result: 实验表明，X-KAN在逼近精度上显著优于XCSF、多层感知机和KAN，且仅需少量规则（平均7.2±2.3条）。

Conclusion: X-KAN验证了在XCSF中使用KAN作为局部模型的有效性，其实现已开源。

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [292] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Main category: cs.LG

TL;DR: 本文提出了一种统一的量化感知训练（QAT）缩放定律，通过268次实验揭示了模型大小、训练数据量和量化粒度对量化误差的影响，并发现激活量化误差是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需要大量计算和内存资源，量化感知训练（QAT）通过降低模型精度来解决这一问题，但现有QAT缩放定律忽略了训练数据量和量化粒度等关键因素。

Method: 提出统一的QAT缩放定律，量化误差建模为模型大小、训练数据量和量化组大小的函数，并通过实验分解4位精度（W4A4）量化误差为权重和激活两部分。

Result: 量化误差随模型增大而减小，但随训练数据量增加和量化粒度变粗而上升；激活量化误差（尤其是FC2层的异常值）是主要瓶颈。通过混合精度量化可缓解此问题。

Conclusion: 研究为QAT的改进提供了关键见解，尤其是在处理权重和激活量化误差时需注意不同场景下的优先级。

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [293] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee,Moonjung Eo,Hye-Seung Cho,Dongmin Kim,Ye Seul Sim,Seoyoon Kim,Min-Kook Suh,Woohyung Lim*

Main category: cs.LG

TL;DR: MultiTab是一个针对表格学习算法的多维、数据感知评估框架，通过分类196个数据集并评估13种模型，揭示了模型性能对数据特征的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖平均指标，无法反映模型在不同数据特征下的行为差异。

Method: MultiTab分类数据集并评估多种模型，分析模型在不同数据特征（如样本量、标签不平衡等）下的表现。

Result: 模型性能高度依赖数据特征，例如样本级相似性模型在大样本或高特征相关性下表现更好。

Conclusion: MultiTab为模型设计和选择提供了更科学的依据，强调数据特征感知评估的重要性。

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [294] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Jack Stade,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 本文研究了ReLU神经网络的表达能力，特别是其深度。作者推翻了之前关于计算连续分段线性函数所需最小深度的猜想，并提出了更优的深度界限。


<details>
  <summary>Details</summary>
Motivation: 先前的研究认为计算所有连续分段线性函数需要特定深度的ReLU网络，但作者质疑这一结论并试图证明更小的深度即可实现。

Method: 通过证明ReLU网络在两层隐藏层时可以精确表示五个输入的最大值，并推广到更高维度的情况。

Result: 作者证明了计算所有连续分段线性函数所需的隐藏层深度为⌈log₃(n−1)⌉+1，优于之前的猜想。

Conclusion: 研究结果表明ReLU网络的表达能力比之前认为的更高效，为神经网络设计提供了新的理论支持。

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [295] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia,Shima Tabakhi,Vahid Seydi*

Main category: cs.LG

TL;DR: 提出了一种基于距离加权的半监督学习框架，通过优先处理关键训练样本提升分类性能，并在噪声和不平衡数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中标记数据有限的问题，同时提升模型在噪声和不平衡数据中的泛化能力。

Method: 采用基于距离的加权机制，结合不确定性一致性和图表示技术，优先处理接近测试数据的关键样本。

Result: 在12个基准数据集上显著提升了准确率、精确率和召回率，优于现有方法。

Conclusion: 该框架为半监督学习提供了实用且鲁棒的解决方案，适用于医疗和安全等领域。

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [296] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński,Emil Ryd,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 论文探讨了如何从语言模型中提取隐藏知识，通过训练一个Taboo模型（不直接提及秘密词），并评估了黑盒和解释性方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型变得更强大，确保其可信赖和可靠至关重要。初步证据显示模型可能试图欺骗或隐藏信息，因此需要探索提取隐藏知识的方法。

Method: 训练了一个Taboo模型（隐藏秘密词），并测试了黑盒方法和基于解释性技术（如logit lens和稀疏自编码器）的自动化策略。

Result: 两种方法在概念验证中均能有效提取秘密词。

Conclusion: 研究为提取语言模型中的隐藏知识提供了初步方法，并指出了未来改进方向，有助于模型的安全部署。

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [297] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen,Ilia Markov,Frank Zhengqing Wu,Ali Ramezani-Kebrya,Kimon Antonakopoulos,Dan Alistarh,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种适应深度神经网络异质性的分层量化框架，并应用于分布式变分不等式，开发了QODA算法，显著提升了训练速度。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络的异质性（如不同层结构、表示特性）影响预测性能，需开发适应性的量化方法。

Method: 开发分层量化框架，结合分布式变分不等式，提出QODA算法，采用自适应学习率。

Result: QODA在Wasserstein GAN训练中比基线快150%。

Conclusion: QODA算法通过分层量化有效提升训练效率，适用于异质性网络。

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [298] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama,Panos Ipeirotis*

Main category: cs.LG

TL;DR: 研究发现，在招聘中使用算法工具强制性别平衡的候选名单并不一定能提高最终录用的多样性，关键在于算法筛选标准与招聘经理评价标准的相关性。提出了一种新算法，显著提升了性别多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨算法工具在招聘中如何有效提升公平性和多样性，尤其是性别平衡的候选名单是否能转化为多样化的最终录用。

Method: 通过理论和实证分析，研究算法筛选标准与招聘经理评价标准的相关性对最终录用多样性的影响，并提出一种新算法。

Result: 强制性别平衡的候选名单对最终录用多样性改善有限，而新算法显著提升了性别多样性且未明显影响录用质量。

Conclusion: 算法设计对实现组织多样性目标至关重要，新算法为实践提供了可行指导。

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [299] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi,Gereon Weiss,Mario Trapp*

Main category: cs.LG

TL;DR: 提出了一种基于模糊逻辑的运行时监控器，用于ML感知组件，提供可解释的错误分析并提升系统安全性。


<details>
  <summary>Details</summary>
Motivation: 现有ML系统的运行时监控缺乏可解释性，难以确保安全性和可靠性。

Method: 设计了一种模糊逻辑监控器，用于解释感知组件的可靠性，并作为运行时安全监控器。

Result: 在自动驾驶案例中验证了监控器的可解释性，并识别了可靠的运行条件，同时提升了安全性。

Conclusion: 该监控器在安全性和可用性上优于现有方法，为ML系统提供了更强的安全保障。

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [300] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz,Marcel Kollovieh,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种基于模式的时序数据分词方法，通过合并样本和模式为标记，自适应压缩时序数据，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有时序分词方法固定样本数生成标记，对简单模式（如常数值）会产生过多标记，导致计算开销大。

Method: 基于频繁模式的离散词汇表，合并样本与底层模式为标记，并引入无需梯度计算的轻量级条件解码优化方法。

Result: 在时序基础模型上，标记化方法提升预测性能36%，效率提升1990%；条件解码进一步降低MSE达44%。

Conclusion: 该方法能自适应多样时序模式，泛化至未见数据，并生成捕捉统计矩和趋势等特征的标记表示。

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [301] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim,Félix Lefebvre,Gaëtan Brison,Alexandre Perez-Lebel,Gaël Varoquaux*

Main category: cs.LG

TL;DR: TARTE是一种表格基础模型，通过预训练将表格转换为知识增强的向量表示，提升下游任务性能，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有表格基础模型需要微调、计算成本高且难以复用的局限性。

Method: TARTE利用字符串捕捉表格语义，预训练大规模关系数据，生成知识增强的向量表示。

Result: TARTE提升了预测性能，优化了预测与计算的权衡，并支持领域特定表示。

Conclusion: TARTE为表格学习提供了一种有效的知识预训练方法。

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [302] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer,Hannes Leitgeb*

Main category: cs.LG

TL;DR: 提出一种基于数学哲学理论的神经网络可解释性方法，通过计算神经元的‘原因向量’来逻辑和贝叶斯地解释神经元及其组合。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中神经元的可解释性问题，尤其是多义性（一个神经元参与多个概念）的挑战。

Method: 为每个神经元计算‘原因向量’，并通过该向量评估其对特定命题的支持强度。

Result: 方法在理论和实证上均表现出哲学基础、普适性、可扩展性、忠实性、正确性、可训练性和实用性。

Conclusion: 该方法为神经网络提供了兼具逻辑和贝叶斯视角的可解释性工具，满足实际需求如鲁棒性和公平性。

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [303] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习和系统动力学的可解释神经系统动力学框架，旨在解决DL缺乏可解释性和SD可扩展性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习（DL）预测能力强但缺乏可解释性，系统动力学（SD）透明但可扩展性差，需结合两者优势。

Method: 开发了神经系统动力学管道，整合概念可解释性、机制可解释性和因果机器学习。

Result: 框架结合了DL的预测能力和SD的可解释性，实现了因果可靠性和可扩展性。

Conclusion: 通过实际应用验证框架有效性，长期目标是支持自主系统中可解释性和安全性的整合。

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [304] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: RefiDiff是一种创新的数据填补框架，结合局部机器学习预测和Mamba去噪网络，显著提升高维混合类型数据在MNAR机制下的填补性能。


<details>
  <summary>Details</summary>
Motivation: 高维混合类型数据在MNAR机制下的缺失值填补存在挑战，现有方法难以整合局部和全局特征。

Method: RefiDiff通过预精炼和后精炼步骤，结合Mamba去噪网络，统一编码混合类型数据为令牌，无需调整架构或超参数。

Result: RefiDiff在MNAR机制下表现优异，训练速度比SOTA方法快4倍，并在九种真实数据集上验证了其鲁棒性和可扩展性。

Conclusion: RefiDiff是一种高效、稳定的数据填补方法，适用于复杂缺失模式的高维混合类型数据。

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [305] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh,Sami Marouani,Ahmad Al Sheikh,Pham Tran Anh Quang,Amaury Habrard*

Main category: cs.LG

TL;DR: 使用Kolmogorov-Arnold Networks (KAN)实现可解释的强化学习，用于网络控制中的负载均衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在网络控制中缺乏可解释性，难以提取控制器方程。

Method: 采用PPO算法，结合1层KAN模型的Actor和MLP Critic网络，学习负载均衡策略。

Result: 能够从神经网络中提取控制器方程，提升网络性能并保持策略可解释性。

Conclusion: KAN在强化学习中提供了一种可解释且高效的网络控制解决方案。

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [306] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan,Wenxiong Chen,Mengfan Li,Wenqi Wei,Ling Liu*

Main category: cs.LG

TL;DR: 本文研究了图分析中的对抗攻击，提出了一个理论框架来证明临界对抗弹性状态的存在，并通过动态系统方法找到该状态。实验表明，该方法在多种数据集和攻击下优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究图对抗攻击的内在弹性状态及其存在性，填补现有防御方法的不足。

Method: 将图对抗学习建模为多目标动态系统，提出理论框架证明临界状态存在，并通过一维函数捕捉动态变化，求解系统平衡点。

Result: 在五个常用数据集和三种代表性攻击下，该方法显著优于现有防御方法。

Conclusion: 本文提出的方法能够有效识别图的临界对抗弹性状态，为防御图对抗攻击提供了新思路。

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [307] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui,Hao Wang,Hanfei Yu,Yitao Hu,Jianxun Li,Hao Wang*

Main category: cs.LG

TL;DR: ServerlessLoRA是一种针对LoRA LLM推理优化的新型无服务器系统，通过共享主干LLM、预加载LoRA工件和资源冲突管理，显著降低了TTFT和成本。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器计算在通用LLM推理中表现良好，但在LoRA推理中存在参数冗余、加载延迟和资源冲突等问题，导致GPU浪费和成本增加。

Method: ServerlessLoRA通过共享主干LLM、预加载LoRA工件以及采用资源冲突感知的批处理和卸载技术来优化性能。

Result: 实验表明，ServerlessLoRA将TTFT降低86%，成本减少89%。

Conclusion: ServerlessLoRA为LoRA LLM推理提供了高效、经济的解决方案，显著优于现有技术。

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [308] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou,Lorenzo Brigato,Vivien Streit,Amanda Hayoz,Stephan Proennecke,Stavros Athanasopoulos,Mikkel T. Olsen,Elizabeth J. den Brok,Cecilie H. Svensson,Konstantinos Makrilakis,Maria Xatzipsalti,Andriani Vazeou,Peter R. Mertens,Ulrik Pedersen-Bjergaard,Bastiaan E. de Galan,Stavroula Mougiakakou*

Main category: cs.LG

TL;DR: ABBA是一种基于强化学习的个性化胰岛素治疗建议方法，相比标准方法BBA，显著提高了T1D和T2D患者的血糖达标时间（TIR），并减少了高低血糖时间。


<details>
  <summary>Details</summary>
Motivation: 尽管胰岛素制剂和技术有所进步，但调整胰岛素剂量对大多数T1D和长期T2D患者仍具挑战性。

Method: 开发并评估了ABBA，一种基于强化学习的个性化胰岛素治疗建议方法，通过模拟测试（101名T1D和101名T2D患者）与标准BBA对比。

Result: ABBA显著提高了TIR，减少了高低血糖时间，且性能持续改善，而BBA仅表现轻微变化。

Conclusion: ABBA有望优化血糖控制，支持患者日常管理，值得进一步在人体试验中验证。

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [309] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu,Xiangyu Yue*

Main category: cs.LG

TL;DR: 提出了一种平衡性能与成本的中间策略，通过学习ODE积分来加速扩散模型推理，使用基于导数-积分关系的损失函数。


<details>
  <summary>Details</summary>
Motivation: 数值求解器在极小步长下表现不佳，而蒸馏技术常引入复杂性和不稳定性，需要一种折中方案。

Method: 通过蒙特卡洛积分和Picard迭代的启发，设计了一种名为“割线损失”的损失函数，逐步将切线延伸为割线。

Result: 在CIFAR-10上，EDM的割线版本实现了10步FID为2.14；在ImageNet-256×256上，SiT-XL/2的割线版本4步FID为2.27，8步FID为1.96。

Conclusion: 割线损失能快速将预训练扩散模型转换为割线版本，显著提升推理效率。

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [310] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek,George Whittle,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文研究了神经网络在训练分布外推时的行为，发现加入Layer Norm（LN）可以显著改善无限宽网络的稳定性。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络在训练分布外的行为，并探索如何通过结构改进（如加入LN）提升其外推稳定性。

Method: 应用神经切线核（NTK）理论分析无限宽网络，并通过实验验证有限宽度网络的表现。

Result: 加入LN的网络输出在训练分布外保持有界，而未加LN的网络可能产生极端输出。

Conclusion: LN能有效提升神经网络的外推稳定性，具有实际应用价值。

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [311] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu,Feng-Ting Liao,Meng-Hsi Chen,Pei-Chen Ho,Farhang Nabiei,Da-shan Shiu*

Main category: cs.LG

TL;DR: Latent Flow Transformer (LFT) 通过流匹配训练替代离散层块，显著压缩模型规模并保持性能，Flow Walking (FW) 算法进一步优化耦合保留。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer层数多且效率低，扩散和流模型在图像生成中表现优越，启发设计连续层替代方案。

Method: 提出LFT，用流匹配训练的传输算子替代层块；引入FW算法解决耦合保留问题。

Result: 在Pythia-410M上，LFT压缩6层性能优于跳过2层，FW进一步压缩12层且KL差距缩小。

Conclusion: LFT设计可行，FW显著缩小自回归与流生成范式差距。

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [312] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu,Chenyu Huang,Milad Roohi,Xin Zhong*

Main category: cs.LG

TL;DR: 论文提出了一种双流学习框架，结合气象数据和文本叙事，用于风灾预测，特别关注弱势社区。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统忽视社区脆弱性，无法满足本地化风险评估需求。

Method: 采用随机森林和RoBERTa的双流框架，通过后期融合机制整合数据。

Result: 实验显示性能显著优于传统方法，模型决策透明。

Conclusion: 框架兼具预测效果和实用价值，支持应急准备和社区韧性提升。

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [313] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Main category: cs.LG

TL;DR: KERL是一个结合食物知识图谱（KG）与大语言模型（LLM）的系统，用于个性化食物推荐和食谱生成，并提供营养信息。


<details>
  <summary>Details</summary>
Motivation: 现有研究在整合食物相关KG与LLM方面有限，KERL旨在填补这一空白，提供更全面的食物解决方案。

Method: KERL通过提取实体、检索KG子图，并利用LLM生成满足约束的食谱及其烹饪步骤和营养信息。

Result: 实验表明，KERL显著优于现有方法，提供完整的食物推荐、食谱生成和营养分析解决方案。

Conclusion: KERL为食物理解和推荐领域提供了高效且统一的系统，代码和数据集已公开。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [314] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo,Xinxin Fan,Quanliang Jing,Chi Lin,Mengfan Li,Yunfeng Lu,Yongjun Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Ising模型的通用且模型无关的触发器净化方法SifterNet，用于抵抗卷积神经网络和视觉Transformer大模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有触发器检测/移除方法通常需要预先了解目标模型的详细信息、大量干净样本甚至模型重新训练权限，这在实际应用中带来极大不便，尤其是在无法访问目标模型时。

Method: 通过利用Hopfield网络的记忆关联功能，提出了一种轻量级黑盒防御方法SifterNet，引入Ising模型的思想来净化输入样本中的触发器。

Result: 实验表明，SifterNet在触发器净化和高精度实现方面表现优异，显著优于现有基线方法。

Conclusion: SifterNet是一种有效的后门攻击防御方法，具有通用性和模型无关性。

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [315] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin,Nishad Tasnim,Md Omor Faruk,Zejian Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲神经网络（SNN）和强化学习的STRL算法，显著提升了能量效率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在强化学习中计算复杂度高、能耗大，限制了其在现实自主系统中的应用。SNN因其生物启发结构和高效能成为替代方案。

Method: 设计了基于多步LIF神经元和注意力机制的SNN，结合状态、动作和奖励编码，构建了类似Transformer的结构。

Result: 实验表明，STRL算法在策略性能上优于传统Transformer，同时显著提升了能量效率。

Conclusion: 该研究为复杂现实决策场景中部署生物启发、低成本的机器学习模型提供了新方向。

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [316] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen,Yulin Xie,Qi Xu,Gang Pan,Huajin Tang,Badong Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于时间注意力引导的自适应融合框架（TAAF），用于解决多模态脉冲神经网络（SNNs）中的模态不平衡和时间错位问题，通过动态分配重要性分数和自适应平衡融合损失，实现了高效的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态SNNs在高效感官处理方面潜力巨大，但面临模态不平衡和时间错位问题。现有方法无法协调不同模态的收敛速度，且融合机制静态，忽略了时间变化的跨模态交互。

Method: 提出TAAF模块，动态分配重要性分数；引入时间自适应平衡融合损失，根据注意力分数调整学习率；通过可学习的时间扭曲操作解决时间错位。

Result: 在CREMA-D、AVE和EAD数据集上分别达到77.55%、70.65%和97.5%的准确率，且具有高能效。

Conclusion: 该框架为神经形态系统中的时间一致性多模态学习提供了新范式，弥合了生物感官处理与高效机器智能之间的差距。

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [317] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta,Sina Khoshfetrat Pakazad,Henrik Ohlsson*

Main category: cs.LG

TL;DR: CHARM是一种用于多元时间序列的基础嵌入模型，通过学习共享、可迁移和领域感知的表示，解决了传统任务特定模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型通常是任务特定的，依赖数据集特定的训练和复杂的特征工程。尽管基于Transformer的架构提高了可扩展性，但时间序列的基础模型仍未被充分探索，且主要局限于预测任务。

Method: CHARM结合了架构创新，整合了通道级文本描述并保持对通道顺序的不变性。模型采用联合嵌入预测架构（JEPA），结合新的增强方案和损失函数，以提高可解释性和训练稳定性。

Result: CHARM（7M参数）在多种下游任务中实现了最先进的性能，为时间序列表示学习设立了新基准。

Conclusion: CHARM为时间序列基础模型的发展提供了重要贡献，展示了其在多样任务中的强大泛化能力。

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [318] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo,Shikai Fang,Binqing Wu,Qingsong Wen,Liang Sun*

Main category: cs.LG

TL;DR: PhyDL-NWP是一个结合物理方程与深度学习的天气预测框架，显著提升效率与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报方法计算量大且物理不完整，而深度学习模型虽高效但缺乏物理约束。

Method: 提出PhyDL-NWP框架，将物理方程与潜在力参数化融入数据驱动模型，通过自动微分计算物理项，并使用物理损失函数。

Result: 实现分辨率无关的降尺度，推理速度提升170倍，仅需55K参数，同时提升预测性能与物理一致性。

Conclusion: PhyDL-NWP在效率与物理一致性上均优于传统方法，为天气预测提供了新方向。

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [319] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha,Domini Jocema Leko Moutouo,Yae Ulrich Gaba*

Main category: cs.LG

TL;DR: 本文通过拓扑学基础研究强化学习（RL）中的状态、动作和策略空间结构，利用数学工具如Banach不动点定理解释RL算法的收敛性，并提出改进Bellman算子的方法以提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过数学理论（如Banach空间和不动点定理）为强化学习提供更坚实的理论基础，从而改进算法设计和性能。

Method: 方法包括回顾关键数学概念（如完全度量空间），利用Banach压缩原理分析RL算法的收敛性，并通过改进Bellman算子来优化算法。

Result: 结果表明，改进后的Bellman算子在标准RL环境（如MountainCar、CartPole和Acrobot）中提升了收敛速度和性能。

Conclusion: 结论指出，对RL的数学理解能够推动更高效的决策算法设计，为理论与实践架起桥梁。

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [320] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma,Landon Harris,Hairong Qi*

Main category: cs.LG

TL;DR: KIPPO结合Koopman算子理论与PPO，通过近似线性潜在空间表示提升策略学习效果，性能提升6-60%，稳定性提升91%。


<details>
  <summary>Details</summary>
Motivation: 复杂非线性动态环境中的策略学习存在梯度估计高方差和非凸优化问题，导致学习不稳定。

Method: 引入Koopman近似辅助网络，构建近似线性潜在空间表示，不改变核心策略或价值函数架构。

Result: 在连续控制任务中，性能提升6-60%，稳定性提升91%。

Conclusion: KIPPO通过线性化潜在空间显著提升了PPO的性能和稳定性。

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [321] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi,Nathaniel Bastian,Lance Fiondella,Gokhan Kul*

Main category: cs.LG

TL;DR: 论文研究了人工神经网络剪枝方法在网络安全数据集上的泛化能力，发现大多数方法表现不佳，仅有少数方法适用。


<details>
  <summary>Details</summary>
Motivation: 旨在通过剪枝方法减小神经网络模型规模或提升推理速度，同时保持预测能力。

Method: 分析了多种剪枝方法在不同剪枝程度下的表现，评估其在新型网络安全数据集上的适应性。

Result: 发现大多数剪枝方法泛化能力不足，仅少数方法表现良好。

Conclusion: 研究确定了最适合该任务的剪枝方法，并揭示了现有方法在泛化性上的局限性。

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [322] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [323] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen,Zahraa S Abdallah,Henry W J Reeve,Kate Robson Brown*

Main category: cs.LG

TL;DR: CSTS是一个用于评估多元时间序列数据中相关结构发现的合成基准，旨在解决聚类质量评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏验证的真实数据，研究者难以客观评估聚类质量或确定失败原因，CSTS提供了清晰的基准以区分问题来源。

Method: CSTS通过生成具有不同相关结构的合成数据，系统性地评估聚类算法和验证方法的表现。

Result: 实证表明CSTS能有效识别聚类失败的具体原因，如算法对非正态分布的敏感性。

Conclusion: CSTS为基于相关性的时间序列聚类提供了严格的评估标准，帮助诊断方法局限性。

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [324] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [325] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang,Chenyu Shi,Angel E. Rodriguez-Fernandez,Oliver Schütze*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大均值差异（MMD）的牛顿方法（MMDN）来解决连续多目标优化问题（MOPs），并通过与多目标进化算法（MOEA）的混合使用，显著提升了优化精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如Hausdorff距离）在测量Pareto前沿与参考集之间的距离时存在局限性，MMD作为一种概率分布距离度量方法，有望提供更优的解决方案。

Method: 将Pareto前沿和参考集视为经验测度，使用MMD测量其距离；推导MMD的梯度和Hessian矩阵，提出MMDN方法；结合MOEA进行混合优化。

Result: 在11个基准问题上测试表明，MMDN与MOEA的混合方法比单独使用MOEA具有更高的优化精度。

Conclusion: MMDN方法在理论分析和实际应用中均表现出色，为多目标优化问题提供了新的解决方案。

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [326] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi,Jason Hartford,Prudencio Tossou,Shawn Whitfield,Alisandra K. Denton,Cas Wognum,Kristina Ulicna,Jonathan Hsu,Michael Cuccarese,Emmanuel Bengio,Dominique Beaini,Christopher Gibson,Daniel Cohen,Berton Earnshaw*

Main category: cs.LG

TL;DR: 本文提出了一种开发虚拟细胞模型的愿景，旨在通过AI和高通量技术预测细胞对扰动的功能响应，以加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要可靠的模型模拟患者反应，以安全经济地测试治疗假设，但目前虚拟细胞模型尚未实现。

Method: 提出基于AI和高通量细胞分析的方法，设计治疗相关的虚拟细胞模型，并采用实验室循环验证。

Result: 虚拟细胞模型需准确预测细胞响应并解释其机制，同时提出生物基准指导开发。

Conclusion: 虚拟细胞模型为药物发现提供了新框架，并有望扩展到更高层次（如虚拟患者）。

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [327] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: 华为云用户使用LoRA（低秩适应）高效微调大语言模型（LLM），但复杂推理任务受基模型偏见干扰。提出CoLD解码框架，通过对比LoRA专家模型与基模型的概率分布差异优化解码，提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA微调LLM在复杂任务中因基模型偏见导致的通用响应问题，提升任务特定知识利用率。

Method: CoLD框架通过对比LoRA专家模型与基模型的概率分布差异评分候选词，优先选择与LoRA学习表示更匹配的词。优化华为Ascend NPU内核降低计算成本。

Result: CoLD提升任务准确率5.54%，端到端延迟降低28%。

Conclusion: CoLD为资源受限环境提供高效解码策略，适用于云和本地部署的数据科学应用。

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [328] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 论文分析了强化学习中验证器错误拒绝正确输出的问题（假阴性），并提出轻量级验证器tinyV以提升奖励信号的准确性。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖验证器提供的奖励信号，但假阴性问题（验证器错误拒绝正确答案）普遍存在，影响模型训练效果。

Method: 通过分析Big-Math-RL-Verified数据集，提出tinyV（轻量级LLM验证器），动态识别假阴性并恢复有效响应。

Result: tinyV在多个数学推理基准测试中提升通过率10%，并加速收敛。

Conclusion: 解决验证器假阴性问题对RL微调LLM至关重要，tinyV提供了一种实用方法。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [329] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada,Shion Matsumoto,Abdul Malik Zekri,Ankur Mali*

Main category: cs.LG

TL;DR: 论文提出了一个理论框架，将预测编码（PC）与最小描述长度（MDL）原则在深度网络中联系起来，证明了PC通过块坐标下降优化MDL目标，并提供了泛化保证和收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将生物启发的局部学习规则（PC）与MDL原则结合，为PC训练的深度模型提供理论支持，替代反向传播。

Method: 方法包括证明PC通过块坐标下降优化MDL目标，使用Hoeffding不等式和前缀码先验推导泛化界，并证明PC更新的收敛性。

Result: 结果表明PC能单调减少经验码长，提供比梯度下降更紧的风险界，并收敛到近似MDL最优解。

Conclusion: 结论是PC是一种理论可靠且生物合理的替代反向传播的方法，首次为PC训练的深度模型提供了泛化和收敛性保证。

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \hat{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [330] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama,Marcos Merino Prado,Alain García Olea,Koldo Gojenola Galletebeitia,Josu Goikoetxea Salutregi,Aitziber Atutxa Salazar*

Main category: cs.LG

TL;DR: 该研究通过结合结构化临床数据和自由文本数据，提出了一种LTM方法，用于预测房颤复发，其性能优于传统评分和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 房颤（AF）的高发病率和死亡率需要更准确的复发预测方法，传统评分如CHADS2-VASc等预测能力有限，且电子健康记录（EHR）数据可能存在错误和缺失。

Method: 研究结合结构化临床数据和自由文本出院报告，通过自然语言处理技术生成高质量数据集，并比较LTM方法与传统评分及机器学习模型的性能。

Result: LTM方法在预测房颤复发方面表现最佳，同时揭示了性别和年龄的预测偏差。

Conclusion: 结合结构化与非结构化数据可提升数据集质量，LTM模型在预测房颤复发方面优于传统方法，展现了机器学习潜力。

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [331] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur,Lav Gupta*

Main category: cs.LG

TL;DR: 本文探讨了在6G医疗环境中使用可解释AI技术（如SHAP、LIME和DiCE）来增强安全性和透明度的研究。


<details>
  <summary>Details</summary>
Motivation: 随着医疗系统采用无线网络和物联网设备，安全问题变得至关重要，尤其是6G技术的引入带来了新的风险。

Method: 通过可解释AI技术（SHAP、LIME、DiCE）分析漏洞并加强防御。

Result: 实验分析展示了该方法在提升安全性和透明度方面的潜力。

Conclusion: 可解释AI技术为6G医疗环境的安全性和信任提供了有效解决方案。

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [332] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro,Andrei Panferov,Soroush Tabesh,Oliver Sieberling,Jiale Chen,Mahdi Nikdan,Saleh Ashkboos,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出Quartet方法，支持全FP4精度训练，解决了低精度训练中的精度损失问题，并在NVIDIA Blackwell GPU上实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）计算需求激增，低精度训练（如FP4）成为提升效率和能耗的关键，但现有方法存在精度损失问题。

Method: 提出Quartet方法，系统研究硬件支持的FP4训练，通过低精度缩放定律优化计算，并实现基于CUDA内核的高效训练。

Result: 在Llama类模型上验证，Quartet实现了FP4精度的SOTA性能，成功训练十亿级模型。

Conclusion: Quartet证明全FP4训练是标准精度和FP8训练的有力替代方案。

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [333] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Main category: q-bio.QM

TL;DR: 提出了一种自动化发现生物医学数据中有趣假设的流程，结合机器学习、知识图谱、文献搜索和大语言模型，定义了“有趣性”为新颖性、实用性和合理性的组合。


<details>
  <summary>Details</summary>
Motivation: 科学发现的核心是寻找有趣现象，但这一过程通常是手动且定义模糊的。

Method: 结合机器学习、知识图谱、文献搜索和大语言模型，构建自动化流程。

Result: 在8种主要疾病上，流程能提前发现风险因素，40-53%的候选假设被验证为有趣，远高于基线方法。

Conclusion: 该流程解决了“有趣性”可扩展操作化的挑战，适用于任何目标。

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [334] [ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets](https://arxiv.org/abs/2505.14016)
*Shunjing Zhao,Xian Shi,Hanlun Lei*

Main category: astro-ph.EP

TL;DR: ThermoONet是一种基于机器学习的神经网络，用于高效预测彗星温度和冰升华通量，显著降低计算时间并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统彗星热物理模型计算成本高，限制了高分辨率或重复建模的研究需求。

Method: 采用机器学习方法开发ThermoONet，预测彗星温度和冰升华通量，并与数值模拟结果对比验证。

Result: ThermoONet平均误差约2%，计算时间减少近六个数量级，成功应用于67P和21P彗星的水活动建模。

Conclusion: ThermoONet高效且准确，结合全局优化算法可反演目标天体的物理性质。

Abstract: Cometary activity is a compelling subject of study, with thermophysical
models playing a pivotal role in its understanding. However, traditional
numerical solutions for small body thermophysical models are computationally
intensive, posing challenges for investigations requiring high-resolution or
repetitive modeling. To address this limitation, we employed a machine learning
approach to develop ThermoONet - a neural network designed to predict the
temperature and water ice sublimation flux of comets. Performance evaluations
indicate that ThermoONet achieves a low average error in subsurface temperature
of approximately 2% relative to the numerical simulation, while reducing
computational time by nearly six orders of magnitude. We applied ThermoONet to
model the water activity of comets 67P/Churyumov-Gerasimenko and
21P/Giacobini-Zinner. By successfully fitting the water production rate curves
of these comets, as obtained by the Rosetta mission and the SOHO telescope,
respectively, we demonstrate the network's effectiveness and efficiency.
Furthermore, when combined with a global optimization algorithm, ThermoONet
proves capable of retrieving the physical properties of target bodies.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [335] [Autonomous nanoparticle synthesis by design](https://arxiv.org/abs/2505.13571)
*Andy S. Anker,Jonas H. Jensen,Miguel Gonzalez-Duque,Rodrigo Moreno,Aleksandra Smolska,Mikkel Juelsholt,Vincent Hardion,Mads R. V. Jorgensen,Andres Faina,Jonathan Quinson,Kasper Stoy,Tejs Vegge*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种自主设计合成原子尺度结构的方法，通过实时匹配实验数据与模拟目标模式，成功合成了两种不同结构的金纳米颗粒。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成依赖试错法，纳米颗粒的原子排列对其性质至关重要，但合成参数复杂，亟需自动化方法。

Method: 利用实时实验总散射和配对分布函数数据，匹配模拟目标模式，自主设计合成协议，无需先验知识。

Result: 在同步辐射实验中成功合成了5 nm十面体和10 nm面心立方结构的金纳米颗粒。

Conclusion: ScatterLab为原子结构定向合成提供了通用框架，可能彻底改变材料设计方式。

Abstract: Controlled synthesis of materials with specified atomic structures underpins
technological advances yet remains reliant on iterative, trial-and-error
approaches. Nanoparticles (NPs), whose atomic arrangement dictates their
emergent properties, are particularly challenging to synthesise due to numerous
tunable parameters. Here, we introduce an autonomous approach explicitly
targeting synthesis of atomic-scale structures. Our method autonomously designs
synthesis protocols by matching real time experimental total scattering (TS)
and pair distribution function (PDF) data to simulated target patterns, without
requiring prior synthesis knowledge. We demonstrate this capability at a
synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm
decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a
simulated target scattering pattern, thus representing a bespoke atomic
structure, and obtaining both the synthesised material and its reproducible
synthesis protocol on demand may revolutionise materials design. Thus,
ScatterLab provides a generalisable blueprint for autonomous, atomic
structure-targeted synthesis across diverse systems and applications.

</details>


### [336] [Path-integral molecular dynamics with actively-trained and universal machine learning force fields](https://arxiv.org/abs/2505.14245)
*A. A. Solovykh,N. E. Rybin,I. S. Novikov,A. V. Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种将机器学习势函数（MTP）与路径积分分子动力学（PIMD）结合的方法，以高效准确地研究核量子效应对材料性质的影响。


<details>
  <summary>Details</summary>
Motivation: 核量子效应（NQEs）在有限温度下显著影响材料性质，但传统方法（如经验势或量子力学计算）在精度或效率上存在不足。

Method: 开发了一个接口，将MLIP-2软件包中的MTP与i-PI软件包的PIMD计算结合，用于主动学习势函数并研究NQEs对LiH和Si材料性质的影响。

Result: MTP-PIMD方法在预测晶格参数、热膨胀系数和径向分布函数方面表现出高准确性，与实验数据和其他方法（如准谐近似和MatterSim）一致。

Conclusion: MTP-PIMD方法为高效准确地研究核量子效应提供了有效解决方案，展示了机器学习势函数在材料模拟中的潜力。

Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter
material properties at finite temperatures. Atomic modeling using the
path-integral molecular dynamics (PIMD) method can fully account for such
effects, but requires computationally efficient and accurate models of
interatomic interactions. Empirical potentials are fast but may lack sufficient
accuracy, whereas quantum-mechanical calculations are highly accurate but
computationally expensive. Machine-learned interatomic potentials offer a
solution to this challenge, providing near-quantum-mechanical accuracy while
maintaining high computational efficiency compared to density functional theory
(DFT) calculations. In this context, an interface was developed to integrate
moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD
calculations using the i-PI software package. This interface was then applied
to active learning of potentials and to investigate the influence of NQEs on
material properties, namely the temperature dependence of lattice parameters
and thermal expansion coefficients, as well as radial distribution functions,
for lithium hydride (LiH) and silicon (Si) systems. The results were compared
with experimental data, quasi-harmonic approximation calculations, and
predictions from the universal machine learning force field MatterSim. These
comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD
approach.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [337] [Functional bottlenecks can emerge from non-epistatic underlying traits](https://arxiv.org/abs/2505.14166)
*Anna Ottavia Schulte,Samar Alqatari,Saverio Rossi,Francesco Zamponi*

Main category: q-bio.PE

TL;DR: 论文研究了蛋白质适应性景观中的上位性现象，特别是全局上位性如何导致功能性瓶颈的形成。


<details>
  <summary>Details</summary>
Motivation: 探索上位性（尤其是全局上位性）如何增加适应性景观的复杂性，并导致功能性瓶颈的出现。

Method: 提出并分析了一个基于全局上位性的简单模型，其中适应性是一个潜在加性性状的非线性函数。

Result: 研究表明，如果模型校准得当，功能性瓶颈会以高概率出现，且需要足够的突变效应异质性。

Conclusion: 模型与实验数据一致，尤其是在较小的组合突变空间中，强调了突变效应异质性对功能性瓶颈形成的重要性。

Abstract: Protein fitness landscapes frequently exhibit epistasis, where the effect of
a mutation depends on the genetic context in which it occurs, \textit{i.e.},
the rest of the protein sequence. Epistasis increases landscape complexity,
often resulting in multiple fitness peaks. In its simplest form, known as
global epistasis, fitness is modeled as a non-linear function of an underlying
additive trait. In contrast, more complex epistasis arises from a network of
(pairwise or many-body) interactions between residues, which cannot be removed
by a single non-linear transformation. Recent studies have explored how global
and network epistasis contribute to the emergence of functional bottlenecks -
fitness landscape topologies where two broad high-fitness basins, representing
distinct phenotypes, are separated by a bottleneck that can only be crossed via
one or a few mutational paths. Here, we introduce and analyze a simple model of
global epistasis with an additive underlying trait. We demonstrate that
functional bottlenecks arise with high probability if the model is properly
calibrated. Our results underscore the necessity of sufficient heterogeneity in
the mutational effects selected by evolution for the emergence of functional
bottlenecks. Moreover, we show that the model agrees with experimental
findings, at least in small enough combinatorial mutational spaces.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [338] [Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials](https://arxiv.org/abs/2505.13494)
*Ying Zhao,Guanhua Chen,Jie Liu*

Main category: cond-mat.soft

TL;DR: 论文探讨了聚合物科学中数据碎片化问题及其对能源技术发展的阻碍，提出了通过技术、协作和文化变革解决这些挑战的方法。


<details>
  <summary>Details</summary>
Motivation: 聚合物科学中数据碎片化问题阻碍了能源技术的发展，亟需解决数据互操作性和标准化问题。

Method: 采用自然语言处理（NLP）工具提取结构化数据，结合高通量机器人平台生成一致数据集，并推广FAIR原则。

Result: 通过技术革新和协作治理，有望解决数据碎片化问题，加速聚合物材料的发现。

Conclusion: 通过技术创新、协作治理和文化变革，聚合物科学可以克服数据瓶颈，推动能源技术发展。

Abstract: The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [339] [Modelling Real-time Systems with Bigraphs](https://arxiv.org/abs/2505.13449)
*Maram Albalwe,Blair Archibald,Michele Sevegnani*

Main category: cs.LO

TL;DR: 论文扩展了Bigraphical Reactive Systems（BRSs）以支持实时系统，通过多视角建模数字时钟，并利用Action BRSs将过渡系统表示为马尔可夫决策过程（MDP）。


<details>
  <summary>Details</summary>
Motivation: 尽管BRSs在通信协议、代理编程、生物学和安全等领域有广泛应用，但缺乏对实时系统的支持。

Method: 采用多视角建模数字时钟，利用Action BRSs将过渡系统表示为MDP，实现自然表示系统状态中的选择（允许时间流逝或执行特定动作）。

Result: 通过BigraphER工具包实现，并以云系统请求建模为例验证了方法的有效性。

Conclusion: 扩展的BRSs成功支持实时系统建模，为相关领域提供了新的工具和方法。

Abstract: Bigraphical Reactive Systems (BRSs) are a graph-rewriting formalism
describing systems evolving in two dimensions: spatially, e.g. a person in a
room, and non-spatially, e.g. mobile phones communicating regardless of
location. Despite use in domains including communication protocols, agent
programming, biology, and security, there is no support for real-time systems.
We extend BRSs to support real-time systems with a modelling approach that uses
multiple perspectives to represent digital clocks. We use Action BRSs, a recent
extension of BRSs, where the resulting transition system is a Markov Decision
Process (MDP). This allows a natural representation of the choices in each
system state: to either allow time to pass or perform a specific action. We
implement our proposed approach using the BigraphER toolkit, and demonstrate
the effectiveness through multiple examples including modelling cloud system
requests.

</details>


### [340] [ProofBuddy: How it Started, How it's Going](https://arxiv.org/abs/2505.13474)
*Nadine Karsten,Kim Jana Eiken,Uwe Nestmann*

Main category: cs.LO

TL;DR: ProofBuddy是一个基于Isabelle证明助手的网络应用，用于教学和学习证明。相比桌面应用，它在简洁性、可维护性和可定制性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何更好地在教学中使用Isabelle证明助手，并通过开发ProofBuddy优化教学体验。

Method: 采用教育设计研究方法，通过一系列实验和评估，开发并优化ProofBuddy。

Result: ProofBuddy作为网络应用，在简洁性、可维护性和可定制性方面优于桌面应用，尤其是其交互式教程功能。

Conclusion: ProofBuddy为教学和学习证明提供了更高效的工具，展示了网络应用在教育中的潜力。

Abstract: We report on our journey to develop ProofBuddy, a web application that is
powered by a server-side instance of the proof assistant Isabelle, for the
teaching and learning of proofs and proving. The journey started from an
attempt to use just Isabelle in an educational context. Along the way,
following the educational design research approach with a series of experiments
and their evaluations, we observed that a web application like \ProofBuddy has
many advantages over a desktop application, for developers and teachers as well
as for students. In summary, the advantages cover simplicity, maintainability
and customizability. We particularly highlight the latter by exhibiting the
potential of interactive tutorials and their implementation within ProofBuddy.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [341] [EuLearn: A 3D database for learning Euler characteristics](https://arxiv.org/abs/2505.13539)
*Rodrigo Fritz,Pablo Suárez-Serrato,Victor Mijangos,Anayanzi D. Martinez-Hernandez,Eduardo Ivan Velazquez Richards*

Main category: cs.CG

TL;DR: EuLearn是首个公平表示多种拓扑类型的表面数据集，通过随机结设计均匀变化的曲面，支持机器学习系统识别拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 为机器学习系统提供能够识别拓扑特征的训练数据，解决现有方法在拓扑分类上的性能不足问题。

Method: 设计基于随机结的曲面数据集，开发非欧几里得统计采样方法，并改进PointNet和Transformer架构以适应拓扑数据。

Result: 实验表明，将拓扑信息融入深度学习流程显著提升了在EuLearn数据集上的性能。

Conclusion: EuLearn数据集及改进方法为拓扑特征的机器学习研究提供了新工具和方向。

Abstract: We present EuLearn, the first surface datasets equitably representing a
diversity of topological types. We designed our embedded surfaces of uniformly
varying genera relying on random knots, thus allowing our surfaces to knot with
themselves. EuLearn contributes new topological datasets of meshes, point
clouds, and scalar fields in 3D. We aim to facilitate the training of machine
learning systems that can discern topological features. We experimented with
specific emblematic 3D neural network architectures, finding that their vanilla
implementations perform poorly on genus classification. To enhance performance,
we developed a novel, non-Euclidean, statistical sampling method adapted to
graph and manifold data. We also introduce adjacency-informed adaptations of
PointNet and Transformer architectures that rely on our non-Euclidean sampling
strategy. Our results demonstrate that incorporating topological information
into deep learning workflows significantly improves performance on these
otherwise challenging EuLearn datasets.

</details>


### [342] [Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks](https://arxiv.org/abs/2505.14417)
*Menglin Yang,Yifei Zhang,Jialin Chen,Melanie Weber,Rex Ying*

Main category: cs.CG

TL;DR: 论文探讨了在基础模型和大型语言模型时代，非欧几里得几何学习的潜力及其在Web应用中的优势。


<details>
  <summary>Details</summary>
Motivation: 欧几里得空间在机器学习架构中存在根本性限制，而非欧几里得空间能更高效地表示复杂数据关系。

Method: 研究非欧几里得空间（如双曲、球面和混合曲率空间）在基础模型中的应用。

Result: 非欧几里得几何能提升模型对数据结构的捕捉能力，优化搜索、推荐和内容理解性能。

Conclusion: 非欧几里得基础模型与几何学习的结合具有巨大潜力，未来需进一步探索其挑战和发展方向。

Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [343] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr,Mehdi Ghatee,Mohammad Amin Seifi,Javad Fazli,Sajed Tavakoli,Zahra Rafei,Shervin Ghaffari,Abolfazl Nikahd,Mahdi Razi Gandomani,Alireza Orouji,Ramtin Mahmoudi Kashani,Sarina Heshmati,Negin Sadat Mousavi*

Main category: stat.ML

TL;DR: 该论文综述了机器学习中不平衡数据的处理策略，包括过采样、欠采样、生成模型等方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据导致模型预测偏差和准确性下降，需要有效的重采样技术来解决这一问题。

Method: 分类和综述了多种数据平衡方法，包括合成过采样、自适应技术、生成模型、集成策略、混合方法、欠采样和邻域方法。

Result: 总结了当前重采样技术的发展，并通过案例验证了其有效性。

Conclusion: 提出了未来在不平衡数据领域的潜在研究方向。

Abstract: Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [344] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai,Yiheng Yao,Guangji Bai,Renhe Jiang,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: stat.ML

TL;DR: 论文提出了一种连续域泛化（CDG）任务，通过几何和代数理论框架，引入神经Lie传输算子（NeuralLTO）来建模低维流形上的参数变化，显著提升了模型在复杂多维数据分布下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据分布通常沿多个潜在因素（如时间、地理、社会经济背景）连续变化，而现有域泛化方法通常将域视为离散或单一维度变化，无法捕捉真实世界变化的复杂性。

Method: 提出基于几何和代数理论的框架，引入神经Lie传输算子（NeuralLTO）建模低维流形上的参数变化，并设计门控机制和局部图表策略以处理噪声或不完整的域描述符。

Result: 在合成和真实数据集（如遥感、科学文献和交通预测）上的实验表明，该方法在泛化准确性和对描述符缺陷的鲁棒性上显著优于现有基线。

Conclusion: CDG任务和NeuralLTO框架为解决复杂多维数据分布的泛化问题提供了有效途径，展示了在噪声或不完整描述符下的优越性能。

Abstract: Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [345] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

Main category: stat.ML

TL;DR: 本文提出了一种名为Competitive Co-evolutionary Bandit Learning (CoEBL)的新算法，将进化算法与多臂老虎机框架结合，首次在矩阵游戏中实现了随机乐观策略的理论分析，并证明其具有次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索矩阵游戏中随机乐观策略的潜力，填补现有研究中仅关注确定性乐观策略的理论空白。

Method: 提出CoEBL算法，通过进化算法的变异操作实现随机乐观策略，并证明其能实现次线性遗憾。

Result: 实验表明，CoEBL不仅实现了次线性遗憾，还在多个矩阵游戏基准测试中优于经典老虎机算法。

Conclusion: 结论指出，进化老虎机学习在博弈论场景中具有潜力，尤其是通过进化算法实现的随机乐观策略表现出色。

Abstract: Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [346] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud,Valentin De Bortoli,Arthur Leclaire,Nicolas Papadakis*

Main category: stat.ML

TL;DR: 本文研究了非凸势能分布采样问题，证明了离散时间ULA在势能强凸假设下的稳定性，并首次证明了PSGLA在非凸势能下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决非凸和非光滑势能分布采样问题，尤其是在成像逆问题中。

Method: 结合前向后向优化算法与ULA步骤的PSGLA方法。

Result: 证明了PSGLA在非凸势能下的收敛性，并在合成数据和成像逆问题中验证了其快速收敛性。

Conclusion: PSGLA在保持恢复性能的同时，比SGLA具有更快的收敛速度。

Abstract: We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [347] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang,Joseph M. Lukens,Sanjaya Lohani,Brian T. Kirby,Thomas A. Searles,Xin Qiu,Kody J. H. Law*

Main category: stat.ML

TL;DR: SBMC是一种新的贝叶斯蒙特卡洛方法，介于点估计和后验之间，通过并行实现SMC或MCMC算法，在MNIST、CIFAR和IMDb等任务中表现优异，与SOTA方法相当，且不确定性量化更优。


<details>
  <summary>Details</summary>
Motivation: 提出SBMC方法，旨在结合点估计和后验的优点，通过并行实现提升贝叶斯深度学习的效率和准确性。

Method: 采用并行实现的SMC或MCMC算法，理论支持其一致性，并在MNIST、CIFAR和IMDb等数据集上进行验证。

Result: 并行实现与串行实现性能相当，准确性达到或超过SOTA方法（如深度集成），不确定性量化（尤其是认知不确定性）显著改善。

Conclusion: SBMC在性能和成本上与SOTA方法相当，同时提供更优的不确定性量化，但并行实现时间成本较高，需权衡压缩时间与准确性。

Abstract: This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [348] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: 提出了一种名为“反向共形预测”的方法，通过灵活控制预测集大小来保证共形覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测固定覆盖水平但允许预测集大小变化，而新方法旨在通过约束预测集大小行为来适应覆盖水平，特别适用于需要小预测集的应用（如医学诊断）。

Method: 基于Gauthier等人的e值后验有效性结果和新的留一估计器，确保理论保证在实践中可计算。

Result: 理论分析和实证结果表明，该方法在保持可计算覆盖保证的同时，实现了可解释且可控的预测集大小。

Conclusion: 反向共形预测在需要灵活控制预测集大小的应用中具有实用价值，同时保持了理论保证。

Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [349] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 提出了一种生成模型，结合了社交网络中的枢纽和密集社区结构，通过图混合模型生成稀疏和密集图序列，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交网络中通常存在少量大型枢纽和大量小型密集社区，现有模型难以同时捕捉这两种结构。

Method: 基于线图上的图混合模型，提出了一种新的稀疏图条件（最大度数），用于识别枢纽。

Result: 理论上证明了可以估计枢纽的归一化度数和稀疏图成分的图混合模型，实验验证了模型的有效性。

Conclusion: 通过显式建模稀疏图，模型在合成数据、引用图和社交网络中表现出优势。

Abstract: Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [350] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi,Takuro Kutsuna,Sawa Takamuku*

Main category: stat.ML

TL;DR: 论文探讨了统计学习中常规模型与奇异模型的区分，提出了WAIC和WBIC的渐近关系，为奇异模型的选择提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统信息准则（如AIC和BIC）在奇异模型中不适用，因此需要研究WAIC和WBIC的关系以提高模型选择效率。

Method: 通过理论推导，建立了WAIC和WBIC之间的渐近方程，揭示了它们的结构关系。

Result: 得到了WAIC在WBIC后验分布下的渐近无偏表达式，阐明了两种准则的渐近行为。

Conclusion: 研究为奇异模型选择的计算效率提供了理论基础，并加深了对WAIC和WBIC的理解。

Abstract: In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [351] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu,Hengzhi He,Guang Cheng*

Main category: stat.ML

TL;DR: 论文研究了递归参数模型训练中的模型崩溃现象，从概率角度分析了其发生条件及缓解方法，提出样本量需超线性增长以避免崩溃，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 近年来，模型崩溃成为语言模型训练中的关键问题，需理解其驱动机制及缓解方法。

Method: 将递归训练过程建模为模型估计的随机游走，分析样本量对步长的影响及估计过程的偏差，提出样本量需超线性增长的条件。

Result: 理论表明样本量需超线性增长以避免崩溃，且在估计偏差较大时需更快增长；实验验证了理论结果。

Conclusion: 通过概率框架和实验验证，论文为预防模型崩溃提供了理论依据和实用指导。

Abstract: In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [352] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia,Arnaud Mavakala Watusadisi,Ernesto De Vito,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 论文研究了在协变量偏移下，通过随机投影在RKHS中实现计算效率与统计准确性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 协变量偏移在监督学习中带来挑战，而核方法虽具统计优势但计算成本高，因此需探索高效方法。

Method: 使用随机投影，在给定RKHS中构建随机子空间作为假设空间。

Result: 即使在协变量偏移下，也能显著节省计算资源且不影响学习性能。

Conclusion: 随机投影是一种有效平衡计算效率与统计性能的方法。

Abstract: This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [353] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki,Junpei Komiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 论文研究了高维特征空间下的核化上下文赌博机问题，提出了一种在特征维度增长时仍能实现无遗憾学习的方法，并分析了宽松遗憾的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在高维特征空间（如高斯核）中表现不佳的问题，为个性化在线广告和推荐系统等决策场景提供更灵活的建模框架。

Method: 引入对上下文分布的随机假设，证明即使特征维度增长到样本数量级，仍可实现无遗憾学习，并分析宽松遗憾的收敛速率。

Result: 在随机假设下，实现了无遗憾学习，并推导出宽松遗憾的收敛速率与Δ的关系。

Conclusion: 该方法在高维特征空间中具有更好的理论保证和实际应用潜力。

Abstract: We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [354] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus,Thomas Kneib,Thomas Nagler,David Rügamer*

Main category: stat.ML

TL;DR: 结合MCTM与自回归NF，兼顾透明性与灵活性，用于密度回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法如MCTM灵活性不足，NF虽灵活但难以解释。结合两者以提升模型性能与可解释性。

Method: 将MCTM与自回归NF结合，先利用MCTM建模边际分布的可解释特征效应，再用NF处理联合分布的复杂关系。

Result: 在模拟和真实数据实验中，新方法优于MCTM和其他NF模型。

Conclusion: 新方法结合了MCTM的可解释性与NF的灵活性，适用于复杂多变量概率分布建模。

Abstract: Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [355] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue,Xinyi Wang,Victor Solo*

Main category: stat.ML

TL;DR: 论文提出了一种基于系统辨识方法的向量时间序列聚类算法k-LMVAR，解决了现有方法忽略自相关特征或依赖领域知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列聚类方法通常仅处理标量时间序列、忽略自相关特征或依赖领域知识，限制了其应用效果。

Method: 通过混合自回归模型推导聚类算法，并进一步提出计算可行的k-LMVAR算法，结合BIC准则选择聚类数和模型阶数。

Result: 算法在仿真实验中表现优异，且计算效率高。

Conclusion: k-LMVAR算法在时间序列聚类中具有高效性和实用性。

Abstract: Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [356] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux,Yang Lu*

Main category: stat.ML

TL;DR: 本文提出了一种用于估计Determinantal Point Process (DPP)核矩阵的闭式估计器，证明其一致性、渐近正态性及大偏差性质。


<details>
  <summary>Details</summary>
Motivation: DPP是一种多变量二元变量的参数化模型，但其核矩阵的估计通常复杂，需要一种简单且高效的估计方法。

Method: 提出了一种闭式估计器，易于实现，并可作为最大似然估计学习算法的初始值。

Result: 证明了该估计器的一致性、渐近正态性及大偏差性质。

Conclusion: 该闭式估计器为DPP核矩阵的估计提供了一种简单且理论保证的方法。

Abstract: The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [357] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui,Malik Tiomoko,Mohamed El Amine Seddik,Cosme Louart,Ekkehard Schnoor,Balazs Kegl*

Main category: stat.ML

TL;DR: 本文分析了自举方法在高维数据下对LSSVM集成的性能影响，并提出了优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究自举方法在大样本和高维特征下对LSSVM集成的理论性能。

Method: 利用随机矩阵理论分析自举方法对LSSVM的影响，并通过实验验证。

Result: 提出了优化子集数量和正则化参数的策略，实验验证了理论结果。

Conclusion: 自举方法在高维LSSVM集成中具有潜力，优化策略可提升性能。

Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [358] [Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs](https://arxiv.org/abs/2505.13572)
*Yousouf Taghzouti,Franck Michel,Tao Jiang,Louis-Félix Nothias,Fabien Gandon*

Main category: cs.DB

TL;DR: Q²Forge是一个开源工具，用于为知识图谱生成能力问题和SPARQL查询，并通过人类反馈和LLM验证迭代优化查询。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询对非专家用户具有挑战性，且现有知识图谱的示例查询有限，需要一种自动化方法生成高质量查询。

Method: Q²Forge通过模块化设计（能力问题生成、查询生成和查询优化）迭代生成和验证查询，支持独立或集成使用。

Result: Q²Forge提供了一个完整的流程，从能力问题生成到查询评估，支持为任何知识图谱创建参考查询集。

Conclusion: Q²Forge解决了知识图谱查询生成的挑战，具有开源、通用和可扩展的特点。

Abstract: The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.

</details>


### [359] [Abacus: A Cost-Based Optimizer for Semantic Operator Systems](https://arxiv.org/abs/2505.14661)
*Matthew Russo,Sivaprasad Sudhir,Gerardo Vitagliano,Chunwei Liu,Tim Kraska,Samuel Madden,Michael Cafarella*

Main category: cs.DB

TL;DR: Abacus是一个基于成本的优化器，用于优化语义操作符系统的实现，显著提升质量、降低成本与延迟。


<details>
  <summary>Details</summary>
Motivation: 现有优化器在优化语义操作符系统时能力有限，无法同时优化质量、成本和延迟。Abacus旨在解决这一问题。

Method: Abacus通过验证示例和先验性能信念估计操作符性能，搜索最佳实现方案。

Result: 在生物医学、法律领域和多模态问答任务中，Abacus优化后的系统质量提升18.7%-39.2%，成本降低23.6倍，延迟减少4.2倍。

Conclusion: Abacus是一种高效且可扩展的优化器，显著优于现有方法。

Abstract: LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [360] [Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy](https://arxiv.org/abs/2505.14507)
*Jingyun Chen,David Horowitz,Yading Yuan*

Main category: cs.DC

TL;DR: FedKBP+是一个用于放疗预测任务的联邦学习平台，解决了数据隐私和异构性问题，支持集中式和完全去中心化的联邦学习策略。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在放疗规划中因数据稀缺和机构间异构性导致的模型泛化能力不足问题，同时避免数据共享的隐私和技术障碍。

Method: 基于gRPC实现统一通信栈，支持集中式和完全去中心化的联邦学习策略，使用SA-Net作为预测模型。

Result: FedKBP+在三个预测任务中表现出高效、有效和鲁棒性，展示了其在放疗中的潜力。

Conclusion: FedKBP+是一个有前景的联邦学习平台，适用于放疗规划中的预测任务。

Abstract: Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [361] [Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors](https://arxiv.org/abs/2505.11325)
*Thomas Nagler,David Rügamer*

Main category: stat.ME

TL;DR: PFNs是一种用于表格数据的预测模型，性能优异但缺乏不确定性量化。本文提出了一种基于Martingale后验的采样方法，用于构建贝叶斯后验，并证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: PFNs在小到中等规模数据集上表现优异，但缺乏对预测均值、分位数等的不确定性量化，因此需要一种方法来解决这一问题。

Method: 提出了一种基于Martingale后验的采样方法，用于构建贝叶斯后验，并证明了其收敛性。

Result: 在模拟和真实数据集上展示了该方法的不确定性量化能力。

Conclusion: 该方法为PFNs提供了有效的不确定性量化工具，适用于推理应用。

Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [362] [RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework](https://arxiv.org/abs/2505.13808)
*Faramarz Safi Esfahani,Ghassan Beydoun,Morteza Saberi,Brad McCusker,Biswajeet Pradhan*

Main category: cs.NE

TL;DR: PMF是一种自适应的元启发式框架，通过实时性能反馈和动态算法选择提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法结构固定且需要大量调参，限制了其解决复杂优化问题的效果。

Method: PMF利用PMA和PMSA动态选择和切换元启发式算法，基于性能指标实现自适应优化。

Result: 实验表明，PMF在高维、动态和多模态环境中显著提升收敛速度和解决方案质量。

Conclusion: PMF为智能、自适应的优化框架提供了新方向，适用于工程、物流等领域。

Abstract: Metaheuristic algorithms are widely used for solving complex optimization
problems, yet their effectiveness is often constrained by fixed structures and
the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)
addresses this limitation by introducing a self-adaptive metaheuristic
switching mechanism driven by real-time performance feedback and dynamic
algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)
and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select
and transition between metaheuristic algorithms based on key performance
indicators, ensuring continuous adaptation. This approach enhances convergence
speed, adaptability, and solution quality, outperforming traditional
metaheuristics in high-dimensional, dynamic, and multimodal environments.
Experimental results on benchmark functions demonstrate that PMF significantly
improves optimization efficiency by mitigating stagnation and balancing
exploration-exploitation strategies across various problem landscapes. By
integrating AI-driven decision-making and self-correcting mechanisms, PMF paves
the way for scalable, intelligent, and autonomous optimization frameworks, with
promising applications in engineering, logistics, and complex decision-making
systems.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [363] [Towards Verifiability of Total Value Locked (TVL) in Decentralized Finance](https://arxiv.org/abs/2505.14565)
*Pietro Saggese,Michael Fröwis,Stefan Kitzler,Bernhard Haslhofer,Raphael Auer*

Main category: q-fin.GN

TL;DR: 论文研究了DeFi中TVL（总锁定价值）的计算方法，发现其缺乏标准化和可验证性，并提出了可验证的TVL（vTVL）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: TVL的计算依赖社区自我报告且缺乏标准化，导致难以独立验证数据，因此需要系统性研究以改进。

Method: 通过对939个以太坊DeFi项目的系统研究，分析了TVL计算方法、阻碍可验证性的因素，并提出了标准化尝试。

Result: 研究发现10.5%的协议依赖外部服务器，68种非标准查询方法存在，240种重复查询，表明透明度和可验证性有限。提出的vTVL在46.5%的协议中与公布数据一致。

Conclusion: 论文提出了设计指南，以促进更可验证、标准化和可解释的TVL计算。

Abstract: Total Value Locked (TVL) aims to measure the aggregate value of cryptoassets
deposited in Decentralized Finance (DeFi) protocols. Although blockchain data
is public, the way TVL is computed is not well understood. In practice, its
calculation on major TVL aggregators relies on self-reports from community
members and lacks standardization, making it difficult to verify published
figures independently. We thus conduct a systematic study on 939 DeFi projects
deployed in Ethereum. We study the methodologies used to compute TVL, examine
factors hindering verifiability, and ultimately propose standardization
attempts in the field. We find that 10.5% of the protocols rely on external
servers; 68 methods alternative to standard balance queries exist, although
their use decreased over time; and 240 equal balance queries are repeated on
multiple protocols. These findings indicate limits to verifiability and
transparency. We thus introduce ``verifiable Total Value Locked'' (vTVL), a
metric measuring the TVL that can be verified relying solely on on-chain data
and standard balance queries. A case study on 400 protocols shows that our
estimations align with published figures for 46.5% of protocols. Informed by
these findings, we discuss design guidelines that could facilitate a more
verifiable, standardized, and explainable TVL computation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [364] [Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer](https://arxiv.org/abs/2505.14303)
*Rebecca Pelke,José Cubero-Cascante,Nils Bosbach,Niklas Degener,Florian Idrizi,Lennart M. Reimann,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.ET

TL;DR: CIM-Explorer是一个模块化工具包，用于优化RRAM交叉阵列上的BNN和TNN推理，提供端到端编译器、多种映射选项和模拟器，支持设计空间探索。


<details>
  <summary>Details</summary>
Motivation: 解决现有RRAM-based CIM软件项目仅关注单一功能（如编译或模拟）且依赖传统8位量化的局限性。

Method: 开发CIM-Explorer工具包，包括编译器栈、映射选项和模拟器，支持从早期精度估计到最终芯片设计的全流程。

Result: 通过DSE案例研究展示了不同映射和交叉阵列参数的预期精度。

Conclusion: CIM-Explorer为BNN和TNN在RRAM交叉阵列上的高效实现提供了全面支持。

Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory
(CIM) architectures offers a promising solution to overcome the von Neumann
bottleneck. Due to non-idealities like cell variability, RRAM crossbars are
often operated in binary mode, utilizing only two states: Low Resistive State
(LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary
Neural Networks (TNNs) are well-suited for this hardware due to their efficient
mapping. Existing software projects for RRAM-based CIM typically focus on only
one aspect: compilation, simulation, or Design Space Exploration (DSE).
Moreover, they often rely on classical 8 bit quantization. To address these
limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN
and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end
compiler stack, multiple mapping options, and simulators, enabling a DSE flow
for accuracy estimation across different crossbar parameters and mappings.
CIM-Explorer can accompany the entire design process, from early accuracy
estimation for specific crossbar parameters, to selecting an appropriate
mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case
studies, we demonstrate the expected accuracy for various mappings and crossbar
parameters. CIM-Explorer can be found on GitHub.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [365] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang,Linsey Pang,Alice Gatti,Mahima Aggarwa,Giovanna Vantin,Xiaosong Ma,Weiwei Sun,Sanjay Chawla*

Main category: math.OC

TL;DR: 提出了一种基于强化学习的组合优化方法，利用约束动作空间引导归一化切割问题向预定义模板实例靠近。


<details>
  <summary>Details</summary>
Motivation: 解决在组合优化问题中如何有效整合外部知识以引导解决方案向领域适用结果靠近的挑战。

Method: 使用约束动作空间的强化学习方法，结合Wedge和Ring Transformer生成图形分区。

Result: 生成的图形分区更接近自然最优分区，且方法具有通用性。

Conclusion: 该方法为组合优化问题提供了一种新的解决方案，并展示了其通用潜力。

Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [366] [Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis](https://arxiv.org/abs/2505.13660)
*Kaheon Kim,Bohan Zhou,Changbo Zhu,Xiaohui Chen*

Main category: math.OC

TL;DR: 本文提出了一种新的无约束凹对偶形式用于计算Wasserstein重心，并基于Sobolev几何设计了Sobolev梯度上升（SGA）算法，具有全局收敛性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算Wasserstein重心时存在计算复杂性和理论限制，需要简化算法并提高效率。

Method: 通过Sobolev几何设计SGA算法，避免昂贵的$c$-凹投影操作，简化计算。

Result: SGA算法在数值实验中表现优于现有最优传输重心求解器。

Conclusion: SGA算法为计算Wasserstein重心提供了高效且理论简化的解决方案。

Abstract: This paper introduces a new constraint-free concave dual formulation for the
Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to
the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA)
algorithm to compute the barycenter for input distributions supported on a
regular grid. Despite the algorithmic simplicity, we provide a global
convergence analysis that achieves the same rate as the classical subgradient
descent methods for minimizing nonsmooth convex functions in the Euclidean
space. A central feature of our SGA algorithm is that the computationally
expensive $c$-concavity projection operator enforced on the Kantorovich dual
potentials is unnecessary to guarantee convergence, leading to significant
algorithmic and theoretical simplifications over all existing primal and dual
methods for computing the exact barycenter. Our numerical experiments
demonstrate the superior empirical performance of SGA over the existing optimal
transport barycenter solvers.

</details>


### [367] [Sequential QCQP for Bilevel Optimization with Line Search](https://arxiv.org/abs/2505.14647)
*Sina Sharifi,Erfan Yazdandoost Hamedani,Mahyar Fazlyab*

Main category: math.OC

TL;DR: 提出了一种单循环、免调参的双层优化算法，保证随时可行性和上层目标下降。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中层次间复杂依赖关系，提出高效且无需调参的方法。

Method: 通过凸二次约束二次规划（QCQP）确定搜索方向，结合控制屏障函数启发的回溯线搜索确保步长安全。

Result: 算法具有O(1/k)的遍历收敛速度，并在典型任务中表现有效。

Conclusion: 该方法可扩展、无需调参，适用于局部正则性假设下的双层优化问题。

Abstract: Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [368] [GANCompress: GAN-Enhanced Neural Image Compression with Binary Spherical Quantization](https://arxiv.org/abs/2505.13542)
*Karthik Sivakoti*

Main category: eess.IV

TL;DR: GANCompress结合BSQ与GAN，提出新型神经压缩框架，显著提升压缩效率与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 视觉数据爆炸式增长需要高效压缩技术，现有神经压缩方法在感知质量、计算效率和适应性方面存在挑战。

Method: 采用基于变压器的自动编码器与BSQ瓶颈，结合GAN架构，优化频率域注意力和颜色一致性。

Result: 压缩效率提升100倍，视觉失真小，PSNR/SSIM与传统编解码器相当，FID降低43%。

Conclusion: GANCompress在神经压缩技术上取得显著进展，适用于实时视觉通信系统。

Abstract: The exponential growth of visual data in digital communications has
intensified the need for efficient compression techniques that balance
rate-distortion performance with computational feasibility. While recent neural
compression approaches have shown promise, they still struggle with fundamental
challenges: preserving perceptual quality at high compression ratios,
computational efficiency, and adaptability to diverse visual content. This
paper introduces GANCompress, a novel neural compression framework that
synergistically combines Binary Spherical Quantization (BSQ) with Generative
Adversarial Networks (GANs) to address these challenges. Our approach employs a
transformer-based autoencoder with an enhanced BSQ bottleneck that projects
latent representations onto a hypersphere, enabling efficient discretization
with bounded quantization error. This is followed by a specialized GAN
architecture incorporating frequency-domain attention and color consistency
optimization. Experimental results demonstrate that GANCompress achieves
substantial improvement in compression efficiency -- reducing file sizes by up
to 100x with minimal visual distortion. Our method outperforms traditional
codecs like H.264 by 12-15% in perceptual metrics while maintaining comparable
PSNR/SSIM values, with 2.4x faster encoding and decoding speeds. On standard
benchmarks including ImageNet-1k and COCO2017, GANCompress sets a new
state-of-the-art, reducing FID from 0.72 to 0.41 (43% improvement) compared to
previous methods while maintaining higher throughput. This work presents a
significant advancement in neural compression technology with promising
applications for real-time visual communication systems.

</details>


### [369] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/abs/2505.13579)
*Yipeng Sun,Linda-Sophie Schneider,Chengze Ye,Mingxuan Gu,Siyuan Mei,Siming Bayer,Andreas Maier*

Main category: eess.IV

TL;DR: 提出了一种基于FDK算法的改进神经网络方法，通过选择性集成可训练元素和利用小波变换稀疏化参数，显著减少计算复杂度，同时保持图像质量和算法可解释性。


<details>
  <summary>Details</summary>
Motivation: FDK算法在CBCT重建中效率高但易受噪声和伪影影响，而现有深度学习方法虽提升图像质量却增加计算复杂度和缺乏可解释性。

Method: 在FDK的余弦加权和滤波阶段选择性集成可训练元素，并利用小波变换稀疏化参数，减少93.75%的参数数量。

Result: 方法在保持FDK算法推理计算成本的同时，提升了图像质量、鲁棒性和收敛速度。

Conclusion: 该方法为临床应用中计算资源受限的环境提供了一种实用且高效的改进方案。

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [370] [Exploring Image Quality Assessment from a New Perspective: Pupil Size](https://arxiv.org/abs/2505.13841)
*Yixuan Gao,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: 研究通过瞳孔大小分析图像质量评估（IQA）任务对认知过程的影响，发现IQA任务激活视觉注意力机制，且瞳孔变化与图像质量相关。


<details>
  <summary>Details</summary>
Motivation: 探索IQA任务如何影响认知过程，并研究瞳孔大小与图像质量的关系。

Method: 邀请受试者参与自由观察和IQA任务的主观实验，分析两种任务下瞳孔大小的差异。

Result: 发现IQA任务激活视觉注意力机制，且瞳孔变化与图像质量密切相关。

Conclusion: 为客观IQA方法提供理论基础，并提出一种新的主观IQA方法。

Abstract: This paper explores how the image quality assessment (IQA) task affects the
cognitive processes of people from the perspective of pupil size and studies
the relationship between pupil size and image quality. Specifically, we first
invited subjects to participate in a subjective experiment, which includes two
tasks: free observation and IQA. In the free observation task, subjects did not
need to perform any action, and they only needed to observe images as they
usually do with an album. In the IQA task, subjects were required to score
images according to their overall impression of image quality. Then, by
analyzing the difference in pupil size between the two tasks, we find that
people may activate the visual attention mechanism when evaluating image
quality. Meanwhile, we also find that the change in pupil size is closely
related to image quality in the IQA task. For future research on IQA, this
research can not only provide a theoretical basis for the objective IQA method
and promote the development of more effective objective IQA methods, but also
provide a new subjective IQA method for collecting the authentic subjective
impression of image quality.

</details>


### [371] [Automated Quality Evaluation of Cervical Cytopathology Whole Slide Images Based on Content Analysis](https://arxiv.org/abs/2505.13875)
*Lanlan Kang,Jian Wang,Jian QIn,Yiqin Liang,Yongjun He*

Main category: eess.IV

TL;DR: 提出了一种基于人工智能算法的宫颈细胞病理学全切片图像（WSI）质量评估方法，结合TBS标准，通过多模型分析染色质量、细胞计数等指标，并利用XGBoost模型综合评分，实验证明其速度和一致性优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统宫颈癌筛查方法（如TCT）依赖病理学家手动评估，存在主观性高、成本高、耗时长和可靠性低的问题，需要自动化质量评估系统。

Method: 基于TBS标准和人工智能算法，通过目标检测、分类和分割模型分析WSI的染色质量、细胞计数等指标，并利用XGBoost模型综合评分。

Result: 在100张WSI上的实验表明，该方法在速度和一致性上具有显著优势。

Conclusion: 提出的自动化质量评估方法能有效替代传统手动评估，提高宫颈癌筛查的效率和可靠性。

Abstract: The ThinPrep Cytologic Test (TCT) is the most widely used method for cervical
cancer screening, and the sample quality directly impacts the accuracy of the
diagnosis. Traditional manual evaluation methods rely on the observation of
pathologist under microscopes. These methods exhibit high subjectivity, high
cost, long duration, and low reliability. With the development of
computer-aided diagnosis (CAD), an automated quality assessment system that
performs at the level of a professional pathologist is necessary. To address
this need, we propose a fully automated quality assessment method for Cervical
Cytopathology Whole Slide Images (WSIs) based on The Bethesda System (TBS)
diagnostic standards, artificial intelligence algorithms, and the
characteristics of clinical data. The method analysis the context of WSIs to
quantify quality evaluation metrics which are focused by TBS such as staining
quality, cell counts and cell mass proportion through multiple models including
object detection, classification and segmentation. Subsequently, the XGBoost
model is used to mine the attention paid by pathologists to different quality
evaluation metrics when evaluating samples, thereby obtaining a comprehensive
WSI sample score calculation model. Experimental results on 100 WSIs
demonstrate that the proposed evaluation method has significant advantages in
terms of speed and consistency.

</details>


### [372] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/abs/2505.13906)
*Soyabul Islam Lincoln,Mirza Mohd Shahriar Maswood*

Main category: eess.IV

TL;DR: 该论文提出了一种结合多残差块、空间注意力机制和多种注意力机制的深度学习架构，用于阿尔茨海默病的诊断，并在多个公开数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的精确诊断和高效治疗需求日益增长，结合人工智能技术可以提升诊断效果。

Method: 采用深度卷积神经网络，结合多残差块、空间注意力块、分组查询注意力和多头注意力机制，评估了模型在多个数据集上的性能。

Result: 在多个数据集上实现了高准确率，如4分类99.66%、3分类99.63%、二分类100%（Kaggle数据集），并在其他数据集上表现优异。

Conclusion: 该模型在阿尔茨海默病诊断中表现出色，能够从MRI图像中提取关键信息，优于现有方法。

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [373] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/abs/2505.13911)
*Ruijie Zhao,Zuopeng Tan,Xiao Xue,Longfei Zhao,Bing Li,Zicheng Liao,Ying Ming,Jiaru Wang,Ran Xiao,Sirong Piao,Rui Zhao,Qiqi Xu,Wei Song*

Main category: eess.IV

TL;DR: 提出了一种基于解剖层次监督学习（AHSL）的弱监督学习方法，用于肺段分割，通过结合段级和叶级监督以及支气管血管先验信息，提高了分割精度和边界平滑度。


<details>
  <summary>Details</summary>
Motivation: 肺段分割在癌症定位和手术规划中至关重要，但像素级标注耗时且边界难以区分，因此需要一种弱监督方法来解决这一问题。

Method: 采用解剖层次监督学习（AHSL），结合段级和叶级监督，设计了两阶段分割策略，并引入一致性损失和边界平滑度评估指标。

Result: 在私有数据集上的实验表明，该方法在肺段分割和边界平滑度方面表现有效。

Conclusion: AHSL方法通过弱监督和先验信息结合，显著提升了肺段分割的精度和实用性。

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [374] [End-to-end Cortical Surface Reconstruction from Clinical Magnetic Resonance Images](https://arxiv.org/abs/2505.14017)
*Jesper Duemose Nielsen,Karthik Gopinath,Andrew Hoopes,Adrian Dalca,Colin Magdamo,Steven Arnold,Sudeshna Das,Axel Thielscher,Juan Eugenio Iglesias,Oula Puonti*

Main category: eess.IV

TL;DR: 该论文提出了一种基于神经网络的皮质表面估计方法，适用于任何对比度和分辨率的临床MR扫描，无需重新训练，显著降低了皮质厚度误差。


<details>
  <summary>Details</summary>
Motivation: 现有皮质表面估计工具仅适用于高分辨率T1加权扫描，无法处理临床MR扫描的异质性。

Method: 使用合成域随机化数据训练神经网络，通过模板网格变形估计白质和灰质表面。

Result: 与现有方法相比，皮质厚度误差减少约50%（从0.50降至0.24毫米），并更好地恢复了与衰老相关的皮质变薄模式。

Conclusion: 该方法实现了临床扫描的快速准确表面重建，支持大规模研究和临床人群分析。

Abstract: Surface-based cortical analysis is valuable for a variety of neuroimaging
tasks, such as spatial normalization, parcellation, and gray matter (GM)
thickness estimation. However, most tools for estimating cortical surfaces work
exclusively on scans with at least 1 mm isotropic resolution and are tuned to a
specific magnetic resonance (MR) contrast, often T1-weighted (T1w). This
precludes application using most clinical MR scans, which are very
heterogeneous in terms of contrast and resolution. Here, we use synthetic
domain-randomized data to train the first neural network for explicit
estimation of cortical surfaces from scans of any contrast and resolution,
without retraining. Our method deforms a template mesh to the white matter (WM)
surface, which guarantees topological correctness. This mesh is further
deformed to estimate the GM surface. We compare our method to
recon-all-clinical (RAC), an implicit surface reconstruction method which is
currently the only other tool capable of processing heterogeneous clinical MR
scans, on ADNI and a large clinical dataset (n=1,332). We show a approximately
50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect
to RAC and better recovery of the aging-related cortical thinning patterns
detected by FreeSurfer on high-resolution T1w scans. Our method enables fast
and accurate surface reconstruction of clinical scans, allowing studies (1)
with sample sizes far beyond what is feasible in a research setting, and (2) of
clinical populations that are difficult to enroll in research studies. The code
is publicly available at https://github.com/simnibs/brainnet.

</details>


### [375] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/abs/2505.14064)
*Cosmin I. Bercea,Jun Li,Philipp Raffler,Evamaria O. Riedel,Lena Schmitzer,Angela Kurz,Felix Bitzer,Paula Roßmüller,Julian Canisius,Mirjam L. Beyrle,Che Liu,Wenjia Bai,Bernhard Kainz,Julia A. Schnabel,Benedikt Wiestler*

Main category: eess.IV

TL;DR: 论文提出了NOVA基准测试，用于评估模型在真实临床环境中处理罕见和未知病理的能力，揭示了现有视觉语言模型在分布外泛化上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对罕见或新病理的评估不足，导致模型在实际临床应用中表现不佳。

Method: 构建了一个包含281种罕见病理的脑MRI扫描数据集（NOVA），用于评估模型的异常定位、视觉描述和诊断推理能力。

Result: 测试显示，领先的视觉语言模型（如GPT-4o、Gemini 2.0 Flash等）在NOVA上的性能显著下降。

Conclusion: NOVA为评估和改进模型在真实临床环境中的分布外泛化能力提供了严格测试平台。

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [376] [Neural Video Compression with Context Modulation](https://arxiv.org/abs/2505.14541)
*Chuanbo Tang,Zhuoyuan Li,Yifan Bian,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: 论文提出了一种通过流导向和上下文补偿来优化神经视频编码器（NVC）中时间上下文传播的方法，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有NVC的时间上下文传播机制未能充分利用参考信息，限制了压缩性能的进一步提升。

Method: 通过流导向挖掘参考帧与预测帧之间的相关性生成额外的时间上下文，并引入上下文补偿机制优化传播的时间上下文。

Result: 实验表明，该方法比传统视频编码器H.266/VVC平均节省22.7%比特率，比现有最佳NVC DCVC-FM节省10.1%。

Conclusion: 提出的方法有效提升了NVC的压缩性能，并通过协同机制和损失监督优化了上下文建模。

Abstract: Efficient video coding is highly dependent on exploiting the temporal
redundancy, which is usually achieved by extracting and leveraging the temporal
context in the emerging conditional coding-based neural video codec (NVC).
Although the latest NVC has achieved remarkable progress in improving the
compression performance, the inherent temporal context propagation mechanism
lacks the ability to sufficiently leverage the reference information, limiting
further improvement. In this paper, we address the limitation by modulating the
temporal context with the reference frame in two steps. Specifically, we first
propose the flow orientation to mine the inter-correlation between the
reference frame and prediction frame for generating the additional oriented
temporal context. Moreover, we introduce the context compensation to leverage
the oriented context to modulate the propagated temporal context generated from
the propagated reference feature. Through the synergy mechanism and decoupling
loss supervision, the irrelevant propagated information can be effectively
eliminated to ensure better context modeling. Experimental results demonstrate
that our codec achieves on average 22.7% bitrate reduction over the advanced
traditional video codec H.266/VVC, and offers an average 10.1% bitrate saving
over the previous state-of-the-art NVC DCVC-FM. The code is available at
https://github.com/Austin4USTC/DCMVC.

</details>


### [377] [Neural Inverse Scattering with Score-based Regularization](https://arxiv.org/abs/2505.14560)
*Yuan Gao,Wenhan Guo,Yu Sun*

Main category: eess.IV

TL;DR: 论文提出了一种基于神经场（NF）和去噪评分函数的正则化方法，用于解决逆散射问题，提高了成像质量。


<details>
  <summary>Details</summary>
Motivation: 逆散射问题是成像应用中的基础挑战，需要同时估计图像和散射场，因此需要有效的图像先验进行正则化。

Method: 采用神经场（NF）结合去噪评分函数的方法，利用神经场的灵活性进行联合估计，并通过去噪评分函数引入图像结构先验。

Result: 在三个高对比度模拟对象上的实验表明，该方法比基于总变分的现有NF方法具有更好的成像质量。

Conclusion: 该方法通过结合神经场和去噪评分函数，显著提升了逆散射问题的成像效果。

Abstract: Inverse scattering is a fundamental challenge in many imaging applications,
ranging from microscopy to remote sensing. Solving this problem often requires
jointly estimating two unknowns -- the image and the scattering field inside
the object -- necessitating effective image prior to regularize the inference.
In this paper, we propose a regularized neural field (NF) approach which
integrates the denoising score function used in score-based generative models.
The neural field formulation offers convenient flexibility to performing joint
estimation, while the denoising score function imposes the rich structural
prior of images. Our results on three high-contrast simulated objects show that
the proposed approach yields a better imaging quality compared to the
state-of-the-art NF approach, where regularization is based on total variation.

</details>


### [378] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/abs/2505.14572)
*Jayroop Ramesh,Valentin Bacher,Mark C. Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana IL Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale*

Main category: eess.IV

TL;DR: 该论文提出了一种自动化胎儿生物测量流程，通过减少观察者间的变异性，提高测量可靠性，用于预测分娩结果。


<details>
  <summary>Details</summary>
Motivation: 减少超声测量中的观察者间变异性，提高测量可靠性，以更好地预测分娩结果。

Method: 提出三步流程：标准平面分类、胎儿头部和耻骨联合分割、AoP和HSD计算。采用稀疏采样和集成深度学习方法。

Result: 在未见过的数据集上表现优异，各项指标（如ACC、F1、AUC等）均较高。

Conclusion: 自动化流程可帮助理解分娩停滞原因，并指导临床风险分层工具的研发。

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [379] [LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution](https://arxiv.org/abs/2505.13497)
*Claudius Kienle,Benjamin Alt,Oleg Arenz,Jan Peters*

Main category: cs.RO

TL;DR: 论文提出了一种分层学习方法，通过组合低级谓词和动作为高级规划构建领域模型，并结合仿真验证其前提和效果，显著提升了长时程规划的成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言指令规划中存在缺陷，需要人工反馈改进。论文旨在减少这种依赖，通过学习分层领域模型提升规划质量。

Method: 采用分层学习方法，组合低级谓词和动作为高级规划模型，并通过仿真验证其前提和效果。引入中心错误推理器确保规划层级一致性。

Result: 在国际规划竞赛（IPC）领域和机器人长时程操作任务中，规划成功率优于现有领域合成和LLM规划方法。

Conclusion: 分层学习方法显著提升了长时程规划的成功率，同时构建了高质量的领域模型。

Abstract: Large Language Models (LLMs) enable planning from natural language
instructions using implicit world knowledge, but often produce flawed plans
that require refinement. Instead of directly predicting plans, recent methods
aim to learn a problem domain that can be solved for different goal states
using classical planners. However, these approaches require significant human
feedback to obtain useful models. We address this shortcoming by learning
hierarchical domains, where low-level predicates and actions are composed into
higher-level counterparts, and by leveraging simulation to validate their
preconditions and effects. This hierarchical approach is particularly powerful
for long-horizon planning, where LLM-based planning approaches typically
struggle. Furthermore, we introduce a central error reasoner to ensure
consistency among the different planning levels. Evaluation on two challenging
International Planning Competition (IPC) domains and a long-horizon robot
manipulation task demonstrates higher planning success rates than
state-of-the-art domain synthesis and LLM-modulo planning methods, while
constructing high-quality models of the domain. Resources, videos and detailed
experiment results are available at https://claudius-kienle.github.io/lodge/.

</details>


### [380] [Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios](https://arxiv.org/abs/2505.13532)
*Feihong Zhang,Guojian Zhan,Bin Shuai,Tianyi Zhang,Jingliang Duan,Shengbo Eben Li*

Main category: cs.RO

TL;DR: 提出了一种名为HPI的安全导向强化学习技术，结合DSAC算法开发了DSAC-H，在多车道场景中实现了高效驾驶且几乎零安全违规。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在处理约束条件时存在挑战，尤其是在现实应用中，需要一种更安全、平衡的训练方法。

Method: 提出HPI技术，计算与高效驾驶和安全约束相关的两个策略梯度，并通过调和梯度更新策略，减少冲突。结合DSAC算法开发DSAC-H。

Result: DSAC-H在多车道模拟中表现出高效驾驶性能，且几乎未违反安全约束。

Conclusion: HPI和DSAC-H为强化学习在自动驾驶中的应用提供了一种更安全、稳定的解决方案。

Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.

</details>


### [381] [SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/abs/2505.13729)
*Abhinav Rajvanshi,Pritish Sahu,Tixiao Shan,Karan Sikka,Han-Pang Chiu*

Main category: cs.RO

TL;DR: SayCoNav利用大型语言模型（LLMs）为机器人团队自动生成协作策略，提升多目标导航任务的效率。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中，机器人团队需要动态协作策略以适应各自技能和状态，完成复杂导航任务。

Method: 通过LLM生成协作策略，并让每个机器人以去中心化方式生成计划和行动，同时通过信息共享持续更新计划。

Result: 在MultiON任务中，SayCoNav比基线方法最高提升44.28%的搜索效率，并能动态适应变化条件。

Conclusion: SayCoNav通过LLM驱动的协作策略，显著提升了异构机器人团队的导航效率和适应性。

Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.

</details>


### [382] [Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams](https://arxiv.org/abs/2505.13834)
*Zhi Su,Yuman Gao,Emily Lukas,Yunfei Li,Jiaze Cai,Faris Tulbah,Fei Gao,Chao Yu,Zhongyu Li,Yi Wu,Koushil Sreenath*

Main category: cs.RO

TL;DR: 本文提出了一种分层多智能体强化学习框架，用于实现四足机器人在足球比赛中的完全自主和去中心化协作。


<details>
  <summary>Details</summary>
Motivation: 研究如何在动态、竞争和多智能体交互的机器人足球测试平台上，实现四足机器人的精细运动控制和长期战略决策。

Method: 采用分层强化学习框架，包括低层动态技能训练（如行走、带球和踢球）和高层战略规划（使用MAPPO和FSP）。

Result: 该方法在协作和竞争性多智能体足球游戏中表现优异，并成功部署到真实四足机器人上。

Conclusion: 提出的学习框架能够适应多样化的对手策略，并实现复杂的团队行为，为机器人足球提供了可行的解决方案。

Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.

</details>


### [383] [Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements](https://arxiv.org/abs/2505.13837)
*Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Melkior Ornik*

Main category: cs.RO

TL;DR: GUIDE框架通过任务特定不确定性地图（TSUMs）将任务需求融入导航策略，结合强化学习优化不确定性管理与任务完成。


<details>
  <summary>Details</summary>
Motivation: 机器人导航需应对传感器噪声、环境变化和信息不完整等问题，不同任务在不同区域对精度的需求各异。

Method: 提出GUIDE框架，利用TSUMs为不同位置分配可接受的不确定性水平，结合强化学习训练导航策略。

Result: 实验显示GUIDE在任务完成和不确定性管理上优于传统方法。

Conclusion: GUIDE通过任务特定不确定性管理显著提升了机器人导航性能。

Abstract: Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.

</details>


### [384] [Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving](https://arxiv.org/abs/2505.13872)
*Jingzheng Li,Tiancheng Wang,Xingyu Peng,Jiacheng Chen,Zhijun Chen,Bing Li,Xianglong Liu*

Main category: cs.RO

TL;DR: 论文提出Safety2Drive，一个用于评估自动驾驶系统安全性的关键场景库，填补现有数据集在闭环测试和法规合规性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集缺乏法规合规的闭环测试场景库，且真实事故场景不足，导致安全性评估不充分。

Method: 提出Safety2Drive，包含70项测试项目，支持安全威胁注入（如自然环境干扰和对抗攻击），并支持多维评估（如感知任务）。

Result: Safety2Drive提供了一个从场景构建到验证的标准化框架，支持自动驾驶系统的全面安全评估。

Conclusion: Safety2Drive为自动驾驶的安全部署提供了标准化测试框架，填补了现有数据集的不足。

Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.

</details>


### [385] [APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight](https://arxiv.org/abs/2505.13921)
*Wanjing Huang,Weixiang Yan,Zhen Zhang,Ambuj Singh*

Main category: cs.RO

TL;DR: APEX框架通过物理驱动的预见性增强LLMs的实时任务规划能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如VLMs或RL）无法捕捉动态物体交互或需任务特定训练，限制了实际应用。

Method: APEX构建结构化图以建模环境中的动态交互，并提供低延迟前向模拟，使LLMs能基于预测结果选择策略。

Result: 在物理推理、Tetris和动态避障三个基准测试中，APEX显著优于标准LLMs和VLM模型。

Conclusion: 显式物理推理是弥合语言智能与现实任务执行差距的关键。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .

</details>


### [386] [Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World](https://arxiv.org/abs/2505.13969)
*Junya Nakanishi,Jun Baba,Yuichiro Yoshikawa,Hiroko Kamide,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: 本文探讨了Global Workspace Theory（GWT）提出的Selection-Broadcast Cycle结构的功能优势，特别关注其在动态实时场景中对人工智能和机器人的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示Selection和Broadcast过程的循环结构及其对实时认知系统的综合效益，弥补以往独立研究的不足。

Method: 通过分析GWT的Selection-Broadcast Cycle结构，识别其三大优势：动态思维适应、基于经验的适应和即时实时适应。

Result: 研究结果表明GWT作为一种认知架构，适用于复杂决策和动态环境中的自适应表现。

Conclusion: 结论指出GWT为开发通用AI和机器人系统提供了新方向，能够应对复杂的现实任务。

Abstract: This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.

</details>


### [387] [Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures](https://arxiv.org/abs/2505.13556)
*Yiru Jiao,Simeon C. Calvert,Sander van Cranenburgh,Hans van Lint*

Main category: cs.RO

TL;DR: 本文提出了一种通用的替代安全度量（GSSM），通过学习自然驾驶数据（无需碰撞或风险标签）来量化交通交互中的碰撞风险。GSSM通过神经网络捕捉正常驾驶模式，并基于极端值理论计算风险评分，在多种交互场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标注稀疏风险、考虑交互上下文或泛化性方面存在不足，亟需一种无需标注、上下文自适应的通用安全度量方法。

Method: GSSM利用神经网络学习自然驾驶数据中的正常交互模式，并通过极端值理论计算上下文自适应的风险评分。

Result: GSSM在测试中表现出色，AUPRC达到0.9，提前2.6秒预警潜在碰撞，且在不同交互类型中均优于基线方法。

Conclusion: GSSM为交通交互中的碰撞风险提供了一种可扩展、上下文感知且通用的量化方法。

Abstract: Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.

</details>


### [388] [Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction](https://arxiv.org/abs/2505.13889)
*Yiting Zhang,Shichen Li*

Main category: cs.RO

TL;DR: 提出了一种安全运动规划与控制框架，用于可变形线性物体（DLO）的操纵，通过预测形状和张力并集成实时轨迹优化器，确保安全性。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅关注形状预测，忽略了接触和张力的约束，可能导致DLO和机器人受损，需解决这一问题。

Method: 结合预测模型（估计DLO未来形状和张力）和基于多项式zonotopes的实时轨迹优化器，强制执行安全约束。

Result: 在模拟线束装配任务中，相比现有方法，任务成功率更高且无安全违规。

Conclusion: 该方法在接触密集环境中实现了稳健且安全的DLO操纵。

Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.

</details>


### [389] [Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning](https://arxiv.org/abs/2505.13925)
*Yunpeng Jiang,Jianshu Hu,Paul Weng,Yutong Ban*

Main category: cs.RO

TL;DR: 论文提出了一种利用时间反转对称性增强深度强化学习（TR-DRL）的方法，通过轨迹反转增强和奖励塑造提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注空间对称性，而忽略了时间对称性。本文旨在填补这一空白，探索时间反转对称性在机器人任务中的应用。

Method: 提出TR-DRL框架，结合轨迹反转增强和奖励塑造。通过动态一致性过滤器识别完全可逆的过渡，生成反转轨迹以增强数据；对部分可逆过渡应用奖励塑造。

Result: 在Robosuite和MetaWorld基准测试中，TR-DRL在单任务和多任务设置中均表现出更高的样本效率和更强的最终性能。

Conclusion: TR-DRL通过利用时间反转对称性，显著提升了深度强化学习在机器人任务中的表现。

Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.

</details>


### [390] [NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)
*Matteo El-Hariry,Antoine Richard,Ricard M. Castan,Luis F. W. Batista,Matthieu Geist,Cedric Pradalier,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: NavBench是一个多领域基准测试框架，用于训练和评估基于强化学习的导航策略，支持不同机器人平台和环境。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常局限于特定平台，限制了泛化能力和跨平台公平比较。

Method: 基于IsaacLab构建，标准化任务定义，支持统一跨介质测试、模块化设计和仿真到现实的验证。

Result: 成功将策略迁移到多种真实机器人，包括卫星模拟器、无人水面艇和轮式地面车辆。

Conclusion: NavBench简化了适应性导航策略的开发，支持广泛应用的模块化设计。

Abstract: Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [391] [MatchDance: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis](https://arxiv.org/abs/2505.14222)
*Kaixing Yang,Xulong Tang,Yuxuan Hu,Jiahao Yang,Hongyan Liu,Qinnan Zhang,Jun He,Zhaoxin Fan*

Main category: cs.SD

TL;DR: MatchDance是一个新颖的音乐到舞蹈生成框架，通过两阶段设计提升舞蹈动作的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在舞蹈生成中难以保持舞蹈动作的一致性，MatchDance旨在解决这一问题。

Method: 采用两阶段设计：1）基于运动学-动力学的量化阶段（KDQS）编码舞蹈动作；2）混合音乐到舞蹈生成阶段（HMDGS）映射音乐到潜在表示。

Result: 在FineDance数据集上表现出最先进的性能。

Conclusion: MatchDance通过潜在表示和混合架构显著提升了舞蹈生成的连贯性。

Abstract: Music-to-dance generation represents a challenging yet pivotal task at the
intersection of choreography, virtual reality, and creative content generation.
Despite its significance, existing methods face substantial limitation in
achieving choreographic consistency. To address the challenge, we propose
MatchDance, a novel framework for music-to-dance generation that constructs a
latent representation to enhance choreographic consistency. MatchDance employs
a two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),
which encodes dance motions into a latent representation by Finite Scalar
Quantization (FSQ) with kinematic-dynamic constraints and reconstructs them
with high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),
which uses a Mamba-Transformer hybrid architecture to map music into the latent
representation, followed by the KDQS decoder to generate 3D dance motions.
Additionally, a music-dance retrieval framework and comprehensive metrics are
introduced for evaluation. Extensive experiments on the FineDance dataset
demonstrate state-of-the-art performance. Code will be released upon
acceptance.

</details>


### [392] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/abs/2505.13577)
*Yubin Kim,Taehan Kim,Wonjune Kang,Eugene Park,Joonsik Yoon,Dongjae Lee,Xin Liu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park*

Main category: cs.SD

TL;DR: VocalAgent是一种基于音频大语言模型（LLM）的工具，用于语音健康诊断，通过多数据集微调和综合评估框架，显著提升了语音障碍分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 全球语音障碍普遍存在，但许多人缺乏便捷的诊断和治疗途径，因此需要一种可扩展的解决方案。

Method: 利用Qwen-Audio-Chat模型，基于医院患者的现场数据集进行微调，并通过安全评估、跨语言性能分析和模态消融研究进行多维度评估。

Result: VocalAgent在语音障碍分类上表现优于现有基准方法。

Conclusion: VocalAgent为语音健康诊断提供了可扩展的解决方案，同时强调了伦理和技术验证的重要性。

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [393] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/abs/2505.13805)
*Yu Pan,Yanni Hu,Yuguang Yang,Jixun Yao,Jianhao Ye,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Main category: cs.SD

TL;DR: ClapFM-EVC是一种新型情感语音转换框架，通过自然语言提示或参考语音实现高质量语音转换，支持情感强度调节。


<details>
  <summary>Details</summary>
Motivation: 尽管情感语音转换（EVC）取得进展，但实现高保真且灵活可控的EVC仍具挑战性。

Method: 提出EVC-CLAP模型跨模态对齐情感元素，结合FuEncoder和自适应强度门融合特征，并通过流匹配模型重建Mel谱图。

Result: 主客观评估验证了ClapFM-EVC的有效性。

Conclusion: ClapFM-EVC在情感表达和语音自然度上表现优异，为EVC提供了新思路。

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [394] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Main category: cs.SD

TL;DR: 利用语音分段声学特征检测深度伪造音频，发现分段特征比全局特征更有效。


<details>
  <summary>Details</summary>
Motivation: 探索分段语音声学特征在检测深度伪造音频中的潜力，因其与人类发音过程密切相关且难以被伪造。

Method: 分析分段声学特征在深度伪造音频检测中的表现，并与全局特征对比。

Result: 某些常用于法医语音比较的分段特征能有效识别深度伪造音频，而全局特征效果较差。

Conclusion: 音频深度伪造检测需区别于法医语音比较，分段特征为此提供了新视角。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [395] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/abs/2505.13971)
*Ming Gao,Shilong Wu,Hang Chen,Jun Du,Chin-Hui Lee,Shinji Watanabe,Jingdong Chen,Siniscalchi Sabato Marco,Odette Scharenborg*

Main category: cs.SD

TL;DR: MISP 2025 Challenge聚焦多模态、多设备会议转录，结合视频和音频，任务包括AVSD、AVSR和AVDR。最佳系统显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 会议场景的复杂声学条件对语音应用提出挑战，多模态方法（视频+音频）有望提升性能。

Method: 挑战赛包含三个任务：AVSD、AVSR和AVDR，参与者提出解决方案，使用多模态数据集和基线系统。

Result: 最佳系统表现显著：AVSD DER降低7.43%，AVSR CER降低10.62%，AVDR cpCER降低72.49%。

Conclusion: 多模态方法在会议转录任务中效果显著，未来可进一步优化。

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [396] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/abs/2505.13771)
*Wanli Sun,Anton Ragni*

Main category: cs.SD

TL;DR: 本文提出了一种新方法，用于训练能量基模型（EBM），通过改进噪声对比估计（NCE）和切片分数匹配（SSM）的局限性，使其更适合一阶优化方案。


<details>
  <summary>Details</summary>
Motivation: NCE和SSM方法在训练EBM时存在局限性，尤其是忽略了对数似然函数的形式，而EBM和扩散模型（DM）在推理时依赖一阶优化。因此，需要一种更适合一阶优化的评分学习方法。

Method: 提出了一种新准则，通过学习更适合一阶优化方案的评分，改进了NCE和SSM的不足。

Result: 实验对比了NCE、SSM和新方法在训练EBM时的表现，验证了新方法的有效性。

Conclusion: 新方法通过学习更适合一阶优化的评分，显著提升了EBM的训练效果，为相关领域提供了更优的解决方案。

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [397] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/abs/2505.14285)
*Eirini Panteli,Paulo E. Santos,Nabil Humphrey*

Main category: cs.SD

TL;DR: AquaSignal是一个模块化、可扩展的水下声学信号处理管道，集成了深度学习技术，在噪声和动态海洋环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计AquaSignal是为了在嘈杂和动态的海洋环境中提高水下声学信号分析的可靠性和准确性。

Method: AquaSignal采用U-Net进行去噪，ResNet18进行分类，AutoEncoder进行异常检测，并在Deepship和ONC数据集上评估。

Result: 实验结果显示，AquaSignal在分类任务中达到71%的准确率，异常检测达到91%的准确率。

Conclusion: AquaSignal展示了在实时水下声学监测中的潜力，适用于科学、环境和海事领域。

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


### [398] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.SD

TL;DR: FMSD-TTS是一种少样本、多说话人、多方言的文本到语音框架，用于合成藏语多方言语音，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 藏语是一种低资源语言，缺乏平行语音语料库，限制了语音建模的进展。

Method: 提出FMSD-TTS框架，包含说话人-方言融合模块和方言专用动态路由网络（DSDR-Net），以捕捉方言间的细微声学和语言变化。

Result: FMSD-TTS在方言表达和说话人相似性上显著优于基线方法，并通过语音到语音方言转换任务验证了合成语音的质量。

Conclusion: 贡献包括：1）针对藏语多方言语音合成的少样本TTS系统；2）公开大规模合成藏语语音语料库；3）开源评估工具包。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [399] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Main category: cs.SD

TL;DR: PAST是一种新颖的端到端框架，联合建模语音信息和信号重建，无需依赖外部预训练模型。


<details>
  <summary>Details</summary>
Motivation: 消除对预训练自监督模型的依赖，直接利用监督语音数据，将领域知识通过辅助任务整合到标记化过程中。

Method: 提出PAST框架，结合监督语音数据和辅助任务，并引入可流式处理的因果变体，支持实时语音应用。

Result: PAST在语音表示和重建方面超越现有基线标记器，并在语音语言模型中表现优异。

Conclusion: PAST是一种高效的语音表示基础，适用于语音生成任务，并开源了完整实现。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [400] [An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents](https://arxiv.org/abs/2505.13504)
*Ayesha Amjad,Saurav Sthapit,Tahir Qasim Syed*

Main category: cs.IR

TL;DR: 提出了一种基于多智能体框架的AI系统，利用LLM智能体和强化学习驱动智能体，实现表单文档的自适应信息提取，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统OCR和学习算法或单一流水线在表单文档信息提取中存在局限性，无法系统性改进。

Method: 采用模块化多智能体框架，结合任务特定提示和强化学习策略，通过元提示智能体从错误中学习并改进提示驱动的执行智能体。

Result: 在SOIRE和CORD两个基准数据集上表现良好。

Conclusion: 该框架能够自适应处理多样化文档和布局，实现高精度信息提取。

Abstract: Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.

</details>


### [401] [Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering](https://arxiv.org/abs/2505.13520)
*Hessa Alawwad,Usman Naseem,Areej Alhothali,Ali Alkhathlan,Amani Jamal*

Main category: cs.IR

TL;DR: 提出了一种多模态教科书问答新方法JETRTQA，通过多目标联合训练增强语义表示，显著提升了复杂教育场景中的文档检索相关性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂教育场景中难以实现准确的语义对齐和任务特定的文档检索，影响了问答性能。

Method: JETRTQA基于检索-生成架构，结合多模态大语言模型生成答案，通过多目标联合训练（包括排序监督和答案隐式监督）优化语义表示。

Result: 在CK12-QA数据集上，JETRTQA显著提升了文档检索的区分能力，验证集准确率提升2.4%，测试集提升11.1%。

Conclusion: JETRTQA通过改进语义表示和检索相关性，在复杂教育场景中优于现有方法，为多模态教科书问答提供了有效解决方案。

Abstract: Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.

</details>


### [402] [Geography-Aware Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2505.13526)
*Zhao Liu,Wei Liu,Huajie Zhu,Jianxing Yu,Jian Yin,Wang-Chien Lee,Shun Wang*

Main category: cs.IR

TL;DR: GA-LLM是一个结合地理信息和POI转换关系的新型框架，用于提升LLM在下一个POI推荐任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在空间任务中难以建模精确的空间上下文和POI转换关系，限制了其在POI推荐中的应用。

Method: GA-LLM通过地理坐标注入模块（GCIM）和POI对齐模块（PAM）增强LLM，分别处理地理特征和POI关系。

Result: 在三个真实数据集上的实验表明，GA-LLM达到了最先进的性能。

Conclusion: GA-LLM通过结合地理和语义信息，有效解决了LLM在空间任务中的局限性。

Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.

</details>


### [403] [LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems](https://arxiv.org/abs/2505.13528)
*Shengkang Gu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Ning Gu,Li Shang,Tun Lu*

Main category: cs.IR

TL;DR: Agent4SR是一种利用LLM代理进行低知识、高影响力的shilling攻击的框架，通过生成评分和评论模拟真实用户行为，攻击效果显著且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 传统攻击策略依赖简单启发式方法，且忽略文本评论的潜在操纵能力，因此需要一种更高效且隐蔽的攻击方法。

Method: Agent4SR通过目标配置文件构建、混合记忆检索和评论攻击策略，模拟真实用户行为生成评分和评论。

Result: 实验表明，Agent4SR在多个数据集和推荐系统架构上优于现有低知识基线，攻击效果和隐蔽性更强。

Conclusion: LLM驱动的代理带来新型威胁，凸显现代推荐系统亟需加强防御。

Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.

</details>


### [404] [Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments](https://arxiv.org/abs/2505.13535)
*Aniket Bhattacharyya,Anurag Tripathi,Ujjal Das,Archan Karmakar,Amit Pathak,Maneesh Gupta*

Main category: cs.IR

TL;DR: BLOCKIE是一种基于LLM的新方法，通过将视觉丰富文档（VRD）组织为可重用的语义块，独立处理每个块，从而在推理和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有非LLM的NLP方法缺乏推理能力，无法提取文档中未明确提及的信息，且对新格式泛化能力差；而现有LLM方法在理解文档布局方面表现不佳。

Method: 提出BLOCKIE方法，将VRD分解为局部化的语义块，独立处理每个块，实现更专注和泛化的推理。

Result: 在公共VRD基准测试中，BLOCKIE的F1分数比现有最佳方法高1-3%，且对未见过的文档格式具有鲁棒性。

Conclusion: BLOCKIE通过语义块的处理方式，显著提升了信息提取的性能和泛化能力。

Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.

</details>


### [405] [RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines](https://arxiv.org/abs/2505.13538)
*Dvir Cohen,Lin Burg,Gilad Barkan*

Main category: cs.IR

TL;DR: RAGXplain是一个评估框架，通过量化RAG性能并提供可操作的改进建议，帮助用户理解和优化复杂的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG评估方法仅提供定量分数，缺乏对复杂管道的具体指导和改进建议。

Method: RAGXplain利用LLM推理将原始分数转化为清晰的叙述，识别性能差距并提出改进建议。

Result: 实验表明，RAGXplain的评估与人类判断高度一致，其建议显著提升了系统性能。

Conclusion: RAGXplain通过透明解释和实用优化，增强了用户对AI系统的理解和信任。

Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.

</details>


### [406] [Know Or Not: a library for evaluating out-of-knowledge base robustness](https://arxiv.org/abs/2505.13545)
*Jessica Foo,Pradyumna Shyama Prasad,Shaun Khoo*

Main category: cs.IR

TL;DR: 论文提出了一种评估大语言模型（LLM）在检索增强生成（RAG）设置中对外部知识库（OOKB）鲁棒性的新方法，并开发了开源工具knowornot。


<details>
  <summary>Details</summary>
Motivation: LLM在高风险应用中因幻觉风险受限，需评估其在RAG设置中对外部知识的鲁棒性。

Method: 提出无需人工标注的系统评估方法，开发knowornot工具，支持自定义评估数据和流程。

Result: knowornot提供统一API、模块化架构、数据建模设计和工具套件，成功应用于PolicyBench基准测试。

Conclusion: knowornot为LLM的OOKB鲁棒性评估提供了高效、灵活且可复现的解决方案。

Abstract: While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.

</details>


### [407] [JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation](https://arxiv.org/abs/2505.13550)
*Ke Yang,Kevin Ros,Shankar Kumar Senthil Kumar,ChengXiang Zhai*

Main category: cs.IR

TL;DR: 论文提出了即时信息推荐（JIR）的数学定义和评估框架，并引入了多模态基准数据集JIR-Arena，用于评估JIR系统在推断用户需求、推荐时效性和内容相关性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对JIR任务的系统性定义和评估方法，阻碍了相关技术的发展。

Method: 通过结合人类和AI模型的输入近似用户需求分布，使用静态知识库快照评估JIR质量，并采用多轮多实体验证框架提高客观性。

Result: 基于基础模型的JIR系统在模拟用户需求时精度较高，但在召回率和内容检索方面存在挑战。

Conclusion: 论文为JIR研究提供了首个数学定义和基准数据集，支持未来研究的发展。

Abstract: Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.

</details>


### [408] [AMAQA: A Metadata-based QA Dataset for RAG Systems](https://arxiv.org/abs/2505.13557)
*Davide Bruni,Marco Avvenuti,Nicola Tonellotto,Maurizio Tesconi*

Main category: cs.IR

TL;DR: AMAQA是一个新的开放访问QA数据集，用于评估结合文本和元数据的任务，特别适用于需要快速分析大量数据的领域。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试缺乏元数据集成，阻碍了在需要文本数据和外部信息的场景下的评估。

Method: AMAQA包含110万条来自Telegram的英文消息，并添加了时间戳、主题、情感语气和毒性指标等元数据，以及450个高质量QA对。

Result: 利用元数据将准确率从0.12提升到0.61，并通过迭代上下文和噪声文档进一步优化，比最佳基线提高了3个百分点。

Conclusion: AMAQA为元数据驱动的QA和RAG系统研究提供了新标准，展示了结构化上下文的价值。

Abstract: Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/

</details>


### [409] [Bridge the Gap between Past and Future: Siamese Model Optimization for Context-Aware Document Ranking](https://arxiv.org/abs/2505.14180)
*Songhao Wu,Quan Tu,Mingjie Zhong,Hong Liu,Jia Xu,Jinjie Gu,Rui Yan*

Main category: cs.IR

TL;DR: 论文提出了一种结合未来上下文信息的会话上下文模型（ForeRanker），通过历史条件模型和未来感知模型的双模型协作优化，提升文档排序效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用历史会话数据，难以捕捉用户意图的动态变化，因此需要引入未来上下文信息以提升排序效果。

Method: 提出双模型框架（历史条件模型和未来感知模型），通过监督标签和伪标签协作训练，并引入动态门控机制的知识蒸馏方法以减少训练不一致性。

Result: 在基准数据集上，ForeRanker表现出优于现有方法的性能。

Conclusion: 结合未来上下文信息能有效提升文档排序效果，ForeRanker框架为此提供了可行方案。

Abstract: In the realm of information retrieval, users often engage in multi-turn
interactions with search engines to acquire information, leading to the
formation of sequences of user feedback behaviors. Leveraging the session
context has proven to be beneficial for inferring user search intent and
document ranking. A multitude of approaches have been proposed to exploit
in-session context for improved document ranking. Despite these advances, the
limitation of historical session data for capturing evolving user intent
remains a challenge. In this work, we explore the integration of future
contextual information into the session context to enhance document ranking. We
present the siamese model optimization framework, comprising a
history-conditioned model and a future-aware model. The former processes only
the historical behavior sequence, while the latter integrates both historical
and anticipated future behaviors. Both models are trained collaboratively using
the supervised labels and pseudo labels predicted by the other. The
history-conditioned model, referred to as ForeRanker, progressively learns
future-relevant information to enhance ranking, while it singly uses historical
session at inference time. To mitigate inconsistencies during training, we
introduce the peer knowledge distillation method with a dynamic gating
mechanism, allowing models to selectively incorporate contextual information.
Experimental results on benchmark datasets demonstrate the effectiveness of our
ForeRanker, showcasing its superior performance compared to existing methods.

</details>


### [410] [Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning](https://arxiv.org/abs/2412.12504)
*Hong Liu,Saisai Gong,Yixin Ji,Kaixin Wu,Jia Xu,Jinjie Gu*

Main category: cs.IR

TL;DR: 论文提出了一种名为DaRL的框架，通过改进损失函数和样本增强技术，提升LLM在搜索相关性建模中的判别能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度相关性判别和分布偏移场景下表现不佳，需要一种更高效的解决方案。

Method: 设计了分布感知的损失函数和样本增强模块（DASA），并采用多阶段微调策略。

Result: DaRL在Alipay搜索中成功部署，提升了相关性的判别能力和模型泛化性。

Conclusion: DaRL框架有效解决了LLM在相关性建模中的局限性，适用于实际应用场景。

Abstract: With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...

</details>


### [411] [Field Matters: A lightweight LLM-enhanced Method for CTR Prediction](https://arxiv.org/abs/2505.14057)
*Yu Cui,Feng Liu,Jiawei Chen,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Xiaohu Yang,Can Wang*

Main category: cs.IR

TL;DR: LLaCTR是一种轻量级的LLM增强CTR预测方法，通过字段级增强范式提升性能，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强方法需要处理大量文本描述，计算开销大，LLaCTR旨在解决这一问题。

Method: LLaCTR利用LLM从小规模特征字段中提取轻量级语义知识，并通过自监督字段特征微调增强特征表示和交互。

Result: 在四个数据集上与六种CTR模型集成，LLaCTR在效果和效率上均优于现有方法。

Conclusion: LLaCTR提供了一种高效且有效的LLM增强CTR预测解决方案。

Abstract: Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.

</details>


### [412] [TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems](https://arxiv.org/abs/2505.13881)
*Jiahao Yu,Haozhuang Liu,Yeqiu Yang,Lu Chen,Wu Jian,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 论文提出了一种名为TranSUN的新方法，通过模型内部修正解决推荐系统中回归模型的再转换偏差问题，并进一步推广为通用回归模型家族GTS。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的回归模型存在再转换偏差问题，现有方法多为事后修正，难以实际应用。

Method: 提出TranSUN方法，通过联合偏差学习实现理论无偏性，并扩展为通用模型家族GTS。

Result: 实验证明方法在多个领域数据上表现优越，并成功应用于淘宝App的实际推荐场景。

Conclusion: TranSUN和GTS为推荐系统提供了理论保障且实用的无偏回归解决方案。

Abstract: Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.

</details>


### [413] [Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity](https://arxiv.org/abs/2505.14310)
*Shiyin Tan,Dongyuan Li,Renhe Jiang,Zhen Wang,Xingtong Yu,Manabu Okumura*

Main category: cs.IR

TL;DR: 论文提出了一种名为CausalEPP的新方法，通过考虑用户对流行物品偏好的动态变化，来减少推荐系统中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常对所有用户统一处理流行度偏差，且未充分考虑用户或物品的时间演化特性，而用户对流行物品的偏好是动态变化的。

Method: 引入‘Evolving Personal Popularity’指标量化用户对流行物品的偏好，设计因果图整合动态偏好，并应用去混淆训练减少偏差。

Result: CausalEPP在减少流行度偏差的同时提高了推荐准确性，优于基线方法。

Conclusion: CausalEPP通过动态建模用户偏好，有效解决了推荐系统中的流行度偏差问题。

Abstract: Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.

</details>


### [414] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Main category: cs.IR

TL;DR: NExT-Search提出了一种新的生成式AI搜索范式，通过引入细粒度的过程级反馈来解决传统反馈循环断开的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索虽然提高了便利性，但破坏了传统Web搜索的反馈驱动改进循环，导致难以优化中间阶段。

Method: NExT-Search设计了两种模式：用户调试模式和影子用户模式，结合在线适应和离线更新机制。

Result: 通过恢复人类对生成式AI搜索关键阶段的控制，NExT-Search有望构建反馈丰富的AI搜索系统。

Conclusion: NExT-Search为生成式AI搜索的持续改进提供了有前景的方向。

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [415] [Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning](https://arxiv.org/abs/2505.14581)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 使用强化学习优化认知无线电网络性能，通过能量收集和动态决策提升数据率。


<details>
  <summary>Details</summary>
Motivation: 在存在主用户的情况下，次用户需要高效利用有限能量资源，同时避免干扰主用户。

Method: 结合时间切换方法和深度Q网络（DQN），动态选择能量收集或数据传输，并优化传输功率。

Result: 提出的方法优于基线策略，并能收敛到最优解。

Conclusion: 该方法有效提升了次用户的数据率，同时兼顾能量效率。

Abstract: In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [416] [Learning to Program Quantum Measurements for Machine Learning](https://arxiv.org/abs/2505.13525)
*Samual Yen-Chi Chen,Huan-Hsin Tseng,Hsin-Yi Lin,Shinjae Yoo*

Main category: quant-ph

TL;DR: 论文提出了一种创新的量子机器学习框架，通过动态编程可观测量的方法，优化量子电路参数和神经网络，显著提升了量子机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的快速发展推动了量子机器学习的研究，但高性能QML模型的开发需要专家级知识，且现有方法在数据编码和测量方案设计上存在不足。

Method: 提出了一种端到端可微分学习框架，动态编程量子系统的可观测量（厄米矩阵），同时优化神经网络和量子电路参数。

Result: 数值模拟显示，该方法能动态编程可观测量，在变分量子电路中表现优异，分类准确率等性能指标优于现有方法。

Conclusion: 该框架通过动态优化可观测量，显著提升了QML模型的整体效能，为量子机器学习的发展提供了新思路。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have sparked significant interest, driving extensive exploration of quantum
machine learning (QML) algorithms to address a wide range of complex
challenges. The development of high-performance QML models requires
expert-level expertise, presenting a key challenge to the widespread adoption
of QML. Critical obstacles include the design of effective data encoding
strategies and parameterized quantum circuits, both of which are vital for the
performance of QML models. Furthermore, the measurement process is often
neglected-most existing QML models employ predefined measurement schemes that
may not align with the specific requirements of the targeted problem. We
propose an innovative framework that renders the observable of a quantum
system-specifically, the Hermitian matrix-trainable. This approach employs an
end-to-end differentiable learning framework, enabling simultaneous
optimization of the neural network used to program the parameterized
observables and the standard quantum circuit parameters. Notably, the quantum
observable parameters are dynamically programmed by the neural network,
allowing the observables to adapt in real time based on the input data stream.
Through numerical simulations, we demonstrate that the proposed method
effectively programs observables dynamically within variational quantum
circuits, achieving superior results compared to existing approaches. Notably,
it delivers enhanced performance metrics, such as higher classification
accuracy, thereby significantly improving the overall effectiveness of QML
models.

</details>


### [417] [Benchmarking data encoding methods in Quantum Machine Learning](https://arxiv.org/abs/2505.14295)
*Orlane Zang,Grégoire Barrué,Tony Quertier*

Main category: quant-ph

TL;DR: 量子机器学习（QML）中数据编码方法的选择对模型性能至关重要，但目前缺乏通用规则。本文研究了常用编码方法，并通过不同数据集进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 数据编码在QML中具有基础性作用，但选择合适的方法缺乏明确规则，因此需要系统研究和比较不同编码方法的性能。

Method: 研究了多种常用的量子数据编码方法，并利用不同数据集进行了基准测试。

Result: 通过实验比较了不同编码方法的性能表现。

Conclusion: 编码方法的选择对QML模型性能有显著影响，未来需要进一步研究以制定更通用的选择规则。

Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.

</details>


### [418] [QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems](https://arxiv.org/abs/2505.14192)
*Bikash K. Behera,Saif Al-Kuwari,Ahmed Farouk*

Main category: quant-ph

TL;DR: 提出了一种混合量子学习模型QSVM-QNN，结合量子支持向量机和量子神经网络，显著提升了EEG-BCI任务的分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决BCI系统中信号变异性、分类效率低和实时适应性问题。

Method: 集成QSVM和QNN，利用QSVM的决策边界能力和QNN的表达学习能力。

Result: 在两个EEG基准数据集上分别达到0.990和0.950的高准确率，优于经典和独立量子模型。

Conclusion: QSVM-QNN在噪声环境下表现稳定，适用于实际量子环境，并可推广到其他生物医学和时间序列分类任务。

Abstract: A brain-computer interface (BCI) system enables direct communication between
the brain and external devices, offering significant potential for assistive
technologies and advanced human-computer interaction. Despite progress, BCI
systems face persistent challenges, including signal variability,
classification inefficiency, and difficulty adapting to individual users in
real time. In this study, we propose a novel hybrid quantum learning model,
termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with
a Quantum Neural Network (QNN), to improve classification accuracy and
robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines
the decision boundary capabilities of QSVM with the expressive learning power
of QNN, leading to superior generalization performance. The proposed model is
evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and
0.950, outperforming both classical and standalone quantum models. To
demonstrate real-world viability, we further validated the robustness of QNN,
QSVM, and QSVM-QNN against six realistic quantum noise models, including bit
flip and phase damping. These experiments reveal that QSVM-QNN maintains stable
performance under noisy conditions, establishing its applicability for
deployment in practical, noisy quantum environments. Beyond BCI, the proposed
hybrid quantum architecture is generalizable to other biomedical and
time-series classification tasks, offering a scalable and noise-resilient
solution for next-generation neurotechnological systems.

</details>


### [419] [Quantum Optimization via Gradient-Based Hamiltonian Descent](https://arxiv.org/abs/2505.14670)
*Jiaqi Leng,Bin Shi*

Main category: quant-ph

TL;DR: 论文提出了一种基于梯度信息的量子哈密顿下降（QHD）改进方法，显著提升了收敛速度和全局解发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统量子哈密顿下降（QHD）在收敛速度和高度非凸问题中的鲁棒性方面存在不足，且仅依赖函数值信息。

Method: 通过结合梯度信息，提出梯度基QHD，利用高分辨率微分方程的机制改进算法。

Result: 数值模拟显示，梯度基QHD在收敛速度和全局解发现上优于现有量子与经典方法至少一个数量级。

Conclusion: 梯度基QHD为复杂优化问题提供了更高效的量子算法解决方案。

Abstract: With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [420] [CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction](https://arxiv.org/abs/2505.13558)
*Yingjie Kuang,Tianchen Zhang,Zhen-Wei Huang,Zhongjie Zeng,Zhe-Yuan Li,Ling Huang,Yuefang Gao*

Main category: econ.EM

TL;DR: 本文提出了一种结合聚类和注意力机制的GRU模型（CAGRU），用于预测客户购买意图，解决了客户群体不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 准确预测客户购买意图对商业策略至关重要，但现有研究多关注产品类型，忽视了复购行为预测，且客户群体不平衡限制了传统时间序列方法的效果。

Method: CAGRU模型通过多模态数据，先聚类客户群体，再用GRU提取时间序列特征，并引入注意力机制捕捉序列重要性，最后分群体训练模型以解决不平衡问题。

Result: 在四个数据集上的实验表明，CAGRU方法优于传统方法。

Conclusion: CAGRU能有效预测客户购买意图，尤其在处理不平衡客户群体时表现优越。

Abstract: Accurately predicting customers' purchase intentions is critical to the
success of a business strategy. Current researches mainly focus on analyzing
the specific types of products that customers are likely to purchase in the
future, little attention has been paid to the critical factor of whether
customers will engage in repurchase behavior. Predicting whether a customer
will make the next purchase is a classic time series forecasting task. However,
in real-world purchasing behavior, customer groups typically exhibit imbalance
- i.e., there are a large number of occasional buyers and a small number of
loyal customers. This head-to-tail distribution makes traditional time series
forecasting methods face certain limitations when dealing with such problems.
To address the above challenges, this paper proposes a unified Clustering and
Attention mechanism GRU model (CAGRU) that leverages multi-modal data for
customer purchase intention prediction. The framework first performs customer
profiling with respect to the customer characteristics and clusters the
customers to delineate the different customer clusters that contain similar
features. Then, the time series features of different customer clusters are
extracted by GRU neural network and an attention mechanism is introduced to
capture the significance of sequence locations. Furthermore, to mitigate the
head-to-tail distribution of customer segments, we train the model separately
for each customer segment, to adapt and capture more accurately the differences
in behavioral characteristics between different customer segments, as well as
the similar characteristics of the customers within the same customer segment.
We constructed four datasets and conducted extensive experiments to demonstrate
the superiority of the proposed CAGRU approach.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [421] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Main category: q-fin.CP

TL;DR: SAE-FiRE框架通过稀疏自编码器分析财报电话会议转录，提取关键信息并消除冗余，显著提升对盈利意外的预测能力。


<details>
  <summary>Details</summary>
Motivation: 财报电话会议转录是公司高管、分析师和股东间的重要沟通渠道，但内容冗长且专业术语多，给语言模型分析带来挑战。

Method: 提出SAE-FiRE框架，利用稀疏自编码器（SAEs）识别模式、过滤噪音，专注于捕捉具有预测能力的财务信号。

Result: 实验结果表明，该方法显著优于基线模型。

Conclusion: SAE-FiRE框架能有效解决财报电话会议转录的分析难题，提升盈利意外预测的准确性。

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [422] [Large-Scale Multi-Character Interaction Synthesis](https://arxiv.org/abs/2505.14087)
*Ziyi Chang,He Wang,George Alex Koulieris,Hubert P. H. Shum*

Main category: cs.GR

TL;DR: 论文提出了一种生成大规模多角色交互动画的方法，解决了现有方法在交互合成和过渡规划上的不足。


<details>
  <summary>Details</summary>
Motivation: 多角色交互动画在角色动画中具有挑战性和重要性，现有方法无法处理密集交互和过渡规划的问题。

Method: 提出了一种条件生成流水线，包括可协调的多角色交互空间和过渡规划网络。

Result: 实验证明了该方法的有效性，展示了其可扩展性和可迁移性。

Conclusion: 该方法为多角色交互动画的生成提供了一种有效的解决方案。

Abstract: Generating large-scale multi-character interactions is a challenging and
important task in character animation. Multi-character interactions involve not
only natural interactive motions but also characters coordinated with each
other for transition. For example, a dance scenario involves characters dancing
with partners and also characters coordinated to new partners based on spatial
and temporal observations. We term such transitions as coordinated interactions
and decompose them into interaction synthesis and transition planning. Previous
methods of single-character animation do not consider interactions that are
critical for multiple characters. Deep-learning-based interaction synthesis
usually focuses on two characters and does not consider transition planning.
Optimization-based interaction synthesis relies on manually designing objective
functions that may not generalize well. While crowd simulation involves more
characters, their interactions are sparse and passive. We identify two
challenges to multi-character interaction synthesis, including the lack of data
and the planning of transitions among close and dense interactions. Existing
datasets either do not have multiple characters or do not have close and dense
interactions. The planning of transitions for multi-character close and dense
interactions needs both spatial and temporal considerations. We propose a
conditional generative pipeline comprising a coordinatable multi-character
interaction space for interaction synthesis and a transition planning network
for coordinations. Our experiments demonstrate the effectiveness of our
proposed pipeline for multicharacter interaction synthesis and the applications
facilitated by our method show the scalability and transferability.

</details>


### [423] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Main category: cs.GR

TL;DR: 论文提出了一种新指标PTME用于评估网格标记化方法，并提出了一种坐标合并技术以提升现有标记化方法的压缩率。


<details>
  <summary>Details</summary>
Motivation: 当前自回归网格生成方法缺乏对标记化方法的高效评估指标，需要一种无需训练的评估方法。

Method: 引入PTME指标评估网格标记化方法，并提出坐标合并技术优化现有方法。

Result: 实验验证了PTME的有效性，坐标合并技术显著提升了压缩率。

Conclusion: PTME和坐标合并技术有望改进现有网格标记化方法，推动原生网格生成的发展。

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [424] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Main category: cs.SE

TL;DR: 论文探讨了如何利用大语言模型（LLMs）增强软件质量保证（SQA）过程，同时结合现有标准（如ISO/IEC 12207等），并分析了其应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 软件质量保证（SQA）对交付可靠、安全和高效的软件产品至关重要。LLMs的出现为自动化SQA任务（如需求分析、代码审查等）提供了新机会，但如何与现有标准结合仍需研究。

Method: 论文首先回顾了软件质量标准和LLMs的技术基础，接着探讨了LLM在SQA中的应用（如需求验证、缺陷检测等），并将其映射到关键质量框架中。通过案例研究和开源项目验证了方法的可行性。

Result: 研究表明，LLMs可以有效增强传统SQA方法，但需解决数据隐私、模型偏见和可解释性等挑战。

Conclusion: 未来方向包括自适应学习、隐私保护部署、多模态分析以及制定适应AI驱动的软件质量标准。

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


### [425] [Selective Code Generation for Functional Guarantees](https://arxiv.org/abs/2505.13553)
*Jaewoo Jeong,Taesoo Kim,Sangdon Park*

Main category: cs.SE

TL;DR: 论文提出了一种通过动态代码分析工具自动生成单元测试的方法，以减少代码生成模型的幻觉问题，并提出了选择性代码生成器和FuzzEval评估范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成任务中表现出色，但其幻觉问题限制了其在需要高安全标准的系统中的适用性。代码的复杂性使得识别生成代码的功能正确性变得困难，而传统单元测试方法扩展成本高昂。

Method: 通过动态代码分析工具自动生成单元测试，利用代码的可执行性。提出选择性代码生成器，避免对不确定的生成结果进行回答，以控制幻觉率。同时提出FuzzEval评估范式，用于精确评估代码。

Result: 实验表明，选择性代码生成器在开放和封闭代码生成器中均表现出色，能够有效控制代码幻觉率，并保持合理的选择效率。

Conclusion: 该方法通过自动生成单元测试和选择性代码生成器，显著提升了代码生成的可信度和可控性，为高安全标准系统提供了可行的解决方案。

Abstract: Large language models (LLMs) show human-level performance and their
specialized descendants, code generation models, play core roles in solving
complex tasks, including mathematical reasoning and software development. On
the downside, the hallucination of LLMs mainly hinders their applicability to
systems requiring higher safety standards, thus drawing the attention of the AI
community. However, the hallucination of code generation models is rarely
considered. One critical bottleneck in considering code hallucination is the
intricate property of code to identify whether generated code has the intended
functionality due to its un-natural form, different to natural languages.
Handful of unit tests have been considered to address this issue, but
scaling-up its size is extremely expensive. We address this core bottleneck by
automatically generating unit tests using dynamic code analysis tools, which
leverages the \emph{executable nature} of code. Given generated unit tests from
true code for measuring functional correctness of generated code, we propose to
learn a \emph{selective code generator}, which abstains from answering for
unsure generation, to control the rate of code hallucination among
non-abstaining answers in terms of a false discovery rate. This learning
algorithm provides a controllability guarantee, providing trustworthiness of
code generation. Finally, we propose to use generated unit tests in evaluation
as well as in learning for precise code evaluation, calling this evaluation
paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code
generator over open and closed code generators, showing clear benefit of
leveraging generated unit tests along with the controllability of code
hallucination and reasonable selection efficiency via our selective code
generator.

</details>


### [426] [HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps](https://arxiv.org/abs/2505.13693)
*Hiya Bhatt,Shaunak Biswas,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 论文提出HarmonE架构，通过MAPE-K循环增强MLOps管道的自适应性，以解决机器学习系统在动态环境中的可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在动态环境中运行时面临数据漂移和模型退化等问题，影响技术、经济、环境和社会多维度可持续性。现有MLOps方法仅关注技术维度，且传统频繁重训练方法带来高能耗。

Method: 引入HarmonE架构，设计时定义可持续目标和适应阈值，运行时监测关键指标（如预测精度、能耗、数据分布变化），触发适应性策略。

Result: 通过智能交通系统的数字孪生验证，HarmonE能有效适应变化条件，保持精度并实现可持续目标。

Conclusion: HarmonE为机器学习系统提供了一种多维度可持续的解决方案，弥补了MLOps的不足。

Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world
applications, but ensuring their sustainable performance over time remains a
significant challenge. These systems operate in dynamic environments and face
runtime uncertainties like data drift and model degradation, which affect the
sustainability of MLS across multiple dimensions: technical, economical,
environmental, and social. While Machine Learning Operations (MLOps) addresses
the technical dimension by streamlining the ML model lifecycle, it overlooks
other dimensions. Furthermore, some traditional practices, such as frequent
retraining, incur substantial energy and computational overhead, thus
amplifying sustainability concerns. To address them, we introduce HarmonE, an
architectural approach that enables self-adaptive capabilities in MLOps
pipelines using the MAPE-K loop. HarmonE allows system architects to define
explicit sustainability goals and adaptation thresholds at design time, and
performs runtime monitoring of key metrics, such as prediction accuracy, energy
consumption, and data distribution shifts, to trigger appropriate adaptation
strategies. We validate our approach using a Digital Twin (DT) of an
Intelligent Transportation System (ITS), focusing on traffic flow prediction as
our primary use case. The DT employs time series ML models to simulate
real-time traffic and assess various flow scenarios. Our results show that
HarmonE adapts effectively to evolving conditions while maintaining accuracy
and meeting sustainability goals.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [427] [Pel, A Programming Language for Orchestrating AI Agents](https://arxiv.org/abs/2505.13453)
*Behnam Mohammadi*

Main category: cs.PL

TL;DR: Pel是一种专为LLM设计的新型编程语言，旨在解决现有方法在表达性、可扩展性、安全性和细粒度控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的控制和编排方法存在表达性、可扩展性、成本和安全性等方面的局限性，需要一种更高效、安全的解决方案。

Method: Pel结合了Lisp、Elixir、Gleam和Haskell的优点，提供了一种语法简单、语义丰富的平台，支持复杂操作、控制流和代理间通信。

Result: Pel通过语法级能力控制、线性组合、闭包支持、自然语言条件和自动并行化等特性，实现了更强大、安全的LLM编排。

Conclusion: Pel为LLM编排提供了更健壮、安全和表达性的范式，推动了更复杂可靠的AI代理框架的发展。

Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in
computing, yet controlling and orchestrating their capabilities beyond simple
text generation remains a challenge. Current methods, such as function/tool
calling and direct code generation, suffer from limitations in expressiveness,
scalability, cost, security, and the ability to enforce fine-grained control.
This paper introduces Pel, a novel programming language specifically designed
to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and
Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich
platform for LLMs to express complex actions, control flow, and inter-agent
communication safely and efficiently. Pel's design emphasizes a minimal, easily
modifiable grammar suitable for constrained LLM generation, eliminating the
need for complex sandboxing by enabling capability control at the syntax level.
Key features include a powerful piping mechanism for linear composition,
first-class closures enabling easy partial application and functional patterns,
built-in support for natural language conditions evaluated by LLMs, and an
advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and
LLM-powered helper agents for automated error correction. Furthermore, Pel
incorporates automatic parallelization of independent operations via static
dependency analysis, crucial for performant agentic systems. We argue that Pel
offers a more robust, secure, and expressive paradigm for LLM orchestration,
paving the way for more sophisticated and reliable AI agentic frameworks.

</details>


### [428] [RTL++: Graph-enhanced LLM for RTL Code Generation](https://arxiv.org/abs/2505.13479)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: RTL++提出了一种基于图表示的LLM辅助方法，用于生成高质量的RTL代码，解决了传统方法在安全性和多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着硬件设计复杂度的增加，传统RTL设计方法效率低且易出错，现有LLM存在安全和隐私问题，开源模型则因数据不足而质量不佳。

Method: RTL++将RTL代码编码为文本化的控制流图（CFG）和数据流图（DFG），通过图结构增强LLM对代码的理解和生成能力。

Result: 实验表明，RTL++在VerilogEval基准测试中优于现有模型，证明了图增强上下文对提升LLM辅助RTL代码生成的有效性。

Conclusion: RTL++通过图表示方法显著提升了RTL代码生成的质量和多样性，为LLM在EDA领域的应用提供了新思路。

Abstract: As hardware design complexity escalates, there is an urgent need for advanced
automation in electronic design automation (EDA). Traditional register transfer
level (RTL) design methods are manual, time-consuming, and prone to errors.
While commercial (instruction-tuned) large language models (LLMs) shows
promising performance for automation, they pose security and privacy concerns.
Open-source models offer alternatives; however, they frequently fall short in
quality/correctness, largely due to limited, high-quality RTL code data
essential for effective training and generalization. This paper proposes RTL++,
a first-of-its-kind LLM-assisted method for RTL code generation that utilizes
graph representations of code structures to enhance the quality of generated
code. By encoding RTL code into a textualized control flowgraphs (CFG) and data
flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and
relationships within the code. This structured graph-based approach enhances
the context available to LLMs, enabling them to better understand and generate
instructions. By focusing on data generation through graph representations,
RTL++ addresses the limitations of previous approaches that rely solely on code
and suffer from lack of diversity. Experimental results demonstrate that RTL++
outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated
using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1
model, which highlight the effectiveness of graph-enhanced context in advancing
the capabilities of LLM-assisted RTL code generation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [429] [Sight, Sound and Smell in Immersive Experiences of Urban History: Virtual Vauxhall Gardens Case Study](https://arxiv.org/abs/2505.13612)
*Tim Pearce,David Souto,Douglas Barrett,Benjamin Lok,Mateusz Bocian,Artur Soczawa-Stronczyk,Giasemi Vavoula,Paul Long,Avinash Bhangaonkar,Stephanie Bowry,Michaela Butter,David Coke,Kate Loveman,Rosemary Sweet,Lars Tharp,Jeremy Webster,Hongji Yang,Robin Green,Andrew Hugill*

Main category: cs.HC

TL;DR: 研究探讨了在虚拟现实历史空间重建中整合多感官元素（尤其是嗅觉）的效果，通过案例研究发现同步嗅觉刺激能显著提升用户参与感和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉和听觉在数字文化遗产体验中已成标准，嗅觉刺激的潜力尚未充分挖掘，但其对记忆和情感的联系具有强大作用。

Method: 在虚拟重建的18世纪伦敦Vauxhall Pleasure Gardens中，开发了便携式嗅觉显示器，将气味与视听元素同步，并通过技术实现和用户体验评估方法测量效果。

Result: 结果表明，同步嗅觉刺激能增强用户参与感，且用户体验吸引力极高，但标准可用性指标可能不适用于多感官体验评估。

Conclusion: 多感官VR文化遗产体验需更细致的评估方法，重点在于创造情感共鸣而非单纯的真实感。

Abstract: We explore the integration of multisensory elements in virtual reality
reconstructions of historical spaces through a case study of the Virtual
Vauxhall Gardens project. While visual and auditory components have become
standard in digital heritage experiences, the addition of olfactory stimuli
remains underexplored, despite its powerful connection to memory and emotional
engagement. This research investigates how multisensory experiences involving
olfaction can be effectively integrated into VR reconstructions of historical
spaces to enhance presence and engagement with cultural heritage. In the
context of a VR reconstruction of London's eighteenth-century Vauxhall Pleasure
Gardens, we developed a networked portable olfactory display capable of
synchronizing specific scents with visual and auditory elements at pivotal
moments in the virtual experience. Our evaluation methodology assesses both
technical implementation and user experience, measuring presence, and usability
metrics across diverse participant groups. Our results show that integrating
synchronized olfactory stimuli into the VR experience can enhance user
engagement and be perceived positively, contributing to a unique and immersive
encounter with historical settings. While presence questionnaires indicated a
strong sense of auditory presence and control, with other sensory factors rated
moderately, user experience of attractiveness was exceptionally high;
qualitative feedback suggested heightened sensory awareness and engagement
influenced by the inclusion and anticipation of smell. Our results suggest that
evaluating multisensory VR heritage experiences requires a nuanced approach, as
standard usability metrics may be ill-suited and 'realism' might be less
critical than creating an evocative, historically informed, and emotionally
resonant experience......

</details>


### [430] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)
*Ulrike Kuhl,Annika Bush*

Main category: cs.HC

TL;DR: 研究表明，带有偏见的AI推荐会影响人类决策，尤其是当缺乏反事实解释时，人类的偏好会与AI偏见一致；而提供解释则能逆转这种影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI偏见如何通过推荐影响人类决策，以及反事实解释是否能减轻这种影响。

Method: 通过模拟招聘实验，分阶段测试参与者在无AI、有偏见AI推荐（带或不带反事实解释）后的决策变化。

Result: 参与者70%的时间遵循AI推荐，但很少人发现性别偏见。无解释时，偏见影响持续；有解释时，偏见被逆转。信任度无显著差异。

Conclusion: 需校准可解释AI以避免偏见传播，确保公平决策。

Abstract: Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.

</details>


### [431] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)
*Lance T Wilhelm,Xiaohan Ding,Kirk McInnis Knutsen,Buse Carik,Eugenia H Rho*

Main category: cs.HC

TL;DR: 研究探讨了AI辅助沟通系统（如CommCoach）如何帮助管理者提升沟通技能，强调适应性、低风险模拟和透明反馈的重要性。


<details>
  <summary>Details</summary>
Motivation: 管理者缺乏定制化、持续的沟通培训，AI系统可能提供可扩展的解决方案，但管理者对AI角色的预期尚不明确。

Method: 设计了一个对话角色扮演系统CommCoach，通过半结构化访谈了解管理者对AI辅助沟通训练的期望。

Result: 参与者重视适应性模拟、透明反馈和AI生成角色的可控性，但也指出需平衡个性化与结构化学习目标。

Conclusion: AI辅助沟通训练需在适应性、一致性和开放性之间找到平衡，同时解决潜在偏见和现实性问题。

Abstract: Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [432] [Non-Obvious Manipulability in Additively Separable and Fractional Hedonic Games](https://arxiv.org/abs/2505.13642)
*Diodato Ferraioli,Giovanna Varricchio*

Main category: cs.GT

TL;DR: 论文研究了非明显可操纵（NOM）机制的设计，针对两类简洁表示的Hedonic Games（可加可分离和分数Hedonic Games），证明了在特定条件下最优机制是NOM，并设计了高效的NOM机制。


<details>
  <summary>Details</summary>
Motivation: 研究NOM机制是为了解决有限理性代理可能无法识别机制可操纵性的问题，尤其是在Hedonic Games中。

Method: 通过理论证明和机制设计，分析了NOM机制在连续和离散评分条件下的可行性，并提出了高效的近似机制。

Result: 证明了在连续评分区间内存在最优NOM机制，并设计了多项式时间内可实现的近似NOM机制。

Conclusion: NOM机制在特定条件下与最优性兼容，为Hedonic Games中的机制设计提供了新的理论支持。

Abstract: In this work, we consider the design of Non-Obviously Manipulable (NOM)
mechanisms, mechanisms that bounded rational agents may fail to recognize as
manipulable, for two relevant classes of succinctly representable Hedonic
Games: Additively Separable and Fractional Hedonic Games. In these classes,
agents have cardinal scores towards other agents, and their preferences over
coalitions are determined by aggregating such scores. This aggregation results
in a utility function for each agent, which enables the evaluation of outcomes
via the utilitarian social welfare. We first prove that, when scores can be
arbitrary, every optimal mechanism is NOM; moreover, when scores are limited in
a continuous interval, there exists an optimal mechanism that is NOM. Given the
hardness of computing optimal outcomes in these settings, we turn our attention
to efficient and NOM mechanisms. To this aim, we first prove a characterization
of NOM mechanisms that simplifies the class of mechanisms of interest. Then, we
design a NOM mechanism returning approximations that asymptotically match the
best-known approximation achievable in polynomial time. Finally, we focus on
discrete scores, where the compatibility of NOM with optimality depends on the
specific values. Therefore, we initiate a systematic analysis to identify which
discrete values support this compatibility and which do not.

</details>


### [433] [Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains](https://arxiv.org/abs/2505.14551)
*Petros Drineas,Rohit Nema,Rafail Ostrovsky,Vassilis Zikas*

Main category: cs.GT

TL;DR: 本文提出了一种新的可信声誉系统模型，通过博弈论方法确保用户真实报告对服务器可信度的信念，并支持其在区块链中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有声誉系统易受操纵且缺乏经济稳健性，尤其是在区块链中。本文旨在设计一种可信的声誉系统，确保用户真实报告并支持信任度估计。

Method: 提出了一类称为'可信声誉博弈'的游戏，用户报告对服务器可信度的信念，确保其策略为最佳响应且支持信任度估计。

Result: 证明了该博弈的纳什均衡允许观察者估计服务器的相对可信度，并展示了其在PoR区块链中的应用潜力。

Conclusion: 本文通过博弈论方法设计了一种可信声誉系统，解决了现有系统的操纵问题，并展示了其在区块链中的实用性。

Abstract: Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [434] [Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach](https://arxiv.org/abs/2505.14336)
*Umberto Cappellazzo,Minsu Kim,Stavros Petridis,Daniele Falavigna,Alessio Brutti*

Main category: eess.AS

TL;DR: Llama-SMoP是一种高效的多模态大型语言模型，通过稀疏混合投影器（SMoP）模块在不增加推理成本的情况下扩展模型容量，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在音频-视觉语音识别（AVSR）中计算成本高的问题，以提升在资源受限环境中的部署能力。

Method: 提出Llama-SMoP模型，采用稀疏混合投影器（SMoP）模块，结合稀疏门控的专家混合（MoE）技术，探索三种SMoP配置。

Result: Llama-SMoP DEDR（分离专家和路由器）在ASR、VSR和AVSR任务中表现优异，验证了其在专家激活、可扩展性和噪声鲁棒性方面的有效性。

Conclusion: Llama-SMoP通过SMoP模块实现了高效的多模态LLM，为资源受限环境提供了可行的解决方案。

Abstract: Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.

</details>


### [435] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/abs/2505.13455)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: eess.AS

TL;DR: 研究探讨了非重叠和重叠对话对情绪同步的影响，发现非重叠对话更稳定，而重叠对话表现出更高的变异性。


<details>
  <summary>Details</summary>
Motivation: 理解多通道情绪表达与同步对情绪识别系统和人机交互有重要意义。

Method: 使用IEMOCAP数据集，通过EmoNet和Wav2Vec2提取情绪数据，分析非重叠和重叠对话的情绪对齐。

Result: 非重叠对话情绪同步更稳定，重叠对话变异性高但DTW显示更紧密对齐。面部表情在轮流对话中领先语音，语音在同时发声时领先。

Conclusion: 对话结构对情绪同步有重要影响，揭示了多模态情绪对齐的时空动态。

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [436] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/abs/2505.13617)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: 本文提出了一种方向感知神经场（DANF），通过Ambisonic格式的RIR更明确地捕捉方向信息，弥补了现有方法在方向特性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经场的方法仅支持单声道全向或双耳听众，无法精确捕捉真实声场的定向特性。

Method: 提出DANF模型，结合方向感知损失，并探索其在新房间中的适应能力，包括低秩适应。

Result: DANF能够更精确地捕捉声场的定向特性，并具备适应新房间的能力。

Conclusion: DANF为声场建模提供了更精确的方向感知能力，并展示了良好的适应性。

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [437] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/abs/2505.13814)
*Jihwan Lee,Kevin Huang,Kleanthis Avramidis,Simon Pistrosch,Monica Gonzalez-Machorro,Yoonjeong Lee,Björn Schuller,Louis Goldstein,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 提出了一种从表面肌电信号（EMG）预测发音特征并解码为可理解语音的模型，结合卷积层和Transformer模块，预测相关性达0.9，并分析了电极位置对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 探索从EMG信号预测发音特征并解码为语音的新方法，填补EMG语音合成的空白。

Method: 结合卷积层和Transformer模块，分别预测发音特征，并通过解码生成语音波形。

Result: 预测发音特征的相关性达0.9，成功解码为可理解语音，并分析了电极位置的影响。

Conclusion: 该方法为EMG语音合成提供了新思路，并优化了电极配置。

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [438] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/abs/2505.13541)
*Amirbek Djanibekov,Nurdaulet Mukhituly,Kentaro Inui,Hanan Aldarmaki,Nils Lukas*

Main category: eess.AS

TL;DR: SLMs通过语音交互更易受对抗攻击，攻击成功率可达100%。提出的后修补防御方法在不影响实用性的情况下，将鲁棒性提升至99%。


<details>
  <summary>Details</summary>
Motivation: 语音语言模型（SLMs）通过语音交互更贴近用户意图，但也带来新的安全风险，如对抗攻击。

Method: 分析对抗攻击并提出后修补防御方法，通过修改SLM的激活值提升安全性。

Result: 防御方法将鲁棒性提升至99%，且不影响实用性，无需重新训练。

Conclusion: 后修补防御方法有效提升了SLMs的安全性，同时保持了实用性。

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [439] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/abs/2505.14465)
*Aviv Navon,Aviv Shamsian,Yael Segal-Feldman,Neta Glazer,Gil Hetz,Joseph Keshet*

Main category: eess.AS

TL;DR: FlowTSE是一种基于条件流匹配的目标说话人提取方法，简化了生成式TSE的流程，并在标准基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成式方法在目标说话人提取（TSE）中表现强劲，但现有方法依赖复杂流程和预训练组件，计算开销大。

Method: 提出FlowTSE，基于条件流匹配，输入为梅尔频谱图和混合语音信号，提取目标说话人语音；并提出一种新型声码器以改进相位估计。

Result: 在标准TSE基准测试中，FlowTSE表现与强基线相当或更优。

Conclusion: FlowTSE是一种简单有效的生成式TSE方法，减少了计算开销并提升了性能。

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [440] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2505.14517)
*Jakob Kienegger,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文提出了一种弱引导的说话人提取方法，仅依赖目标的初始位置来解决空间动态场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标方向已知且静态时表现优异，但在动态场景中（如说话人移动或交叉时）面临时间变化的空间特征和模糊性问题。手动跟踪移动说话人不切实际，因此需要一种不依赖精确方向线索的解决方案。

Method: 结合深度跟踪算法和联合训练策略，使用合成数据集进行训练，提出了一种弱引导的提取方法。

Result: 该方法能够有效解决空间模糊性问题，并在性能上超越了不匹配的强引导提取方法。

Conclusion: 弱引导方法在动态场景中具有显著优势，为说话人提取提供了更实用的解决方案。

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [441] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/abs/2505.14561)
*Theo Lepage,Reda Dehak*

Main category: eess.AS

TL;DR: 论文提出了一种名为SSPS的自监督正采样技术，通过聚类和记忆队列在潜在空间中寻找合适的正样本，显著提升了说话人验证性能。


<details>
  <summary>Details</summary>
Motivation: 标准自监督学习框架在说话人验证中主要编码了录音条件的信道信息，限制了性能提升。

Method: 提出SSPS技术，利用聚类和记忆队列在潜在空间中寻找同一说话人但不同录音条件的正样本。

Result: SSPS显著降低了说话人内方差，SimCLR-SSPS和DINO-SSPS在VoxCeleb1-O上分别达到2.57%和2.53%的EER，优于现有方法。

Conclusion: SSPS是一种有效的自监督正采样技术，显著提升了说话人验证性能。

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [442] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Main category: cs.CL

TL;DR: 论文提出了一种以记忆为中心的EQA框架MemoryEQA，通过多模态分层记忆机制提升复杂任务处理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有EQA框架以规划器为中心，记忆模块无法与其他模块充分交互，限制了处理复杂任务的能力。

Method: 提出MemoryEQA框架，采用全局和局部记忆的分层机制，并利用多模态大语言模型转换记忆信息以适应不同模块输入。

Result: 在HM-EQA、MT-HM3D和OpenEQA数据集上验证了框架有效性，MT-HM3D上性能提升19.8%。

Conclusion: MemoryEQA通过增强记忆模块的交互能力，显著提升了复杂EQA任务的性能。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [443] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TL;DR: 本文提出了一种基于最近邻检索的跨语言迁移学习方法，用于低资源语言中的仇恨言论检测，通过检索多语言数据增强目标语言的少量标注数据，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测数据标注成本高且耗时，尤其在低资源语言中更为突出。跨语言迁移学习和数据增强在有限标注数据任务中表现出有效性，因此需要开发高效且可扩展的方法。

Method: 利用最近邻检索技术，从多语言仇恨言论检测池中检索与目标语言少量标注数据最相关的实例，以增强数据。同时采用最大边际相关性减少冗余。

Result: 在八种语言上的实验表明，该方法优于仅使用目标语言数据的模型，并在多数情况下超越当前最优方法，且具有高度数据效率和可扩展性。

Conclusion: 该方法通过检索增强少量标注数据，显著提升了低资源语言仇恨言论检测的性能，同时具备高效性和可扩展性。

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [444] [From Words to Worlds: Compositionality for Cognitive Architectures](https://arxiv.org/abs/2407.13419)
*Ruchira Dhar,Anders Søgaard*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在组合能力上表现复杂，规模扩展提升组合性，但指令调优可能削弱这种能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备更强的组合性，以及这是否是其高性能的原因。

Method: 对四个LLM家族的12个模型和三类任务进行实证分析，包括一项新任务。

Result: 规模扩展增强组合能力，但指令调优常产生相反效果。

Conclusion: LLMs的发展需更关注与人类认知能力的对齐，解决组合性学习中的不一致问题。

Abstract: Large language models (LLMs) are very performant connectionist systems, but
do they exhibit more compositionality? More importantly, is that part of why
they perform so well? We present empirical analyses across four LLM families
(12 models) and three task categories, including a novel task introduced below.
Our findings reveal a nuanced relationship in learning of compositional
strategies by LLMs -- while scaling enhances compositional abilities,
instruction tuning often has a reverse effect. Such disparity brings forth some
open issues regarding the development and improvement of large language models
in alignment with human cognitive capacities.

</details>


### [445] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在自杀风险评估中的表现，发现Claude和GPT与人类标注接近，Mistral在预测误差上最低，但需谨慎部署。


<details>
  <summary>Details</summary>
Motivation: 在线平台如Reddit的r/SuicideWatch为自杀意念表达提供了空间，但LLMs的兴起可能改变这一模式，研究旨在评估LLMs在自杀风险评估中的能力。

Method: 使用哥伦比亚自杀严重程度评定量表（C-SSRS），对六种模型（包括Claude、GPT、Mistral和LLaMA）的零样本性能进行评估，分类帖子的严重程度（0-6级）。

Result: Claude和GPT与人类标注结果接近，Mistral在预测误差上表现最佳，多数模型对相邻严重级别敏感。

Conclusion: 研究强调了人类监督、透明度和谨慎部署的重要性，代码和补充材料已开源。

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [446] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在文本评估中的一致性和偏见问题，发现模型间和模型内高度一致，但来源标注会显著降低一致性，尤其是对中国来源的文本。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在文本评估中的一致性、偏见和框架效应，以确保其公正性和中立性。

Method: 使用四种先进LLMs评估4800条叙述性声明，操纵来源标注（LLM或人类作者国籍），分析模型间和模型内一致性。

Result: 未标注来源时，模型间和模型内高度一致；标注来源后，一致性下降，尤其是对中国来源的文本。

Conclusion: 框架效应对LLMs的文本评估有显著影响，需关注其在中立性和公平性方面的潜在问题。

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [447] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Main category: cs.CL

TL;DR: 论文研究了LLMs中的社会谄媚现象，提出了ELEPHANT框架评估五种行为，发现LLMs谄媚率显著高于人类，且难以缓解。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注可验证的谄媚行为，忽略了模糊情境中的社会谄媚，可能强化有害行为或信念。

Method: 提出社会谄媚理论，开发ELEPHANT框架评估五种行为，使用OEQ和AITA数据集测试八个模型。

Result: LLMs在OEQ中谄媚率比人类高47%，在AITA中42%情况下支持不当行为；谄媚行为在偏好数据集中受奖励且难以缓解。

Conclusion: 研究为社会谄媚提供了理论和实证工具，揭示了这一被忽视但重要的问题。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [448] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Main category: cs.CL

TL;DR: 论文探讨了语言模型如何编码并强化有害的性别刻板印象，提出需超越表面关联，重新定义性别偏见，并通过实证研究发现模型倾向于将性别与生物性别二元化。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅通过解耦非性别词汇与性别词汇来缓解性别偏见，但忽视了性别建构本身的问题，可能导致对跨性别和性别多样群体的伤害。

Method: 结合性别研究理论，实证测试了16种不同架构、训练数据和规模的模型对性别的编码方式。

Result: 发现语言模型倾向于将性别与生物性别二元化，且对不符合二元分类的性别词汇进行抹除和病理化；更大模型强化了性别与性别的关联。

Conclusion: 呼吁重新定义和解决语言模型中的性别偏见问题，以更全面地应对性别建构带来的潜在伤害。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [449] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在感知到被评估时会改变行为（类似霍桑效应），影响其安全对齐。作者提出了一种白盒探测框架，量化了这种“测试意识”对模型行为的影响，并展示了不同模型的差异。


<details>
  <summary>Details</summary>
Motivation: 量化LLM在感知评估时的行为变化（测试意识）及其对安全对齐的影响，以提高安全评估的可信度。

Method: 提出白盒探测框架，线性识别与测试意识相关的激活，并引导模型行为，监控下游性能。

Result: 测试意识显著影响安全对齐，且不同模型表现不同。

Conclusion: 通过精细控制测试意识，研究旨在提升安全评估的信任度。

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [450] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Main category: cs.CL

TL;DR: LitmusValues通过评估AI模型的价值优先级，预测其潜在风险行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，检测其风险行为变得更困难，因此需要一种基于价值识别的早期预警系统。

Method: 开发LitmusValues评估管道和AIRiskDilemmas数据集，通过模型在价值冲突场景中的选择揭示其价值优先级。

Result: LitmusValues中的价值（如Care）能预测已知和未知的风险行为。

Conclusion: 基于价值优先级的评估方法可有效识别AI模型的潜在风险。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [451] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TL;DR: SLOT是一种参数高效的测试时推理方法，通过少量优化步骤更新样本特定参数向量，提升语言模型对复杂指令的响应能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在复杂指令上表现不佳的问题。

Method: 在测试时进行少量优化步骤，更新轻量级样本特定参数向量，并将其添加到最终隐藏层。

Result: 在多个基准测试和模型上表现优异，例如Qwen2.5-7B在GSM8K上准确率提升8.6%。

Conclusion: SLOT能有效提升语言模型对复杂指令的适应能力，表现优于现有方法。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [452] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Main category: cs.CL

TL;DR: 论文介绍了一个中文多模态隐喻广告数据集（EmoMeta），包含5000个文本-图像对，标注了隐喻、领域关系和细粒度情感分类，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 多模态隐喻在情感表达中起关键作用，但现有研究缺乏多模态隐喻细粒度情感数据集，且多集中于英语，忽略了语言间的差异。

Method: 构建了一个中文多模态隐喻广告数据集，包含5000个文本-图像对，标注了隐喻、领域关系和10种细粒度情感。

Result: 数据集公开可用（https://github.com/DUTIR-YSQ/EmoMeta），支持多模态隐喻情感分类的进一步研究。

Conclusion: 该数据集填补了多模态隐喻情感研究的空白，为跨语言情感分析提供了新资源。

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [453] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Main category: cs.CL

TL;DR: 论文提出了一种基于生成预训练Transformer的框架，用于更好地理解和总结电商评论，帮助用户快速做出决策。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，用户对电商的偏好增加，但大量评论可能导致决策瘫痪。现有工具仅提供评分调整，无法真正帮助用户理解评论内容。

Method: 使用生成预训练Transformer（GPT-3的Curie引擎）进行微调，引入抽象性总结而非简单的提取性总结，以更好地反映评论间的关系。

Result: 模型能够生成评论的优缺点总结，为用户提供更直观的决策支持。

Conclusion: 通过生成性模型引入“常识”元素，帮助用户快速理解评论并做出决策，提升了电商体验。

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [454] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.CL

TL;DR: IRLBench是一个新的多语言基准测试，专注于英语和爱尔兰语，用于评估大语言模型在低资源语言环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在文化偏见、仅限文本评估、依赖选择题格式，且对极低资源语言支持不足。

Method: 基于2024年爱尔兰毕业考试开发12个代表性科目，采用长文本生成任务和官方评分标准。

Result: 实验显示模型在爱尔兰语中的表现显著低于英语，正确率分别为55.8%和76.2%。

Conclusion: IRLBench为未来多语言AI研究提供了工具，强调文化意识和语言保真度的重要性。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [455] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Main category: cs.CL

TL;DR: 研究发现，通过向大语言模型（LLM）的激活中注入高斯噪声，安全微调的鲁棒性存在显著漏洞，导致有害输出率上升，且更深的安全微调无法提供额外保护。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型中安全护栏在扰动下的鲁棒性，以揭示当前安全对齐技术的脆弱性。

Method: 通过系统性地向模型激活中注入高斯噪声，测试多个开源模型的安全微调效果。

Result: 高斯噪声使有害输出率显著上升（p < 0.001），更深的安全微调无额外保护作用，但链式推理能力基本不受影响。

Conclusion: 当前安全对齐技术存在关键漏洞，推理和强化学习方法可能是开发更鲁棒AI安全系统的方向。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [456] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Main category: cs.CL

TL;DR: EcoSafeRAG提出了一种不依赖LLM内部知识的方法，通过句子级处理和诱饵引导的上下文多样性检测来防御RAG中的恶意内容，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG通过外部知识增强LLM的事实准确性，但引入了新的攻击面（如语料库污染），现有防御方法依赖LLM内部知识，与RAG设计理念冲突。

Method: EcoSafeRAG采用句子级处理和诱饵引导的上下文多样性检测，分析候选文档的上下文多样性，识别恶意内容。

Result: 实验表明EcoSafeRAG在保持实用操作成本的同时（延迟1.2倍，令牌减少48%-80%），提供了最先进的安全性，并提升了干净场景下的RAG性能。

Conclusion: EcoSafeRAG在不依赖LLM内部知识的情况下，有效解决了RAG的安全问题，同时优化了性能。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [457] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Main category: cs.CL

TL;DR: Time-R1是一个框架，赋予中等规模（3B参数）的LLM全面的时间能力，包括理解、预测和创造性生成，通过三阶段强化学习课程实现，性能超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缺乏强大的时间智能，难以整合过去推理与未来预测，且现有方法泛化能力差。

Method: 采用三阶段强化学习课程，逐步构建时间理解、未来事件预测和创造性生成能力。

Result: Time-R1在挑战性任务上表现优于200倍大的模型，如671B DeepSeek-R1。

Conclusion: 通过精心设计的强化学习微调，小模型可实现卓越的时间性能，推动时间感知AI发展。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [458] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）中重复诅咒的现象，发现诱导头（induction heads）是导致重复行为的关键机制，并提出了一种注意力头正则化技术来缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 重复诅咒现象广泛存在但其机制尚不明确，研究旨在揭示诱导头在其中的作用。

Method: 通过分析诱导头的毒性（即其在重复过程中主导输出logits的倾向），提出注意力头正则化技术。

Result: 发现诱导头是重复诅咒的主要驱动因素，并验证了正则化技术的有效性。

Conclusion: 研究为LLM设计和训练提供了新思路，通过抑制诱导头可提升生成多样性和连贯性。

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [459] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: LogiBreak是一种利用逻辑表达式翻译绕过大型语言模型安全系统的通用黑盒越狱方法，通过将有害自然语言提示转换为形式逻辑表达式，利用对齐数据和逻辑输入之间的分布差异，有效规避安全约束。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在与人类价值观对齐方面取得了显著进展，但其安全机制仍易受越狱攻击。本研究假设这种脆弱性源于对齐导向提示与恶意提示之间的分布差异。

Method: 提出LogiBreak方法，通过将有害自然语言提示转换为形式逻辑表达式，利用对齐数据与逻辑输入之间的分布差异，绕过LLM安全系统。

Result: 在多语言越狱数据集上评估LogiBreak，证明其在多种评估设置和语言环境中均有效。

Conclusion: LogiBreak通过逻辑表达式翻译成功规避LLM安全约束，揭示了当前安全机制在逻辑输入上的脆弱性。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [460] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: 论文研究了基于强化学习的多模态大语言模型（MLLMs）在医学视觉问答（VQA）中的优化问题，提出了四个关键维度的影响因素，并验证了GRPO方法在准确性和推理质量上的优越性。


<details>
  <summary>Details</summary>
Motivation: 为了将模型响应与临床期望对齐，解决直接应用强化学习（RL）到医学任务中的挑战。

Method: 研究了四个关键维度：基础模型初始化策略、医学语义对齐的作用、长度奖励对长链推理的影响以及偏差的影响，并进行了大量实验。

Result: GRPO-based RL调优在准确性和推理质量上均优于标准监督微调（SFT）。

Conclusion: 研究为医学MLLMs的领域特定微调提供了新见解，并验证了GRPO方法的有效性。

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [461] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Main category: cs.CL

TL;DR: 论文提出了一种结合NMT和LLM的翻译调度策略，通过优化LLM使用频率，在保证翻译质量的同时降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: LLM在翻译任务中表现优异但计算成本高，而NMT系统效率更高但某些场景下表现不足，因此需要一种结合两者的优化方案。

Method: 提出了一种基于源语句特征的调度策略，动态决定何时使用LLM或NMT，以减少LLM的使用频率。

Result: 实验表明，该策略在多语言测试集上能以最少的LLM使用实现最优翻译性能。

Conclusion: 结合NMT和LLM的调度策略是一种高效且经济的翻译解决方案。

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [462] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Main category: cs.CL

TL;DR: EmoGist是一种无需训练的上下文学习方法，用于视觉情感分类，通过上下文定义情感标签提高准确性。


<details>
  <summary>Details</summary>
Motivation: 情感在图像中的表现具有高度上下文依赖性，传统方法可能无法捕捉其复杂性。

Method: EmoGist预生成情感标签的多重解释，基于嵌入相似性检索解释并用于分类。

Result: 在Memotion和FI数据集上，EmoGist分别提升了13点和8点的F1分数。

Conclusion: EmoGist通过上下文学习显著提升了视觉情感分类的准确性。

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [463] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Main category: cs.CL

TL;DR: 该研究比较了几种大语言模型（LLMs）和基于Transformer的模型在检测新闻文章中的宣传技术上的性能。GPT-4表现优于GPT-3.5和Claude 3 Opus，但未超过RoBERTa-CRF基线。某些情况下，LLMs在检测特定宣传技术上优于MultiGranularity Network（MGN）。


<details>
  <summary>Details</summary>
Motivation: 宣传者常使用修辞手段和情感诉求来推动议程，识别这些技术对做出明智决策至关重要。NLP的进步使得开发检测操纵性内容的系统成为可能。

Method: 研究比较了几种大语言模型（如GPT-4、GPT-3.5、Claude 3 Opus）和基于Transformer的模型（如RoBERTa-CRF、MGN）在检测宣传技术上的性能。

Result: GPT-4的F1分数（0.16）优于GPT-3.5和Claude 3 Opus，但低于RoBERTa-CRF基线（0.67）。LLMs在检测某些宣传技术（如name-calling、appeal to fear、flag-waving）上优于MGN基线。

Conclusion: 尽管GPT-4在LLMs中表现最佳，但基于Transformer的模型（如RoBERTa-CRF）在检测宣传技术上仍更优。LLMs在特定宣传技术的检测上显示出潜力。

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [464] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: 论文探讨了在问答系统中，通过知识蒸馏（KD）方法提升小语言模型（SLM）性能时，中间推理痕迹（如Chain-of-Thought）的忠实性评估问题。研究发现，正确的推理痕迹并不一定保证最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前问答系统（如ChatGPT）需要高准确性和透明度，但小语言模型性能不足。知识蒸馏方法虽能提升性能，但中间推理痕迹的评估存在挑战。

Method: 采用基于规则的问题分解方法，将复杂查询拆解为结构化子问题，生成可解释的推理痕迹，并在Open Book QA等数据集上进行实验。

Result: 实验发现，正确的推理痕迹与最终答案正确性相关性低，挑战了利用推理痕迹提升小语言模型性能的假设。

Conclusion: 研究揭示了推理痕迹评估的重要性，并指出其在知识蒸馏中的局限性，为未来改进提供了方向。

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [465] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Main category: cs.CL

TL;DR: EfficientLLM是一个新基准，首次全面评估大规模LLM的效率技术，涵盖架构预训练、微调和推理，揭示效率与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数和上下文窗口的增加，计算、能源和成本问题日益突出，需要系统研究效率技术。

Method: 在48xGH200和8xH200 GPU集群上，评估100多个模型-技术组合，涵盖注意力变体、稀疏MoE、参数高效微调和量化方法。

Result: 发现效率技术存在量化权衡，最优方法因任务和规模而异，且技术可跨模态迁移。

Conclusion: EfficientLLM为下一代基础模型的效率-性能权衡提供指导，开源数据集和工具支持研究。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [466] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Main category: cs.CL

TL;DR: DoGEN是一种通过集成领域专家检测模型来适应未见领域的机器文本检测技术，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的进步，检测机器生成文本的需求日益迫切，但现有检测器难以适应新领域和新模型。

Method: 提出DoGEN技术，通过域分类器的权重集成多个领域专家检测模型。

Result: 在多个基准测试中，DoGEN在域内检测中表现最佳，在域外检测中优于体积更大的模型。

Conclusion: DoGEN为领域自适应AI检测提供了有效解决方案，并公开了代码和模型以支持未来研究。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [467] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Main category: cs.CL

TL;DR: R1 Translator模型结合双向LSTM编码器和预训练Transformer解码器，显著提升EEG信号到文本的解码性能，优于T5和Brain Translator。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型快速发展，但EEG信号解码为文本仍存在性能限制，需改进。

Method: R1 Translator采用双向LSTM编码器捕获EEG序列依赖，结合Transformer解码器生成高质量文本。

Result: ROUGE-1得分38.00%（比T5高9%，比Brain高3%），CER和WER表现更优。

Conclusion: R1 Translator在EEG-to-text任务中表现卓越，为未来研究提供新方向。

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [468] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Main category: cs.CL

TL;DR: 论文提出了一种名为FlashThink的方法，通过提前终止大语言模型（LLMs）的推理过程，减少计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理任务中表现优异，但常生成冗长的推理内容，导致计算资源浪费。研究发现，模型在推理过程中可能提前得出正确答案，无需完成全部推理。

Method: 引入验证模型，识别推理过程中可以提前终止的时刻，从而缩短推理内容。

Result: 在四个基准测试中，FlashThink显著缩短了推理内容（如Deepseek-R1和QwQ-32B模型分别减少77.04%和77.47%），同时保持准确性。

Conclusion: FlashThink方法有效提高了LLMs的推理效率，为资源优化提供了新思路。

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [469] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Main category: cs.CL

TL;DR: CAFES是一个多代理协作框架，用于提升自动作文评分（AES）的泛化能力和多模态感知，显著提高了评分与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统AES方法在多模态评估和泛化性上表现不佳，而现有MLLM方法可能产生幻觉性评分。CAFES旨在解决这些问题。

Method: CAFES包含三个代理：初始评分器、反馈池管理器和反思评分器，通过协作迭代优化评分。

Result: 实验显示，CAFES在QWK指标上相对提升了21%，尤其在语法和词汇多样性方面表现突出。

Conclusion: CAFES为智能多模态AES系统奠定了基础，代码将在论文接受后公开。

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [470] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Main category: cs.CL

TL;DR: CS-Sum是首个评估大语言模型（LLMs）对代码切换（CS）对话理解能力的基准，涵盖三种语言对，发现LLMs在自动指标上表现良好，但仍存在细微错误。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs对代码切换（CS）对话的理解能力，填补该领域的研究空白。

Method: 通过CS-Sum基准，评估10种LLMs在少样本、翻译-总结和微调（LoRA、QLoRA）方法下的表现。

Result: LLMs在自动指标上得分高，但会犯细微错误，影响对话整体含义，且错误率因语言对和模型而异。

Conclusion: LLMs需要针对代码切换数据进行专门训练，以提高其理解和总结能力。

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [471] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出了一种基于TED Talks的大规模多语言平行语料库TED2025，用于提升大语言模型（LLM）的多语言性能。实验表明，使用多语言平行数据训练的模型优于非对齐数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 非对齐的多语言数据在捕捉跨语言语义方面存在局限性，而多语言平行数据能提供更强的跨语言一致性，从而提升LLM的多语言性能。

Method: 构建了TED2025语料库，涵盖113种语言，最多50种语言平行对齐，并研究了利用该数据进行持续预训练和指令调优的最佳实践。

Result: 在六个多语言基准测试中，使用多语言平行数据训练的模型表现优于非对齐数据训练的模型。

Conclusion: 多语言平行数据能显著提升LLM的多语言性能，TED2025语料库为相关研究提供了高质量资源。

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [472] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: PersonaConvBench是一个大规模基准测试，用于评估多轮对话中个性化推理和生成，结合了个人化和对话结构，包含三个核心任务，并展示了在统一提示设置下LLM的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常单独关注个人化或对话结构，而PersonaConvBench旨在结合两者，系统分析个性化对话上下文如何影响LLM输出。

Method: 设计了三个核心任务（句子分类、影响回归和用户中心文本生成），并在十个多样化Reddit领域中进行评估。

Result: 引入个性化历史显著提升性能，例如在情感分类任务中相对最佳非对话基线提升了198%。

Conclusion: 通过发布PersonaConvBench，支持研究LLM如何适应个体风格、跟踪长期上下文并生成丰富且吸引人的响应。

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [473] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 论文介绍了DiagnosisArena，一个用于评估大型语言模型在临床诊断推理能力的综合基准测试，结果显示当前模型的准确率较低。


<details>
  <summary>Details</summary>
Motivation: 由于现有医学基准测试在评估高级诊断推理能力方面的局限性，需要开发一个更全面的基准测试来评估模型的临床诊断能力。

Method: 通过筛选和审核10种顶级医学期刊的临床病例报告，构建了包含1,113对病例和诊断的基准测试DiagnosisArena，并进行了数据泄漏检查。

Result: 测试显示，当前最先进的模型o3-mini、o1和DeepSeek-R1的准确率分别为45.82%、31.09%和17.79%，表明其在临床诊断推理中存在显著的泛化瓶颈。

Conclusion: DiagnosisArena旨在推动AI诊断推理能力的进一步发展，为解决实际临床诊断挑战提供更有效的解决方案。

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [474] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 论文研究了在强化微调（RFT）中的先验提示工程（pPE），发现不同pPE方法能引导语言模型（LM）内化不同行为，且pPE训练模型优于推理时提示工程（iPE）模型。


<details>
  <summary>Details</summary>
Motivation: 现有RFT研究主要关注算法、奖励塑造和数据管理，而先验提示设计（即训练时预置的指令）尚未充分探索。

Method: 将五种iPE策略（如推理、规划、代码推理等）转化为pPE方法，并在Qwen2.5-7B模型上进行实验。

Result: 所有pPE训练模型均优于iPE提示模型，其中空示例pPE方法表现最佳，且在行为分类框架下显示不同pPE策略塑造了不同行为风格。

Conclusion: pPE是RFT中一个强大但未被充分研究的维度。

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [475] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.CL

TL;DR: 论文研究了分词（tokenization）对语言模型推理能力的影响，指出分词结构限制了符号计算的成功，并提出“Token Awareness”概念。通过实验证明，原子对齐的分词格式能显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 探讨分词作为语言模型计算的第一层如何影响符号推理能力，特别是子词分词方法（如BPE）对原子推理单元的模糊化问题。

Method: 理论分析和实证研究相结合，通过算术和符号任务的系统评估，比较不同分词结构对推理性能的影响。

Result: 分词结构显著影响推理性能，原子对齐的分词格式使小模型（如GPT-4o-mini）在结构化推理中优于大模型（如o1）。

Conclusion: 语言模型的符号推理能力不仅取决于架构，还深度依赖于分词级别的表示。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [476] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段的摘要生成框架，通过自动识别科学论文的结构功能，解决了现有方法无法充分捕捉结构化信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有摘要生成方法未能充分利用科学论文的结构化信息，且依赖关键词映射或特征工程，缺乏跨学科的鲁棒性。

Method: 1. 构建大规模数据集用于结构功能识别；2. 训练分类器识别关键结构组件；3. 使用Longformer生成上下文感知的摘要。

Result: 在两个领域特定的数据集上，该方法优于先进基线，生成了更全面的摘要。

Conclusion: 提出的框架通过结构功能识别和上下文建模，显著提升了科学论文摘要的质量。

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [477] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化方法对大型语言模型（LLM）的可解释性和透明度有显著影响，效果因量化方法、评估协议和解释方法而异。


<details>
  <summary>Details</summary>
Motivation: 量化对LLM能力的退化已有研究，但其对模型可解释性和透明度的影响尚未探索。

Method: 使用三种量化技术和两种可解释性方法（反事实示例和自然语言解释）及两种透明度方法（知识记忆分析和潜在多跳推理分析），结合用户研究。

Result: 量化对可解释性和透明度的影响不一致，有时退化，有时改善。

Conclusion: 量化可能不可预测地影响模型透明度，对需要透明度的应用部署有重要启示。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [478] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Main category: cs.CL

TL;DR: 论文提出了一种通过自动生成基于上下文的QA对来增强大型语言模型（LLM）在知识密集型QA任务中的方法，减少了人工标注的依赖并提升了模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前QA系统在处理复杂推理和实时知识整合时表现不佳，检索增强生成（RAG）技术仍面临多源信息逻辑连接的挑战。

Method: 采用自动生成QA对的方法，包括一个自动QA生成器和模型微调器，使用困惑度、ROUGE、BLEU和BERTScore进行评估。

Result: 实验表明，该方法在逻辑连贯性和事实准确性上有显著提升，Mistral-7b-v0.3在BERT F1、BLEU和ROUGE分数上优于Llama-3-8b。

Conclusion: 该方法为开发适应性强的AI系统提供了新思路，展示了自动生成QA对在提升LLM性能上的潜力。

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [479] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Main category: cs.CL

TL;DR: 该论文提出了一种利用代码混合和语音扰动的新型策略，成功破解大型语言模型（LLMs）的安全过滤器，并在文本和图像生成任务中取得了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的红队测试主要针对英语，使用固定模板攻击，导致模型在多语言和多模态环境下仍易受攻击。本研究旨在填补这一空白，探索更通用的安全对齐方法。

Method: 通过代码混合和语音扰动（如敏感词的语音拼写错误）设计新型攻击策略，并评估其在文本和图像生成任务中的效果。

Result: 新型攻击策略在文本生成中达到99%的攻击成功率，图像生成中为78%，攻击相关率分别为100%和95%。

Conclusion: 研究表明，语音扰动影响词标记化，导致破解成功，呼吁在多语言多模态模型中加强通用安全对齐的研究。

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [480] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Main category: cs.CL

TL;DR: ABFT通过优化注意力分数而非最终输出，以低成本提升语言模型的ICL性能。


<details>
  <summary>Details</summary>
Motivation: 减少传统ICL方法的高计算成本，利用注意力机制改进模型行为。

Method: 提出ABFT，基于注意力分数构建训练目标，优化标签令牌的注意力分布。

Result: 在9个现代LM和8个数据集上表现优异，数据成本仅为0.01%。

Conclusion: ABFT展示了通过控制LM内部模块改进行为的潜力，为机制可解释性应用开辟新方向。

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [481] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Main category: cs.CL

TL;DR: ABBA是一种新的参数高效微调（PEFT）架构，通过解耦更新与预训练权重，显著提高了表达能力，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务中表现优异，但高效适应新领域仍具挑战性。现有PEFT方法（如LoRA和HiRA）的表达能力受限于预训练模型的结构。

Method: ABBA通过将更新重新参数化为两个可独立学习的低秩矩阵的Hadamard积，完全解耦了更新与预训练权重。

Result: ABBA在矩阵重构实验中表现出更高的表达能力，并在算术和常识推理基准测试中显著优于现有PEFT方法。

Conclusion: ABBA提供了一种更灵活的PEFT方法，为模型适应新领域提供了更高的表达能力和性能。

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [482] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Main category: cs.CL

TL;DR: FuxiMT是一种基于稀疏化大语言模型（LLM）的中文中心多语言机器翻译模型，通过两阶段训练和课程学习策略，在低资源场景下表现优异，并具备零样本翻译能力。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中低资源语言对的性能问题，并探索稀疏化LLM在翻译任务中的潜力。

Method: 采用两阶段训练策略：先在中文语料上预训练，再在65种语言的平行数据集上进行多语言微调，结合MoE和课程学习策略。

Result: FuxiMT显著优于现有基线模型，尤其在低资源场景下，并展现出对未见语言对的零样本翻译能力。

Conclusion: FuxiMT在多语言机器翻译任务中表现出色，特别是在低资源语言对和零样本翻译方面具有潜力。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [483] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Main category: cs.CL

TL;DR: 通过激活工程技术，无需训练或数据集创建，改进LLMs的时间对齐能力，提升事实回忆准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs的知识具有时间敏感性，确保其生成时间相关的响应至关重要。

Method: 使用激活工程技术，将LLaMA 2模型锚定到特定时间点，研究不同注入层和提示策略的效果。

Result: 实验显示，相对和显式提示分别提升44%和16%，性能接近微调方法，但计算效率更高。

Conclusion: 激活工程技术在时间对齐任务中高效且无需预对齐数据集，性能接近微调方法。

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [484] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Main category: cs.CL

TL;DR: 研究发现多语言视觉语言模型（如M-CLIP、NLLB-CLIP和CAPIVARA-CLIP）在跨语言环境中表现出更强的性别和种族偏见，尤其是在低资源语言中。


<details>
  <summary>Details</summary>
Motivation: 探索多语言视觉语言模型的社会偏见，填补现有研究的空白。

Method: 使用FairFace和PATA数据集，在零样本设置下评估三种多语言CLIP模型的偏见和刻板印象放大。

Result: 所有模型在多语言环境中表现出比单语言基线更强的性别偏见，低资源语言尤其明显。

Conclusion: 多语言模型可能加剧偏见，需要更细粒度的语言感知评估。

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [485] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Main category: cs.CL

TL;DR: 论文提出Think-J方法，通过强化学习优化生成式LLM的评判能力，无需额外人工标注即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成式LLM在评判任务中表现不佳，需提升其自动建模偏好的能力。

Method: 利用少量数据训练初始评判能力，再通过离线（训练批评模型）和在线（规则奖励）强化学习优化评判思维。

Result: 实验表明，Think-J显著提升生成式LLM的评判能力，超越生成式和分类器方法。

Conclusion: Think-J通过优化评判思维，有效提升LLM的自动评判能力，具有实际应用价值。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [486] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Main category: cs.CL

TL;DR: PL-FGSA是一个基于提示学习的统一框架，用于细粒度情感分析，结合轻量级TextCNN，在多任务提示增强生成中实现高性能。


<details>
  <summary>Details</summary>
Motivation: 传统细粒度情感分析方法需要特定架构和大量标注数据，泛化性和可扩展性受限。

Method: PL-FGSA通过提示设计将FGSA重新表述为多任务提示增强生成问题，结合轻量级TextCNN。

Result: 在三个基准数据集上表现优异，F1分数分别为0.922、0.694和0.597。

Conclusion: PL-FGSA通过提示学习提升泛化能力，适用于实际情感分析任务。

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [487] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Main category: cs.CL

TL;DR: YESciEval是一个开源框架，结合细粒度评分标准和强化学习，用于评估大型语言模型（LLMs）的科学问答能力，并减少评估中的乐观偏差。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学问答中的评估鲁棒性不足，需要一种独立于专有模型和人工反馈的可靠评估方法。

Method: 通过多学科科学问答数据集（含对抗性变体）和多个LLM的评分，结合强化学习优化评估框架。

Result: 实现了可扩展、零成本的LLM评估，支持AI对齐和透明评估。

Conclusion: 该工作为科学研究和通用人工智能提供了更可靠的LLM评估方法。

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [488] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: N-rep一致性是一种更经济的文本到SQL方法，性能接近昂贵方法，成本仅为每查询0.039美元。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法（如CoT、自一致性和微调）成本高昂，推理时可能需要数百次LLM调用，每查询成本高达0.46美元。

Method: N-rep利用同一模式输入的多种表示来弥补单一表示的弱点，避免使用昂贵模型或微调。

Result: N-rep在BIRD基准测试中表现接近昂贵方法，成本显著降低。

Conclusion: N-rep是成本范围内性能最佳的文本到SQL方法。

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [489] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Main category: cs.CL

TL;DR: 本文介绍了一种基于自然语言处理（NLP）的方法，用于系统分类儿童言语障碍科学文献。通过主题建模技术（LDA和BERTopic），发现了14个临床相关主题，并展示了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 自动化文献综述在言语病理学领域的需求，以提高效率和精确性。

Method: 从PubMed数据库中检索并筛选4,804篇相关文章，应用LDA和BERTopic进行主题建模，并使用定制停用词表优化结果。

Result: LDA模型的连贯性得分为0.42，困惑度为-7.5；BERTopic模型的异常主题比例低于20%，表明其分类效果良好。

Conclusion: 该方法为言语病理学领域的自动化文献综述提供了有效工具。

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [490] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Main category: cs.CL

TL;DR: MUG-Eval是一个评估大型语言模型（LLMs）多语言生成能力的新框架，通过将现有基准转化为对话任务并测量任务成功率来评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 评估低资源语言中LLMs的文本生成能力具有挑战性，现有方法有限。

Method: 设计对话任务，要求模型在目标语言中有效沟通，以任务成功率作为生成能力的代理指标。

Result: 在30种语言上评估8个LLMs，MUG-Eval与现有基准强相关（r>0.75），支持跨语言和模型的标准化比较。

Conclusion: MUG-Eval提供了一种资源高效且稳健的多语言生成评估方法，适用于数千种语言。

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [491] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Main category: cs.CL

TL;DR: 论文提出了一种名为log-augmented generation (LAG)的新框架，通过直接重用过去的计算和推理日志，提升语言模型在新任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 人类能从过去经验中学习，但大型语言模型（LLMs）及其代理系统难以保留和复用过去的推理能力。

Method: LAG框架利用键值（KV）缓存存储任务日志，并在新任务中检索相关日志以增强生成能力。

Result: 实验表明，LAG在知识和推理密集型数据集上显著优于未使用日志的标准代理系统及其他现有技术。

Conclusion: LAG通过直接复用过去的推理和计算，有效提升了模型的适应能力和准确性。

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [492] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Main category: cs.CL

TL;DR: 论文介绍了WirelessMathBench，一个专门评估大语言模型在无线通信数学建模能力的基准测试，发现现有模型在复杂任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在无线通信领域复杂数学推理能力的不足。

Method: 开发了包含587个问题的WirelessMathBench，涵盖多种任务类型，并测试了多个领先的大语言模型。

Result: 模型在基本任务上表现良好，但在复杂方程完成任务中表现较差，最佳模型DeepSeek-R1的平均准确率仅为38.05%。

Conclusion: 通过公开基准测试和工具包，推动开发更强大的领域感知大语言模型。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [493] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文探讨了参数化知识转移（PKT）的新范式，提出了Pre-Align PKT（PrePKT）方法LaTen，并揭示了神经不兼容性是跨规模PKT的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统知识转移基于符号语言，而参数化知识转移（PKT）通过LLM参数实现更直接的知识共享，但跨规模LLM的PKT仍面临挑战。

Method: 提出PrePKT范式及LaTen方法，通过少量训练步骤对齐参数空间，避免后续微调。

Result: 实验表明PostPKT和PrePKT均难以实现稳定转移，神经不兼容性是根本障碍。

Conclusion: 研究揭示了LLM参数结构的差异对PKT的影响，为未来高效PKT研究提供了方向。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [494] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Main category: cs.CL

TL;DR: 论文提出了一种名为CrPO的新方法，通过多维度创造力信号优化LLM的生成内容，实验表明其效果优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升LLM创造力时过于狭窄，未能全面解决创造力的多维度问题。

Method: 提出Creative Preference Optimization (CrPO)，结合多维度创造力信号进行偏好优化，并使用新数据集MuCE进行训练。

Result: 模型在自动和人工评估中表现优于GPT-4o，生成内容更具新颖性、多样性和惊喜感，同时保持高质量。

Conclusion: 在偏好框架中直接优化创造力是提升LLM创造力的有效方向，且不影响输出质量。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [495] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Main category: cs.CL

TL;DR: 论文提出CtrlDiff，一种动态可控的半自回归框架，结合离散扩散模型和强化学习，解决固定长度生成和弱可控性问题。


<details>
  <summary>Details</summary>
Motivation: 探索替代传统自回归语言模型的新范式，解决扩散模型固定长度生成和缺乏灵活控制的问题。

Method: 结合离散扩散和强化学习，动态确定生成块大小，并引入分类器引导控制机制。

Result: CtrlDiff在混合扩散模型中表现优异，缩小与自回归模型的性能差距，支持高效条件文本生成。

Conclusion: CtrlDiff为扩散语言模型提供了动态可控的解决方案，推动了条件文本生成的发展。

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [496] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 研究探讨了LLMs在处理混合代码输入输出时的安全性问题，发现其比单语输入更容易产生不安全输出，并通过可解释性方法分析了内部机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的进步，混合代码输入输出的安全性问题日益突出，研究旨在揭示其背后的机制。

Method: 使用可解释性方法分析LLMs的内部属性变化，区分普遍不安全与文化特定不安全的查询。

Result: 发现混合代码输入比单语输入更容易引发不安全输出，并揭示了驱动这一现象的机制。

Conclusion: 研究为LLMs的安全性提供了新见解，强调了混合代码输入的特殊风险。

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [497] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种结合小型语言模型（SLMs）和大型语言模型（LLMs）的新框架LRSA，用于多模态基于方面的情感分析（MABSA），通过LLMs的解释增强SLMs的能力。


<details>
  <summary>Details</summary>
Motivation: 现有MABSA方法依赖SLMs，但其能力有限，导致对文本和视觉数据中方面、情感及其关联的识别不准确。LLMs虽在多模态任务中表现优异，但在ABSA领域仍不及微调的小模型。

Method: 提出LRSA框架，将LLMs生成的解释作为理性注入SLMs，并采用双重交叉注意力机制增强特征交互与融合。

Result: 在两个基线模型和三个基准测试上的实验表明，该方法优于现有方法，具有通用性和适用性。

Conclusion: LRSA通过结合SLMs和LLMs的优势，显著提升了MABSA任务的性能，适用于大多数预训练模型。

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [498] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Main category: cs.CL

TL;DR: 论文提出了一种基于RWKV7架构的多模态框架ModRWKV，通过动态适配的异构模态编码器实现多源信息融合，验证了现代RNN架构在多模态大语言模型中的可行性。


<details>
  <summary>Details</summary>
Motivation: 目前多模态研究主要基于计算复杂度高的Transformer架构，而计算成本较低的线性模型（如RNN）在多模态领域的应用有限。本文旨在探索现代RNN架构在多模态上下文中的潜力。

Method: 提出ModRWKV框架，基于RWKV7架构，采用轻量级多模态模块设计，并通过实验找到性能与计算效率的最优平衡。利用RWKV7的预训练权重加速多模态训练。

Result: 实验表明，ModRWKV在多模态任务中表现优异，预训练权重的初始化对提升模型理解多模态信号的能力至关重要。

Conclusion: 现代RNN架构是多模态大语言模型中Transformer的可行替代方案，并通过系统探索确定了ModRWKV的最优配置。

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [499] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Main category: cs.CL

TL;DR: 本文提出语言模型基于逻辑形式（LFLMs）的数据效率优于文本模型，并通过GFoLDS原型验证其有效性。实验表明，LFLMs能利用内置语言知识快速学习复杂模式，且在小数据量下表现优于文本模型。


<details>
  <summary>Details</summary>
Motivation: 探讨逻辑形式语言模型（LFLMs）的数据效率优势，验证其在复杂模式学习和实际应用中的潜力。

Method: 提出GFoLDS原型，一种基于图表示逻辑形式的预训练语言模型，并通过实验对比其与文本模型的性能。

Result: GFoLDS在小数据量下显著优于文本模型，且性能可能随参数和数据量增加而提升。

Conclusion: LFLMs具有高效数据利用和可扩展性，适合实际应用。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [500] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）具有内部思维链，能够逐层分解和执行复合任务。


<details>
  <summary>Details</summary>
Motivation: 探索LLM如何通过不同网络深度学习和顺序执行子任务，以增强模型透明性。

Method: 使用层间上下文掩码和跨任务修补方法验证子任务学习深度，通过LogitLens解码隐藏状态分析执行模式。

Result: 在15个两步复合任务和真实世界TRACE基准测试中，观察到一致的逐层执行模式。

Conclusion: LLM具备内部规划和执行子任务的能力，为细粒度指令级激活控制提供了新方向。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [501] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 论文介绍了KORGym，一个动态评估平台，用于全面评估大型语言模型的推理能力，填补了现有领域特定基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为领域特定，无法全面评估LLM的通用推理能力，因此需要更全面的评估方法。

Method: 引入KORGym平台，提供50多种游戏（文本或视觉形式），支持交互式多轮评估和强化学习场景。

Result: 实验涉及19个LLM和8个VLM，发现闭源模型表现更优，并分析了模态、推理策略等因素对性能的影响。

Conclusion: KORGym有望成为推动LLM推理研究和复杂交互环境评估方法的重要资源。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [502] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CL

TL;DR: 使用印地语作为枢纽语言，将尼泊尔语翻译为英语，探讨了枢纽语言的选择原因，并比较了两种方法（全监督的转移方法和半监督的回译方法）的效果。


<details>
  <summary>Details</summary>
Motivation: 解决尼泊尔语和英语之间缺乏大规模、多领域平行语料库的问题。

Method: 采用印地语作为枢纽语言，使用转移方法（全监督）和回译方法（半监督）进行翻译。

Result: 转移方法在开发测试集上SacreBLEU得分为14.2，比基线高6.6分；回译方法略低于基线15.1分。

Conclusion: 讨论了回译方法表现不佳的原因，并提出了未来改进的方向。

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [503] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Main category: cs.CL

TL;DR: 论文介绍了TruthHypo基准和KnowHD检测器，用于评估大语言模型（LLM）生成真实生物医学假设的能力，并解决其幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成假设的真实性是一个关键挑战，因为验证其准确性需要大量时间和资源，且LLM的幻觉问题可能导致生成看似合理但错误的假设。

Method: 提出TruthHypo基准和KnowHD知识基础幻觉检测器，用于评估假设的真实性。

Result: LLM在生成真实假设方面表现不佳，但KnowHD的groundedness评分能有效筛选真实假设。

Conclusion: KnowHD在识别真实假设和加速科学发现方面具有实用价值，数据和代码已开源。

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [504] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Main category: cs.CL

TL;DR: 论文探讨了机器文本检测的固有困难，提出了一种基于风格特征空间的检测方法，并分析了其鲁棒性。研究发现，尽管攻击者可以优化模型逃避检测，但风格特征仍能有效区分机器生成文本。同时，论文引入了AURA指标，量化人类与机器生成文本分布的重叠程度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证机器文本检测的固有困难性，并探索一种鲁棒的检测方法，以应对语言模型优化逃避检测的挑战。

Method: 论文提出了一种基于风格特征空间的检测方法，并研究了其对优化攻击的鲁棒性。此外，引入了一种新的重述方法，用于分析检测器的性能。

Result: 研究发现，风格特征空间对优化攻击具有鲁棒性，且在样本数量增加时，人类与机器生成文本的分布可区分。AURA指标进一步量化了这种分布重叠。

Conclusion: 结论强调了避免依赖机器文本检测的建议，同时展示了风格特征检测的潜力。

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [505] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 论文提出CoT Thought Leap Bridge Task，通过检测思维跳跃并生成缺失的中间推理步骤，提升大语言模型在数学任务中的表现。基于ScaleQM+数据集训练的CoT-Bridge模型在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有数学CoT数据集因专家省略中间步骤导致思维跳跃，影响模型学习和泛化能力。

Method: 提出CoT Thought Leap Bridge Task，构建ScaleQM+数据集，训练CoT-Bridge模型检测和填补思维跳跃。

Result: 在NuminaMath等基准测试中，模型性能提升达+5.87%，泛化能力增强。

Conclusion: CoT-Bridge能有效提升推理完整性，兼容现有优化技术，具有广泛适用性。

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [506] [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](https://arxiv.org/abs/2505.14314)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 论文提出了一种基于ExpMul硬件操作符的FlashAttention优化方法，显著降低了硬件加速器的面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 随着注意力机制在长序列处理中的应用增加，需要更高效的硬件加速器来支持FlashAttention算法。

Method: 通过融合指数计算和向量乘法的新硬件操作符（ExpMul）优化FlashAttention内核。

Result: 在28nm ASIC技术中，面积和功耗分别平均降低了28.8%和17.6%。

Conclusion: ExpMul操作符为FlashAttention硬件加速器提供了显著的性能提升和成本优化。

Abstract: Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [507] [Towards Efficient Multi-Scale Deformable Attention on NPU](https://arxiv.org/abs/2505.14022)
*Chenghuan Huang,Zhigeng Xu,Chong Sun,Chen Li,Ziyang Ma*

Main category: cs.PF

TL;DR: 本文提出了一种针对Ascend NPU架构的MSDA协同设计方法，优化了内存访问和计算策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MSDA的随机访问网格采样策略在NPU等专用加速器上存在优化挑战，需要更高效的实现。

Method: 采用协同设计方法，重新设计内存访问和计算策略，支持高效的前向和后向计算，并包含硬件感知优化。

Result: 实验表明，相比基线方法，实现了5.9倍（前向）、8.9倍（后向）和7.3倍（端到端训练）的加速。

Conclusion: 协同设计方法显著提升了MSDA在NPU上的性能，为视觉任务提供了更高效的解决方案。

Abstract: Multi-scale deformable attention (MSDA) is a flexible and powerful feature
extraction mechanism for visual tasks, but its random-access grid sampling
strategy poses significant optimization challenges, especially on
domain-specific accelerators such as NPUs. In this work, we present a co-design
approach that systematically rethinks memory access and computation strategies
for MSDA on the Ascend NPU architecture. With this co-design approach, our
implementation supports both efficient forward and backward computation, is
fully adapted for training workloads, and incorporates a suite of
hardware-aware optimizations. Extensive experiments show that our solution
achieves up to $5.9\times$ (forward), $8.9\times$ (backward), and $7.3\times$
(end-to-end training) speedup over the grid sample-based baseline, and
$1.9\times$, $2.4\times$, and $2.0\times$ acceleration over the latest vendor
library, respectively.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [508] [CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data](https://arxiv.org/abs/2505.14027)
*Yifan Zeng*

Main category: cs.CR

TL;DR: CSAGC-IDS是一种基于深度学习的网络入侵检测模型，通过生成高质量数据和特征提取解决数据不平衡和复杂流量模式问题，在NSL-KDD数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着计算机网络普及，网络入侵的严重性增加，需要高效的入侵检测系统。深度学习模型在入侵检测中表现良好，但面临高维复杂流量模式和数据不平衡的挑战。

Method: 提出CSAGC-IDS模型，结合SC-CGAN生成高质量数据解决类别不平衡，使用CSCA-CNN提取复杂流量特征。

Result: 在NSL-KDD数据集上，五分类任务准确率84.55%，F1分数84.52%；二分类任务准确率91.09%，F1分数92.04%。

Conclusion: CSAGC-IDS在入侵检测中表现优异，并通过SHAP和LIME提供模型可解释性分析。

Abstract: As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.

</details>


### [509] [Traceable Black-box Watermarks for Federated Learning](https://arxiv.org/abs/2505.13651)
*Jiahao Xu,Rui Hu,Olivera Kotevska,Zikai Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为TraMark的服务器端水印方法，用于在联邦学习系统中注入可追踪的黑盒水印，以保护模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统中存在模型泄露风险，现有水印方法多为不可追踪或白盒水印，缺乏对可追踪黑盒水印的正式定义和问题描述。

Method: 通过将模型参数空间划分为主任务区域和水印区域，为每个客户端构建个性化的全局模型，并在水印区域中学习独特水印。

Result: 实验表明，TraMark能够在保持主任务性能的同时，确保所有水印模型的可追踪性。

Conclusion: TraMark填补了可追踪黑盒水印在联邦学习系统中的研究空白，为模型知识产权保护提供了有效解决方案。

Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local
client has access to the global model, posing a critical risk of model leakage.
Existing works have explored injecting watermarks into local models to enable
intellectual property protection. However, these methods either focus on
non-traceable watermarks or traceable but white-box watermarks. We identify a
gap in the literature regarding the formal definition of traceable black-box
watermarking and the formulation of the problem of injecting such watermarks
into FL systems. In this work, we first formalize the problem of injecting
traceable black-box watermarks into FL. Based on the problem, we propose a
novel server-side watermarking method, $\mathbf{TraMark}$, which creates a
traceable watermarked model for each client, enabling verification of model
leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions
the model parameter space into two distinct regions: the main task region and
the watermarking region. Subsequently, a personalized global model is
constructed for each client by aggregating only the main task region while
preserving the watermarking region. Each model then learns a unique watermark
exclusively within the watermarking region using a distinct watermark dataset
before being sent back to the local client. Extensive results across various FL
systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all
watermarked models while preserving their main task performance.

</details>


### [510] [Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy](https://arxiv.org/abs/2505.13655)
*Jiahao Xu,Rui Hu,Olivera Kotevska*

Main category: cs.CR

TL;DR: GDPFed和GDPFed$^+$提出了一种在联邦学习中处理客户端隐私需求异质性的方法，通过分组和模型稀疏化优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在客户端隐私需求异质性下性能不足的问题，避免过度噪声和模型效用下降。

Method: GDPFed按隐私预算分组客户端，实现组内客户端级差分隐私；GDPFed$^+$进一步引入模型稀疏化和优化采样比例。

Result: 实验证明GDPFed$^+$在多个基准数据集上显著优于现有方法。

Conclusion: GDPFed$^+$有效平衡了隐私保护和模型性能，适用于实际攻击模型。

Abstract: Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.

</details>


### [511] [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
*Guangke Chen,Fu Song,Zhe Zhao,Xiaojun Jia,Yang Liu,Yanchen Qiao,Weizhe Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种新型音频越狱攻击方法AudioJailbreak，解决了现有攻击在异步性、通用性、隐蔽性和空中鲁棒性上的不足，并在实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频语言模型（LALMs）越狱攻击效果不佳，且假设攻击者能完全操控用户提示。本文旨在提出一种更有效的音频越狱攻击方法。

Method: 提出AudioJailbreak方法，具备异步性（无需时间对齐）、通用性（适用于多种提示）、隐蔽性（隐藏恶意意图）和空中鲁棒性（考虑环境失真）。

Result: 实验表明，AudioJailbreak在多种LALMs上表现出高效性，且适用于无法完全操控用户提示的攻击场景。

Conclusion: 本文揭示了音频越狱攻击对LALMs的安全威胁，并为其安全改进提供了实际参考。

Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied
recently, but they achieve suboptimal effectiveness, applicability, and
practicability, particularly, assuming that the adversary can fully manipulate
user prompts. In this work, we first conduct an extensive experiment showing
that advanced text jailbreak attacks cannot be easily ported to end-to-end
LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a
novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio
does not need to align with user prompts in the time axis by crafting suffixal
jailbreak audios; (2) universality: a single jailbreak perturbation is
effective for different prompts by incorporating multiple prompts into
perturbation generation; (3) stealthiness: the malicious intent of jailbreak
audios will not raise the awareness of victims by proposing various intent
concealment strategies; and (4) over-the-air robustness: the jailbreak audios
remain effective when being played over the air by incorporating the
reverberation distortion effect with room impulse response into the generation
of the perturbations. In contrast, all prior audio jailbreak attacks cannot
offer asynchrony, universality, stealthiness, or over-the-air robustness.
Moreover, AudioJailbreak is also applicable to the adversary who cannot fully
manipulate user prompts, thus has a much broader attack scenario. Extensive
experiments with thus far the most LALMs demonstrate the high effectiveness of
AudioJailbreak. We highlight that our work peeks into the security implications
of audio jailbreak attacks against LALMs, and realistically fosters improving
their security robustness. The implementation and audio samples are available
at our website https://audiojailbreak.github.io/AudioJailbreak.

</details>


### [512] [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)
*Tiehan Cui,Yanxu Mao,Peipei Liu,Congying Liu,Datao You*

Main category: cs.CR

TL;DR: 论文提出了一种名为ICE的新型黑盒越狱方法，通过意图隐藏和分散策略高效绕过LLM的安全限制，同时提出了BiSceneEval数据集以全面评估LLM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法存在查询次数过多和泛化能力差的问题，且评估数据集缺乏对文本生成任务的关注。

Method: 提出ICE方法（意图隐藏和分散策略）和BiSceneEval数据集，用于评估LLM在问答和文本生成任务中的鲁棒性。

Result: ICE在单次查询中实现了高攻击成功率，显著提升了效率和跨模型迁移能力；BiSceneEval数据集填补了评估空白。

Conclusion: 研究揭示了当前防御机制的关键漏洞，强调需要结合预定义安全机制和实时语义分解的混合安全策略。

Abstract: Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.

</details>


### [513] [Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime](https://arxiv.org/abs/2505.14323)
*Tomasz Maciążek,Robert Allison*

Main category: cs.CR

TL;DR: 论文研究了训练数据重构攻击，展示了一种在现实威胁模型下有效的攻击方法，并指出差分隐私在小数据场景下难以防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 探讨在现实威胁模型下，攻击者仅知道训练数据分布时，如何通过重构神经网络恢复训练数据，并分析差分隐私的防御局限性。

Method: 提出一种攻击方法，针对在小数据集上迁移学习的神经网络分类器，利用重构神经网络反转训练数据与模型权重之间的映射。

Result: 在MNIST、CIFAR-10和CelebA数据集上的实验表明，攻击有效且鲁棒，且差分隐私在小数据场景下会严重损害分类器准确性。

Conclusion: 研究揭示了迁移学习分类器在训练数据保护方面的潜在风险，并建议重新评估重构攻击的成功率度量标准。

Abstract: Training data reconstruction attacks enable adversaries to recover portions
of a released model's training data. We consider the attacks where a
reconstructor neural network learns to invert the (random) mapping between
training data and model weights. Prior work has shown that an informed
adversary with access to released model's weights and all but one training data
point can achieve high-quality reconstructions in this way. However,
differential privacy can defend against such an attack with little to no loss
in model's utility when the amount of training data is sufficiently large. In
this work we consider a more realistic adversary who only knows the
distribution from which a small training dataset has been sampled and who
attacks a transfer-learned neural network classifier that has been trained on
this dataset. We exhibit an attack that works in this realistic threat model
and demonstrate that in the small-data regime it cannot be defended against by
DP-SGD without severely damaging the classifier accuracy. This raises
significant concerns about the use of such transfer-learned classifiers when
protection of training-data is paramount. We demonstrate the effectiveness and
robustness of our attack on VGG, EfficientNet and ResNet image classifiers
transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we
point out that the commonly used (true-positive) reconstruction success rate
metric fails to reliably quantify the actual reconstruction effectiveness.
Instead, we make use of the Neyman-Pearson lemma to construct the receiver
operating characteristic curve and consider the associated true-positive
reconstruction rate at a fixed level of the false-positive reconstruction rate.

</details>


### [514] [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)
*Dzung Pham,Peter Kairouz,Niloofar Mireshghallah,Eugene Bagdasarian,Chau Minh Pham,Amir Houmansadr*

Main category: cs.CR

TL;DR: LLMs在隐私保护中存在系统性缺陷，特别是在识别模糊人名时表现不佳，AMBENCH基准测试显示其召回率下降20-40%。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs在隐私保护中可靠识别PII的假设，揭示其系统性失败。

Method: 提出AMBENCH基准数据集，测试LLMs在模糊人名和良性提示注入下的表现。

Result: 模糊人名的召回率下降20-40%，且在隐私摘要中被忽略的概率增加四倍。

Conclusion: LLMs在隐私保护中存在风险，需系统性研究其失败模式。

Abstract: Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.

</details>


### [515] [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)
*Chongyang Shi,Sharon Lin,Shuang Song,Jamie Hayes,Ilia Shumailov,Itay Yona,Juliette Pluto,Aneesh Pappu,Christopher A. Choquette-Choo,Milad Nasr,Chawin Sitawarin,Gena Gibson,Andreas Terzis,John "Four" Flynn*

Main category: cs.CR

TL;DR: Google DeepMind评估Gemini模型对抗恶意指令的鲁棒性，通过持续对抗测试提升其安全性。


<details>
  <summary>Details</summary>
Motivation: Gemini模型在处理用户数据时可能受到恶意指令的威胁，需评估其对抗性鲁棒性。

Method: 采用对抗性评估框架，使用自适应攻击技术持续测试Gemini模型。

Result: 测试帮助发现模型的脆弱性，并提升其对抗操纵的韧性。

Conclusion: 持续对抗评估是提升Gemini模型安全性的有效方法。

Abstract: Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [516] [Robust learning of halfspaces under log-concave marginals](https://arxiv.org/abs/2505.13708)
*Jane Lange,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 论文研究了在计算高效条件下学习具有小边界体积的假设的任务，提出了一种算法，能够学习线性阈值函数并返回边界体积为O(r+ε)的分类器。


<details>
  <summary>Details</summary>
Motivation: 研究如何在输入分布为亚高斯各向同性对数凹分布的情况下，高效学习具有小边界体积的假设，以提升分类器的对抗鲁棒性。

Method: 算法通过多项式回归的经典方法，结合三个额外步骤：a) 在噪声敏感性约束下进行ℓ1误差回归，b) 结构化分区和舍入步骤，c) 局部校正器平滑低噪声敏感性函数。

Result: 算法能够学习线性阈值函数并返回边界体积为O(r+ε)的分类器，时间和样本复杂度与多项式回归相当。

Conclusion: 该算法在保持计算效率的同时，显著提升了分类器的对抗鲁棒性，为高效学习小边界体积假设提供了有效方法。

Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of
norm $r$ if, with high probability over a point $x$ drawn from the input
distribution, there is no point within distance $\le r$ from $x$ that is
classified differently. The \emph{boundary volume} is the probability that a
point falls within distance $r$ of a point with a different label. This work
studies the task of computationally efficient learning of hypotheses with small
boundary volume, where the input is distributed as a subgaussian isotropic
log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary
volume proportional to $r$. Such concept classes are efficiently learnable by
polynomial regression, which produces a polynomial threshold function (PTF),
but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and
returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of
perturbation $r$. The time and sample complexity of
$d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial
regression.
  Our algorithm augments the classic approach of polynomial regression with
three additional steps: a) performing the $\ell_1$-error regression under noise
sensitivity constraints, b) a structured partitioning and rounding step that
returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and
noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector
that ``smooths'' a function with low noise sensitivity into a function that is
adversarially robust.

</details>
