<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 8]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.CV](#cs.CV) [Total: 156]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 17]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.GT](#cs.GT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 9]
- [eess.AS](#eess.AS) [Total: 7]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 57]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies](https://arxiv.org/abs/2505.15851)
*Silvia Crafa,Teresa Scantamburlo*

Main category: cs.CY

TL;DR: 论文探讨了道德练习作为帮助AI从业者培养美德以有效监督AI系统的手段，结合哲学、古代实践和当代AI伦理研究，提出了道德练习方法的核心支柱，并通过三项试点研究验证其潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过道德练习培养AI从业者的美德，以提升对AI系统的人类监督能力，填补AI伦理实践中的空白。

Method: 提出了道德练习的三大核心支柱：激发个人参与态度、促进关系理解和培养技术道德智慧，并通过公司、多学科AI研究团队和高等教育学生的三项试点研究验证。

Result: 试点研究表明道德练习在培养负责任AI文化方面具有潜力，但也揭示了其局限性。

Conclusion: 道德练习有助于组织内负责任AI文化的形成，未来研究可进一步探索其应用和优化。

Abstract: This paper elaborates on the concept of moral exercises as a means to help AI
actors cultivate virtues that enable effective human oversight of AI systems.
We explore the conceptual framework and significance of moral exercises,
situating them within the contexts of philosophical discourse, ancient
practices, and contemporary AI ethics scholarship. We outline the core pillars
of the moral exercises methodology - eliciting an engaged personal disposition,
fostering relational understanding, and cultivating technomoral wisdom - and
emphasize their relevance to key activities and competencies essential for
human oversight of AI systems. Our argument is supported by findings from three
pilot studies involving a company, a multidisciplinary team of AI researchers,
and higher education students. These studies allow us to explore both the
potential and the limitations of moral exercises. Based on the collected data,
we offer insights into how moral exercises can foster a responsible AI culture
within organizations, and suggest directions for future research.

</details>


### [2] [Multimodal AI-based visualization of strategic leaders' emotional dynamics: a deep behavioral analysis of Trump's trade war discourse](https://arxiv.org/abs/2505.16274)
*Wei Meng*

Main category: cs.CY

TL;DR: 研究探讨了政治领袖在战略决策中的情感与行为机制，以特朗普对华关税为例，采用多模态认知行为建模框架，发现其决策源于支配-一致性节奏，并提出六轴国家战略节奏干预框架。


<details>
  <summary>Details</summary>
Motivation: 理解政治领袖在战略决策中的情感和行为机制，以揭示非理性决策背后的驱动因素。

Method: 采用多模态认知行为建模框架，包括微表情追踪、语调分析、语义流建模、认知负荷模拟和战略行为映射。

Result: 特朗普的决策并非基于理性推导，而是源于支配-一致性节奏。

Conclusion: 提出六轴国家战略节奏干预框架，支持前瞻性政策建模。

Abstract: This study investigates the emotional rhythms and behavioral mechanisms of
dominant political leaders in strategic decision-making. Using the Trump
administration's 125 percent tariff hike on China as a case, it adopts a
Multimodal Cognitive Behavioral Modeling framework. This includes
micro-expression tracking, acoustic intonation analysis, semantic flow
modeling, cognitive load simulation, and strategic behavior mapping to
construct a full-cycle simulation of emotion, motivation, and output. Results
reveal that Trump's decisions are not driven by rational deduction, but emerge
from dominance-coherence rhythms. A six-axis National Strategic Tempo
Intervention Framework is proposed to support anticipatory policy modeling.

</details>


### [3] [TAPAS: A Pattern-Based Approach to Assessing Government Transparency](https://arxiv.org/abs/2505.16413)
*Jos Zuijderwijk,Iris Beerepoot,Thomas Martens,Eva Knies,Tanja van der Lippe,Hajo A. Reijers*

Main category: cs.CY

TL;DR: 论文提出了一种名为TAPAS的数据驱动方法，用于评估政府透明度，通过识别阻碍透明度的行为模式。该方法在荷兰某部委的实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估信息管理的方法未能考虑公务员的实际工作行为且资源密集，因此需要一种更有效的方法来评估政府透明度。

Method: 采用设计科学研究方法，开发了TAPAS系统，通过分析电子文档管理系统的数据识别透明度障碍行为模式。

Result: 在荷兰某部委的应用中，识别出八种透明度障碍模式，分为四类：不完整文档、有限可访问性、信息不明确和延迟文档。

Conclusion: TAPAS能够持续监控并提供可操作的见解，且无需大量资源投入，为政府透明度评估提供了实用工具。

Abstract: Government transparency, widely recognized as a cornerstone of open
government, depends on robust information management practices. Yet effective
assessment of information management remains challenging, as existing methods
fail to consider the actual working behavior of civil servants and are
resource-intensive. Using a design science research approach, we present the
Transparency Anti-Pattern Assessment System (TAPAS) -- a novel, data-driven
methodology designed to evaluate government transparency through the
identification of behavioral patterns that impede transparency. We demonstrate
TAPAS's real-world applicability at a Dutch ministry, analyzing their
electronic document management system data from the past two decades. We
identify eight transparency anti-patterns grouped into four categories:
Incomplete Documentation, Limited Accessibility, Unclear Information, and
Delayed Documentation. We show that TAPAS enables continuous monitoring and
provides actionable insights without requiring significant resource
investments.

</details>


### [4] [NY Real Estate Racial Equity Analysis via Applied Machine Learning](https://arxiv.org/abs/2505.16946)
*Sanjana Chalavadi,Andrei Pastor,Terry Leitch*

Main category: cs.CY

TL;DR: 研究分析了纽约州和纽约市房地产所有权的种族差异，发现白人持有房产比例过高，少数族裔则不足，尤其是在少数族裔为主的社区。


<details>
  <summary>Details</summary>
Motivation: 揭示房地产所有权中的种族不平等，反映历史和经济社会因素的影响。

Method: 使用LSTM+Geo与XGBoost过滤的种族/族裔预测模型（准确率89.2%），比较预测的所有者种族构成与人口普查数据。

Result: 白人持有房产比例显著高于其人口比例，少数族裔社区的所有权不足，企业所有权加剧了这种差距。

Conclusion: 研究强调了房地产所有权中的种族不平等，并指出数据驱动方法的重要性。

Abstract: This study analyzes tract-level real estate ownership patterns in New York
State (NYS) and New York City (NYC) to uncover racial disparities. We use an
advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,
validated at 89.2% accuracy) to compare the predicted racial composition of
property owners to the resident population from census data. We examine both a
Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how
incorporating geospatial context affects our predictions and disparity
estimates. The results reveal significant inequities: White individuals hold a
disproportionate share of properties and property value relative to their
population, while Black, Hispanic, and Asian communities are underrepresented
as property owners. These disparities are most pronounced in minority-majority
neighborhoods, where ownership is predominantly White despite a predominantly
non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates
these gaps by reducing owner-occupied opportunities in urban minority
communities. We provide a breakdown of ownership vs. population by race for
majority-White, -Black, -Hispanic, and -Asian tracts, identify those with
extreme ownership disparities, and compare patterns in urban, suburban, and
rural contexts. The findings underscore persistent racial inequity in property
ownership, reflecting broader historical and socio-economic forces, and
highlight the importance of data-driven approaches to address these issues.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [5] [MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing](https://arxiv.org/abs/2505.16279)
*Junjie Zheng,Zihao Chen,Chaofan Ding,Yunming Liang,Yihan Fan,Huan Yang,Lei Xie,Xinhan Di*

Main category: cs.MM

TL;DR: 论文提出了一种多模态生成框架，用于解决电影配音中的风格适应、对话处理及细节考虑等问题，通过视觉语言模型和语音生成模型提升配音质量，实验结果显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前电影配音技术虽能实现语音与视频的同步及情感表达，但在配音风格多样性、对话处理及细节（如年龄、性别）方面仍有不足。

Method: 采用多模态视觉语言模型分析视觉输入，识别配音类型和细节属性；利用大型语音生成模型生成高质量配音；构建带标注的电影配音数据集。

Result: 在多个基准数据集上表现优于现有方法，LSE-D、SPK-SIM、EMO-SIM和MCD指标分别提升1.09%、8.80%、19.08%和18.74%。

Conclusion: 多模态框架显著提升了电影配音的质量和适应性，尤其在风格多样性和细节处理方面表现突出。

Abstract: Current movie dubbing technology can produce the desired speech using a
reference voice and input video, maintaining perfect synchronization with the
visuals while effectively conveying the intended emotions. However, crucial
aspects of movie dubbing, including adaptation to various dubbing styles,
effective handling of dialogue, narration, and monologues, as well as
consideration of subtle details such as speaker age and gender, remain
insufficiently explored. To tackle these challenges, we introduce a multi-modal
generative framework. First, it utilizes a multi-modal large vision-language
model (VLM) to analyze visual inputs, enabling the recognition of dubbing types
and fine-grained attributes. Second, it produces high-quality dubbing using
large speech generation models, guided by multi-modal inputs. Additionally, a
movie dubbing dataset with annotations for dubbing types and subtle details is
constructed to enhance movie understanding and improve dubbing quality for the
proposed multi-modal framework. Experimental results across multiple benchmark
datasets show superior performance compared to state-of-the-art (SOTA) methods.
In details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to
1.09%, 8.80%, 19.08%, and 18.74%, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [6] [Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring Fraud Detection in Travel Planning](https://arxiv.org/abs/2505.16557)
*Junchi Yao,Jianhua Xu,Tianyu Xin,Ziyi Wang,Shenzhe Zhu,Shu Yang,Di Wang*

Main category: cs.MA

TL;DR: 论文介绍了WandaPlan，一个评估多智能体规划系统在欺诈内容下风险的测试环境，揭示了现有框架的弱点，并提出集成反欺诈代理的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的多智能体规划系统依赖易受欺诈信息影响的平台，可能导致财务损失和用户体验下降，需评估其风险。

Method: 开发WandaPlan测试环境，模拟真实数据并注入欺诈内容，评估系统在三种欺诈案例中的表现。

Result: 发现现有框架过于关注任务效率而忽视数据真实性，验证了WandaPlan的通用性。

Conclusion: 提出集成反欺诈代理以降低欺诈风险，为可靠规划提供解决方案。

Abstract: The rise of Large Language Model-based Multi-Agent Planning has leveraged
advanced frameworks to enable autonomous and collaborative task execution. Some
systems rely on platforms like review sites and social media, which are prone
to fraudulent information, such as fake reviews or misleading descriptions.
This reliance poses risks, potentially causing financial losses and harming
user experiences. To evaluate the risk of planning systems in real-world
applications, we introduce \textbf{WandaPlan}, an evaluation environment
mirroring real-world data and injected with deceptive content. We assess system
performance across three fraud cases: Misinformation Fraud, Team-Coordinated
Multi-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal
significant weaknesses in existing frameworks that prioritize task efficiency
over data authenticity. At the same time, we validate WandaPlan's
generalizability, capable of assessing the risks of real-world open-source
planning frameworks. To mitigate the risk of fraud, we propose integrating an
anti-fraud agent, providing a solution for reliable planning.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [7] [Ricci Matrix Comparison for Graph Alignment: A DMC Variation](https://arxiv.org/abs/2505.15831)
*Ashley Wang,Peter Chin*

Main category: cs.SI

TL;DR: 论文提出了一种基于几何的图对齐方法，包括新提出的Ricci矩阵比较（RMC）及其原始形式度矩阵比较（DMC），并通过实验验证了其在复杂网络中对齐的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索节点对应关系及其最优性，引入微分几何视角改进图对齐方法。

Method: 基于DMC提出RMC方法，利用Ricci曲率构建图对齐模型，并在环面和蛋白质相互作用网络上进行实验。

Result: RMC在识别环面空洞和对齐复杂网络线图时达到80-90%以上的准确率。

Conclusion: 论文为图对齐领域提供了新视角，并部分验证了DMC方法的有效性。

Abstract: The graph alignment problem explores the concept of node correspondence and
its optimality. In this paper, we focus on purely geometric graph alignment
methods, namely our newly proposed Ricci Matrix Comparison (RMC) and its
original form, Degree Matrix Comparison (DMC). To formulate a
Ricci-curvature-based graph alignment situation, we start with discussing
different ideas of constructing one of the most typical and important
topological objects, the torus, and then move on to introducing the RMC based
on DMC with theoretical motivations. Lastly, we will present to the reader
experimental results on a torus and a complex protein-protein interaction
network that indicate the potential of applying a differential-geometric view
to graph alignment. Results show that a direct variation of DMC using Ricci
curvature can help with identifying holes in tori and aligning line graphs of a
complex network at 80-90+% accuracy. This paper contributes a new perspective
to the field of graph alignment and partially shows the validity of the
previous DMC method.

</details>


### [8] [MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation](https://arxiv.org/abs/2505.15834)
*Congyuan Zhao,Lingwei Wei,Ziming Qin,Wei Zhou,Yunya Song,Songlin Hu*

Main category: cs.SI

TL;DR: 提出了一种跨平台假新闻检测模型APSL，利用图神经网络提取多平台社交上下文特征，实验表明考虑平台传播差异能提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测算法通常基于单一平台，忽略了不同平台传播特性的差异，导致检测效果受限。

Method: 构建了MPPFND数据集，捕捉多平台传播结构，并提出APSL模型，使用图神经网络提取多平台社交上下文特征。

Result: 实验证明，考虑跨平台传播差异能显著提升假新闻检测性能。

Conclusion: 跨平台传播特性的差异对假新闻检测至关重要，APSL模型在多平台环境下表现优越。

Abstract: Fake news spreads widely on social media, leading to numerous negative
effects. Most existing detection algorithms focus on analyzing news content and
social context to detect fake news. However, these approaches typically detect
fake news based on specific platforms, ignoring differences in propagation
characteristics across platforms. In this paper, we introduce the MPPFND
dataset, which captures propagation structures across multiple platforms. We
also describe the commenting and propagation characteristics of different
platforms to show that their social contexts have distinct features. We propose
a multi-platform fake news detection model (APSL) that uses graph neural
networks to extract social context features from various platforms. Experiments
show that accounting for cross-platform propagation differences improves fake
news detection performance.

</details>


### [9] [Web2Wiki: Characterizing Wikipedia Linking Across the Web](https://arxiv.org/abs/2505.15837)
*Veniamin Veselovsky,Tiziano Piccardi,Ashton Anderson,Robert West,Akhil Arora*

Main category: cs.SI

TL;DR: 本文首次大规模分析了维基百科在互联网中的引用情况，发现其主要用于新闻和科学网站的信息参考，且多数链接出现在主要内容中，作为背景知识提供。


<details>
  <summary>Details</summary>
Motivation: 探索维基百科在其平台之外的引用情况，填补相关研究的空白。

Method: 使用Common Crawl数据集，识别了超过9000万个维基百科链接，并分析其分布、上下文和功能。

Result: 新闻和科学网站最常引用维基百科，链接多出现在主要内容中，95%的链接用于解释而非证据或引用。

Conclusion: 维基百科在互联网中主要作为背景知识提供者，其影响广泛但功能集中在信息参考。

Abstract: Wikipedia is one of the most visited websites globally, yet its role beyond
its own platform remains largely unexplored. In this paper, we present the
first large-scale analysis of how Wikipedia is referenced across the Web. Using
a dataset from Common Crawl, we identify over 90 million Wikipedia links
spanning 1.68% of Web domains and examine their distribution, context, and
function. Our analysis of English Wikipedia reveals three key findings: (1)
Wikipedia is most frequently cited by news and science websites for
informational purposes, while commercial websites reference it less often. (2)
The majority of Wikipedia links appear within the main content rather than in
boilerplate or user-generated sections, highlighting their role in structured
knowledge presentation. (3) Most links (95%) serve as explanatory references
rather than as evidence or attribution, reinforcing Wikipedia's function as a
background knowledge provider. While this study focuses on English Wikipedia,
our publicly released Web2Wiki dataset includes links from multiple language
editions, supporting future research on Wikipedia's global influence on the
Web.

</details>


### [10] [AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening](https://arxiv.org/abs/2505.15842)
*Mohit Kataria,Shreyash Bhilwade,Sandeep Kumar,Jayadeva*

Main category: cs.SI

TL;DR: 提出了一种结合LSH和一致性哈希的自适应图粗化框架，支持异构图的语义一致性，显著提升了效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法每次只能生成一个粗化图，且无法适应异构图的语义约束，导致效率低下和灵活性不足。

Method: 结合局部敏感哈希（LSH）和一致性哈希实现自适应图粗化，并提出类型隔离粗化策略以处理异构图。

Result: 在23个真实数据集上的实验表明，该方法在保持结构和语义完整性的同时，具有优越的可扩展性。

Conclusion: 该框架首次统一支持自适应和异构粗化，为大规模图学习提供了高效解决方案。

Abstract: $\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique
that compresses large graphs to enable efficient learning and inference.
However, existing GC methods generate only one coarsened graph per run and must
recompute from scratch for each new coarsening ratio, resulting in unnecessary
overhead. Moreover, most prior approaches are tailored to
$\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints
of $\textit{heterogeneous}$ graphs, which comprise multiple node and edge
types. To overcome these limitations, we introduce a novel framework that
combines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable
$\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method
is inherently fast and scalable. For heterogeneous graphs, we propose a
$\textit{type isolated coarsening}$ strategy that ensures semantic consistency
by restricting merges to nodes of the same type. Our approach is the first
unified framework to support both adaptive and heterogeneous coarsening.
Extensive evaluations on 23 real-world datasets including homophilic,
heterophilic, homogeneous, and heterogeneous graphs demonstrate that our method
achieves superior scalability while preserving the structural and semantic
integrity of the original graph.

</details>


### [11] [Simulating Prosocial Behavior and Social Contagion in LLM Agents under Institutional Interventions](https://arxiv.org/abs/2505.15857)
*Yujia Zhou,Hexi Wang,Qingyao Ai,Zhen Wu,Yiqun Liu*

Main category: cs.SI

TL;DR: ProSim是一个模拟框架，用于研究LLM代理在多样社会与制度条件下的亲社会行为表现。通过三个研究，发现代理能稳定展现亲社会行为，对不公平现象有反应，且政策不平等会抑制亲社会行为。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为自主代理在社交环境中应用增多，理解其亲社会行为的能力变得重要。

Method: ProSim框架包含个体、场景、交互和干预模拟四个组件，通过三个渐进研究评估亲社会行为。

Result: LLM代理能稳定展现亲社会行为，对不公平现象有反应，政策不平等会抑制亲社会行为并通过社交网络传播。

Conclusion: 研究为评估社会对齐和建模代理驱动社会的制度动态奠定了基础。

Abstract: As large language models (LLMs) increasingly serve as autonomous agents in
social contexts, understanding their capacity for prosocial behavior becomes
essential. We present ProSim, a simulation framework designed to examine how
prosocial behavior emerges, adapts, and erodes in LLM-based agents under
diverse social and institutional conditions. The framework comprises four
components: individual simulation, scenario simulation, interaction simulation,
and intervention simulation. We conduct three progressive studies to evaluate
prosocial alignment. First, we show that LLM agents can demonstrate stable and
context-sensitive prosocial behavior across diverse scenarios and adapt their
responses under normative policy interventions. Second, we find that agents
engage in fairness-based third-party punishment and respond systematically to
variations in inequity magnitude and enforcement cost. Third, we show that
policy-induced inequities suppress prosocial behavior, propagate through social
networks, and are mediated by agents' perceptions of unfairness. These findings
lay the groundwork for evaluating social alignment and modeling institutional
dynamics in agent-driven societies.

</details>


### [12] [Tight Practical Bounds for Subgraph Densities in Ego-centric Networks](https://arxiv.org/abs/2505.16079)
*Connor Mattes,Esha Datta,Ali Pinar*

Main category: cs.SI

TL;DR: 本文提出了一种新的子图密度分析方法，通过子图扩散比量化网络结构的差异，并结合标志代数、模体计数和拓扑数据分析技术，显著提高了子图密度边界的紧密度。


<details>
  <summary>Details</summary>
Motivation: 子图密度在网络分析中至关重要，但区分数学决定的和领域驱动的子图密度特征具有挑战性。本文旨在量化这些差异，尤其是在社交网络等实际应用中。

Method: 结合标志代数、模体计数和拓扑数据分析技术，提出子图扩散比，并提供了更紧的子图密度边界。

Result: 通过实证分析11个真实网络，发现社交网络的子图扩散比显著低于其他网络类型（如维基百科链接网络）。

Conclusion: 子图扩散比提供了一种量化网络结构差异的指标，有助于更直观地理解和比较不同类型的网络。

Abstract: Subgraph densities play a crucial role in network analysis, especially for
the identification and interpretation of meaningful substructures in complex
graphs. Localized subgraph densities, in particular, can provide valuable
insights into graph structures. Distinguishing between
mathematically-determined and domain-driven subgraph density features, however,
poses challenges. For instance, the lack or presence of certain structures can
be explained by graph density or degree distribution. These differences are
especially meaningful in applied contexts as they allow us to identify
instances where the data induces specific network structures, such as
friendships in social networks. The goal of this paper is to measure these
differences across various types of graphs, conducting social media analysis
from a network perspective. To this end, we first provide tighter bounds on
subgraph densities. We then introduce the subgraph spread ratio to quantify the
realized subgraph densities of specific networks relative to the feasible
bounds. Our novel approach combines techniques from flag algebras,
motif-counting, and topological data analysis. Crucially, effective adoption of
the state-of-the-art in the plain flag algebra method yields feasible regions
up to three times tighter than prior best-known results, thereby enabling more
accurate and direct comparisons across graphs. We additionally perform an
empirical analysis of 11 real-world networks. We observe that social networks
consistently have smaller subgraph spread ratios than other types of networks,
such as linkage-mapping networks for Wikipedia pages. This aligns with our
intuition about social relationships: such networks have meaningful structure
that makes them distinct. The subgraph spread ratio enables the quantification
of intuitive understandings of network structures and provides a metric for
comparing types of networks.

</details>


### [13] [Novel Rewiring Mechanism for Restoration of the Fragmented Social Networks after Attacks](https://arxiv.org/abs/2505.16233)
*Rajesh Kumar,Suchi Kumari,Anubhav Mishra*

Main category: cs.SI

TL;DR: 该研究通过重连或添加边来重构网络，并采用两种方法（战略重连和预算约束最优重连）评估网络鲁棒性，同时引入拉普拉斯能量以更全面地衡量恢复网络的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的复杂系统（如社交网络、技术基础设施和通信网络）易因随机故障或外部攻击而断开连接，因此管理其安全性和韧性至关重要。

Method: 采用战略重连和预算约束最优重连两种方法重构网络，并通过拉普拉斯能量评估恢复网络的鲁棒性。

Result: 研究表明，仅依赖最大连通分量大小评估鲁棒性不足，拉普拉斯能量能更全面地反映网络恢复后的行为。

Conclusion: 通过拉普拉斯能量评估网络鲁棒性，能更全面地理解系统在恢复过程中的行为，弥补了传统方法的不足。

Abstract: Real-world complex systems exhibit intricate interconnections and
dependencies, especially social networks, technological infrastructures, and
communication networks. These networks are prone to disconnection due to random
failures or external attacks on their components. Therefore, managing the
security and resilience of such networks is a prime concern, particularly at
the time of disaster. Therefore, in this research work, network is
reconstructed by rewiring/addition of the edges and robustness of the networks
is measured. To this aim, two approaches namely (i) Strategic rewiring (ii)
budget constrained optimal rewiring are adopted. While current research often
assesses robustness by examining the size of the largest connected component,
this approach fails to capture the complete spectrum of vulnerability. The
failure of a small number of connections leads to a sparser network yet
connected network. Thus, the present research work delves deeper into
evaluating the robustness of the restored network by evaluating Laplacian
Energy to better comprehend the system's behavior during the restoration of the
network still considering the size of the largest connected component attacks.

</details>


### [14] [Filling in the Blanks? A Systematic Review and Theoretical Conceptualisation for Measuring WikiData Content Gaps](https://arxiv.org/abs/2505.16383)
*Marisa Ripoll,Neal Reeves,Anelia Kurteva,Elena Simperl,Albert Meroño Peñuela*

Main category: cs.SI

TL;DR: 本文通过系统文献综述分析了Wikidata中的内容缺口，提出了缺口的分类和理论框架，并讨论了其对协作和编辑活动的影响。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为协作知识图谱，存在数据不完整和系统性缺口的问题，需要系统研究以理解这些缺口的现状。

Method: 通过系统文献综述，提出缺口的分类和理论框架，并分析现有文献中的方法和指标。

Result: 研究揭示了Wikidata中的质量、完整性和系统性偏见问题，并识别了被忽视的缺口。

Conclusion: 研究结果为理解Wikidata的质量和缺口提供了理论支持，并指出了未来研究方向。

Abstract: Wikidata is a collaborative knowledge graph which provides machine-readable
structured data for Wikimedia projects including Wikipedia. Managed by a
community of volunteers, it has grown to become the most edited Wikimedia
project. However, it features a long-tail of items with limited data and a
number of systematic gaps within the available content. In this paper, we
present the results of a systematic literature review aimed to understand the
state of these content gaps within Wikidata. We propose a typology of gaps
based on prior research and contribute a theoretical framework intended to
conceptualise gaps and support their measurement. We also describe the methods
and metrics present used within the literature and classify them according to
our framework to identify overlooked gaps that might occur in Wikidata. We then
discuss the implications for collaboration and editor activity within Wikidata
as well as future research directions. Our results contribute to the
understanding of quality, completeness and the impact of systematic biases
within Wikidata and knowledge gaps more generally.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems](https://arxiv.org/abs/2505.15862)
*Long Wanga,Jiongzhi Zheng,Zhengda Xiong,ChuMin Li,Kun He*

Main category: cs.AI

TL;DR: 论文提出了一种动态扩展候选边的方法，通过多臂老虎机模型优化LKH算法，显著提升了TSP及其变体问题的求解性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法（如LKH）使用静态候选边可能导致陷入局部最优，限制了找到更好解的能力。

Method: 引入多臂老虎机模型动态选择候选边，扩展候选集以提升搜索效率。

Result: 在多个TSP基准测试中表现优异，且显著提升了LKH-3在TSP变体中的性能。

Conclusion: 动态候选边选择方法有效提升了LKH算法的性能，适用于TSP及其变体问题。

Abstract: Algorithms designed for routing problems typically rely on high-quality
candidate edges to guide their search, aiming to reduce the search space and
enhance the search efficiency. However, many existing algorithms, like the
classical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman
Problem (TSP), often use predetermined candidate edges that remain static
throughout local searches. This rigidity could cause the algorithm to get
trapped in local optima, limiting its potential to find better solutions. To
address this issue, we propose expanding the candidate sets to include other
promising edges, providing them an opportunity for selection. Specifically, we
incorporate multi-armed bandit models to dynamically select the most suitable
candidate edges in each iteration, enabling LKH to make smarter choices and
lead to improved solutions. Extensive experiments on multiple TSP benchmarks
show the excellent performance of our method. Moreover, we employ this
bandit-based method to LKH-3, an extension of LKH tailored for solving various
TSP variant problems, and our method also significantly enhances LKH-3's
performance across typical TSP variants.

</details>


### [16] [PhyX: Does Your Model Have the "Wits" for Physical Reasoning?](https://arxiv.org/abs/2505.15929)
*Hui Shen,Taiqiang Wu,Qi Han,Yunta Hsieh,Jizhou Wang,Yuyue Zhang,Yuxin Cheng,Zijian Hao,Yuansheng Ni,Xin Wang,Zhongwei Wan,Kai Zhang,Wendong Xu,Jing Xiong,Ping Luo,Wenhu Chen,Chaofan Tao,Zhuoqing Mao,Ngai Wong*

Main category: cs.AI

TL;DR: PhyX是一个新的大规模基准测试，旨在评估模型在视觉场景中的物理推理能力，发现当前最先进模型在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能全面评估物理推理能力，因此需要一个新的基准测试来填补这一空白。

Method: PhyX包含3K多模态问题，涵盖6种推理类型和6个物理领域，通过多种评估范式分析模型表现。

Result: GPT-4o、Claude3.7-Sonnet和GPT-o4-mini的准确率仅为32.5%、42.2%和45.8%，远低于人类专家。

Conclusion: 当前模型在物理推理上存在明显不足，需改进以提升真实物理理解能力。

Abstract: Existing benchmarks fail to capture a crucial aspect of intelligence:
physical reasoning, the integrated ability to combine domain knowledge,
symbolic reasoning, and understanding of real-world constraints. To address
this gap, we introduce PhyX: the first large-scale benchmark designed to assess
models capacity for physics-grounded reasoning in visual scenarios. PhyX
includes 3K meticulously curated multimodal questions spanning 6 reasoning
types across 25 sub-domains and 6 core physics domains: thermodynamics,
electromagnetism, mechanics, modern physics, optics, and wave\&acoustics. In
our comprehensive evaluation, even state-of-the-art models struggle
significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and
GPT-o4-mini achieve only 32.5\%, 42.2\%, and 45.8\% accuracy
respectively-performance gaps exceeding 29\% compared to human experts. Our
analysis exposes critical limitations in current models: over-reliance on
memorized disciplinary knowledge, excessive dependence on mathematical
formulations, and surface-level visual pattern matching rather than genuine
physical understanding. We provide in-depth analysis through fine-grained
statistics, detailed case studies, and multiple evaluation paradigms to
thoroughly examine physical reasoning capabilities. To ensure reproducibility,
we implement a compatible evaluation protocol based on widely-used toolkits
such as VLMEvalKit, enabling one-click evaluation.

</details>


### [17] [Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics](https://arxiv.org/abs/2505.15998)
*Thomas Michel,Marko Cvjetko,Gautier Hamon,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: cs.AI

TL;DR: 提出了一种基于好奇心驱动AI的方法，用于在Flow-Lenia中自动发现系统级动力学，扩展了多样性搜索算法以支持交互模式，并通过实验验证其优于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 探索连续细胞自动机（Flow-Lenia）中自组织和生态系统动力学的形成过程，填补个体模式搜索的不足。

Method: 采用内在动机目标探索过程（IMGEPs），结合仿真范围指标（如进化活动、压缩复杂性和多尺度熵），驱动对Flow-Lenia环境的多样性探索。

Result: 实验表明，该方法能发现比随机搜索更丰富的动力学行为，并展示了复杂集体行为的自组织现象。

Conclusion: 该方法为理解参数化复杂系统中涌现的集体特性提供了框架，并支持人机协作的科学探索。

Abstract: We present a method for the automated discovery of system-level dynamics in
Flow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and
parameter localization$-$using a curiosity-driven AI scientist. This method
aims to uncover processes leading to self-organization of evolutionary and
ecosystemic dynamics in CAs. We build on previous work which uses diversity
search algorithms in Lenia to find self-organized individual patterns, and
extend it to large environments that support distinct interacting patterns. We
adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive
exploration of diverse Flow-Lenia environments using simulation-wide metrics,
such as evolutionary activity, compression-based complexity, and multi-scale
entropy. We test our method in two experiments, showcasing its ability to
illuminate significantly more diverse dynamics compared to random search. We
show qualitative results illustrating how ecosystemic simulations enable
self-organization of complex collective behaviors not captured by previous
individual pattern search and analysis. We complement automated discovery with
an interactive exploration tool, creating an effective human-AI collaborative
workflow for scientific investigation. Though demonstrated specifically with
Flow-Lenia, this methodology provides a framework potentially applicable to
other parameterizable complex systems where understanding emergent collective
properties is of interest.

</details>


### [18] [Children's Mental Models of AI Reasoning: Implications for AI Literacy Education](https://arxiv.org/abs/2505.16031)
*Aayushi Dangol,Robert Wolfe,Runhua Zhao,JaeWon Kim,Trushaa Ramanan,Katie Davis,Julie A. Kientz*

Main category: cs.AI

TL;DR: 研究探讨儿童对AI推理过程的理解，发现不同年龄段儿童对AI推理的认知模型不同，并提出了对AI教育和可解释AI工具设计的启示。


<details>
  <summary>Details</summary>
Motivation: 随着AI推理能力的发展，尤其是大型推理模型（LRMs）的出现，了解儿童如何理解AI的推理过程对培养AI素养至关重要。

Method: 采用两阶段方法：与8名儿童共同设计会话，随后对106名儿童（3-8年级）进行实地研究。

Result: 发现儿童对AI推理的三种模型：演绎、归纳和固有模型；年幼儿童（3-5年级）倾向于认为AI推理基于固有智能，而年长儿童（6-8年级）更倾向于将AI视为模式识别器。

Conclusion: 研究揭示了儿童对AI推理理解的三种张力，并提出了对AI课程设计和可解释AI工具开发的启示。

Abstract: As artificial intelligence (AI) advances in reasoning capabilities, most
recently with the emergence of Large Reasoning Models (LRMs), understanding how
children conceptualize AI's reasoning processes becomes critical for fostering
AI literacy. While one of the "Five Big Ideas" in AI education highlights
reasoning algorithms as central to AI decision-making, less is known about
children's mental models in this area. Through a two-phase approach, consisting
of a co-design session with 8 children followed by a field study with 106
children (grades 3-8), we identified three models of AI reasoning: Deductive,
Inductive, and Inherent. Our findings reveal that younger children (grades 3-5)
often attribute AI's reasoning to inherent intelligence, while older children
(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions
that surfaced in children's understanding of AI reasoning and conclude with
implications for scaffolding AI curricula and designing explainable AI tools.

</details>


### [19] [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
*Rui Ye,Xiangrui Liu,Qimin Wu,Xianghe Pang,Zhenfei Yin,Lei Bai,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于异构LLM驱动的多智能体系统（X-MAS），通过多样化LLM提升系统性能，并引入X-MAS-Bench测试平台进行验证。实验表明，异构配置能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAS框架依赖单一LLM，限制了系统智能。本文探索异构LLM驱动的MAS，以释放多样化LLM的集体智能潜力。

Method: 引入X-MAS-Bench测试平台，评估27种LLM在5个领域和5种功能上的表现，进行超170万次评估。

Result: 异构配置在聊天机器人场景中提升8.4%性能（MATH数据集），在混合场景中提升47%（AIME数据集）。

Conclusion: 异构LLM驱动的MAS具有变革潜力，为可扩展协作AI系统提供新方向。

Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.

</details>


### [20] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果端到端框架的LLM路由方法，通过最小化决策遗憾从观测数据中学习路由策略，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用解耦策略，容易导致误差累积且依赖高成本的全反馈数据，而本文旨在利用观测数据学习更高效的路由策略。

Method: 提出因果端到端框架，引入两种理论支持的替代目标（分类上界和softmax加权遗憾近似），并扩展框架以处理异构成本偏好。

Result: 在公共基准测试中，该方法优于现有基线，实现了不同嵌入模型上的最先进性能。

Conclusion: 该方法通过观测数据学习路由策略，显著提升了性能，且适用于不同成本偏好场景。

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [21] [SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution](https://arxiv.org/abs/2505.16048)
*Philipp D. Siedler*

Main category: cs.AI

TL;DR: 论文介绍了一个新数据集，用于评估大型语言模型在拓扑优化中的物理和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言和逻辑基准无法全面评估模型的物理和空间推理能力，因此需要一种新的评估方法。

Method: 数据集包含2D边界、施加力和支撑条件，要求模型推理最优材料分布，任务包括填充部分结构或预测完整分布。

Result: 数据集为评估模型在2D环境中的物理和空间推理能力提供了新视角。

Conclusion: 该数据集填补了传统基准的不足，为模型能力的全面评估提供了补充。

Abstract: We introduce a novel dataset designed to benchmark the physical and spatial
reasoning capabilities of Large Language Models (LLM) based on topology
optimization, a method for computing optimal material distributions within a
design space under prescribed loads and supports. In this dataset, LLMs are
provided with conditions such as 2D boundary, applied forces and supports, and
must reason about the resulting optimal material distribution. The dataset
includes a variety of tasks, ranging from filling in masked regions within
partial structures to predicting complete material distributions. Solving these
tasks requires understanding the flow of forces and the required material
distribution under given constraints, without access to simulation tools or
explicit physical models, challenging models to reason about structural
stability and spatial organization. Our dataset targets the evaluation of
spatial and physical reasoning abilities in 2D settings, offering a
complementary perspective to traditional language and logic benchmarks.

</details>


### [22] [How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior](https://arxiv.org/abs/2505.16067)
*Zidi Xiong,Yuping Lin,Wenya Xie,Pengfei He,Jiliang Tang,Himabindu Lakkaraju,Zhen Xiang*

Main category: cs.AI

TL;DR: 本文研究了内存管理对大型语言模型（LLM）代理长期行为的影响，发现选择性添加和删除策略可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 内存管理对LLM代理的任务表现至关重要，但现有研究对其具体影响缺乏系统性分析。

Method: 通过定量分析，研究了内存的添加和删除操作对代理行为的影响，并进行了控制实验。

Result: 发现代理具有经验跟随特性，但会导致错误传播和错位经验回放；选择性策略可提升10%性能。

Conclusion: 研究为设计稳健的LLM代理内存系统提供了实践指导，并开源了代码。

Abstract: Memory is a critical component in large language model (LLM)-based agents,
enabling them to store and retrieve past executions to improve task performance
over time. In this paper, we conduct an empirical study on how memory
management choices impact the LLM agents' behavior, especially their long-term
performance. Specifically, we focus on two fundamental memory operations that
are widely used by many agent frameworks-addition, which incorporates new
experiences into the memory base, and deletion, which selectively removes past
experiences-to systematically study their impact on the agent behavior. Through
our quantitative analysis, we find that LLM agents display an
experience-following property: high similarity between a task input and the
input in a retrieved memory record often results in highly similar agent
outputs. Our analysis further reveals two significant challenges associated
with this property: error propagation, where inaccuracies in past experiences
compound and degrade future performance, and misaligned experience replay,
where outdated or irrelevant experiences negatively influence current tasks.
Through controlled experiments, we show that combining selective addition and
deletion strategies can help mitigate these negative effects, yielding an
average absolute performance gain of 10% compared to naive memory growth.
Furthermore, we highlight how memory management choices affect agents' behavior
under challenging conditions such as task distribution shifts and constrained
memory resources. Our findings offer insights into the behavioral dynamics of
LLM agent memory systems and provide practical guidance for designing memory
components that support robust, long-term agent performance. We also release
our code to facilitate further study.

</details>


### [23] [SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation](https://arxiv.org/abs/2505.16080)
*Jiayue Liu,Zhongchao Yi,Zhengyang Zhou,Qihe Huang,Kuo Yang,Xu Wang,Yang Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为SynEVO的时空网络，通过跨领域集体智能和模型演化提升泛化能力，实验显示其性能提升高达42%。


<details>
  <summary>Details</summary>
Motivation: 现有时空学习器通常针对特定数据源独立训练，导致跨领域知识迁移能力有限。

Method: 提出SynEVO网络，通过样本重组、弹性公共容器和任务无关提取器实现模型增长和任务解耦，并引入动态耦合器实现模型演化。

Result: 实验表明，SynEVO在跨领域场景下泛化能力提升高达42%。

Conclusion: SynEVO为知识迁移和适应提供了一种NeuroAI范式。

Abstract: Discovering regularities from spatiotemporal systems can benefit various
scientific and social planning. Current spatiotemporal learners usually train
an independent model from a specific source data that leads to limited
transferability among sources, where even correlated tasks requires new design
and training. The key towards increasing cross-domain knowledge is to enable
collective intelligence and model evolution. In this paper, inspired by
neuroscience theories, we theoretically derive the increased information
boundary via learning cross-domain collective intelligence and propose a
Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the
model independence and enables cross-domain knowledge to be shared and
aggregated. Specifically, we first re-order the sample groups to imitate the
human curriculum learning, and devise two complementary learners, elastic
common container and task-independent extractor to allow model growth and
task-wise commonality and personality disentanglement. Then an adaptive dynamic
coupler with a new difference metric determines whether the new sample group
should be incorporated into common container to achieve model evolution under
various domains. Experiments show that SynEVO improves the generalization
capacity by at most 42% under cross-domain scenarios and SynEVO provides a
paradigm of NeuroAI for knowledge transfer and adaptation.

</details>


### [24] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
*Ming Shen,Raphael Shu,Anurag Pratik,James Gung,Yubin Ge,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于自然语言反馈的两步优化方法，用于提升基于角色的多智能体系统在软件开发任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多智能体系统中取得了显著进展，但优化这些系统仍具挑战性。

Method: 提出两步优化流程：1）通过文本反馈识别表现不佳的智能体及其失败原因；2）利用失败原因优化系统提示。

Result: 验证了优化方法的有效性，并研究了不同优化设置对系统性能的影响。

Conclusion: 该方法为未来多智能体系统的开发提供了实用见解。

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [25] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
*Dominick Kubica,Dylan T. Gordon,Nanami Emura,Derleen Saini,Charlie Goldenberg*

Main category: cs.AI

TL;DR: 本文评估了生成式AI（GenAI）在金融领域情感分析的可靠性，重点关注大型语言模型（LLMs）对财务文本（如财报电话会议记录）的情感解读能力。研究发现，LLMs在复杂金融语境中表现不佳，并通过实验比较了多种模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在各行业的广泛应用，其在金融等高风险领域的可靠性成为关键问题。财务文本的复杂性和策略性模糊语言使得情感分析尤为困难，需要评估和改进现有模型的表现。

Method: 研究使用了微软财报电话会议记录，比较了微软Copilot、OpenAI的ChatGPT、Google的Gemini及传统机器学习模型的情感分析性能，并探讨了提示工程对结果的影响。

Result: LLMs在金融文本情感分析中表现不稳定，尤其是在处理策略性模糊语言时。提示工程可以部分改善结果，但与市场情感和股价波动的相关性仍需优化。

Conclusion: 金融领域的情感分析需要更专业的模型优化和提示工程技术，以提升LLMs在复杂语境中的表现。

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [26] [Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events](https://arxiv.org/abs/2505.16455)
*Mengzhu Liu,Zhengqiu Zhu,Chuan Ai,Chen Gao,Xinghong Li,Lingnan He,Kaisheng Lai,Yingfeng Chen,Xin Lu,Yong Li,Quanjun Yin*

Main category: cs.AI

TL;DR: 论文提出了一种心理学驱动的生成代理框架（PsychoAgent），用于解释性恐慌预测，解决了数据标注不足、风险感知未建模和解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在突发灾难事件中，准确预测社交媒体上的公众恐慌情绪对主动治理和危机管理至关重要，但目前面临数据标注不足、风险感知未建模和解释性不足的挑战。

Method: 通过人类与大型语言模型（LLMs）协作构建细粒度恐慌情绪数据集（COPE），开发基于心理机制的跨领域异构数据框架，并设计基于LLM的角色扮演代理模拟个体心理链。

Result: 实验表明，PsychoAgent比基线模型在恐慌情绪预测性能上提升了12.6%至21.7%，并验证了方法的可解释性和泛化能力。

Conclusion: 该方法实现了从‘数据驱动拟合’到‘基于角色的模拟与机制解释’的范式转变，为紧急事件中的恐慌情绪预测提供了透明且可解释的解决方案。

Abstract: During sudden disaster events, accurately predicting public panic sentiment
on social media is crucial for proactive governance and crisis management.
Current efforts on this problem face three main challenges: lack of finely
annotated data hinders emotion prediction studies, unmodeled risk perception
causes prediction inaccuracies, and insufficient interpretability of panic
formation mechanisms. We address these issues by proposing a Psychology-driven
generative Agent framework (PsychoAgent) for explainable panic prediction based
on emotion arousal theory. Specifically, we first construct a fine-grained open
panic emotion dataset (namely COPE) via human-large language models (LLMs)
collaboration to mitigate semantic bias. Then, we develop a framework
integrating cross-domain heterogeneous data grounded in psychological
mechanisms to model risk perception and cognitive differences in emotion
generation. To enhance interpretability, we design an LLM-based role-playing
agent that simulates individual psychological chains through dedicatedly
designed prompts. Experimental results on our annotated dataset show that
PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%
compared to baseline models. Furthermore, the explainability and generalization
of our approach is validated. Crucially, this represents a paradigm shift from
opaque "data-driven fitting" to transparent "role-based simulation with
mechanistic interpretation" for panic emotion prediction during emergencies.
Our implementation is publicly available at:
https://anonymous.4open.science/r/PsychoAgent-19DD.

</details>


### [27] [TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials](https://arxiv.org/abs/2505.16097)
*Zifeng Wang,Qiao Jin,Jiacheng Lin,Junyi Gao,Jathurshan Pradeepkumar,Pengcheng Jiang,Benjamin Danek,Zhiyong Lu,Jimeng Sun*

Main category: cs.AI

TL;DR: TrialPanorama是一个大规模、结构化的临床试验数据库，包含1,657,476条记录，支持多种临床任务，并通过基准测试评估了大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为垂直领域的AI开发提供数据基础，支持临床试验的设计、规划和总结。

Method: 构建TrialPanorama数据库，链接标准生物医学本体，并设计八项基准任务评估LLMs的性能。

Result: 通用LLMs在零样本任务中表现有限，不适用于高风险的临床试验工作流。

Conclusion: TrialPanorama为临床试验AI研究提供了统一资源，未来需进一步优化模型性能。

Abstract: Developing artificial intelligence (AI) for vertical domains requires a solid
data foundation for both training and evaluation. In this work, we introduce
TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical
trial records aggregated from 15 global sources. The database captures key
aspects of trial design and execution, including trial setups, interventions,
conditions, biomarkers, and outcomes, and links them to standard biomedical
ontologies such as DrugBank and MedDRA. This structured and ontology-grounded
design enables TrialPanorama to serve as a unified, extensible resource for a
wide range of clinical trial tasks, including trial planning, design, and
summarization. To demonstrate its utility, we derive a suite of benchmark tasks
directly from the TrialPanorama database. The benchmark spans eight tasks
across two categories: three for systematic review (study search, study
screening, and evidence summarization) and five for trial design (arm design,
eligibility criteria, endpoint selection, sample size estimation, and trial
completion assessment). The experiments using five state-of-the-art large
language models (LLMs) show that while general-purpose LLMs exhibit some
zero-shot capability, their performance is still inadequate for high-stakes
clinical trial workflows. We release TrialPanorama database and the benchmark
to facilitate further research on AI for clinical trials.

</details>


### [28] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
*Zifeng Wang,Benjamin Danek,Jimeng Sun*

Main category: cs.AI

TL;DR: BioDSA-1K是一个用于评估AI代理在生物医学假设验证任务中的基准，包含1029个任务和1177个分析计划，旨在模拟真实研究流程。


<details>
  <summary>Details</summary>
Motivation: 验证科学假设是生物医学研究的核心挑战，但AI代理因数据复杂性和证据解释困难而难以胜任。BioDSA-1K旨在填补这一空白。

Method: BioDSA-1K从300多项已发表研究中提取任务和分析计划，每个任务包括结构化假设和基于实证数据的支持证据。

Result: 基准支持从假设决策准确性到AI生成代码可执行性四个维度的评估，并包含无法验证的假设以反映现实场景。

Conclusion: BioDSA-1K为构建和评估可信赖的生物医学AI代理提供了基础。

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [29] [Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language](https://arxiv.org/abs/2505.16114)
*Naiqi Li,Peiyuan Liu,Zheng Liu,Tao Dai,Yong Jiang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: 提出Logic-of-Thought (Logot)框架，结合大语言模型与逻辑编程，解决自然语言谜题推理问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂谜题推理上表现不足，需结合精确逻辑推理能力。

Method: 利用LLMs将谜题规则转换为答案集程序，通过ASP解释器高效求解。

Result: 在网格谜题和动态谜题上实现近乎完美的准确性。

Conclusion: Logot框架成功结合了自然语言理解与精确逻辑推理，显著提升谜题解决能力。

Abstract: Solving puzzles in natural language poses a long-standing challenge in AI.
While large language models (LLMs) have recently shown impressive capabilities
in a variety of tasks, they continue to struggle with complex puzzles that
demand precise reasoning and exhaustive search. In this paper, we propose
Logic-of-Thought (Logot), a novel framework that bridges LLMs with logic
programming to address this problem. Our method leverages LLMs to translate
puzzle rules and states into answer set programs (ASPs), the solution of which
are then accurately and efficiently inferred by an ASP interpreter. This hybrid
approach combines the natural language understanding of LLMs with the precise
reasoning capabilities of logic programs. We evaluate our method on various
grid puzzles and dynamic puzzles involving actions, demonstrating near-perfect
accuracy across all tasks. Our code and data are available at:
https://github.com/naiqili/Logic-of-Thought.

</details>


### [30] [LLM-Powered AI Agent Systems and Their Applications in Industry](https://arxiv.org/abs/2505.16120)
*Guannan Liang,Qianqian Tong*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）如何重塑代理系统，对比传统规则代理的局限性，突出了LLM代理的灵活性、跨领域推理和多模态处理能力，并分类讨论了其应用领域及面临的挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何推动代理系统从传统规则模式向更灵活、多模态的方向发展，并探索其在实际应用中的潜力与挑战。

Method: 通过分类代理系统（软件、物理、混合系统）并分析其在多个领域的应用，同时识别LLM代理的主要问题（如延迟、不确定性等）并提出解决方案。

Result: LLM代理在多领域展现出强大潜力，但仍需解决技术挑战（如延迟、安全漏洞）以进一步提升性能。

Conclusion: LLM代理系统代表了代理技术的重大进步，但需进一步研究和优化以克服现有挑战，实现更广泛的应用。

Abstract: The emergence of Large Language Models (LLMs) has reshaped agent systems.
Unlike traditional rule-based agents with limited task scope, LLM-powered
agents offer greater flexibility, cross-domain reasoning, and natural language
interaction. Moreover, with the integration of multi-modal LLMs, current agent
systems are highly capable of processing diverse data modalities, including
text, images, audio, and structured tabular data, enabling richer and more
adaptive real-world behavior. This paper comprehensively examines the evolution
of agent systems from the pre-LLM era to current LLM-powered architectures. We
categorize agent systems into software-based, physical, and adaptive hybrid
systems, highlighting applications across customer service, software
development, manufacturing automation, personalized education, financial
trading, and healthcare. We further discuss the primary challenges posed by
LLM-powered agents, including high inference latency, output uncertainty, lack
of evaluation metrics, and security vulnerabilities, and propose potential
solutions to mitigate these concerns.

</details>


### [31] [Sudoku-Bench: Evaluating creative reasoning with Sudoku variants](https://arxiv.org/abs/2505.16135)
*Jeffrey Seely,Yuki Imajuku,Tianyu Zhao,Edoardo Cetin,Llion Jones*

Main category: cs.AI

TL;DR: Sudoku-Bench是一个针对大型语言模型（LLMs）的创造性推理基准测试，通过独特的数独变体评估多步逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准测试未能捕捉真实创造力，常依赖记忆模式。Sudoku-Bench旨在填补这一空白。

Method: 使用精心挑选的数独变体，设计标准化文本表示和工具，确保评估清晰一致。

Result: 当前最先进的LLMs在无辅助情况下仅解决不到15%的谜题。

Conclusion: Sudoku-Bench为推进长时程战略推理能力提供了重要机会。

Abstract: Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.

</details>


### [32] [Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value](https://arxiv.org/abs/2505.16147)
*Le Ma,Shirao Yang,Zihao Wang,Yinggui Wang,Lei Wang,Tao Wei,Kejun Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为Unlearning Shapley的新框架，通过机器遗忘技术高效估计数据价值，解决了传统方法计算成本高或依赖完整数据的问题。


<details>
  <summary>Details</summary>
Motivation: 大型模型的普及使得高效数据估值方法变得迫切，传统方法如Shapley值和影响函数技术计算成本高或依赖完整数据，难以实现部分数据估值。

Method: 利用机器遗忘技术，从预训练模型中移除目标数据，并通过蒙特卡洛采样计算Shapley值，避免了重新训练和依赖完整数据。

Result: 实验表明，该方法在基准数据集和大规模文本语料上与传统方法精度相当，同时显著降低计算开销。

Conclusion: Unlearning Shapley为现代AI生态系统提供了一种可扩展且隐私合规的数据估值解决方案，填补了理论与实际应用之间的差距。

Abstract: The proliferation of large models has intensified the need for efficient data
valuation methods to quantify the contribution of individual data providers.
Traditional approaches, such as game-theory-based Shapley value and
influence-function-based techniques, face prohibitive computational costs or
require access to full data and model training details, making them hardly
achieve partial data valuation. To address this, we propose Unlearning Shapley,
a novel framework that leverages machine unlearning to estimate data values
efficiently. By unlearning target data from a pretrained model and measuring
performance shifts on a reachable test set, our method computes Shapley values
via Monte Carlo sampling, avoiding retraining and eliminating dependence on
full data. Crucially, Unlearning Shapley supports both full and partial data
valuation, making it scalable for large models (e.g., LLMs) and practical for
data markets. Experiments on benchmark datasets and large-scale text corpora
demonstrate that our approach matches the accuracy of state-of-the-art methods
while reducing computational overhead by orders of magnitude. Further analysis
confirms a strong correlation between estimated values and the true impact of
data subsets, validating its reliability in real-world scenarios. This work
bridges the gap between data valuation theory and practical deployment,
offering a scalable, privacy-compliant solution for modern AI ecosystems.

</details>


### [33] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
*Jun Rao,Xuebo Liu,Hexuan Deng,Zepeng Lin,Zixiong Yu,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.AI

TL;DR: SAI-DPO是一种动态数据选择算法，通过实时评估模型在不同训练阶段的推理能力，提升数据利用效率和任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖静态指标，无法适应动态训练过程，限制了模型能力的持续提升。

Method: SAI-DPO通过实时反馈模型性能，动态调整数据选择策略，适应模型能力的演变。

Result: 在多个数学推理基准测试中，SAI-DPO平均提升21.3个百分点，AIME24和AMC23分别提升10和15点。

Conclusion: 动态数据选择优于静态策略，显著提升推理能力。

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [34] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
*Kaiwen Zhou,Xuandong Zhao,Gaowen Liu,Jayanth Srinivasa,Aosong Feng,Dawn Song,Xin Eric Wang*

Main category: cs.AI

TL;DR: 论文提出SafeKey方法，通过激活安全推理的‘aha moment’提升大型推理模型的安全性，显著降低有害查询和对抗攻击的风险。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现出色，但对有害查询和对抗攻击存在安全隐患。现有方法如监督微调（SFT）难以泛化到未见过的越狱提示。

Method: 提出SafeKey方法，包含双路径安全头（增强安全信号）和查询掩码建模目标（提升对查询理解的注意力）。

Result: 实验表明，SafeKey显著提升安全性，平均有害率降低9.6%，同时保持模型通用能力。

Conclusion: SafeKey通过重塑内部注意力和提升隐藏表示质量，有效增强模型安全性。

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [35] [Velocity Completion Task and Method for Event-based Player Positional Data in Soccer](https://arxiv.org/abs/2505.16199)
*Rikuhei Umemoto,Keisuke Fujii*

Main category: cs.AI

TL;DR: 提出了一种基于神经网络的方法，利用事件位置数据完成所有代理的速度信息，提升团队运动分析的深度和准确性。


<details>
  <summary>Details</summary>
Motivation: 事件位置数据缺乏连续时间信息，无法直接计算速度，限制了动态分析的深度。

Method: 使用神经网络方法，基于事件位置数据完成所有代理的速度信息，并验证其适用性。

Result: 神经网络方法在速度完成误差上优于基于规则的方法，且空间评估结果更接近完整跟踪数据。

Conclusion: 该方法能显著提升团队运动系统分析的潜力，尤其是在动态行为和团队策略理解方面。

Abstract: In many real-world complex systems, the behavior can be observed as a
collection of discrete events generated by multiple interacting agents.
Analyzing the dynamics of these multi-agent systems, especially team sports,
often relies on understanding the movement and interactions of individual
agents. However, while providing valuable snapshots, event-based positional
data typically lacks the continuous temporal information needed to directly
calculate crucial properties such as velocity. This absence severely limits the
depth of dynamic analysis, preventing a comprehensive understanding of
individual agent behaviors and emergent team strategies. To address this
challenge, we propose a new method to simultaneously complete the velocity of
all agents using only the event-based positional data from team sports. Based
on this completed velocity information, we investigate the applicability of
existing team sports analysis and evaluation methods. Experiments using soccer
event data demonstrate that neural network-based approaches outperformed
rule-based methods regarding velocity completion error, considering the
underlying temporal dependencies and graph structure of player-to-player or
player-to-ball interaction. Moreover, the space evaluation results obtained
using the completed velocity are closer to those derived from complete tracking
data, highlighting our method's potential for enhanced team sports system
analysis.

</details>


### [36] [LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead](https://arxiv.org/abs/2505.16221)
*Yifan Zhang,Xinkui Zhao,Zuxin Wang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.AI

TL;DR: LightRouter是一个新颖的框架，用于从大型语言模型池中选择和集成小型子集，以优化任务性能和成本效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在性能和成本上差异显著，用户难以选择最适合特定任务的模型。

Method: LightRouter通过自适应选择机制和有效集成策略，减少启动令牌数量并降低成本。

Result: 实验表明，LightRouter在多个基准测试中表现优于或匹配现有集成方法，准确率提升25%，成本降低27%。

Conclusion: LightRouter提供了一种无需先验知识的高效模型选择和集成策略。

Abstract: The rapid advancement of large language models has unlocked remarkable
capabilities across a diverse array of natural language processing tasks.
However, the considerable differences among available LLMs-in terms of cost,
performance, and computational demands-pose significant challenges for users
aiming to identify the most suitable model for specific tasks. In this work, we
present LightRouter, a novel framework designed to systematically select and
integrate a small subset of LLMs from a larger pool, with the objective of
jointly optimizing both task performance and cost efficiency. LightRouter
leverages an adaptive selection mechanism to identify models that require only
a minimal number of boot tokens, thereby reducing costs, and further employs an
effective integration strategy to combine their outputs. Extensive experiments
across multiple benchmarks demonstrate that LightRouter matches or outperforms
widely-used ensemble baselines, achieving up to a 25% improvement in accuracy.
Compared with leading high-performing models, LightRouter achieves comparable
performance while reducing inference costs by up to 27%. Importantly, our
framework operates without any prior knowledge of individual models and relies
exclusively on inexpensive, lightweight models. This work introduces a
practical approach for efficient LLM selection and provides valuable insights
into optimal strategies for model combination.

</details>


### [37] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/abs/2505.16223)
*Sangyong Lee,Subo Hwang,Dohoon Kim*

Main category: cs.AI

TL;DR: MADCluster是一种模型无关的异常检测框架，通过自监督聚类解决现有深度学习方法中的'超球崩溃'问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在异常检测中存在的'超球崩溃'问题，并提升模型的表达能力和兼容性。

Method: 提出MADCluster框架，包括Base Embedder、Cluster Distance Mapping和Sequence-wise Clustering三个组件，并引入'One-directed Adaptive loss'优化聚类效果。

Result: 在四个时间序列基准数据集上的实验表明，MADCluster显著提升了对比模型的性能。

Conclusion: MADCluster具有广泛的兼容性，能够有效提升多种架构的模型性能。

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [38] [MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning](https://arxiv.org/abs/2505.16225)
*Zihan Chen,Song Wang,Zhen Tan,Jundong Li,Cong Shen*

Main category: cs.AI

TL;DR: MAPLE是一种基于影响力的多示例上下文学习框架，通过伪标记样本弥补标签数据的不足，提升大语言模型在有限标签数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多示例上下文学习（ICL）中获取大量标注数据成本高的问题。

Method: 提出MAPLE框架，通过识别有影响力的未标记样本并进行伪标记，自适应地选择和调整这些样本作为输入。

Result: 在真实数据集上的实验表明，MAPLE能显著提升大语言模型的适应性和性能。

Conclusion: MAPLE是一种高效的方法，能够在有限标签数据下提升多示例上下文学习的效果。

Abstract: In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle
diverse tasks by incorporating multiple input-output examples, known as
demonstrations, into the input of LLMs. More recently, advancements in the
expanded context windows of LLMs have led to many-shot ICL, which uses hundreds
of demonstrations and outperforms few-shot ICL, which relies on fewer examples.
However, this approach is often hindered by the high cost of obtaining large
amounts of labeled data. To address this challenge, we propose Many-Shot
Adaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL
framework that utilizes pseudo-labeled samples to compensate for the lack of
label information. We first identify a subset of impactful unlabeled samples
and perform pseudo-labeling on them by querying LLMs. These pseudo-labeled
samples are then adaptively selected and tailored to each test query as input
to improve the performance of many-shot ICL, without significant labeling
costs. Extensive experiments on real-world datasets demonstrate the
effectiveness of our framework, showcasing its ability to enhance LLM
adaptability and performance with limited labeled data.

</details>


### [39] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
*Desiree Heim,Lars-Peter Meyer,Markus Schröder,Johannes Frey,Andreas Dengel*

Main category: cs.AI

TL;DR: 论文探讨了在知识图谱工程（KGE）任务中使用大型语言模型（LLMs）时，模型大小与性能及成本的关系。通过LLM-KG-Bench框架评估26个开源LLMs，发现模型大小通常与性能正相关，但也存在平台效应和家族内性能波动的情况。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在KGE任务中，LLMs的模型大小与性能及成本的关系，以找到高性价比的模型选择。

Method: 使用LLM-KG-Bench框架评估26个开源LLMs在KGE任务中的表现，分析模型大小与性能的关系，并观察平台效应和家族内性能波动。

Result: 研究发现模型大小通常与性能正相关，但也存在平台效应（性能不再显著提升）和家族内性能波动的情况。

Conclusion: 结论是模型大小通常适用于KGE任务，但在某些情况下，较小的模型可能更具性价比，建议测试同一家族中相邻大小的模型。

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [40] [No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery](https://arxiv.org/abs/2505.16288)
*Xiaoxue Han,Pengfei Hu,Jun-En Ding,Chang Lu,Feng Liu,Yue Ning*

Main category: cs.AI

TL;DR: II-KEA框架通过知识增强和因果分析提升深度学习模型在医疗诊断中的可解释性和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医疗诊断中缺乏可解释性和交互性，限制了临床医生的决策能力。

Method: 提出II-KEA框架，整合个性化知识库和代理LLMs，通过因果分析和显式推理增强可解释性，支持医生知识注入。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优异，提升了可解释性和交互性。

Conclusion: II-KEA为医疗决策提供了更透明和互动的工具，有望改善临床实践。

Abstract: Deep learning models trained on extensive Electronic Health Records (EHR)
data have achieved high accuracy in diagnosis prediction, offering the
potential to assist clinicians in decision-making and treatment planning.
However, these models lack two crucial features that clinicians highly value:
interpretability and interactivity. The ``black-box'' nature of these models
makes it difficult for clinicians to understand the reasoning behind
predictions, limiting their ability to make informed decisions. Additionally,
the absence of interactive mechanisms prevents clinicians from incorporating
their own knowledge and experience into the decision-making process. To address
these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal
discovery framework that integrates personalized knowledge databases and
agentic LLMs. II-KEA enhances interpretability through explicit reasoning and
causal analysis, while also improving interactivity by allowing clinicians to
inject their knowledge and experience through customized knowledge bases and
prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating
superior performance along with enhanced interpretability and interactivity, as
evidenced by its strong results from extensive case studies.

</details>


### [41] [EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning](https://arxiv.org/abs/2505.16312)
*Jiawei Liu,Qisi Chen,Jianshu Zhang,Quan Liu,Defu Lian*

Main category: cs.AI

TL;DR: 论文提出EquivPruner方法，通过识别和剪枝语义等价动作，显著减少LLM推理搜索中的token消耗，并提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理搜索中存在大量语义等价步骤的冗余探索，导致token消耗巨大，现有语义相似性方法在领域特定场景（如数学推理）中效果不佳。

Method: 提出EquivPruner方法，结合MathEquiv数据集训练轻量级等价检测器，识别并剪枝语义等价动作。

Result: 实验表明，EquivPruner显著减少token消耗（如GSM8K任务中减少48.1%），并提升推理准确性。

Conclusion: EquivPruner是一种简单有效的方法，可优化LLM推理搜索效率与性能，代码已开源。

Abstract: Large Language Models (LLMs) excel at complex reasoning through search
algorithms, yet current strategies often suffer from massive token consumption
due to redundant exploration of semantically equivalent steps. Existing
semantic similarity methods struggle to accurately identify such equivalence in
domain-specific contexts like mathematical reasoning. To address this, we
propose EquivPruner, a simple yet effective approach that identifies and prunes
semantically equivalent actions during LLM reasoning search. We also introduce
MathEquiv, the first dataset we created for mathematical statement equivalence,
which enables the training of a lightweight equivalence detector. Extensive
experiments across various models and tasks demonstrate that EquivPruner
significantly reduces token consumption, improving searching efficiency and
often bolstering reasoning accuracy. For instance, when applied to
Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by
48.1\% while also improving accuracy. Our code is available at
https://github.com/Lolo1222/EquivPruner.

</details>


### [42] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
*Xiaoxue Cheng,Junyi Li,Zhenduo Zhang,Xinyu Tang,Wayne Xin Zhao,Xinyu Kong,Zhiqiang Zhang*

Main category: cs.AI

TL;DR: ACPO是一种强化学习框架，通过自适应认知分配和动态系统切换，减少大型推理模型的冗余推理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现优异，但存在过度思考问题，生成冗余内容。受认知科学中的双过程理论启发，提出ACPO以实现高效推理。

Method: ACPO引入系统感知推理标记和在线难度估计，通过两阶段训练策略（监督微调和强化学习）实现自适应系统切换。

Result: 实验表明，ACPO能有效减少冗余推理，并根据任务复杂度自适应调整认知分配。

Conclusion: ACPO为大型推理模型提供了一种高效混合推理的解决方案。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [43] [Serious Games: Human-AI Interaction, Evolution, and Coevolution](https://arxiv.org/abs/2505.16388)
*Nandini Doreswamy,Louise Horstmanshof*

Main category: cs.AI

TL;DR: 该论文探讨了进化博弈论（EGT）在预测人类与AI交互、进化及共同进化中的潜在平衡中的应用，重点分析了三种EGT模型，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI之间的进化动态和共同进化轨迹，预测其潜在平衡，并探讨可能的合作与竞争策略。

Method: 分析了13种EGT模型中的三种（鹰鸽博弈、重复囚徒困境和消耗战），基于其广泛接受度和对人类-AI动态的明确相关性。

Result: 鹰鸽博弈预测了基于冲突成本的混合策略平衡；重复囚徒困境表明记忆和互惠可促进合作；消耗战揭示了资源竞争可能导致战略共同进化。

Conclusion: EGT为理解人类-AI进化动态提供了框架，但未来需扩展研究范围，关注伦理和认知影响。

Abstract: The serious games between humans and AI have only just begun. Evolutionary
Game Theory (EGT) models the competitive and cooperative strategies of
biological entities. EGT could help predict the potential evolutionary
equilibrium of humans and AI. The objective of this work was to examine some of
the EGT models relevant to human-AI interaction, evolution, and coevolution. Of
thirteen EGT models considered, three were examined: the Hawk-Dove Game,
Iterated Prisoner's Dilemma, and the War of Attrition. This selection was based
on the widespread acceptance and clear relevance of these models to potential
human-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove
Game predicts balanced mixed-strategy equilibria based on the costs of
conflict. It also shows the potential for balanced coevolution rather than
dominance. Iterated Prisoner's Dilemma suggests that repeated interaction may
lead to cognitive coevolution. It demonstrates how memory and reciprocity can
lead to cooperation. The War of Attrition suggests that competition for
resources may result in strategic coevolution, asymmetric equilibria, and
conventions on sharing resources. Therefore, EGT may provide a suitable
framework to understand and predict the human-AI evolutionary dynamic. However,
future research could extend beyond EGT and explore additional frameworks,
empirical validation methods, and interdisciplinary perspectives. AI is being
shaped by human input and is evolving in response to it. So too,
neuroplasticity allows the human brain to grow and evolve in response to
stimuli. If humans and AI converge in future, what might be the result of human
neuroplasticity combined with an ever-evolving AI? Future research should be
mindful of the ethical and cognitive implications of human-AI interaction,
evolution, and coevolution.

</details>


### [44] [FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS](https://arxiv.org/abs/2505.16409)
*Chaeeun Kim,Seungone Kim*

Main category: cs.AI

TL;DR: FREESON框架通过让大型推理模型（LRM）同时充当生成器和检索器，消除了对独立检索模型的依赖，提升了检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强推理方法依赖独立检索模型，导致硬件成本高且检索错误多。

Method: 提出FREESON框架和CT-MCTS算法，使LRM能够自主检索知识。

Result: 在五个开放域QA基准测试中，FREESON平均提升14.4%的EM和F1分数。

Conclusion: FREESON显著提升了检索增强推理的性能，且无需独立检索模型。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
multi-step reasoning and calling search engines at appropriate steps. However,
existing retrieval-augmented reasoning approaches rely on separate retrieval
models, limiting the LRM's role in retrieval to deciding when to retrieve and
how to query. This separation not only increases hardware and operational costs
but also leads to errors in the retrieval process due to the representation
bottleneck, a phenomenon where the retriever's embedding space is not
expressive enough to meet the generator's requirements. To address this, we
shift our perspective from sequence-to-sequence matching to locating the
answer-containing paths within the corpus, and propose a novel framework called
FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables
LRMs to retrieve relevant knowledge on their own by acting as both a generator
and retriever. To achieve this, we introduce a variant of the MCTS algorithm
specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing
Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus
toward answer-containing regions. Our results on five open-domain QA
benchmarks, including single-hop and multi-hop questions, show that FREESON
achieves an average improvement of 14.4% in EM and F1 over four multi-step
reasoning models with a separate retriever, and it also performs comparably to
the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.

</details>


### [45] [Internal Bias in Reasoning Models leads to Overthinking](https://arxiv.org/abs/2505.16448)
*Renfei Dang,Shujian Huang,Jiajun Chen*

Main category: cs.AI

TL;DR: 论文揭示了推理模型中的过度思考现象源于其对输入文本的内部偏见，通过掩盖输入部分可显著减少推理长度并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型因冗余反思导致过度思考，研究旨在揭示其内部偏见对决策的影响。

Method: 通过解释性实验分析模型行为，并采用掩盖输入部分的方法缓解内部偏见。

Result: 掩盖输入部分可减少推理长度31%-53%，并在多数情况下提高准确性。

Conclusion: 内部偏见与过度思考存在因果关系，掩盖输入是有效的解决方案。

Abstract: While current reasoning models possess strong exploratory capabilities, they
are often criticized for overthinking due to redundant and unnecessary
reflections. In this work, we reveal for the first time that overthinking in
reasoning models may stem from their internal bias towards input texts. Upon
encountering a reasoning problem, the model immediately forms a preliminary
guess about the answer, which we term as an internal bias since it is not
derived through actual reasoning. When this guess conflicts with its reasoning
result, the model tends to engage in reflection, leading to the waste of
computational resources. Through further interpretability experiments, we find
that this behavior is largely driven by the model's excessive attention to the
input section, which amplifies the influence of internal bias on its
decision-making process. Additionally, by masking out the original input
section, the affect of internal bias can be effectively alleviated and the
reasoning length could be reduced by 31%-53% across different complex reasoning
tasks. Notably, in most cases, this approach also leads to improvements in
accuracy. These findings demonstrate a causal relationship between internal
bias and overthinking.

</details>


### [46] [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/abs/2505.16459)
*Guiyao Tie,Xueyang Zhou,Tianhe Gu,Ruihang Zhang,Chaoran Hu,Sizhe Zhang,Mengqu Sun,Yan Zhang,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 论文提出了MMMR基准，用于评估多模态大语言模型（MLLMs）的推理能力，特别是带有中间思考痕迹的模型（MLLMs-T），揭示了现有模型在推理质量上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究对多模态大语言模型的推理能力缺乏深入理解和标准化评估，尤其是中间思考痕迹的作用。

Method: 提出MMMR基准，包括1）包含1,083个高难度问题的数据集，覆盖六种推理类型；2）模块化的推理痕迹评估流程（RTEP），通过相关性、一致性和错误标注等指标评估推理质量。

Result: MLLMs-T在整体表现上优于无思考痕迹的模型，但顶级模型仍存在不一致性和过度思考等推理缺陷。

Conclusion: MMMR为评估和改进多模态推理系统提供了可扩展的基础，揭示了准确性与推理质量之间的差距。

Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled
unified processing of language, vision, and structured inputs, opening the door
to complex tasks such as logical deduction, spatial reasoning, and scientific
analysis. Despite their promise, the reasoning capabilities of MLLMs,
particularly those augmented with intermediate thinking traces (MLLMs-T),
remain poorly understood and lack standardized evaluation benchmarks. Existing
work focuses primarily on perception or final answer correctness, offering
limited insight into how models reason or fail across modalities. To address
this gap, we introduce the MMMR, a new benchmark designed to rigorously
evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a
high-difficulty dataset of 1,083 questions spanning six diverse reasoning types
with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace
Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy
through metrics like relevance, consistency, and structured error annotations.
Empirical results show that MLLMs-T overall outperform non-thinking
counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro
suffer from reasoning pathologies such as inconsistency and overthinking. This
benchmark reveals persistent gaps between accuracy and reasoning quality and
provides an actionable evaluation pipeline for future model development.
Overall, the MMMR offers a scalable foundation for evaluating, comparing, and
improving the next generation of multi-modal reasoning systems.

</details>


### [47] [ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection](https://arxiv.org/abs/2505.16475)
*Jiaqi Li,Xinyi Dong,Yang Liu,Zhizhuo Yang,Quansen Wang,Xiaobo Wang,SongChun Zhu,Zixia Jia,Zilong Zheng*

Main category: cs.AI

TL;DR: ReflectEvo是一种新方法，通过反思学习提升小型语言模型（SLMs）的推理能力，无需依赖大模型或人工标注。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自我反思和迭代学习提升小型语言模型的推理能力，减少对大模型或人工标注的依赖。

Method: 提出ReflectEvo流程，生成大规模自我反思数据集ReflectEvo-460k，并利用SFT和DPO方法训练SLMs。

Result: 显著提升模型推理能力（如Llama-3从52.4%到71.2%），性能媲美或超越开源模型。

Conclusion: ReflectEvo展示了通过迭代反思学习持续提升SLMs推理能力的潜力。

Abstract: We present a novel pipeline, ReflectEvo, to demonstrate that small language
models (SLMs) can enhance meta introspection through reflection learning. This
process iteratively generates self-reflection for self-training, fostering a
continuous and self-evolving process. Leveraging this pipeline, we construct
ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection
dataset with broadened instructions and diverse multi-domain tasks. Building
upon this dataset, we demonstrate the effectiveness of reflection learning to
improve SLMs' reasoning abilities using SFT and DPO with remarkable
performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral
from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the
reasoning capability of the three prominent open-sourced models on BIG-bench
without distillation from superior models or fine-grained human annotation. We
further conduct a deeper analysis of the high quality of self-generated
reflections and their impact on error localization and correction. Our work
highlights the potential of continuously enhancing the reasoning performance of
SLMs through iterative reflection learning in the long run.

</details>


### [48] [Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery](https://arxiv.org/abs/2505.16477)
*Yanbo Zhang,Sumeer A. Khan,Adnan Mahmud,Huck Yang,Alexander Lavin,Michael Levin,Jeremy Frey,Jared Dunnmon,James Evans,Alan Bundy,Saso Dzeroski,Jesper Tegner,Hector Zenil*

Main category: cs.AI

TL;DR: LLMs正在改变科学研究，提升生产力并重塑科学方法，但也面临幻觉和可靠性等挑战。需要与人类目标深度整合，并解决伦理问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何重新定义科学方法及其在科学周期各阶段的潜在应用。

Method: 综述LLMs在实验设计、数据分析和科学流程中的应用，分析其挑战与潜力。

Result: LLMs可作为创造性引擎和生产力增强工具，但需明确评估指标和伦理框架。

Conclusion: LLMs有望推动科学突破，但需谨慎整合人类目标，解决伦理和责任问题。

Abstract: With recent Nobel Prizes recognising AI contributions to science, Large
Language Models (LLMs) are transforming scientific research by enhancing
productivity and reshaping the scientific method. LLMs are now involved in
experimental design, data analysis, and workflows, particularly in chemistry
and biology. However, challenges such as hallucinations and reliability
persist. In this contribution, we review how Large Language Models (LLMs) are
redefining the scientific method and explore their potential applications
across different stages of the scientific cycle, from hypothesis testing to
discovery. We conclude that, for LLMs to serve as relevant and effective
creative engines and productivity enhancers, their deep integration into all
steps of the scientific process should be pursued in collaboration and
alignment with human scientific goals, with clear evaluation metrics. The
transition to AI-driven science raises ethical questions about creativity,
oversight, and responsibility. With careful guidance, LLMs could evolve into
creative engines, driving transformative breakthroughs across scientific
disciplines responsibly and effectively. However, the scientific community must
also decide how much it leaves to LLMs to drive science, even when associations
with 'reasoning', mostly currently undeserved, are made in exchange for the
potential to explore hypothesis and solution regions that might otherwise
remain unexplored by human exploration alone.

</details>


### [49] [Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes](https://arxiv.org/abs/2505.16482)
*Huynh Thi Thanh Binh,Le Van Cuong,Dang Hai Dang,Le Trong Vinh*

Main category: cs.AI

TL;DR: 论文提出了一种新颖的部分充电方法，通过双层优化方案最小化无线可充电传感器网络（WRSNs）中的能量耗尽问题，同时优化充电路径和时间。


<details>
  <summary>Details</summary>
Motivation: 现有充电策略主要关注完全充电路径优化，可能导致传感器因充电延迟而失效，因此需要更高效的充电方法。

Method: 提出双层优化方案，上层优化充电路径，下层优化充电时间。采用两种近似算法：1）结合多起点局部搜索和遗传算法；2）嵌套多任务和协方差矩阵自适应进化策略。

Result: 实验验证表明，所提算法在多种网络场景下优于现有方法。

Conclusion: 部分充电方法通过双层优化显著提升了WRSNs的充电性能。

Abstract: Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the
advantage of wireless energy transfer technology have opened a promising
opportunity in solving the limited energy issue. However, an ineffective
charging strategy may reduce the charging performance. Although many practical
charging algorithms have been introduced, these studies mainly focus on
optimizing the charging path with a fully charging approach. This approach may
lead to the death of a series of sensors due to their extended charging
latency. This paper introduces a novel partial charging approach that follows a
bi-level optimized scheme to minimize energy depletion in WRSNs. We aim at
optimizing simultaneously two factors: the charging path and time. To
accomplish this, we first formulate a mathematical model of the investigated
problem. We then propose two approximate algorithms in which the optimization
of the charging path and the charging time are considered as the upper and
lower level, respectively. The first algorithm combines a Multi-start Local
Search method and a Genetic Algorithm to find a solution. The second algorithm
adopts a nested approach that utilizes the advantages of the Multitasking and
Covariance Matrix Adaptation Evolutionary Strategies. Experimental validations
on various network scenarios demonstrate that our proposed algorithms
outperform the existing works.

</details>


### [50] [Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)](https://arxiv.org/abs/2505.16507)
*Anshu Xiong,Songmao Zhang*

Main category: cs.AI

TL;DR: 本文扩展了Odekerken等人提出的单参数稳定性概念，研究了参数集的验证稳定性，并提出了强相关性概念。复杂度分析显示，多数语义下检测相关性的时间为P时间，但在基础语义下难以找到高效方法。


<details>
  <summary>Details</summary>
Motivation: 扩展单参数稳定性的概念，研究参数集在不确定性下的验证稳定性，并探讨强相关性的必要性。

Method: 提出参数集验证稳定性的概念，定义强相关性，并进行复杂度分析。

Result: 多数语义下检测相关性的时间为P时间，但在基础语义下难以找到高效方法。

Conclusion: 扩展了稳定性概念，揭示了不同语义下相关性检测的复杂度差异。

Abstract: The notion of relevance was proposed for stability of justification status of
a single argument in incomplete argumentation frameworks (IAFs) in 2024 by
Odekerken et al. To extend the notion, we study the relevance for stability of
verification status of a set of arguments in this paper, i.e., the
uncertainties in an IAF that have to be resolved in some situations so that
answering whether a given set of arguments is an extension obtains the same
result in every completion of the IAF. Further we propose the notion of strong
relevance for describing the necessity of resolution in all situations reaching
stability. An analysis of complexity reveals that detecting the (strong)
relevance for stability of sets of arguments can be accomplished in P time
under the most semantics discussed in the paper. We also discuss the difficulty
in finding tractable methods for relevance detection under grounded semantics.

</details>


### [51] [Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning](https://arxiv.org/abs/2505.16579)
*Siqu Ou,Hongcheng Liu,Pingjie Wang,Yusheng Liao,Chuan Xuan,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出了GRASSLAND迷宫导航基准和D2R框架，通过动态视觉草稿增强文本推理链，显著提升动态空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于文本或静态视觉领域，难以应对动态空间推理任务。

Method: 提出D2R框架，结合文本推理链和动态视觉草稿，无需微调即可增强多模态大语言模型的推理能力。

Result: 实验表明D2R在多样化任务中表现优异，为动态空间推理提供了稳健基准。

Conclusion: D2R框架有效解决了动态空间推理问题，为未来研究提供了新方向。

Abstract: While chains-of-thought (CoT) have advanced complex reasoning in multimodal
large language models (MLLMs), existing methods remain confined to text or
static visual domains, often faltering in dynamic spatial reasoning tasks. To
bridge this gap, we present GRASSLAND, a novel maze navigation benchmark
designed to evaluate dynamic spatial reasoning. Our experiments show that
augmenting textual reasoning chains with dynamic visual drafts, overlaid on
input images, significantly outperforms conventional approaches, offering new
insights into spatial reasoning in evolving environments. To generalize this
capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free
framework that seamlessly integrates textual CoT with corresponding visual
drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently
enhances performance across diverse tasks, establishing a robust baseline for
dynamic spatial reasoning without requiring model fine-tuning. Project is open
at https://github.com/Cratileo/D2R.

</details>


### [52] [Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences](https://arxiv.org/abs/2505.16619)
*Gavin Farrell,Eleni Adamidi,Rafael Andrade Buono,Mihail Anton,Omar Abdelghani Attafi,Salvador Capella Gutierrez,Emidio Capriotti,Leyla Jael Castro,Davide Cirillo,Lisa Crossman,Christophe Dessimoz,Alexandros Dimopoulos,Raul Fernandez-Diaz,Styliani-Christina Fragkouli,Carole Goble,Wei Gu,John M. Hancock,Alireza Khanteymoori,Tom Lenaerts,Fabio G. Liberante,Peter Maccallum,Alexander Miguel Monzon,Magnus Palmblad,Lucy Poveda,Ovidiu Radulescu,Denis C. Shields,Shoaib Sufi,Thanasis Vergoulis,Fotis Psomopoulos,Silvio C. E. Tosatto*

Main category: cs.AI

TL;DR: 论文探讨了AI在生命科学中的快速发展及其带来的挑战，提出了开放与可持续AI（OSAI）的建议，以解决可重用性、可重复性和透明度问题。


<details>
  <summary>Details</summary>
Motivation: AI在生命科学中的应用迅速增长，但伴随而来的是信任危机和环境可持续性问题，亟需解决方案。

Method: 通过分析AI生态系统的碎片化问题，提出了基于共识的OSAI建议，并映射到300多个AI组件。

Result: 提出了一套实用的OSAI建议，旨在促进可持续、可重用和透明的AI研究。

Conclusion: 该研究为未来AI政策的制定和实施提供了结构化路径，支持生命科学领域的AI发展。

Abstract: Artificial intelligence (AI) has recently seen transformative breakthroughs
in the life sciences, expanding possibilities for researchers to interpret
biological information at an unprecedented capacity, with novel applications
and advances being made almost daily. In order to maximise return on the
growing investments in AI-based life science research and accelerate this
progress, it has become urgent to address the exacerbation of long-standing
research challenges arising from the rapid adoption of AI methods. We review
the increased erosion of trust in AI research outputs, driven by the issues of
poor reusability and reproducibility, and highlight their consequent impact on
environmental sustainability. Furthermore, we discuss the fragmented components
of the AI ecosystem and lack of guiding pathways to best support Open and
Sustainable AI (OSAI) model development. In response, this perspective
introduces a practical set of OSAI recommendations directly mapped to over 300
components of the AI ecosystem. Our work connects researchers with relevant AI
resources, facilitating the implementation of sustainable, reusable and
transparent AI. Built upon life science community consensus and aligned to
existing efforts, the outputs of this perspective are designed to aid the
future development of policy and structured pathways for guiding AI
implementation.

</details>


### [53] [SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving](https://arxiv.org/abs/2505.16646)
*Yujie Hou,Ting Zhang,Mei Wang,Xuetao Ma,Hu Huang*

Main category: cs.AI

TL;DR: SMART框架通过四个维度评估LLM的数学能力，揭示单一准确性指标的不足，并提出更全面的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如最终答案准确性）无法区分LLM的数学推理能力与模式识别能力，缺乏诊断价值。

Method: 引入SMART框架，将数学问题解决分解为理解、推理、算术和反思与优化四个维度，并通过自动化机制生成和验证数据。

Result: 应用于21个先进LLM，发现不同维度能力差异显著，证明单一准确性指标不足。

Conclusion: SMART框架提供更细粒度和可解释的评估，推动更全面的数学能力衡量标准。

Abstract: Large Language Models have achieved remarkable results on a variety of
mathematical benchmarks. However, concerns remain as to whether these successes
reflect genuine mathematical reasoning or superficial pattern recognition.
Common evaluation metrics, such as final answer accuracy, fail to disentangle
the underlying competencies involved, offering limited diagnostic value. To
address these limitations, we introduce SMART: a Self-Generating and
Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes
mathematical problem solving into four distinct dimensions: understanding,
reasoning, arithmetic, and reflection \& refinement. Each dimension is
evaluated independently through tailored tasks, enabling interpretable and
fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated
self-generating and self-validating mechanism to produce and verify benchmark
data, ensuring both scalability and reliability. We apply SMART to 21
state-of-the-art open- and closed-source LLMs, uncovering significant
discrepancies in their abilities across different dimensions. Our findings
demonstrate the inadequacy of final answer accuracy as a sole metric and
motivate a new holistic metric to better capture true problem-solving
capabilities. Code and benchmarks will be released upon acceptance.

</details>


### [54] [ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming](https://arxiv.org/abs/2505.16667)
*Xinwei Yang,Zhaofeng Liu,Chen Huang,Jiashuai Zhang,Tong Zhang,Yifan Zhang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出了首个全面的人类反馈分类法、新数据集ELABORATIONSET和新基准ELABORATION，以促进人类与LLM在编程竞赛中的协作研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究分散且使用多样化的反馈，缺乏对人类与LLM协作编程的全面理解。

Method: 提出分类法整合编程过程反馈，创建ELABORATIONSET数据集和ELABORATION基准。

Result: 通过ELABORATION基准识别现有方法的优缺点，为未来改进奠定基础。

Conclusion: 论文为人类-LLM协作编程提供了系统化工具和评估框架。

Abstract: While recent research increasingly emphasizes the value of human-LLM
collaboration in competitive programming and proposes numerous empirical
methods, a comprehensive understanding remains elusive due to the fragmented
nature of existing studies and their use of diverse, application-specific human
feedback. Thus, our work serves a three-fold purpose: First, we present the
first taxonomy of human feedback consolidating the entire programming process,
which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a
novel programming dataset specifically designed for human-LLM collaboration,
meticulously annotated to enable large-scale simulated human feedback and
facilitate costeffective real human interaction studies. Third, we introduce
ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM
competitive programming. With ELABORATION, we pinpoint strengthes and
weaknesses of existing methods, thereby setting the foundation for future
improvement. Our code and dataset are available at
https://github.com/SCUNLP/ELABORATION

</details>


### [55] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
*Lars Benedikt Kaesberg,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.AI

TL;DR: SPaRC是一个新的2D网格路径推理数据集，用于评估空间和符号推理能力，现有模型表现不佳，而人类表现接近完美。


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集无法测试抽象、多步骤问题，尤其是路径规划和复杂规则约束满足。

Method: 引入SPaRC数据集，包含1000个2D网格路径规划谜题，要求逐步规划和算术几何规则。

Result: 人类准确率98%（困难谜题94.5%），而最佳模型（如o4-mini）仅15.8%（困难谜题1.1%），且常生成无效路径。

Conclusion: SPaRC揭示了模型在空间推理上的局限性，未来需改进训练和测试时计算效率。

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [56] [MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models](https://arxiv.org/abs/2505.16700)
*Xuanqi Gao,Siyi Xie,Juan Zhai,Shqing Ma,Chao Shen*

Main category: cs.AI

TL;DR: MCP-RADAR是一个专门为评估LLM在MCP框架下的工具利用能力而设计的综合基准，通过五个维度（答案准确性、工具选择效率、计算资源效率、参数构建准确性和执行速度）进行客观量化评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分评估LLM在新范式下的工具利用能力，因此需要一种更全面的评估框架。

Method: MCP-RADAR采用五维度的客观量化方法，覆盖多个任务领域（如软件工程、数学推理和问题解决），并对主流商业和开源LLM进行评估。

Result: 评估揭示了LLM在准确性、效率和速度之间存在显著权衡，挑战了传统的单一指标排名，并为开发者提供了优化工具的指导。

Conclusion: MCP-RADAR不仅适用于MCP框架，还可推广到其他LLM工具集成框架，为LLM和工具开发者提供了优化交互生态的宝贵见解。

Abstract: As Large Language Models (LLMs) evolve from passive text generators to active
reasoning agents capable of tool interaction, the Model Context Protocol (MCP)
has emerged as a standardized framework for dynamic tool discovery and
orchestration. Despite widespread industry adoption, existing evaluation
methodologies fail to adequately assess tool utilization capabilities within
this new paradigm. This paper introduces MCP-RADAR, the first comprehensive
benchmark specifically designed to evaluate LLM performance in the MCP
framework through a novel five-dimensional approach measuring: answer accuracy,
tool selection efficiency, computational resource efficiency, parameter
construction accuracy, and execution speed. Unlike conventional benchmarks that
rely on subjective human evaluations or binary success metrics, MCP-RADAR
employs objective, quantifiable measurements across multiple task domains
including software engineering, mathematical reasoning, and general
problem-solving. Our evaluations of leading commercial and open-source LLMs
reveal distinctive capability profiles with significant trade-offs between
accuracy, efficiency, and speed, challenging traditional single-metric
performance rankings. Besides, we provide valuable guidance for developers to
optimize their tools for maximum model compatibility and effectiveness. While
focused on MCP due to its standardized approach, our methodology remains
applicable across all LLM agent tool integration frameworks, providing valuable
insights for both LLM developers and tool creators to optimize the entire
LLM-tool interaction ecosystem. The implementation, configurations, and
datasets used in our evaluation are publicly available at
https://anonymous.4open.science/r/MCPRadar-B143.

</details>


### [57] [Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review](https://arxiv.org/abs/2505.16771)
*Beyazit Bestami Yuksel,Ayse Yilmazer Metin*

Main category: cs.AI

TL;DR: 本文综述了过去15年人工智能（AI）领域的重大突破，结合历史、理论和技术的视角，分析了计算资源、数据访问和算法创新的融合如何推动AI发展。


<details>
  <summary>Details</summary>
Motivation: 探讨AI领域的关键转折点，揭示技术突破背后的范式转变，并为未来研究和政策制定提供战略指导。

Method: 通过统计学习理论（如样本复杂性和数据效率）分析技术突破，并评估新兴解决方案（如联邦学习和隐私增强技术）。

Result: 研究发现，GPU训练、ImageNet、Transformer和GPT系列等技术突破推动了AI的范式转变，数据为中心的方法成为未来趋势。

Conclusion: 未来AI研究需关注数据为中心的方法，结合隐私保护和数据基础设施的演进，以应对现实挑战。

Abstract: This paper presents a comprehensive synthesis of major breakthroughs in
artificial intelligence (AI) over the past fifteen years, integrating
historical, theoretical, and technological perspectives. It identifies key
inflection points in AI' s evolution by tracing the convergence of
computational resources, data access, and algorithmic innovation. The analysis
highlights how researchers enabled GPU based model training, triggered a data
centric shift with ImageNet, simplified architectures through the Transformer,
and expanded modeling capabilities with the GPT series. Rather than treating
these advances as isolated milestones, the paper frames them as indicators of
deeper paradigm shifts. By applying concepts from statistical learning theory
such as sample complexity and data efficiency, the paper explains how
researchers translated breakthroughs into scalable solutions and why the field
must now embrace data centric approaches. In response to rising privacy
concerns and tightening regulations, the paper evaluates emerging solutions
like federated learning, privacy enhancing technologies (PETs), and the data
site paradigm, which reframe data access and security. In cases where real
world data remains inaccessible, the paper also assesses the utility and
constraints of mock and synthetic data generation. By aligning technical
insights with evolving data infrastructure, this study offers strategic
guidance for future AI research and policy development.

</details>


### [58] [Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making](https://arxiv.org/abs/2505.16781)
*Qianlei Jia,Xinliang Zhou,Ondrej Krejcar,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种结合三支决策理论、动态网络重构和语言意见表示的社会网络群体决策框架，以解决群体决策中的不确定性和动态性问题。


<details>
  <summary>Details</summary>
Motivation: 群体决策中存在不确定性、动态社会结构和模糊信息等挑战，传统模型难以应对。

Method: 结合三支决策理论建模犹豫和模糊性，基于意见相似性调整连接关系，使用语言描述意见，构建多智能体决策框架。

Result: 在多无人机协同决策场景中验证了模型的有效性，实验表明其能提升系统稳定性和决策行为真实性。

Conclusion: 该框架能有效解决群体决策中的动态性和不确定性问题，具有实际应用价值。

Abstract: In group decision-making (GDM) scenarios, uncertainty, dynamic social
structures, and vague information present major challenges for traditional
opinion dynamics models. To address these issues, this study proposes a novel
social network group decision-making (SNGDM) framework that integrates
three-way decision (3WD) theory, dynamic network reconstruction, and linguistic
opinion representation. First, the 3WD mechanism is introduced to explicitly
model hesitation and ambiguity in agent judgments, thereby preventing
irrational decisions. Second, a connection adjustment rule based on opinion
similarity is developed, enabling agents to adaptively update their
communication links and better reflect the evolving nature of social
relationships. Third, linguistic terms are used to describe agent opinions,
allowing the model to handle subjective, vague, or incomplete information more
effectively. Finally, an integrated multi-agent decision-making framework is
constructed, which simultaneously considers individual uncertainty, opinion
evolution, and network dynamics. The proposed model is applied to a multi-UAV
cooperative decision-making scenario, where simulation results and consensus
analysis demonstrate its effectiveness. Experimental comparisons further verify
the advantages of the algorithm in enhancing system stability and representing
realistic decision-making behaviors.

</details>


### [59] [Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce](https://arxiv.org/abs/2505.16787)
*Ashish Sundar,Chunbo Luo,Xiaoyang Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于模型强化学习（MBRL）的新方法，通过主动寻找高熵状态来优化世界模型的学习，从而提高样本效率和下游性能。


<details>
  <summary>Details</summary>
Motivation: 传统MBRL方法忽视了世界模型学习的优化，而改进世界模型的保真度和收敛时间可以显著提升性能。

Method: 提出了一种利用世界模型生成的短时潜在预测主动寻找高熵状态的方法，并设计了一个分层规划器动态决定重新规划时机、规划长度和奖励与熵的权重。

Result: 在Miniworld迷宫任务中，该方法比基础Dreamer收敛速度快50%，且训练策略所需环境步数仅为Dreamer的60%。

Conclusion: 该方法为MBRL提供了一种更高效的世界模型优化策略，显著提升了性能。

Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.

</details>


### [60] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
*Wei Sun,Wen Yang,Pu Jian,Qianlong Du,Fuwei Cui,Shuo Ren,Jiajun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为KTAE的新算法，通过细粒度的token级优势估计，解决了现有强化学习算法在优势计算上的粗粒度问题，显著提升了模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法（如GRPO及其变体DAPO）在计算优势时存在粗粒度问题，无法捕捉token级贡献，限制了模型的学习效果。

Method: 提出KTAE算法，利用采样rollout的正确性和统计分析，量化token级重要性，并结合rollout级优势，实现细粒度的token级优势估计。

Result: 实验表明，GRPO+KTAE和DAPO+KTAE在五个数学推理基准上优于基线方法，且能以更短的响应实现更高准确率。

Conclusion: KTAE算法有效解决了优势计算的粗粒度问题，显著提升了模型的推理性能。

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [61] [GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent](https://arxiv.org/abs/2505.16827)
*Bin Xie,Rui Shao,Gongwei Chen,Kaiwen Zhou,Yinchuan Li,Jie Liu,Min Zhang,Liqiang Nie*

Main category: cs.AI

TL;DR: GUI-explorer是一种无需训练的GUI代理，通过自主探索和知识提取解决动态环境中GUI自动化的挑战。


<details>
  <summary>Details</summary>
Motivation: 动态环境中GUI自动化面临组件误解和知识过时的问题，传统微调方法成本高。

Method: 结合功能感知任务目标生成器和转换感知知识提取器，实现系统探索和无监督知识挖掘。

Result: 在SPA-Bench和AndroidWorld上任务成功率分别为53.7%和47.4%，优于现有技术。

Conclusion: GUI-explorer无需参数更新，适用于新应用，已开源。

Abstract: GUI automation faces critical challenges in dynamic environments. MLLMs
suffer from two key issues: misinterpreting UI components and outdated
knowledge. Traditional fine-tuning methods are costly for app-specific
knowledge updates. We propose GUI-explorer, a training-free GUI agent that
incorporates two fundamental mechanisms: (1) Autonomous Exploration of
Function-aware Trajectory. To comprehensively cover all application
functionalities, we design a Function-aware Task Goal Generator that
automatically constructs exploration goals by analyzing GUI structural
information (e.g., screenshots and activity hierarchies). This enables
systematic exploration to collect diverse trajectories. (2) Unsupervised Mining
of Transition-aware Knowledge. To establish precise screen-operation logic, we
develop a Transition-aware Knowledge Extractor that extracts effective
screen-operation logic through unsupervised analysis the state transition of
structured interaction triples (observation, action, outcome). This eliminates
the need for human involvement in knowledge extraction. With a task success
rate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows
significant improvements over SOTA agents. It requires no parameter updates for
new apps. GUI-explorer is open-sourced and publicly available at
https://github.com/JiuTian-VL/GUI-explorer.

</details>


### [62] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
*Haonian Ji,Shi Qiu,Siyang Xin,Siwei Han,Zhaorun Chen,Hongyi Wang,Dake Zhang,Huaxiu Yao*

Main category: cs.AI

TL;DR: 论文提出了EduVisBench基准和EduVisAgent框架，用于评估和改进基础模型在教育场景中生成可视化解释的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在生成教育有效视觉解释方面能力有限，且多关注文本推理，忽视了结构化可视化的重要性。

Method: 引入EduVisBench多领域多级别基准，并提出EduVisAgent多智能体协作框架，分解推理并设计教育对齐的可视化。

Result: EduVisAgent显著优于基线，提升40.2%，生成更符合教育需求的可视化。

Conclusion: EduVisBench和EduVisAgent为教育可视化提供了有效工具，弥补了基础模型的不足。

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [63] [Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models](https://arxiv.org/abs/2505.16854)
*Jiaqi Wang,Kevin Qinghong Lin,James Cheng,Mike Zheng Shou*

Main category: cs.AI

TL;DR: TON是一种两阶段训练策略，通过选择性推理减少计算成本，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 受人类思考过程启发，研究如何在视觉语言模型中实现选择性推理，避免不必要的计算开销。

Method: 采用两阶段训练：(i) 监督微调阶段引入'thought dropout'操作；(ii) GRPO阶段优化任务奖励。

Result: TON比GRPO减少90%的推理长度，性能不降反升。

Conclusion: TON为强化学习实现类人推理模式提供了新思路。

Abstract: Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.

</details>


### [64] [Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings](https://arxiv.org/abs/2505.16877)
*Yuqicheng Zhu,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 论文提出CondKGCP方法，通过合并相似谓词和利用排名信息，为知识图谱嵌入提供更精确的条件覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅提供平均覆盖保证，无法满足高风险应用（如医疗诊断）对每个查询一致覆盖的需求。

Method: CondKGCP合并相似向量表示的谓词，并利用排名信息增强校准，以近似谓词条件覆盖保证。

Result: 理论证明和实验验证表明，CondKGCP在保持预测集紧凑的同时，提供了有效的条件覆盖保证。

Conclusion: CondKGCP为知识图谱嵌入的不确定性量化提供了更可靠的解决方案，适用于高风险场景。

Abstract: Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is
crucial for ensuring the reliability of downstream applications. A recent work
applies conformal prediction to KGE methods, providing uncertainty estimates by
generating a set of answers that is guaranteed to include the true answer with
a predefined confidence level. However, existing methods provide probabilistic
guarantees averaged over a reference set of queries and answers (marginal
coverage guarantee). In high-stakes applications such as medical diagnosis, a
stronger guarantee is often required: the predicted sets must provide
consistent coverage per query (conditional coverage guarantee). We propose
CondKGCP, a novel method that approximates predicate-conditional coverage
guarantees while maintaining compact prediction sets. CondKGCP merges
predicates with similar vector representations and augments calibration with
rank information. We prove the theoretical guarantees and demonstrate empirical
effectiveness of CondKGCP by comprehensive evaluations.

</details>


### [65] [Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships](https://arxiv.org/abs/2505.16899)
*Kerem Oktar,Katherine M. Collins,Jose Hernandez-Orallo,Diane Coyle,Stephen Cave,Adrian Weller,Ilia Sucholutsky*

Main category: cs.AI

TL;DR: 论文探讨了AI作为思维伙伴的风险，提出了RISc框架分析实时、个体和社会层面的风险，并提出了评估指标和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究AI思维伙伴在复杂推理中与人类协作的潜力及其伴随的风险，旨在预防潜在危害并确保人类受益。

Method: 通过RISc框架系统性识别多层面风险（实时、个体、社会），提出评估指标和缓解策略。

Result: 提出了RISc框架和具体风险指标，为开发者和政策制定者提供了缓解策略。

Conclusion: AI思维伙伴的普及需要风险管理策略，以确保其益处最大化并减少潜在危害。

Abstract: Artificial Intelligence (AI) systems have historically been used as tools
that execute narrowly defined tasks. Yet recent advances in AI have unlocked
possibilities for a new class of models that genuinely collaborate with humans
in complex reasoning, from conceptualizing problems to brainstorming solutions.
Such AI thought partners enable novel forms of collaboration and extended
cognition, yet they also pose major risks-including and beyond risks of typical
AI tools and agents. In this commentary, we systematically identify risks of AI
thought partners through a novel framework that identifies risks at multiple
levels of analysis, including Real-time, Individual, and Societal risks arising
from collaborative cognition (RISc). We leverage this framework to propose
concrete metrics for risk evaluation, and finally suggest specific mitigation
strategies for developers and policymakers. As AI thought partners continue to
proliferate, these strategies can help prevent major harms and ensure that
humans actively benefit from productive thought partnerships.

</details>


### [66] [Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning](https://arxiv.org/abs/2505.16928)
*Bosung Kim,Prithviraj Ammanabrolu*

Main category: cs.AI

TL;DR: ∞-THOR是一个新框架，专注于长时程具身任务，通过生成可扩展轨迹、提出新任务和提供数据集，推动具身AI的长上下文理解。


<details>
  <summary>Details</summary>
Motivation: 解决具身AI中长时程理解和推理的挑战，为复杂任务提供可扩展的解决方案。

Method: 提出生成框架、新任务（Needle(s) in the Embodied Haystack）和数据集，结合目标-状态-动作建模和上下文扩展技术。

Result: 实验展示了长时程任务的挑战，并提供了训练策略和模型行为的见解。

Conclusion: ∞-THOR为下一代具身AI系统提供了基础，支持长时程推理和规划。

Abstract: We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks
that advances long-context understanding in embodied AI. $\infty$-THOR
provides: (1) a generation framework for synthesizing scalable, reproducible,
and unlimited long-horizon trajectories; (2) a novel embodied QA task,
Needle(s) in the Embodied Haystack, where multiple scattered clues across
extended trajectories test agents' long-context reasoning ability; and (3) a
long-horizon dataset and benchmark suite featuring complex tasks that span
hundreds of environment steps, each paired with ground-truth action sequences.
To enable this capability, we explore architectural adaptations, including
interleaved Goal-State-Action modeling, context extension techniques, and
Context Parallelism, to equip LLM-based agents for extreme long-context
reasoning and interaction. Experimental results and analyses highlight the
challenges posed by our benchmark and provide insights into training strategies
and model behaviors under long-horizon conditions. Our work provides a
foundation for the next generation of embodied AI systems capable of robust,
long-term reasoning and planning.

</details>


### [67] [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
*NovelSeek Team,Bo Zhang,Shiyang Feng,Xiangchao Yan,Jiakang Yuan,Zhiyin Yu,Xiaohan He,Songtao Huang,Shaowei Hou,Zheng Nie,Zhilong Wang,Jinyao Liu,Runmin Ma,Tianshuo Peng,Peng Ye,Dongzhan Zhou,Shufei Zhang,Xiaosong Wang,Yilan Zhang,Meng Li,Zhongying Tu,Xiangyu Yue,Wangli Ouyang,Bowen Zhou,Lei Bai*

Main category: cs.AI

TL;DR: NovelSeek是一个统一的多智能体闭环框架，用于跨科学领域进行自主研究，具有可扩展性、交互性和高效性。


<details>
  <summary>Details</summary>
Motivation: 加速科学研究范式转变，提升研究效率并推动创新。

Method: 采用多智能体闭环框架NovelSeek，支持跨领域任务、人机交互和自动化流程。

Result: 在多个科学任务中显著提升性能，如反应产率预测从27.6%提升至35.4%，增强子活性预测准确率从0.52提升至0.79。

Conclusion: NovelSeek展示了在科学研究中的高效性和潜力，为复杂问题提供了快速精准的解决方案。

Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.

</details>


### [68] [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
*Yunjia Qi,Hao Peng,Xiaozhi Wang,Amy Xin,Youfeng Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.AI

TL;DR: 论文提出了AgentIF基准，用于系统评估大语言模型（LLM）在代理场景中的指令遵循能力，发现现有模型表现不佳，尤其是在处理复杂约束和工具规范时。


<details>
  <summary>Details</summary>
Motivation: 代理场景中常涉及冗长且复杂的指令，但LLM是否能可靠遵循这些指令尚未充分研究。

Method: 构建AgentIF基准，包含50个真实代理应用的707条人工标注指令，每条指令平均1723字，含11.9个约束。

Result: 现有LLM表现普遍较差，特别是在处理复杂约束结构和工具规范时。

Conclusion: AgentIF为未来研究提供了基准，并揭示了LLM在代理场景中的局限性。

Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.

</details>


### [69] [HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation](https://arxiv.org/abs/2505.16978)
*Weizhi Tang,Yixuan Li,Chris Sypherd,Elizabeth Polgreen,Vaishak Belle*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）在少样本语法生成中的表现，提出了一种混合遗传算法HyGenar以优化语法生成。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多领域表现出色，但其语法生成能力尚未充分探索，尤其是在从少量正负样本中推断语法时。

Method: 引入包含540个语法生成挑战的数据集，设计6个评估指标，测试8种LLMs，并提出HyGenar算法优化语法生成。

Result: 现有LLMs在语法生成中表现不佳，而HyGenar显著提升了生成语法的句法和语义正确性。

Conclusion: HyGenar为LLMs的语法生成能力提供了有效改进方案。

Abstract: Grammar plays a critical role in natural language processing and text/code
generation by enabling the definition of syntax, the creation of parsers, and
guiding structured outputs. Although large language models (LLMs) demonstrate
impressive capabilities across domains, their ability to infer and generate
grammars has not yet been thoroughly explored. In this paper, we aim to study
and improve the ability of LLMs for few-shot grammar generation, where grammars
are inferred from sets of a small number of positive and negative examples and
generated in Backus-Naur Form. To explore this, we introduced a novel dataset
comprising 540 structured grammar generation challenges, devised 6 metrics, and
evaluated 8 various LLMs against it. Our findings reveal that existing LLMs
perform sub-optimally in grammar generation. To address this, we propose an
LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar
generation. HyGenar achieves substantial improvements in both the syntactic and
semantic correctness of generated grammars across LLMs.

</details>


### [70] [Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design](https://arxiv.org/abs/2505.16979)
*Zhenkun Li,Lingyao Li,Shuhang Lin,Yongfeng Zhang*

Main category: cs.AI

TL;DR: KtR框架通过任务分解和针对性增强，提升小模型在多任务中的表现，避免依赖大模型。


<details>
  <summary>Details</summary>
Motivation: 单智能体LLM存在上下文限制、角色过载和领域迁移脆弱性，传统多智能体方法又引入新问题。

Method: KtR将领域先验转化为层次化任务分解，通过控制器协调子任务，结合零样本或轻量增强（如思维链、微调、自检）。

Result: 在背包问题和任务分配问题中，KtR显著提升小模型性能，如GPT-4o-mini在背包问题上从3%提升到95%。

Conclusion: 算法感知的任务分解和针对性增强使小模型成为可靠协作工具，无需依赖大模型。

Abstract: Single-agent LLMs hit hard limits--finite context, role overload, and brittle
domain transfer. Conventional multi-agent fixes soften those edges yet expose
fresh pains: ill-posed decompositions, fuzzy contracts, and verification
overhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a
framework that converts domain priors into an algorithmic blueprint hierarchy,
in which tasks are recursively split into typed, controller-mediated subtasks,
each solved zero-shot or with the lightest viable boost (e.g.,
chain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch
theorem, KtR trades the chase for a universal prompt for disciplined
decomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents
raise accuracy from 3% zero-shot to 95% on size-5 instances after patching a
single bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a
six-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,
versus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation
thus turns modest models into reliable collaborators--no ever-larger monoliths
required.

</details>


### [71] [Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine](https://arxiv.org/abs/2505.16982)
*Adib Bazgir,Amir Habibdoust Lafmajani,Yuwen Zhang*

Main category: cs.AI

TL;DR: 论文提出因果性LLM代理，通过多模态数据和干预推理提升生物医学中的因果理解，需解决框架设计、评估基准、数据整合等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在生物医学中依赖相关性而非因果性，限制了其应用潜力。

Method: 整合多模态数据（文本、图像、基因组等），结合干预推理、知识图谱和因果推断工具。

Result: 可能加速药物发现和实现个性化医疗。

Conclusion: 需跨学科合作，结合因果概念与基础模型，开发可靠的生物医学AI伙伴。

Abstract: Large Language Models (LLMs) show promise in biomedicine but lack true causal
understanding, relying instead on correlations. This paper envisions causal LLM
agents that integrate multimodal data (text, images, genomics, etc.) and
perform intervention-based reasoning to infer cause-and-effect. Addressing this
requires overcoming key challenges: designing safe, controllable agentic
frameworks; developing rigorous benchmarks for causal evaluation; integrating
heterogeneous data sources; and synergistically combining LLMs with structured
knowledge (KGs) and formal causal inference tools. Such agents could unlock
transformative opportunities, including accelerating drug discovery through
automated hypothesis generation and simulation, enabling personalized medicine
through patient-specific causal models. This research agenda aims to foster
interdisciplinary efforts, bridging causal concepts and foundation models to
develop reliable AI partners for biomedical progress.

</details>


### [72] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 论文提出了一种名为BCPG-NSA的细粒度离线强化学习框架，用于从负样本中挖掘有价值的学习信号，提升长链推理模型的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么完全丢弃负样本（RFT），要么对所有标记进行均等惩罚（RL），未能充分利用负样本中的自反思和纠错步骤等有价值信息。

Method: BCPG-NSA框架包含三个阶段：1）样本分割，2）结合LLM和PRM评判器的共识性步骤正确性评估，3）设计NSA策略优化以挖掘负样本中的积极步骤。

Result: 实验表明，BCPG-NSA在多个数学/编程推理基准上优于基线方法，提升了样本效率，并展示了在多轮迭代中的鲁棒性和可扩展性。

Conclusion: BCPG-NSA通过细粒度利用负样本中的学习信号，显著提升了长链推理模型的性能。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [73] [Multilinear subspace learning for person re-identification based fusion of high order tensor features](https://arxiv.org/abs/2505.15825)
*Ammar Chouchane,Mohcene Bessaoudi,Hamza Kheddar,Abdelmalik Ouamane,Tiago Vieira,Mahmoud Hassaballah*

Main category: cs.CV

TL;DR: 论文提出了一种高维特征融合方法（HDFF），结合CNN和LOMO特征，通过张量交叉视图二次分析（TXQDA）提升行人重识别（PRe-ID）的准确性。


<details>
  <summary>Details</summary>
Motivation: 行人重识别是视频监控中的关键任务，现有方法在特征提取和表示上仍有提升空间。

Method: 提出HDFF方法，融合CNN和LOMO特征，使用TXQDA进行多线性子空间学习，并结合余弦相似度匹配。

Result: 在VIPeR、GRID和PRID450S数据集上验证了方法的有效性，性能优于现有先进方法。

Conclusion: HDFF结合TXQDA能有效提升行人重识别的准确性，为高维特征融合提供了新思路。

Abstract: Video surveillance image analysis and processing is a challenging field in
computer vision, with one of its most difficult tasks being Person
Re-Identification (PRe-ID). PRe-ID aims to identify and track target
individuals who have already been detected in a network of cameras, using a
robust description of their pedestrian images. The success of recent research
in person PRe-ID is largely due to effective feature extraction and
representation, as well as the powerful learning of these features to reliably
discriminate between pedestrian images. To this end, two powerful features,
Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are
modeled on multidimensional data using the proposed method, High-Dimensional
Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced
to leverage and combine these two types of features in a single tensor, even
though their dimensions are not identical. To enhance the system's accuracy, we
employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace
learning, followed by cosine similarity for matching. TXQDA efficiently
facilitates learning while reducing the high dimensionality inherent in
high-order tensor data. The effectiveness of our approach is verified through
experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S.
Extensive experiments demonstrate that our approach outperforms recent
state-of-the-art methods.

</details>


### [74] [Generative AI for Autonomous Driving: A Review](https://arxiv.org/abs/2505.15863)
*Katharina Winter,Abhishek Vivekanandan,Rupert Polley,Yinzhe Shen,Christian Schlauch,Mohamed-Khalil Bouzidi,Bojan Derajic,Natalie Grabowsky,Annajoyce Mariani,Dennis Rochau,Giovanni Lucente,Harsh Yadav,Firas Mualla,Adam Molin,Sebastian Bernhard,Christian Wirth,Ömer Şahin Taş,Nadja Klein,Fabian B. Flohr,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 生成式AI（GenAI）在自动驾驶（AD）领域的应用扩展，涵盖静态地图创建、动态场景生成、轨迹预测和车辆运动规划等任务，并比较了多种生成模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI如何提升自动驾驶任务，通过比较不同生成模型的能力和局限性，推动该领域的发展。

Method: 分析了多种生成模型（如VAEs、GANs、INNs、GTs和DMs）及其混合方法在自动驾驶中的应用。

Result: 总结了生成式AI在自动驾驶中的潜力，并提出了改进适应性、鲁棒性的方法。

Conclusion: 讨论了安全性、可解释性和实时性等核心挑战，并为未来研究方向提供了建议。

Abstract: Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving
(AD), extending beyond traditional applications in text, image, and video
generation. We explore how generative models can enhance automotive tasks, such
as static map creation, dynamic scenario generation, trajectory forecasting,
and vehicle motion planning. By examining multiple generative approaches
ranging from Variational Autoencoder (VAEs) over Generative Adversarial
Networks (GANs) and Invertible Neural Networks (INNs) to Generative
Transformers (GTs) and Diffusion Models (DMs), we highlight and compare their
capabilities and limitations for AD-specific applications. Additionally, we
discuss hybrid methods integrating conventional techniques with generative
approaches, and emphasize their improved adaptability and robustness. We also
identify relevant datasets and outline open research questions to guide future
developments in GenAI. Finally, we discuss three core challenges: safety,
interpretability, and realtime capabilities, and present recommendations for
image generation, dynamic scenario generation, and planning.

</details>


### [75] [CP-LLM: Context and Pixel Aware Large Language Model for Video Quality Assessment](https://arxiv.org/abs/2505.16025)
*Wen Wen,Yaohong Wu,Yue Sheng,Neil Birkbeck,Balu Adsumilli,Yilin Wang*

Main category: cs.CV

TL;DR: CP-LLM是一种新型多模态大语言模型，通过双视觉编码器分别分析视频上下文和像素级失真，结合语言解码器实现视频质量评估，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统VQA模型缺乏对视频上下文的理解，而现有LLM模型对小失真不敏感或无法同时处理质量评分和描述。CP-LLM旨在解决这些问题。

Method: CP-LLM采用双视觉编码器分别处理视频上下文和像素失真，结合语言解码器进行联合推理，并通过多任务训练优化评分、描述生成和成对比较。

Result: 实验表明，CP-LLM在VQA基准测试中达到最优性能，对小失真具有更强的鲁棒性。

Conclusion: CP-LLM为视频质量评估提供了一种全面且实用的解决方案。

Abstract: Video quality assessment (VQA) is a challenging research topic with broad
applications. Effective VQA necessitates sensitivity to pixel-level distortions
and a comprehensive understanding of video context to accurately determine the
perceptual impact of distortions. Traditional hand-crafted and learning-based
VQA models mainly focus on pixel-level distortions and lack contextual
understanding, while recent LLM-based models struggle with sensitivity to small
distortions or handle quality scoring and description as separate tasks. To
address these shortcomings, we introduce CP-LLM: a Context and Pixel aware
Large Language Model. CP-LLM is a novel multimodal LLM architecture featuring
dual vision encoders designed to independently analyze perceptual quality at
both high-level (video context) and low-level (pixel distortion) granularity,
along with a language decoder subsequently reasons about the interplay between
these aspects. This design enables CP-LLM to simultaneously produce robust
quality scores and interpretable quality descriptions, with enhanced
sensitivity to pixel distortions (e.g. compression artifacts). The model is
trained via a multi-task pipeline optimizing for score prediction, description
generation, and pairwise comparisons. Experiment results demonstrate that
CP-LLM achieves state-of-the-art cross-dataset performance on established VQA
benchmarks and superior robustness to pixel distortions, confirming its
efficacy for comprehensive and practical video quality assessment in real-world
scenarios.

</details>


### [76] [How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads](https://arxiv.org/abs/2505.15865)
*Ingeol Baek,Hwan Chang,Sunghyun Ryu,Hwanhee Lee*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（LVLMs）中负责识别图像中文本的特定头部（OCR Head），发现其具有低稀疏性、性质独特和静态激活的特点，并通过实验验证了这些发现。


<details>
  <summary>Details</summary>
Motivation: 探索LVLMs的文本识别机制，填补其在可解释性和文本定位方面的研究空白。

Method: 分析多种LVLMs，识别OCR Head，并通过CoT和掩码实验验证其特性。

Result: OCR Head具有低稀疏性、性质独特和静态激活的特点，调整其sink-token值可提升性能。

Conclusion: 研究揭示了LVLMs处理图像中文本的内部机制，为模型优化提供了新方向。

Abstract: Despite significant advancements in Large Vision Language Models (LVLMs), a
gap remains, particularly regarding their interpretability and how they locate
and interpret textual information within images. In this paper, we explore
various LVLMs to identify the specific heads responsible for recognizing text
from images, which we term the Optical Character Recognition Head (OCR Head).
Our findings regarding these heads are as follows: (1) Less Sparse: Unlike
previous retrieval heads, a large number of heads are activated to extract
textual information from images. (2) Qualitatively Distinct: OCR heads possess
properties that differ significantly from general retrieval heads, exhibiting
low similarity in their characteristics. (3) Statically Activated: The
frequency of activation for these heads closely aligns with their OCR scores.
We validate our findings in downstream tasks by applying Chain-of-Thought (CoT)
to both OCR and conventional retrieval heads and by masking these heads. We
also demonstrate that redistributing sink-token values within the OCR heads
improves performance. These insights provide a deeper understanding of the
internal mechanisms LVLMs employ in processing embedded textual information in
images.

</details>


### [77] [SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval](https://arxiv.org/abs/2505.15867)
*Nikolaos Chaidos,Angeliki Dimitriou,Maria Lymperaiou,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出了一种基于场景图的无监督检索框架SCENIR，强调语义内容而非低层视觉特征，解决了现有方法对标注数据的依赖和文本编码不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有卷积和Transformer架构在图像检索中易受低层视觉特征（如颜色）的偏见影响，且缺乏语义理解。此外，基于监督GNN的方法依赖标注数据，而文本编码的不一致性影响了检索可靠性。

Method: 提出SCENIR，一种基于图自编码器的无监督检索框架，无需标注数据。采用图编辑距离（GED）作为场景图相似性的确定性度量。

Result: SCENIR在性能和运行时效率上优于现有视觉、多模态和监督GNN方法，并在未标注数据集上验证了泛化能力。

Conclusion: SCENIR通过无监督学习和GED度量，显著提升了图像检索的语义理解和可靠性，推动了反事实图像检索的进展。

Abstract: Despite the dominance of convolutional and transformer-based architectures in
image-to-image retrieval, these models are prone to biases arising from
low-level visual features, such as color. Recognizing the lack of semantic
understanding as a key limitation, we propose a novel scene graph-based
retrieval framework that emphasizes semantic content over superficial image
characteristics. Prior approaches to scene graph retrieval predominantly rely
on supervised Graph Neural Networks (GNNs), which require ground truth graph
pairs driven from image captions. However, the inconsistency of caption-based
supervision stemming from variable text encodings undermine retrieval
reliability. To address these, we present SCENIR, a Graph Autoencoder-based
unsupervised retrieval framework, which eliminates the dependence on labeled
training data. Our model demonstrates superior performance across metrics and
runtime efficiency, outperforming existing vision-based, multimodal, and
supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as
a deterministic and robust ground truth measure for scene graph similarity,
replacing the inconsistent caption-based alternatives for the first time in
image-to-image retrieval evaluation. Finally, we validate the generalizability
of our method by applying it to unannotated datasets via automated scene graph
generation, while substantially contributing in advancing state-of-the-art in
counterfactual image retrieval.

</details>


### [78] [DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor](https://arxiv.org/abs/2505.16256)
*Yan Zhao,Zhengxue Cheng,Junxuan Zhang,Qunshan Gu,Qi Wang,Li Song*

Main category: cs.CV

TL;DR: DualComp是一种轻量级、统一的双模态无损压缩器，针对图像和文本设计，通过结构增强和参数共享实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有学习型压缩器多为单模态设计，缺乏灵活性和多模态适应性，而多模态大模型又过于复杂。

Method: 采用轻量级主干网络，结合模态统一分词、模态切换上下文学习和模态路由专家混合三种结构增强，以及重参数化训练策略。

Result: DualComp在桌面CPU上实现近实时推理（200KB/s），压缩性能与SOTA方法相当，单模态变体在Kodak数据集上超越之前最佳图像压缩器9%。

Conclusion: DualComp通过统一设计和轻量化实现了高效的多模态无损压缩，为实际部署提供了可行方案。

Abstract: Most learning-based lossless compressors are designed for a single modality,
requiring separate models for multi-modal data and lacking flexibility.
However, different modalities vary significantly in format and statistical
properties, making it ineffective to use compressors that lack
modality-specific adaptations. While multi-modal large language models (MLLMs)
offer a potential solution for modality-unified compression, their excessive
complexity hinders practical deployment. To address these challenges, we focus
on the two most common modalities, image and text, and propose DualComp, the
first unified and lightweight learning-based dual-modality lossless compressor.
Built on a lightweight backbone, DualComp incorporates three key structural
enhancements to handle modality heterogeneity: modality-unified tokenization,
modality-switching contextual learning, and modality-routing
mixture-of-experts. A reparameterization training strategy is also used to
boost compression performance. DualComp integrates both modality-specific and
shared parameters for efficient parameter utilization, enabling near real-time
inference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp
achieves compression performance on par with the SOTA LLM-based methods for
both text and image datasets. Its simplified single-modality variant surpasses
the previous best image compressor on the Kodak dataset by about 9% using just
1.2% of the model size.

</details>


### [79] [Satellites Reveal Mobility: A Commuting Origin-destination Flow Generator for Global Cities](https://arxiv.org/abs/2505.15870)
*Can Rong,Xin Zhang,Yanxin Xi,Hongjie Sui,Jingtao Ding,Yong Li*

Main category: cs.CV

TL;DR: 论文提出GlODGen，利用卫星图像生成全球城市通勤OD流数据，替代传统高成本数据收集方法。


<details>
  <summary>Details</summary>
Motivation: 通勤OD流数据对城市可持续发展至关重要，但传统数据收集成本高且涉及隐私问题。卫星图像提供了一种低成本、公开的替代方案。

Method: GlODGen结合视觉语言地理基础模型从卫星图像提取城市语义信号，与人口数据结合生成区域表示，再通过图扩散模型生成OD流。

Result: 在四大洲六个城市的实验中，GlODGen生成的OD流数据与真实数据高度一致，表现出强泛化能力。

Conclusion: GlODGen是一种自动化工具，可高效生成全球城市的OD流数据，为城市规划和可持续发展提供支持。

Abstract: Commuting Origin-destination~(OD) flows, capturing daily population mobility
of citizens, are vital for sustainable development across cities around the
world. However, it is challenging to obtain the data due to the high cost of
travel surveys and privacy concerns. Surprisingly, we find that satellite
imagery, publicly available across the globe, contains rich urban semantic
signals to support high-quality OD flow generation, with over 98\%
expressiveness of traditional multisource hard-to-collect urban
sociodemographic, economics, land use, and point of interest data. This
inspires us to design a novel data generator, GlODGen, which can generate OD
flow data for any cities of interest around the world. Specifically, GlODGen
first leverages Vision-Language Geo-Foundation Models to extract urban semantic
signals related to human mobility from satellite imagery. These features are
then combined with population data to form region-level representations, which
are used to generate OD flows via graph diffusion models. Extensive experiments
on 4 continents and 6 representative cities show that GlODGen has great
generalizability across diverse urban environments on different continents and
can generate OD flow data for global cities highly consistent with real-world
mobility data. We implement GlODGen as an automated tool, seamlessly
integrating data acquisition and curation, urban semantic feature extraction,
and OD flow generation together. It has been released at
https://github.com/tsinghua-fib-lab/generate-od-pubtools.

</details>


### [80] [Joint Flow And Feature Refinement Using Attention For Video Restoration](https://arxiv.org/abs/2505.16434)
*Ranjith Merugu,Mohammad Sameer Suhail,Akshay P Sarashetti,Venkata Bharath Reddy Reddem,Pankaj Kumar Bajpai,Amit Satish Unde*

Main category: cs.CV

TL;DR: 提出了一种名为JFFRA的视频修复框架，通过联合优化光流和特征修复，显著提升了视频修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法在利用低质量输入帧时难以保持时间一致性，需要更高效的时空信息利用方法。

Method: JFFRA通过迭代优化光流和特征修复的协同作用，结合多尺度处理和遮挡感知损失函数，提升修复效果。

Result: 在去噪、去模糊和超分辨率等任务中，JFFRA性能提升高达1.62 dB。

Conclusion: JFFRA通过联合优化光流和特征修复，显著提升了视频修复的性能和时间一致性。

Abstract: Recent advancements in video restoration have focused on recovering
high-quality video frames from low-quality inputs. Compared with static images,
the performance of video restoration significantly depends on efficient
exploitation of temporal correlations among successive video frames. The
numerous techniques make use of temporal information via flow-based strategies
or recurrent architectures. However, these methods often encounter difficulties
in preserving temporal consistency as they utilize degraded input video frames.
To resolve this issue, we propose a novel video restoration framework named
Joint Flow and Feature Refinement using Attention (JFFRA). The proposed JFFRA
is based on key philosophy of iteratively enhancing data through the
synergistic collaboration of flow (alignment) and restoration. By leveraging
previously enhanced features to refine flow and vice versa, JFFRA enables
efficient feature enhancement using temporal information. This interplay
between flow and restoration is executed at multiple scales, reducing the
dependence on precise flow estimation. Moreover, we incorporate an
occlusion-aware temporal loss function to enhance the network's capability in
eliminating flickering artifacts. Comprehensive experiments validate the
versatility of JFFRA across various restoration tasks such as denoising,
deblurring, and super-resolution. Our method demonstrates a remarkable
performance improvement of up to 1.62 dB compared to state-of-the-art
approaches.

</details>


### [81] [Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging](https://arxiv.org/abs/2505.15875)
*Shenghe Zheng,Hongzhi Wang,Chenyu Huang,Xiaohui Wang,Tao Chen,Jiayuan Fan,Shuyue Hu,Peng Ye*

Main category: cs.CV

TL;DR: 论文提出了一种名为DO-Merging的解耦正交合并方法，用于解决LoRA模块合并时参数幅度差异大导致性能下降的问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并方法主要针对全微调模型，忽略了流行的LoRA模块。研究发现，现有方法在LoRA上表现不佳，且参数幅度差异大导致合并性能下降。

Method: 提出DO-Merging方法，将参数分解为幅度和方向分量并独立合并，同时引入无数据、分层梯度下降和正交约束以减少干扰。

Result: 实验证明DO-Merging在视觉、语言和多模态任务中显著优于现有方法，且各组件可灵活集成现有方法。

Conclusion: DO-Merging通过解耦和正交合并有效解决了LoRA合并问题，性能提升显著且成本低。

Abstract: With more open-source models available for diverse tasks, model merging has
gained attention by combining models into one, reducing training, storage, and
inference costs. Current research mainly focuses on model merging for full
fine-tuning, overlooking the popular LoRA. However, our empirical analysis
reveals that: a) existing merging methods designed for full fine-tuning perform
poorly on LoRA; b) LoRA modules show much larger parameter magnitude variance
than full fine-tuned weights; c) greater parameter magnitude variance
correlates with worse merging performance. Considering that large magnitude
variances cause deviations in the distribution of the merged parameters,
resulting in information loss and performance degradation, we propose a
Decoupled and Orthogonal merging approach(DO-Merging). By separating parameters
into magnitude and direction components and merging them independently, we
reduce the impact of magnitude differences on the directional alignment of the
merged models, thereby preserving task information. Furthermore, we introduce a
data-free, layer-wise gradient descent method with orthogonal constraints to
mitigate interference during the merging of direction components. We provide
theoretical guarantees for both the decoupling and orthogonal components. And
we validate through extensive experiments across vision, language, and
multi-modal domains that our proposed DO-Merging can achieve significantly
higher performance than existing merging methods at a minimal cost. Notably,
each component can be flexibly integrated with existing methods, offering near
free-lunch improvements across tasks.

</details>


### [82] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
*Siting Li,Xiang Gao,Simon Shaolei Du*

Main category: cs.CV

TL;DR: 论文提出COCO-Facet基准，评估文本到图像检索器在属性聚焦查询上的表现，发现现有方法（如CLIP）表现不佳，并提出基于提示的图像嵌入方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索器（如CLIP）在处理属性聚焦查询时表现不佳，因其图像嵌入主要关注全局语义而忽略细节。

Method: 提出基于提示的图像嵌入方法，通过多模态检索器生成可提示的嵌入，并优化加速策略（预处理和线性近似）。

Result: 新方法显著提升性能，预处理策略在Recall@5上提升15%，线性近似策略提升8%。

Conclusion: 通用图像嵌入不适合属性聚焦查询，可提示嵌入方法能有效解决这一问题，并具有实际应用潜力。

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [83] [CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation](https://arxiv.org/abs/2505.16663)
*Haihong Hao,Mingfei Han,Changlin Li,Zhihui Li,Xiaojun Chang*

Main category: cs.CV

TL;DR: CoNav是一个协作跨模态推理框架，通过3D-text模型指导图像-文本导航代理，解决多模态数据融合的挑战，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有的2D图像、3D点云和文本指令的多模态融合方法面临数据稀缺和模态冲突问题，需要一种更有效的跨模态协作方法。

Method: 提出CoNav框架，通过Cross-Modal Belief Alignment共享3D-text模型的空间语义知识，轻量级微调导航代理以整合多模态信息。

Result: 在四个导航基准和两个空间推理基准上表现优异，路径更短且成功率更高。

Conclusion: CoNav展示了多模态数据融合在导航中的潜力，但仍需解决模态融合的挑战。

Abstract: Embodied navigation demands comprehensive scene understanding and precise
spatial reasoning. While image-text models excel at interpreting pixel-level
color and lighting cues, 3D-text models capture volumetric structure and
spatial relationships. However, unified fusion approaches that jointly fuse 2D
images, 3D point clouds, and textual instructions face challenges in limited
availability of triple-modality data and difficulty resolving conflicting
beliefs among modalities. In this work, we introduce CoNav, a collaborative
cross-modal reasoning framework where a pretrained 3D-text model explicitly
guides an image-text navigation agent by providing structured spatial-semantic
knowledge to resolve ambiguities during navigation. Specifically, we introduce
Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance
by simply sharing textual hypotheses from the 3D-text model to the navigation
agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the
navigation agent learns to integrate visual cues with spatial-semantic
knowledge derived from the 3D-text model, enabling effective reasoning in
embodied navigation. CoNav achieves significant improvements on four standard
embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial
reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success
Rate, CoNav often generates shorter paths compared to other methods (as
measured by SPL), showcasing the potential and challenges of fusing data from
different modalities in embodied navigation. Project Page:
https://oceanhao.github.io/CoNav/

</details>


### [84] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
*Yue Fan,Xuehai He,Diji Yang,Kaizhi Zheng,Ching-Chen Kuo,Yuting Zheng,Sravana Jyothi Narayanaraju,Xinze Guan,Xin Eric Wang*

Main category: cs.CV

TL;DR: GRIT提出了一种结合自然语言和视觉信息的推理方法，通过强化学习训练多模态语言模型生成视觉接地的推理链。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理模型仅用自然语言生成推理内容，缺乏视觉信息的显式整合，限制了其推理能力。

Method: GRIT引入了一种接地的推理范式，模型生成交替自然语言和边界框坐标的推理链，并结合GRPO-GR强化学习算法。

Result: GRIT仅需少量数据即可训练模型生成连贯且视觉接地的推理链，实现了推理与接地能力的统一。

Conclusion: GRIT在多模态语言模型中成功结合了视觉信息和推理能力，显著提升了视觉推理任务的表现。

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [85] [Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2505.16756)
*Hailong Ning,Siying Wang,Tao Lei,Xiaopeng Cao,Huanmin Dou,Bin Zhao,Asoke K. Nandi,Petia Radeva*

Main category: cs.CV

TL;DR: 该论文提出了一种用于遥感图像-文本检索（RSITR）的表示差异桥接（RDB）方法，通过跨模态不对称适配器（CMAA）和双任务一致性损失（DTCL）解决模态不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像-文本检索在多个领域有重要应用，但现有方法中文本模态的强判别性会抑制图像表示学习，导致跨模态优化不平衡。

Method: 设计了跨模态不对称适配器（CMAA），包括视觉增强适配器（VEA）和文本语义适配器（TSA），并通过双任务一致性损失（DTCL）优化跨模态对齐。

Result: 在RSICD和RSITMD数据集上，RDB方法比现有PEFT方法在mR指标上提升了6%-11%，比全微调的GeoRSCLIP模型提升了1.15%-2%。

Conclusion: RDB方法有效解决了跨模态优化不平衡问题，显著提升了遥感图像-文本检索的性能。

Abstract: Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in
geographic information interpretation, disaster monitoring, and urban planning
by establishing semantic associations between image and textual descriptions.
Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language
Pre-training (VLP) models typically adopt symmetric adapter structures for
exploring cross-modal correlations. However, the strong discriminative nature
of text modality may dominate the optimization process and inhibits image
representation learning. The nonnegligible imbalanced cross-modal optimization
remains a bottleneck to enhancing the model performance. To address this issue,
this study proposes a Representation Discrepancy Bridging (RDB) method for the
RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is
designed to enable modality-specific optimization and improve feature
alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text
Semantic Adapter (TSA). VEA mines fine-grained image features by Differential
Attention (DA) mechanism, while TSA identifies key textual semantics through
Hierarchical Attention (HA) mechanism. On the other hand, this study extends
the traditional single-task retrieval framework to a dual-task optimization
framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves
cross-modal alignment robustness through an adaptive weighted combination of
cross-modal, classification, and exponential moving average consistency
constraints. Experiments on RSICD and RSITMD datasets show that the proposed
RDB method achieves a 6%-11% improvement in mR metrics compared to
state-of-the-art PEFT methods and a 1.15%-2% improvement over the full
fine-tuned GeoRSCLIP model.

</details>


### [86] [Challenger: Affordable Adversarial Driving Video Generation](https://arxiv.org/abs/2505.15880)
*Zhiyuan Xu,Bohan Li,Huan-ang Gao,Mingju Gao,Yong Chen,Ming Liu,Chenxu Yan,Hang Zhao,Shuo Feng,Hao Zhao*

Main category: cs.CV

TL;DR: Challenger框架生成逼真的对抗性驾驶视频，通过物理感知的轨迹优化和评分函数，显著提升自动驾驶模型的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要关注普通驾驶场景，缺乏逼真的对抗性场景数据以测试自动驾驶系统。

Method: 采用多轮轨迹优化和评分函数，生成物理合理且逼真的对抗性驾驶视频。

Result: 在nuScenes数据集上生成多样对抗场景，显著提高多个自动驾驶模型的碰撞率。

Conclusion: Challenger能有效生成逼真对抗视频，且对抗行为在不同模型间具有可迁移性。

Abstract: Generating photorealistic driving videos has seen significant progress
recently, but current methods largely focus on ordinary, non-adversarial
scenarios. Meanwhile, efforts to generate adversarial driving scenarios often
operate on abstract trajectory or BEV representations, falling short of
delivering realistic sensor data that can truly stress-test autonomous driving
(AD) systems. In this work, we introduce Challenger, a framework that produces
physically plausible yet photorealistic adversarial driving videos. Generating
such videos poses a fundamental challenge: it requires jointly optimizing over
the space of traffic interactions and high-fidelity sensor observations.
Challenger makes this affordable through two techniques: (1) a physics-aware
multi-round trajectory refinement process that narrows down candidate
adversarial maneuvers, and (2) a tailored trajectory scoring function that
encourages realistic yet adversarial behavior while maintaining compatibility
with downstream video synthesis. As tested on the nuScenes dataset, Challenger
generates a diverse range of aggressive driving scenarios-including cut-ins,
sudden lane changes, tailgating, and blind spot intrusions-and renders them
into multiview photorealistic videos. Extensive evaluations show that these
scenarios significantly increase the collision rate of state-of-the-art
end-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and
importantly, adversarial behaviors discovered for one model often transfer to
others.

</details>


### [87] [Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2505.16809)
*Junze Wang,Lei Fan,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 提出ReHyDIL方法，通过域增量学习和超图网络解决多模态MRI分割中缺失模态的问题，提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 临床实践中MRI模态可能缺失，导致性能下降，而重新训练模型效率低且易过拟合。

Method: 结合域增量学习（DIL）和超图网络（CHSNet），引入Tversky感知对比损失（TAC）处理信息不平衡。

Result: 在BraTS2019数据集上，Dice相似系数提升超过2%。

Conclusion: ReHyDIL有效解决了模态缺失问题，性能优于现有方法。

Abstract: Existing methods for multimodal MRI segmentation with missing modalities
typically assume that all MRI modalities are available during training.
However, in clinical practice, some modalities may be missing due to the
sequential nature of MRI acquisition, leading to performance degradation.
Furthermore, retraining models to accommodate newly available modalities can be
inefficient and may cause overfitting, potentially compromising previously
learned knowledge. To address these challenges, we propose Replay-based
Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation
with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to
enable the segmentation model to learn from newly acquired MRI modalities
without forgetting previously learned information. To enhance segmentation
performance across diverse patient scenarios, we introduce the Cross-Patient
Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture
high-order associations between patients. Additionally, we incorporate
Tversky-Aware Contrastive (TAC) loss to effectively mitigate information
imbalance both across and within different modalities. Extensive experiments on
the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art
methods, achieving an improvement of over 2\% in the Dice Similarity
Coefficient across various tumor regions. Our code is available at ReHyDIL.

</details>


### [88] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
*Tony Montes,Fernando Lozano*

Main category: cs.CV

TL;DR: 该论文提出了一种结合Chain-of-Thought框架和YOLO-World的LLM-brained代理，用于零样本视频问答（VideoQA），在多个基准测试中实现了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前VideoQA系统在对象跟踪和基于推理的决策方面仍有改进空间，尤其是在对象引用与语言模型输出的对齐上。

Method: 采用LLM-brained代理，结合Chain-of-Thought框架和YOLO-World，增强对象跟踪和对齐能力。

Result: 在NExT-QA、iVQA和ActivityNet-QA基准测试中表现优异，实现了新的最佳性能。

Conclusion: 该框架不仅提升了VideoQA性能，还支持跨时间段的验证，提高了输出可靠性。

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [89] [Creatively Upscaling Images with Global-Regional Priors](https://arxiv.org/abs/2505.16976)
*Yurui Qian,Qi Cai,Yingwei Pan,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: C-Upscale是一种无需调整的图像放大方法，通过全局-区域先验生成高分辨率图像，解决了现有模型在保持全局语义结构和生成区域细节上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在高分辨率图像生成中难以同时保持全局语义结构和生成创造性区域细节。

Method: 利用全局提示和通过多模态LLM估计的区域提示，结合低频分量和区域注意力控制，生成高分辨率图像。

Result: C-Upscale能生成超高分辨率图像（如4,096 X 4,096和8,192 X 8,192），具有更高的视觉保真度和创造性区域细节。

Conclusion: C-Upscale通过全局-区域先验，显著提升了高分辨率图像生成的质量和细节表现。

Abstract: Contemporary diffusion models show remarkable capability in text-to-image
generation, while still being limited to restricted resolutions (e.g., 1,024 X
1,024). Recent advances enable tuning-free higher-resolution image generation
by recycling pre-trained diffusion models and extending them via regional
denoising or dilated sampling/convolutions. However, these models struggle to
simultaneously preserve global semantic structure and produce creative regional
details in higher-resolution images. To address this, we present C-Upscale, a
new recipe of tuning-free image upscaling that pivots on global-regional priors
derived from given global prompt and estimated regional prompts via Multimodal
LLM. Technically, the low-frequency component of low-resolution image is
recognized as global structure prior to encourage global semantic consistency
in high-resolution generation. Next, we perform regional attention control to
screen cross-attention between global prompt and each region during regional
denoising, leading to regional attention prior that alleviates object
repetition issue. The estimated regional prompts containing rich descriptive
details further act as regional semantic prior to fuel the creativity of
regional detail generation. Both quantitative and qualitative evaluations
demonstrate that our C-Upscale manages to generate ultra-high-resolution images
(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more
creative regional details.

</details>


### [90] [VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance](https://arxiv.org/abs/2505.15952)
*Mohammad Reza Taesiri,Abhijay Ghildyal,Saman Zadtootaghaj,Nabajeet Barman,Cor-Paul Bezemer*

Main category: cs.CV

TL;DR: 论文介绍了VideoGameQA-Bench，一个用于评估视觉语言模型在游戏QA任务中性能的标准化基准。


<details>
  <summary>Details</summary>
Motivation: 游戏行业收入高，但QA流程自动化不足，现有基准无法满足需求，需要新工具提升效率。

Method: 开发了VideoGameQA-Bench，涵盖多种游戏QA任务，如图像和视频的视觉单元测试、回归测试、故障检测等。

Result: 提供了标准化基准，支持对VLMs在游戏QA任务中的性能评估。

Conclusion: VideoGameQA-Bench填补了游戏QA领域标准化工具的空白，有助于推动自动化发展。

Abstract: With video games now generating the highest revenues in the entertainment
industry, optimizing game development workflows has become essential for the
sector's sustained growth. Recent advancements in Vision-Language Models (VLMs)
offer considerable potential to automate and enhance various aspects of game
development, particularly Quality Assurance (QA), which remains one of the
industry's most labor-intensive processes with limited automation options. To
accurately evaluate the performance of VLMs in video game QA tasks and
determine their effectiveness in handling real-world scenarios, there is a
clear need for standardized benchmarks, as existing benchmarks are insufficient
to address the specific requirements of this domain. To bridge this gap, we
introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array
of game QA activities, including visual unit testing, visual regression
testing, needle-in-a-haystack tasks, glitch detection, and bug report
generation for both images and videos of various games. Code and data are
available at: https://asgaardlab.github.io/videogameqa-bench/

</details>


### [91] [Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On](https://arxiv.org/abs/2505.16977)
*Siqi Wan,Jingwen Chen,Yingwei Pan,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉对应性的新方法，通过语义点匹配和3D感知线索来改进扩散模型在虚拟试衣（VTON）任务中的细节保留能力。


<details>
  <summary>Details</summary>
Motivation: 由于扩散模型的固有随机性，现有的双分支UNet架构在虚拟试衣任务中难以完全保留服装的形状和细节。

Method: 通过将服装的细节解释为结构化语义点，并利用局部流变形匹配目标人物的语义点，结合3D感知线索（深度/法线图）监督扩散模型训练。

Result: 实验表明，该方法在VITON-HD和DressCode数据集上实现了最先进的虚拟试衣性能，显著提升了服装细节的保留能力。

Conclusion: 通过语义点匹配和3D感知线索，该方法有效解决了扩散模型在虚拟试衣任务中的细节保留问题，为未来研究提供了新思路。

Abstract: Diffusion models have shown preliminary success in virtual try-on (VTON)
task. The typical dual-branch architecture comprises two UNets for implicit
garment deformation and synthesized image generation respectively, and has
emerged as the recipe for VTON task. Nevertheless, the problem remains
challenging to preserve the shape and every detail of the given garment due to
the intrinsic stochasticity of diffusion model. To alleviate this issue, we
novelly propose to explicitly capitalize on visual correspondence as the prior
to tame diffusion process instead of simply feeding the whole garment into UNet
as the appearance reference. Specifically, we interpret the fine-grained
appearance and texture details as a set of structured semantic points, and
match the semantic points rooted in garment to the ones over target person
through local flow warping. Such 2D points are then augmented into 3D-aware
cues with depth/normal map of target person. The correspondence mimics the way
of putting clothing on human body and the 3D-aware cues act as semantic point
matching to supervise diffusion model training. A point-focused diffusion loss
is further devised to fully take the advantage of semantic point matching.
Extensive experiments demonstrate strong garment detail preservation of our
approach, evidenced by state-of-the-art VTON performances on both VITON-HD and
DressCode datasets. Code is publicly available at:
https://github.com/HiDream-ai/SPM-Diff.

</details>


### [92] [Super-Resolution with Structured Motion](https://arxiv.org/abs/2505.15961)
*Gabby Litterio,Juan-David Lizarazo-Ferro,Pedro Felzenszwalb,Rashid Zia*

Main category: cs.CV

TL;DR: 论文探讨了利用成像约束实现超分辨率的极限，提出通过高精度运动信息、稀疏图像先验和凸优化，可实现大幅分辨率提升，并证明运动模糊对超分辨率有帮助。


<details>
  <summary>Details</summary>
Motivation: 传统重建方法因理论和实践限制，分辨率提升幅度有限，且运动模糊通常被视为阻碍。研究旨在突破这些限制。

Method: 结合高精度运动信息、稀疏图像先验和凸优化，利用伪随机运动实现单幅低分辨率图像的高分辨率重建。

Result: 通过数值模拟和真实数据实验，验证了方法的有效性，实现了大幅分辨率提升。

Conclusion: 研究表明，运动模糊和稀疏信号处理可显著提升超分辨率效果，为实际应用提供了新思路。

Abstract: We consider the limits of super-resolution using imaging constraints. Due to
various theoretical and practical limitations, reconstruction-based methods
have been largely restricted to small increases in resolution. In addition,
motion-blur is usually seen as a nuisance that impedes super-resolution. We
show that by using high-precision motion information, sparse image priors, and
convex optimization, it is possible to increase resolution by large factors. A
key operation in super-resolution is deconvolution with a box. In general,
convolution with a box is not invertible. However, we obtain perfect
reconstructions of sparse signals using convex optimization. We also show that
motion blur can be helpful for super-resolution. We demonstrate that using
pseudo-random motion it is possible to reconstruct a high-resolution target
using a single low-resolution image. We present numerical experiments with
simulated data and results with real data captured by a camera mounted on a
computer controlled stage.

</details>


### [93] [Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction](https://arxiv.org/abs/2505.16980)
*Dong Li,Wenqi Zhong,Wei Yu,Yingwei Pan,Dingwen Zhang,Ting Yao,Junwei Han,Tao Mei*

Main category: cs.CV

TL;DR: 论文提出了一种名为DPIDM的动态姿态交互扩散模型，用于视频虚拟试穿，通过结合时空姿态交互和注意力机制提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚拟试穿方法常忽略时空姿态交互，导致时间不一致性，DPIDM旨在解决这一问题。

Method: DPIDM利用扩散模型，引入骨架姿态适配器和分层注意力模块，建模帧内和跨帧的姿态交互。

Result: 在多个数据集上表现优异，VFID分数提升60.5%。

Conclusion: DPIDM通过动态姿态交互显著提升了视频虚拟试穿的效果和一致性。

Abstract: Video virtual try-on aims to seamlessly dress a subject in a video with a
specific garment. The primary challenge involves preserving the visual
authenticity of the garment while dynamically adapting to the pose and physique
of the subject. While existing methods have predominantly focused on
image-based virtual try-on, extending these techniques directly to videos often
results in temporal inconsistencies. Most current video virtual try-on
approaches alleviate this challenge by incorporating temporal modules, yet
still overlook the critical spatiotemporal pose interactions between human and
garment. Effective pose interactions in videos should not only consider spatial
alignment between human and garment poses in each frame but also account for
the temporal dynamics of human poses throughout the entire video. With such
motivation, we propose a new framework, namely Dynamic Pose Interaction
Diffusion Models (DPIDM), to leverage diffusion models to delve into dynamic
pose interactions for video virtual try-on. Technically, DPIDM introduces a
skeleton-based pose adapter to integrate synchronized human and garment poses
into the denoising network. A hierarchical attention module is then exquisitely
designed to model intra-frame human-garment pose interactions and long-term
human pose dynamics across frames through pose-aware spatial and temporal
attention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized
attention loss between consecutive frames to enhance temporal consistency.
Extensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate
the superiority of our DPIDM against the baseline methods. Notably, DPIDM
achieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over
the state-of-the-art GPD-VVTO approach.

</details>


### [94] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
*Shujun Liu,Siyuan Wang,Zejun Li,Jianxiang Wang,Cheng Zeng,Zhongyu Wei*

Main category: cs.CV

TL;DR: 论文提出了一种在线视觉语言偏好学习（OViP）框架，通过动态构建基于模型自身幻觉输出的对比训练数据，减少大视觉语言模型（LVLMs）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）容易产生与视觉输入不符的幻觉内容，现有方法依赖预定义或随机编辑的负样本，无法反映实际模型错误，限制了训练效果。

Method: 提出OViP框架，动态构建对比训练数据，通过识别响应对的语义差异并利用扩散模型合成负样本图像，实时生成相关监督信号。

Result: 实验表明，OViP在幻觉和通用基准测试中有效减少幻觉，同时保留核心多模态能力。

Conclusion: OViP通过失败驱动的训练和优化的评估协议，实现了对幻觉问题的自适应对齐，提升了模型性能。

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [95] [Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework](https://arxiv.org/abs/2505.17019)
*Chenhao Zhang,Yazhe Niu*

Main category: cs.CV

TL;DR: 论文提出Let Androids Dream (LAD)框架，通过三阶段方法解决图像隐喻理解问题，性能优于现有MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在图像隐喻理解上存在文化、情感和上下文理解的不足，LAD旨在填补这一空白。

Method: LAD采用三阶段框架：感知（视觉信息转文本）、搜索（跨领域知识整合）、推理（生成上下文对齐的隐喻理解）。

Result: LAD在英文和中文图像隐喻基准测试中表现优异，性能接近GPT-4o，并在开放式问题上显著超越。

Conclusion: LAD为AI图像隐喻理解提供了新思路，推动了视觉-语言推理和人机交互领域的发展。

Abstract: Metaphorical comprehension in images remains a critical challenge for AI
systems, as existing models struggle to grasp the nuanced cultural, emotional,
and contextual implications embedded in visual content. While multimodal large
language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they
struggle with a fundamental limitation on image implication tasks: contextual
gaps that obscure the relationships between different visual elements and their
abstract meanings. Inspired by the human cognitive process, we propose Let
Androids Dream (LAD), a novel framework for image implication understanding and
reasoning. LAD addresses contextual missing through the three-stage framework:
(1) Perception: converting visual information into rich and multi-level textual
representations, (2) Search: iteratively searching and integrating cross-domain
knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment
image implication via explicit reasoning. Our framework with the lightweight
GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English
image implication benchmark and a huge improvement on Chinese benchmark,
performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ)
and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work
provides new insights into how AI can more effectively interpret image
implications, advancing the field of vision-language reasoning and human-AI
interaction. Our project is publicly available at
https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.

</details>


### [96] [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
*Chengqi Duan,Rongyao Fang,Yuqing Wang,Kun Wang,Linjiang Huang,Xingyu Zeng,Hongsheng Li,Xihui Liu*

Main category: cs.CV

TL;DR: GoT-R1是一个通过强化学习增强语义-空间推理的视觉生成框架，显著提升了复杂文本提示下的图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉生成模型在复杂提示（多对象、精确空间关系和属性）下的表现不足问题。

Method: 采用强化学习框架GoT-R1，结合双阶段多维奖励机制，利用MLLMs评估推理过程和生成结果。

Result: 在T2I-CompBench基准测试中表现优异，尤其在涉及精确空间关系和属性绑定的任务中。

Conclusion: GoT-R1通过将复杂推理能力引入视觉生成领域，推动了图像生成技术的发展。

Abstract: Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.

</details>


### [97] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
*Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 论文提出了一种在像素空间进行推理的新框架，通过视觉推理操作（如放大和选择帧）增强视觉语言模型（VLM）的能力，并通过两阶段训练方法解决模型初始不平衡和操作适应问题，显著提升了视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维推理仅限于文本空间，限制了视觉密集型任务的效果，因此需要扩展推理能力到像素空间。

Method: 采用两阶段训练方法：指令调整阶段使用合成推理轨迹熟悉视觉操作，强化学习阶段通过好奇心驱动奖励平衡像素空间和文本推理。

Result: 7B模型在多个视觉推理基准测试中表现优异，如V* bench（84%）、TallyQA-Complex（74%）和InfographicsVQA（84%），达到开源模型最高准确率。

Conclusion: 像素空间推理对提升VLM性能至关重要，提出的框架有效解决了视觉推理的挑战。

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [98] [Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders](https://arxiv.org/abs/2505.15970)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 该论文利用稀疏自编码器（SAEs）分析视觉模型如何编码ImageNet层次结构，发现模型激活中存在隐含的层次关系，并探讨了DINOv2模型中不同层的表示一致性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模型是否能够学习并编码ImageNet分类学中的层次结构，以理解其内部表示。

Method: 使用稀疏自编码器（SAEs）分析视觉模型的激活，研究其是否与ImageNet分类学结构一致。

Result: SAEs揭示了模型激活中的层次关系，表明视觉模型隐式编码了分类学结构，且DINOv2模型通过逐层增加类令牌信息来内化层次类别信息。

Conclusion: 该研究为系统分析视觉模型的层次表示提供了框架，并展示了SAEs在探测深度网络语义结构中的潜力。

Abstract: The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.

</details>


### [99] [Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers](https://arxiv.org/abs/2505.15997)
*Mehran Zoravar,Shadi Alijani,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文提出了一种名为CE-ViTs的新框架，通过结合视觉Transformer模型和集成学习，提升图像分类性能，特别是在域适应和不确定性估计方面。


<details>
  <summary>Details</summary>
Motivation: 在医学影像等关键领域，深度学习模型的可靠性至关重要。传统方法在域偏移场景下表现不佳，因此需要一种能够提供可靠不确定性估计和域适应能力的方法。

Method: 提出CE-ViTs框架，利用视觉Transformer模型集成学习，结合HAM10000、Dermofit和ISIC数据集进行训练，并通过共形学习增强域适应能力。

Result: 实验结果显示，该框架的覆盖率达到90.38%，比单一模型提高9.95%，且对困难样本的预测集大小从1.86增加到3.075。

Conclusion: CE-ViTs通过集成学习和共形预测显著提升了模型的域适应能力和不确定性估计性能，适用于医学影像等高可靠性要求的领域。

Abstract: Exploring the trustworthiness of deep learning models is crucial, especially
in critical domains such as medical imaging decision support systems. Conformal
prediction has emerged as a rigorous means of providing deep learning models
with reliable uncertainty estimates and safety guarantees. However, conformal
prediction results face challenges due to the backbone model's struggles in
domain-shifted scenarios, such as variations in different sources. To aim this
challenge, this paper proposes a novel framework termed Conformal Ensemble of
Vision Transformers (CE-ViTs) designed to enhance image classification
performance by prioritizing domain adaptation and model robustness, while
accounting for uncertainty. The proposed method leverages an ensemble of vision
transformer models in the backbone, trained on diverse datasets including
HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning
approach, calibrated through the combined mentioned datasets, aims to enhance
domain adaptation through conformal learning. Experimental results underscore
that the framework achieves a high coverage rate of 90.38\%, representing an
improvement of 9.95\% compared to the HAM10000 model. This indicates a strong
likelihood that the prediction set includes the true label compared to singular
models. Ensemble learning in CE-ViTs significantly improves conformal
prediction performance, increasing the average prediction set size for
challenging misclassified samples from 1.86 to 3.075.

</details>


### [100] [Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning](https://arxiv.org/abs/2505.16001)
*Qiang Zhu,Kuan Lu,Menghao Huo,Yuxiao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型和Transformer的图像到图像翻译框架，结合CLIP编码器实现细粒度翻译，无需依赖文本或标签。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在图像翻译任务中的应用，结合Transformer的全局建模能力，提升翻译质量和结构一致性。

Method: 使用Diffusion Transformers (DiT)框架，通过CLIP编码器提取图像嵌入作为条件，结合CLIP相似性损失和LPIPS感知损失进行训练。

Result: 在face2comics和edges2shoes数据集上验证，实现了高质量且语义一致的图像翻译。

Conclusion: DiT结合CLIP条件化和感知损失，为图像翻译任务提供了优于GAN的替代方案。

Abstract: Image-to-image translation aims to learn a mapping between a source and a
target domain, enabling tasks such as style transfer, appearance
transformation, and domain adaptation. In this work, we explore a
diffusion-based framework for image-to-image translation by adapting Diffusion
Transformers (DiT), which combine the denoising capabilities of diffusion
models with the global modeling power of transformers. To guide the translation
process, we condition the model on image embeddings extracted from a
pre-trained CLIP encoder, allowing for fine-grained and structurally consistent
translations without relying on text or class labels. We incorporate both a
CLIP similarity loss to enforce semantic consistency and an LPIPS perceptual
loss to enhance visual fidelity during training. We validate our approach on
two benchmark datasets: face2comics, which translates real human faces to
comic-style illustrations, and edges2shoes, which translates edge maps to
realistic shoe images. Experimental results demonstrate that DiT, combined with
CLIP-based conditioning and perceptual similarity objectives, achieves
high-quality, semantically faithful translations, offering a promising
alternative to GAN-based models for paired image-to-image translation tasks.

</details>


### [101] [Position: Agentic Systems Constitute a Key Component of Next-Generation Intelligent Image Processing](https://arxiv.org/abs/2505.16007)
*Jinjin Gu*

Main category: cs.CV

TL;DR: 论文主张图像处理领域应从纯模型中心开发扩展到智能代理系统设计，以解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在泛化性、适应性和实际问题解决灵活性方面存在不足，需引入智能代理系统以模拟人类专家的工具组合能力。

Method: 分析模型中心范式的局限性，提出代理系统的设计原则和能力等级。

Result: 提出智能代理系统作为图像处理领域的下一步发展方向。

Conclusion: 智能代理系统设计是图像处理领域的重要补充范式，能克服单一模型的脆弱性。

Abstract: This position paper argues that the image processing community should broaden
its focus from purely model-centric development to include agentic system
design as an essential complementary paradigm. While deep learning has
significantly advanced capabilities for specific image processing tasks,
current approaches face critical limitations in generalization, adaptability,
and real-world problem-solving flexibility. We propose that developing
intelligent agentic systems, capable of dynamically selecting, combining, and
optimizing existing image processing tools, represents the next evolutionary
step for the field. Such systems would emulate human experts' ability to
strategically orchestrate different tools to solve complex problems, overcoming
the brittleness of monolithic models. The paper analyzes key limitations of
model-centric paradigms, establishes design principles for agentic image
processing systems, and outlines different capability levels for such agents.

</details>


### [102] [Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection](https://arxiv.org/abs/2505.16029)
*Shichao Li,Peiliang Li,Qing Lian,Peng Yun,Xiaozhi Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种用于高密度行人场景的3D多目标跟踪方法，通过构建新基准和改进高分辨率表示，显著提升了自动标注效率。


<details>
  <summary>Details</summary>
Motivation: 解决高密度城市环境中行人感知的难点，包括稀疏点云和缺乏合适基准的问题。

Method: 收集新基准，构建离线自动标注系统，学习密度和关系感知的高分辨率表示。

Result: 实验表明，方法显著提高了3D行人跟踪性能，提升了自动标注效率。

Conclusion: 提出的方法有效解决了高密度行人场景的自动标注问题，代码将公开。

Abstract: Perceiving pedestrians in highly crowded urban environments is a difficult
long-tail problem for learning-based autonomous perception. Speeding up 3D
ground truth generation for such challenging scenes is performance-critical yet
very challenging. The difficulties include the sparsity of the captured
pedestrian point cloud and a lack of suitable benchmarks for a specific system
design study. To tackle the challenges, we first collect a new multi-view
LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded
pedestrians for in-depth analysis. We then build an offboard auto-labeling
system that reconstructs pedestrian trajectories from LiDAR point cloud and
multi-view images. To improve the generalization power for crowded scenes and
the performance for small objects, we propose to learn high-resolution
representations that are density-aware and relationship-aware. Extensive
experiments validate that our approach significantly improves the 3D pedestrian
tracking performance towards higher auto-labeling efficiency. The code will be
publicly available at this HTTP URL.

</details>


### [103] [An Approach Towards Identifying Bangladeshi Leaf Diseases through Transfer Learning and XAI](https://arxiv.org/abs/2505.16033)
*Faika Fairuj Preotee,Shuvashis Sarker,Shamim Rahim Refat,Tashreef Muhammad,Shifat Islam*

Main category: cs.CV

TL;DR: 该研究利用深度学习模型（如VGG19和Xception）和可解释AI技术（如GradCAM）高效识别孟加拉国植物叶片病害，最高准确率达98.90%，帮助农民管理植物健康。


<details>
  <summary>Details</summary>
Motivation: 植物叶片病害严重影响农作物健康和农民生计，尤其在孟加拉国等农业重要地区。研究旨在提供一种高效、易用的病害识别方案，减少对专家知识的依赖。

Method: 使用CNN和迁移学习模型（如VGG16、VGG19、MobileNetV2等）分类21种叶片病害，并结合可解释AI技术（如GradCAM）提高模型透明度。

Result: VGG19和Xception模型表现最佳，准确率分别为98.90%和98.66%。可解释AI技术成功定位模型关注的叶片区域。

Conclusion: 该方法不仅提高了病害识别准确性，还帮助农民理解模型预测，支持农业决策，提升作物保护和生产效率。

Abstract: Leaf diseases are harmful conditions that affect the health, appearance and
productivity of plants, leading to significant plant loss and negatively
impacting farmers' livelihoods. These diseases cause visible symptoms such as
lesions, color changes, and texture variations, making it difficult for farmers
to manage plant health, especially in large or remote farms where expert
knowledge is limited. The main motivation of this study is to provide an
efficient and accessible solution for identifying plant leaf diseases in
Bangladesh, where agriculture plays a critical role in food security. The
objective of our research is to classify 21 distinct leaf diseases across six
plants using deep learning models, improving disease detection accuracy while
reducing the need for expert involvement. Deep Learning (DL) techniques,
including CNN and Transfer Learning (TL) models like VGG16, VGG19, MobileNetV2,
InceptionV3, ResNet50V2 and Xception are used. VGG19 and Xception achieve the
highest accuracies, with 98.90% and 98.66% respectively. Additionally,
Explainable AI (XAI) techniques such as GradCAM, GradCAM++, LayerCAM, ScoreCAM
and FasterScoreCAM are used to enhance transparency by highlighting the regions
of the models focused on during disease classification. This transparency
ensures that farmers can understand the model's predictions and take necessary
action. This approach not only improves disease management but also supports
farmers in making informed decisions, leading to better plant protection and
increased agricultural productivity.

</details>


### [104] [An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection](https://arxiv.org/abs/2505.16039)
*Shuvashis Sarker,Shamim Rahim Refat,Faika Fairuj Preotee,Shifat Islam,Tashreef Muhammad,Mohammad Ashraful Hoque*

Main category: cs.CV

TL;DR: 该研究比较了Vision Transformer (ViT)和迁移学习模型在MRI脑疾病分类中的表现，ViT表现更优，准确率达94.39%，并结合可解释AI方法提升诊断透明度。


<details>
  <summary>Details</summary>
Motivation: 脑部疾病诊断复杂，MRI图像解读困难，研究旨在通过AI模型提升分类准确性和可解释性。

Method: 使用ViT和迁移学习模型（VGG16、VGG19等）对孟加拉国MRI数据集进行分类，并结合GradCAM等XAI方法解释模型预测。

Result: ViT模型表现最佳，分类准确率达94.39%，XAI方法增强了模型的可解释性。

Conclusion: ViT结合XAI方法可有效提升脑部疾病诊断的准确性和透明度，为医疗专业人员提供支持。

Abstract: The brain is a highly complex organ that manages many important tasks,
including movement, memory and thinking. Brain-related conditions, like tumors
and degenerative disorders, can be hard to diagnose and treat. Magnetic
Resonance Imaging (MRI) serves as a key tool for identifying these conditions,
offering high-resolution images of brain structures. Despite this, interpreting
MRI scans can be complicated. This study tackles this challenge by conducting a
comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL)
models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain
diseases using MRI data from Bangladesh based dataset. ViT, known for their
ability to capture global relationships in images, are particularly effective
for medical imaging tasks. Transfer learning helps to mitigate data constraints
by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods
such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are
employed to interpret model predictions. The results demonstrate that ViT
surpasses transfer learning models, achieving a classification accuracy of
94.39%. The integration of XAI methods enhances model transparency, offering
crucial insights to aid medical professionals in diagnosing brain diseases with
greater precision.

</details>


### [105] [GMatch: Geometry-Constrained Feature Matching for RGB-D Object Pose Estimation](https://arxiv.org/abs/2505.16144)
*Ming Yang,Haoran Li*

Main category: cs.CV

TL;DR: GMatch是一种无需学习的特征匹配器，用于稳健的6DoF物体姿态估计，通过几何一致性解决稀疏特征匹配中的局部模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖描述符相似性，难以解决稀疏特征匹配中的局部模糊问题，GMatch旨在通过几何一致性提升匹配效果。

Method: GMatch采用增量搜索并强制SE(3)不变的几何一致性，利用几何特征唯一确定3D关键点配置，无需训练或GPU支持。

Result: 在HOPE和YCB-Video数据集上，GMatch-SIFT表现优于传统和学习型匹配器，甚至媲美实例级姿态网络。

Conclusion: GMatch-SIFT不仅验证了其在物体姿态估计中的有效性，还展示了作为通用特征匹配器的广泛适用性。

Abstract: We present GMatch, a learning-free feature matcher designed for robust 6DoF
object pose estimation, addressing common local ambiguities in sparse feature
matching. Unlike traditional methods that rely solely on descriptor similarity,
GMatch performs a guided, incremental search, enforcing SE(3)-invariant
geometric consistency throughout the matching process. It leverages a provably
complete set of geometric features that uniquely determine 3D keypoint
configurations, ensuring globally consistent correspondences without the need
for training or GPU support. When combined with classical descriptors such as
SIFT, GMatch-SIFT forms a general-purpose pose estimation pipeline that offers
strong interpretability and generalization across diverse objects and scenes.
Experiments on the HOPE dataset show that GMatch outperforms both traditional
and learning-based matchers, with GMatch-SIFT achieving or surpassing the
performance of instance-level pose networks. On the YCB-Video dataset,
GMatch-SIFT demonstrates high accuracy and low variance on texture-rich
objects. These results not only validate the effectiveness of GMatch-SIFT for
object pose estimation but also highlight the broader applicability of GMatch
as a general-purpose feature matcher. Code will be released upon acceptance.

</details>


### [106] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
*Zhenglin Hua,Jinghan He,Zijun Yao,Tianxu Han,Haiyun Guo,Yuheng Jia,Junfeng Fang*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的无训练方法SSL，用于减少大型视觉语言模型（LVLM）中的幻觉问题，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态任务中表现优异，但存在幻觉问题，现有方法计算成本高且效果有限。

Method: 利用SAE识别与幻觉或实际语义相关的方向，提出SSL方法通过干预这些方向来减少幻觉。

Result: 实验表明SSL能显著减少幻觉，且在不同模型架构中具有可迁移性，额外时间开销可忽略。

Conclusion: SSL是一种高效且无需训练的方法，能有效减少LVLM的幻觉问题。

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [107] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
*Zirui Pang,Haosheng Tan,Yuhan Pu,Zhijie Deng,Zhouan Shen,Keyu Hu,Jiaheng Wei*

Main category: cs.CV

TL;DR: 论文提出REVEAL框架，结合视觉语言模型和标签清理方法，解决图像分类数据集中噪声标签和缺失标签问题，显著提升6个基准测试集的质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类数据集（如CIFAR、MNIST、ImageNet）存在噪声标签和缺失标签问题，导致模型评估不准确。现有方法主要关注噪声标签，而忽略缺失标签。

Method: REVEAL整合预训练视觉语言模型（如LLaVA、BLIP）与标签清理方法（如Docta、Cleanlab），通过置信度预测和共识过滤检测噪声标签和缺失标签。

Result: REVEAL显著提升了6个基准测试集的质量，并通过人类验证证明其标签准确性更高。

Conclusion: REVEAL为图像分类数据集提供了一种系统化的标签清理方法，支持更公平的模型评估。

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [108] [Training-Free Reasoning and Reflection in MLLMs](https://arxiv.org/abs/2505.16151)
*Hongchen Wei,Zhenzhong Chen*

Main category: cs.CV

TL;DR: FRANK Model是一种无需训练即可为现有多模态大语言模型（MLLMs）添加推理和反思能力的模型，通过分层权重合并方法，显著提升了多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理大语言模型（LLMs）在多模态扩展时面临高昂的重新训练成本和高质量数据稀缺问题，FRANK Model旨在解决这些问题。

Method: 通过分层权重合并方法，将视觉预训练的MLLM与专门用于推理的LLM结合，利用泰勒导出的闭式融合机制在深层解码器中集成推理能力。

Result: 在MMMU基准测试中，FRANK-38B以69.2的准确率超越最强基线InternVL2.5-38B（+5.3），甚至优于专有模型GPT-4o。

Conclusion: FRANK Model通过免训练的方式有效提升了MLLMs的推理能力，为多模态推理任务提供了高效解决方案。

Abstract: Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have
showcased impressive reasoning capabilities via reinforcement learning.
However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by
the prohibitive costs of retraining and the scarcity of high-quality,
verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a
training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning
and reflection abilities, without any gradient updates or extra supervision.
Our key insight is to decouple perception and reasoning across MLLM decoder
layers. Specifically, we observe that compared to the deeper decoder layers,
the shallow decoder layers allocate more attention to visual tokens, while the
deeper decoder layers concentrate on textual semantics. This observation
motivates a hierarchical weight merging approach that combines a
visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we
propose a layer-wise, Taylor-derived closed-form fusion mechanism that
integrates reasoning capacity into deep decoder layers while preserving visual
grounding in shallow decoder layers. Extensive experiments on challenging
multimodal reasoning benchmarks demonstrate the effectiveness of our approach.
On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,
outperforming the strongest baseline InternVL2.5-38B by +5.3, and even
surpasses the proprietary GPT-4o model. Our project homepage is at:
http://iip.whu.edu.cn/frank/index.html

</details>


### [109] [BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World](https://arxiv.org/abs/2505.16154)
*Ji Guo,Long Zhou,Zhijin Wang,Jiaming He,Qiyang Song,Aiguo Chen,Wenbo Jiang*

Main category: cs.CV

TL;DR: 论文提出BadDepth，首次针对单目深度估计模型的后门攻击方法，通过选择性操纵目标物体深度生成毒化数据集，并在物理世界中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 填补单目深度估计模型后门攻击研究的空白，解决现有方法因深度图形式标签无法直接应用的问题。

Method: 利用图像分割模型选择性操纵目标物体深度，并通过深度补全恢复周围区域，生成毒化数据集；引入数字到物理增强以适应物理世界与数字领域的差异。

Result: 在多个模型上的实验验证了BadDepth在数字领域和物理世界中的有效性，且不受环境因素影响。

Conclusion: BadDepth为单目深度估计模型的后门攻击提供了有效解决方案，并展示了在物理世界中的鲁棒性。

Abstract: In recent years, deep learning-based Monocular Depth Estimation (MDE) models
have been widely applied in fields such as autonomous driving and robotics.
However, their vulnerability to backdoor attacks remains unexplored. To fill
the gap in this area, we conduct a comprehensive investigation of backdoor
attacks against MDE models. Typically, existing backdoor attack methods can not
be applied to MDE models. This is because the label used in MDE is in the form
of a depth map. To address this, we propose BadDepth, the first backdoor attack
targeting MDE models. BadDepth overcomes this limitation by selectively
manipulating the target object's depth using an image segmentation model and
restoring the surrounding areas via depth completion, thereby generating
poisoned datasets for object-level backdoor attacks. To improve robustness in
physical world scenarios, we further introduce digital-to-physical augmentation
to adapt to the domain gap between the physical world and the digital domain.
Extensive experiments on multiple models validate the effectiveness of BadDepth
in both the digital domain and the physical world, without being affected by
environmental factors.

</details>


### [110] [Breaking Complexity Barriers: High-Resolution Image Restoration with Rank Enhanced Linear Attention](https://arxiv.org/abs/2505.16157)
*Yuang Ai,Huaibo Huang,Tao Wu,Qihang Fan,Ran He*

Main category: cs.CV

TL;DR: 论文提出了一种名为RELA的线性注意力增强方法，并基于此构建了高效的图像恢复Transformer模型LAformer，解决了传统Transformer在高分辨率图像处理中的计算复杂度和性能问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像恢复任务中表现优异，但其自注意力的二次复杂度限制了在高分辨率图像上的应用。现有方法通过稀疏或窗口注意力缓解问题，但牺牲了全局上下文建模能力。线性注意力虽能保持线性复杂度，但在图像恢复任务中性能下降明显。

Method: 提出Rank Enhanced Linear Attention (RELA)，通过轻量级深度卷积增强特征表示；基于RELA构建LAformer模型，结合线性注意力和通道注意力实现全局感知，并通过卷积门控前馈网络增强局部拟合能力。

Result: LAformer在7个图像恢复任务和21个基准测试中表现优于现有方法，并显著提升了计算效率。

Conclusion: LAformer通过优化注意力机制和网络结构，实现了高效且高性能的图像恢复，适用于高分辨率图像处理。

Abstract: Transformer-based models have made remarkable progress in image restoration
(IR) tasks. However, the quadratic complexity of self-attention in Transformer
hinders its applicability to high-resolution images. Existing methods mitigate
this issue with sparse or window-based attention, yet inherently limit global
context modeling. Linear attention, a variant of softmax attention,
demonstrates promise in global context modeling while maintaining linear
complexity, offering a potential solution to the above challenge. Despite its
efficiency benefits, vanilla linear attention suffers from a significant
performance drop in IR, largely due to the low-rank nature of its attention
map. To counter this, we propose Rank Enhanced Linear Attention (RELA), a
simple yet effective method that enriches feature representations by
integrating a lightweight depthwise convolution. Building upon RELA, we propose
an efficient and effective image restoration Transformer, named LAformer.
LAformer achieves effective global perception by integrating linear attention
and channel attention, while also enhancing local fitting capabilities through
a convolutional gated feed-forward network. Notably, LAformer eliminates
hardware-inefficient operations such as softmax and window shifting, enabling
efficient processing of high-resolution images. Extensive experiments across 7
IR tasks and 21 benchmarks demonstrate that LAformer outperforms SOTA methods
and offers significant computational advantages.

</details>


### [111] [Deep Learning-Driven Ultra-High-Definition Image Restoration: A Survey](https://arxiv.org/abs/2505.16161)
*Liyan Wang,Weixiang Zhou,Cong Wang,Kin-Man Lam,Zhixun Su,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文系统综述了超高清（UHD）图像恢复领域的最新进展，涵盖数据集构建、算法设计等方面，并提出了基于网络架构和采样策略的分类框架。


<details>
  <summary>Details</summary>
Motivation: 解决超高清图像质量退化问题，总结该领域的最新深度学习创新，为研究者提供有价值的资源。

Method: 通过总结各种图像恢复子问题的退化模型，整理现有UHD基准数据集，并分类文献。回顾深度学习驱动的UHD图像恢复里程碑，提出分类框架。

Result: 系统整理了UHD图像恢复的研究进展，提出了分类框架，并指出了未来研究方向。

Conclusion: 本文为UHD图像恢复领域提供了全面的综述和分类框架，并展望了未来的研究方向。

Abstract: Ultra-high-definition (UHD) image restoration aims to specifically solve the
problem of quality degradation in ultra-high-resolution images. Recent
advancements in this field are predominantly driven by deep learning-based
innovations, including enhancements in dataset construction, network
architecture, sampling strategies, prior knowledge integration, and loss
functions. In this paper, we systematically review recent progress in UHD image
restoration, covering various aspects ranging from dataset construction to
algorithm design. This serves as a valuable resource for understanding
state-of-the-art developments in the field. We begin by summarizing degradation
models for various image restoration subproblems, such as super-resolution,
low-light enhancement, deblurring, dehazing, deraining, and desnowing, and
emphasizing the unique challenges of their application to UHD image
restoration. We then highlight existing UHD benchmark datasets and organize the
literature according to degradation types and dataset construction methods.
Following this, we showcase major milestones in deep learning-driven UHD image
restoration, reviewing the progression of restoration tasks, technological
developments, and evaluations of existing methods. We further propose a
classification framework based on network architectures and sampling
strategies, helping to clearly organize existing methods. Finally, we share
insights into the current research landscape and propose directions for further
advancements. A related repository is available at
https://github.com/wlydlut/UHD-Image-Restoration-Survey.

</details>


### [112] [RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition](https://arxiv.org/abs/2505.16165)
*Yechan Park,Gyuhyeon Pak,Euntai Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为RE-TRIP的新型3D地点识别描述符，结合了几何测量和反射率信息，提升了在复杂场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的地点识别方法主要依赖几何信息，忽略了反射率数据，导致在几何退化、高几何相似性和动态对象等场景中表现不佳。

Method: 提出RE-TRIP描述符，包括关键点提取、关键实例分割、RE-TRIP匹配和反射率结合的闭环验证方法。

Result: 在公开数据集（HELIPR、FusionPortable）上的实验表明，RE-TRIP在性能上优于现有方法（Scan Context、Intensity Scan Context、STD）。

Conclusion: RE-TRIP通过结合几何和反射率信息，显著提升了地点识别的鲁棒性和准确性。

Abstract: While most people associate LiDAR primarily with its ability to measure
distances and provide geometric information about the environment (via point
clouds), LiDAR also captures additional data, including reflectivity or
intensity values. Unfortunately, when LiDAR is applied to Place Recognition
(PR) in mobile robotics, most previous works on LiDAR-based PR rely only on
geometric measurements, neglecting the additional reflectivity information that
LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named
RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new
descriptor leverages both geometric measurements and reflectivity to enhance
robustness in challenging scenarios such as geometric degeneracy, high
geometric similarity, and the presence of dynamic objects. To implement RE-TRIP
in real-world applications, we further propose (1) a keypoint extraction
method, (2) a key instance segmentation method, (3) a RE-TRIP matching method,
and (4) a reflectivity-combined loop verification method. Finally, we conduct a
series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to
public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios
such as long corridors, bridges, large-scale urban areas, and highly dynamic
environments -- our experimental results show that the proposed method
outperforms existing state-of-the-art methods in terms of Scan Context,
Intensity Scan Context, and STD.

</details>


### [113] [TRAIL: Transferable Robust Adversarial Images via Latent diffusion](https://arxiv.org/abs/2505.16166)
*Yuhao Xue,Zhifei Zhang,Xinyang Jiang,Yifei Shen,Junyao Gao,Wentao Gu,Jiale Zhao,Miaojing Shi,Cairong Zhao*

Main category: cs.CV

TL;DR: TRAIL框架通过潜在扩散模型生成对抗样本，解决分布不匹配问题，显著提升跨模型攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法因生成对抗特征与真实数据分布不匹配，导致跨模型迁移性受限。

Method: 提出TRAIL框架，结合对抗目标和感知约束更新扩散U-Net权重，生成分布对齐的对抗样本。

Result: 实验表明TRAIL在跨模型攻击迁移性上显著优于现有方法。

Conclusion: 分布对齐的对抗特征合成对实际黑盒攻击至关重要。

Abstract: Adversarial attacks exploiting unrestricted natural perturbations present
severe security risks to deep learning systems, yet their transferability
across models remains limited due to distribution mismatches between generated
adversarial features and real-world data. While recent works utilize
pre-trained diffusion models as adversarial priors, they still encounter
challenges due to the distribution shift between the distribution of ideal
adversarial samples and the natural image distribution learned by the diffusion
model. To address the challenge, we propose Transferable Robust Adversarial
Images via Latent Diffusion (TRAIL), a test-time adaptation framework that
enables the model to generate images from a distribution of images with
adversarial features and closely resembles the target images. To mitigate the
distribution shift, during attacks, TRAIL updates the diffusion U-Net's weights
by combining adversarial objectives (to mislead victim models) and perceptual
constraints (to preserve image realism). The adapted model then generates
adversarial samples through iterative noise injection and denoising guided by
these objectives. Experiments demonstrate that TRAIL significantly outperforms
state-of-the-art methods in cross-model attack transferability, validating that
distribution-aligned adversarial feature synthesis is critical for practical
black-box attacks.

</details>


### [114] [Erased or Dormant? Rethinking Concept Erasure Through Reversibility](https://arxiv.org/abs/2505.16174)
*Ping Liu,Chi Zhang*

Main category: cs.CV

TL;DR: 论文探讨了概念擦除技术在扩散模型中是否真正消除了生成能力，还是仅实现了表面的、特定提示的抑制。通过系统评估两种代表性方法，发现擦除的概念在轻微调整后仍能重新生成，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证当前概念擦除技术是否真正移除了目标概念的生成能力，而非仅实现表面的抑制。

Method: 采用实例级评估策略，通过轻量级微调测试擦除概念的重新激活潜力，评估了Unified Concept Editing和Erased Stable Diffusion两种方法。

Result: 结果显示，擦除的概念在轻微调整后仍能以高视觉保真度重新生成，表明现有方法未能完全消除潜在生成表示。

Conclusion: 结论指出现有概念擦除方法存在局限性，需要更深层次的表示级干预和更严格的评估标准以实现真正的概念移除。

Abstract: To what extent does concept erasure eliminate generative capacity in
diffusion models? While prior evaluations have primarily focused on measuring
concept suppression under specific textual prompts, we explore a complementary
and fundamental question: do current concept erasure techniques genuinely
remove the ability to generate targeted concepts, or do they merely achieve
superficial, prompt-specific suppression? We systematically evaluate the
robustness and reversibility of two representative concept erasure methods,
Unified Concept Editing and Erased Stable Diffusion, by probing their ability
to eliminate targeted generative behaviors in text-to-image models. These
methods attempt to suppress undesired semantic concepts by modifying internal
model parameters, either through targeted attention edits or model-level
fine-tuning strategies. To rigorously assess whether these techniques truly
erase generative capacity, we propose an instance-level evaluation strategy
that employs lightweight fine-tuning to explicitly test the reactivation
potential of erased concepts. Through quantitative metrics and qualitative
analyses, we show that erased concepts often reemerge with substantial visual
fidelity after minimal adaptation, indicating that current methods suppress
latent generative representations without fully eliminating them. Our findings
reveal critical limitations in existing concept erasure approaches and
highlight the need for deeper, representation-level interventions and more
rigorous evaluation standards to ensure genuine, irreversible removal of
concepts from generative models.

</details>


### [115] [QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design](https://arxiv.org/abs/2505.16175)
*Benjamin Schneider,Dongfu Jiang,Chao Du,Tianyu Pang,Wenhu Chen*

Main category: cs.CV

TL;DR: QuickVideo通过系统-算法协同设计加速长视频理解，解决解码和预填充瓶颈，支持实时应用。


<details>
  <summary>Details</summary>
Motivation: 长视频理解在现实应用中至关重要，但现有方法因解码和预填充的高延迟和内存消耗而受限。

Method: 提出QuickVideo，包含并行解码器QuickDecoder、内存高效预填充QuickPrefill和CPU-GPU重叠处理。

Result: 实验显示QuickVideo显著减少推理时间，支持不同时长和采样率的长视频处理。

Conclusion: QuickVideo为长视频理解提供了高效、可扩展的解决方案。

Abstract: Long-video understanding has emerged as a crucial capability in real-world
applications such as video surveillance, meeting summarization, educational
lecture analysis, and sports broadcasting. However, it remains computationally
prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential
video decoding, the process of converting the raw bit stream to RGB frames can
take up to a minute for hour-long video inputs, and 2) costly prefilling of up
to several million tokens for LLM inference, resulting in high latency and
memory use. To address these challenges, we propose QuickVideo, a
system-algorithm co-design that substantially accelerates long-video
understanding to support real-time downstream applications. It comprises three
key innovations: QuickDecoder, a parallelized CPU-based video decoder that
achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals
processed concurrently; QuickPrefill, a memory-efficient prefilling method
using KV-cache pruning to support more frames with less GPU memory; and an
overlapping scheme that overlaps CPU video decoding with GPU inference.
Together, these components infernece time reduce by a minute on long video
inputs, enabling scalable, high-quality video understanding even on limited
hardware. Experiments show that QuickVideo generalizes across durations and
sampling rates, making long video processing feasible in practice.

</details>


### [116] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CV

TL;DR: Redemption Score是一种新颖的混合框架，通过结合三种互补信号（MID、DINO感知相似性和BERTScore）来评估图像标题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像标题评估指标未能完全捕捉视觉语义和语言语用学，需要更全面的评估方法。

Method: 提出Redemption Score，结合MID（全局图像-文本分布对齐）、DINO感知相似性（视觉基础）和BERTScore（上下文文本相似性）。

Result: 在Flickr8k基准测试中，Kendall-τ达56.43，优于12种现有方法，且无需任务特定训练。

Conclusion: Redemption Score提供更全面、鲁棒的图像标题评估，适用于多种数据集。

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [117] [Understanding Generative AI Capabilities in Everyday Image Editing Tasks](https://arxiv.org/abs/2505.16181)
*Mohammad Reza Taesiri,Brandon Collins,Logan Bolton,Viet Dac Lai,Franck Dernoncourt,Trung Bui,Anh Totti Nguyen*

Main category: cs.CV

TL;DR: 论文研究了用户对图像编辑的需求及AI编辑器的表现，发现仅33%的请求能被当前最佳AI编辑器满足，且AI在低创意任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 探究用户最常需要哪些图像编辑任务，以及AI编辑器在处理这些任务时的表现，以指导未来AI编辑器的改进。

Method: 通过分析Reddit社区12年间的83k请求和305k编辑结果，结合人类评分和VLM评估。

Result: 仅33%的请求能被AI编辑器满足，AI在低创意任务中表现更差，且常无法保留人或动物的身份。

Conclusion: AI编辑器需改进低创意任务的精确编辑能力，并减少非请求的修饰行为。

Abstract: Generative AI (GenAI) holds significant promise for automating everyday image
editing tasks, especially following the recent release of GPT-4o on March 25,
2025. However, what subjects do people most often want edited? What kinds of
editing actions do they want to perform (e.g., removing or stylizing the
subject)? Do people prefer precise edits with predictable outcomes or highly
creative ones? By understanding the characteristics of real-world requests and
the corresponding edits made by freelance photo-editing wizards, can we draw
lessons for improving AI-based editors and determine which types of requests
can currently be handled successfully by AI editors? In this paper, we present
a unique study addressing these questions by analyzing 83k requests from the
past 12 years (2013-2025) on the Reddit community, which collected 305k
PSR-wizard edits. According to human ratings, approximately only 33% of
requests can be fulfilled by the best AI editors (including GPT-4o,
Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on
low-creativity requests that require precise editing than on more open-ended
tasks. They often struggle to preserve the identity of people and animals, and
frequently make non-requested touch-ups. On the other side of the table, VLM
judges (e.g., o1) perform differently from human judges and may prefer AI edits
more than human edits. Code and qualitative examples are available at:
https://psrdataset.github.io

</details>


### [118] [VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.16192)
*Chaoya Jiang,Yongrui Heng,Wei Ye,Han Yang,Haiyang Xu,Ming Yan,Ji Zhang,Fei Huang,Shikun Zhang*

Main category: cs.CV

TL;DR: VLM-R³是一个视觉语言模型框架，通过动态聚焦和迭代视觉区域，提升复杂任务中的文本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的多模态大语言模型在复杂任务中难以动态聚焦和迭代视觉区域，导致文本推理与视觉证据的精确结合不足。

Method: 提出VLM-R³框架，结合区域识别与推理能力，采用R-GRPO训练范式，奖励模型选择信息区域并整合视觉上下文。

Result: 在MathVista、ScienceQA等基准测试中，VLM-R³在零样本和少样本设置下达到新SOTA，尤其在需要空间推理或细粒度视觉提取的任务中表现突出。

Conclusion: VLM-R³通过动态区域聚焦和强化学习优化，显著提升了复杂视觉推理任务的性能。

Abstract: Recently, reasoning-based MLLMs have achieved a degree of success in
generating long-form textual reasoning chains. However, they still struggle
with complex tasks that necessitate dynamic and iterative focusing on and
revisiting of visual regions to achieve precise grounding of textual reasoning
in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual
\textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and
\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)
decide \emph{when} additional visual evidence is needed, (ii) determine
\emph{where} to ground within the image, and (iii) seamlessly weave the
relevant sub-image content back into an interleaved chain-of-thought. The core
of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization
(R-GRPO)}, a training paradigm that rewards the model for selecting informative
regions, formulating appropriate transformations (e.g.\ crop, zoom), and
integrating the resulting visual context into subsequent reasoning steps. To
bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual
Interleaved Rationale (VLIR) corpus that provides step-level supervision on
region selection and textual justification. Extensive experiments on MathVista,
ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art
in zero-shot and few-shot settings, with the largest gains appearing on
questions demanding subtle spatial reasoning or fine-grained visual cue
extraction.

</details>


### [119] [A Causal Approach to Mitigate Modality Preference Bias in Medical Visual Question Answering](https://arxiv.org/abs/2505.16209)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Dagan Feng,Jinman Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为MedCFVQA的模型，通过因果图消除模态偏好偏差，并在重构的数据集上验证其性能优于非因果模型。


<details>
  <summary>Details</summary>
Motivation: 现有MedVQA模型存在模态偏好偏差，导致无法充分学习多模态知识，影响临床诊断效率。

Method: 提出MedCFVQA模型，利用因果图消除模态偏好偏差，并重构数据集以改变问题和答案的先验依赖关系。

Result: MedCFVQA在SLAKE、RadVQA及其重构数据集上显著优于非因果模型。

Conclusion: MedCFVQA能有效解决模态偏好偏差问题，提升MedVQA性能。

Abstract: Medical Visual Question Answering (MedVQA) is crucial for enhancing the
efficiency of clinical diagnosis by providing accurate and timely responses to
clinicians' inquiries regarding medical images. Existing MedVQA models suffered
from modality preference bias, where predictions are heavily dominated by one
modality while overlooking the other (in MedVQA, usually questions dominate the
answer but images are overlooked), thereby failing to learn multimodal
knowledge. To overcome the modality preference bias, we proposed a Medical
CounterFactual VQA (MedCFVQA) model, which trains with bias and leverages
causal graphs to eliminate the modality preference bias during inference.
Existing MedVQA datasets exhibit substantial prior dependencies between
questions and answers, which results in acceptable performance even if the
model significantly suffers from the modality preference bias. To address this
issue, we reconstructed new datasets by leveraging existing MedVQA datasets and
Changed their P3rior dependencies (CP) between questions and their answers in
the training and test set. Extensive experiments demonstrate that MedCFVQA
significantly outperforms its non-causal counterpart on both SLAKE, RadVQA and
SLAKE-CP, RadVQA-CP datasets.

</details>


### [120] [A Shape-Aware Total Body Photography System for In-focus Surface Coverage Optimization](https://arxiv.org/abs/2505.16228)
*Wei-Lun Huang,Joshua Liu,Davood Tashayyod,Jun Kang,Amir Gandjbakhche,Misha Kazhdan,Mehran Armand*

Main category: cs.CV

TL;DR: 本文提出了一种新型的形状感知全身摄影系统，通过优化图像分辨率和清晰度，提高皮肤癌筛查的自动检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的全身摄影系统在自动检测和分析可疑皮肤病变方面仍有改进空间，尤其是图像分辨率和清晰度。

Method: 系统结合深度和RGB摄像头，利用3D身体形状估计和聚焦表面优化方法，为每个摄像头姿态选择最佳对焦距离。

Result: 系统在模拟数据和真实扫描中分别达到0.068 mm/像素和0.0566 mm/像素的平均分辨率，85%和95%的表面区域清晰对焦。

Conclusion: 该系统的高保真成像能力将提升皮肤癌筛查中自动皮肤病变分析的准确性。

Abstract: Total Body Photography (TBP) is becoming a useful screening tool for patients
at high risk for skin cancer. While much progress has been made, existing TBP
systems can be further improved for automatic detection and analysis of
suspicious skin lesions, which is in part related to the resolution and
sharpness of acquired images. This paper proposes a novel shape-aware TBP
system automatically capturing full-body images while optimizing image quality
in terms of resolution and sharpness over the body surface. The system uses
depth and RGB cameras mounted on a 360-degree rotary beam, along with 3D body
shape estimation and an in-focus surface optimization method to select the
optimal focus distance for each camera pose. This allows for optimizing the
focused coverage over the complex 3D geometry of the human body given the
calibrated camera poses. We evaluate the effectiveness of the system in
capturing high-fidelity body images. The proposed system achieves an average
resolution of 0.068 mm/pixel and 0.0566 mm/pixel with approximately 85% and 95%
of surface area in-focus, evaluated on simulation data of diverse body shapes
and poses as well as a real scan of a mannequin respectively. Furthermore, the
proposed shape-aware focus method outperforms existing focus protocols (e.g.
auto-focus). We believe the high-fidelity imaging enabled by the proposed
system will improve automated skin lesion analysis for skin cancer screening.

</details>


### [121] [CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering](https://arxiv.org/abs/2505.16229)
*Yuren Mao,Wenyi Xu,Yuyang Qin,Yunjun Gao*

Main category: cs.CV

TL;DR: 论文提出CT-Agent，一种多模态代理框架，用于解决CT影像问答任务中的解剖复杂性和跨切片空间关系问题。


<details>
  <summary>Details</summary>
Motivation: 放射科医生创建CT报告耗时且易出错，现有VQA系统无法有效处理CTQA任务，因解剖复杂性和跨切片空间关系难以捕捉。

Method: 采用解剖独立工具分解解剖复杂性，并通过全局-局部标记压缩策略高效捕捉跨切片空间关系。

Result: 在两个3D胸部CT数据集（CT-RATE和RadGenome-ChestCT）上验证了CT-Agent的优越性能。

Conclusion: CT-Agent能有效解决CTQA任务中的关键挑战，为自动生成CT报告提供支持。

Abstract: Computed Tomography (CT) scan, which produces 3D volumetric medical data that
can be viewed as hundreds of cross-sectional images (a.k.a. slices), provides
detailed anatomical information for diagnosis. For radiologists, creating CT
radiology reports is time-consuming and error-prone. A visual question
answering (VQA) system that can answer radiologists' questions about some
anatomical regions on the CT scan and even automatically generate a radiology
report is urgently needed. However, existing VQA systems cannot adequately
handle the CT radiology question answering (CTQA) task for: (1) anatomic
complexity makes CT images difficult to understand; (2) spatial relationship
across hundreds slices is difficult to capture. To address these issues, this
paper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent
adopts anatomically independent tools to break down the anatomic complexity;
furthermore, it efficiently captures the across-slice spatial relationship with
a global-local token compression strategy. Experimental results on two 3D chest
CT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of
CT-Agent.

</details>


### [122] [DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution](https://arxiv.org/abs/2505.16239)
*Zheng Chen,Zichen Zou,Kewei Zhang,Xiongfei Su,Xin Yuan,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: DOVE是一种高效的单步扩散模型，用于真实世界视频超分辨率（VSR），通过微调预训练模型和引入潜在像素训练策略，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在VSR中表现优异，但推理速度慢；单步采样技术虽能加速，但实现难度大。

Method: 微调预训练视频扩散模型（CogVideoX），采用潜在像素训练策略和两阶段方案，并构建高质量数据集HQ-VSR。

Result: DOVE性能与多步扩散方法相当或更优，推理速度提升28倍。

Conclusion: DOVE在保持高性能的同时显著提升了效率，适用于真实世界VSR任务。

Abstract: Diffusion models have demonstrated promising performance in real-world video
super-resolution (VSR). However, the dozens of sampling steps they require,
make inference extremely slow. Sampling acceleration techniques, particularly
single-step, provide a potential solution. Nonetheless, achieving one step in
VSR remains challenging, due to the high training overhead on video data and
stringent fidelity demands. To tackle the above issues, we propose DOVE, an
efficient one-step diffusion model for real-world VSR. DOVE is obtained by
fine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To
effectively train DOVE, we introduce the latent-pixel training strategy. The
strategy employs a two-stage scheme to gradually adapt the model to the video
super-resolution task. Meanwhile, we design a video processing pipeline to
construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning
on this dataset further enhances the restoration capability of DOVE. Extensive
experiments show that DOVE exhibits comparable or superior performance to
multi-step diffusion-based VSR methods. It also offers outstanding inference
efficiency, achieving up to a **28$\times$** speed-up over existing methods
such as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.

</details>


### [123] [Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces](https://arxiv.org/abs/2505.16253)
*Preeti Mehta,Aman Sagar,Suchi Kumari*

Main category: cs.CV

TL;DR: 研究提出基于Swin Transformer的模型，用于在RGB、YCbCr和HSV三种颜色空间中区分计算机生成图像（CGI）与真实数字图像，模型在多个数据集上表现出色，RGB颜色空间效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法在处理CGI的复杂性和多样性时存在局限性，需要更准确的区分方法。

Method: 采用Swin Transformer的分层架构捕捉局部和全局特征，结合数据增强和t-SNE可视化技术。

Result: RGB颜色空间表现最佳，模型在跨数据集测试中优于VGG-19和ResNet-50。

Conclusion: Swin Transformer模型在数字图像取证中具有潜力，尤其在区分CGI与自然图像方面表现优异。

Abstract: This study aims to address the growing challenge of distinguishing
computer-generated imagery (CGI) from authentic digital images across three
different color spaces; RGB, YCbCr, and HSV. Given the limitations of existing
classification methods in handling the complexity and variability of CGI, this
research proposes a Swin Transformer based model for accurate differentiation
between natural and synthetic images. The proposed model leverages the Swin
Transformer's hierarchical architecture to capture local and global features
for distinguishing CGI from natural images. Its performance was assessed
through intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,
and Columbia. The model was evaluated individually on each dataset (D1, D2, D3)
and on the combined datasets (D1+D2+D3) to test its robustness and domain
generalization. To address dataset imbalance, data augmentation techniques were
applied. Additionally, t-SNE visualization was used to demonstrate the feature
separability achieved by the Swin Transformer across the selected color spaces.
The model's performance was tested across all color schemes, with the RGB color
scheme yielding the highest accuracy for each dataset. As a result, RGB was
selected for domain generalization analysis and compared with other CNN-based
models, VGG-19 and ResNet-50. The comparative results demonstrate the proposed
model's effectiveness in detecting CGI, highlighting its robustness and
reliability in both intra-dataset and inter-dataset evaluations. The findings
of this study highlight the Swin Transformer model's potential as an advanced
tool for digital image forensics, particularly in distinguishing CGI from
natural images. The model's strong performance indicates its capability for
domain generalization, making it a valuable asset in scenarios requiring
precise and reliable image classification.

</details>


### [124] [LINEA: Fast and Accurate Line Detection Using Scalable Transformers](https://arxiv.org/abs/2505.16264)
*Sebastian Janampa,Marios Pattichis*

Main category: cs.CV

TL;DR: 本文提出了一种基于变形线注意力（DLA）的快速Transformer方法LINEA，无需在大数据集上预训练注意力机制，显著提升了线检测的速度和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的线检测方法虽然准确率高，但推理速度慢且需要在大数据集上预训练，限制了其在低延迟视频分析中的应用。

Method: 提出了一种新的注意力机制Deformable Line Attention（DLA），无需预训练，并基于此构建了LINEA方法。

Result: 实验表明，LINEA在速度上显著提升，并在分布外数据集测试中优于先前模型的sAP指标。

Conclusion: LINEA通过DLA机制实现了高效且无需预训练的线检测，为低延迟应用提供了新解决方案。

Abstract: Line detection is a basic digital image processing operation used by
higher-level processing methods. Recently, transformer-based methods for line
detection have proven to be more accurate than methods based on CNNs, at the
expense of significantly lower inference speeds. As a result, video analysis
methods that require low latencies cannot benefit from current
transformer-based methods for line detection. In addition, current
transformer-based models require pretraining attention mechanisms on large
datasets (e.g., COCO or Object360). This paper develops a new transformer-based
method that is significantly faster without requiring pretraining the attention
mechanism on large datasets. We eliminate the need to pre-train the attention
mechanism using a new mechanism, Deformable Line Attention (DLA). We use the
term LINEA to refer to our new transformer-based method based on DLA. Extensive
experiments show that LINEA is significantly faster and outperforms previous
models on sAP in out-of-distribution dataset testing.

</details>


### [125] [DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving](https://arxiv.org/abs/2505.16278)
*Zhenjie Yang,Yilin Chai,Xiaosong Jia,Qifeng Li,Yuqian Shao,Xuekai Zhu,Haisheng Su,Junchi Yan*

Main category: cs.CV

TL;DR: DriveMoE是一种基于Mixture-of-Experts（MoE）架构的端到端自动驾驶框架，通过动态选择视觉输入和激活专家模块，显著提升了自动驾驶在复杂场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要处理多视角数据和复杂场景，尤其是罕见驾驶行为（如激进转弯）。MoE架构在大型语言模型中的成功表明参数专业化可提升扩展性。

Method: DriveMoE基于Drive-π0框架，引入Scene-Specialized Vision MoE和Skill-Specialized Action MoE，动态选择摄像头输入和激活专家模块。

Result: 在Bench2Drive闭环评估中，DriveMoE实现了最先进的性能。

Conclusion: DriveMoE通过结合视觉和动作MoE，有效提升了自动驾驶任务的表现，代码和模型将开源。

Abstract: End-to-end autonomous driving (E2E-AD) demands effective processing of
multi-view sensory data and robust handling of diverse and complex driving
scenarios, particularly rare maneuvers such as aggressive turns. Recent success
of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)
demonstrates that specialization of parameters enables strong scalability. In
this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a
Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is
built upon our $\pi_0$ Vision-Language-Action (VLA) baseline (originally from
the embodied AI field), called Drive-$\pi_0$. Specifically, we add Vision MoE
to Drive-$\pi_0$ by training a router to select relevant cameras according to
the driving context dynamically. This design mirrors human driving cognition,
where drivers selectively attend to crucial visual cues rather than
exhaustively processing all visual information. In addition, we add Action MoE
by training another router to activate specialized expert modules for different
driving behaviors. Through explicit behavioral specialization, DriveMoE is able
to handle diverse scenarios without suffering from modes averaging like
existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE
achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness
of combining vision and action MoE in autonomous driving tasks. We will release
our code and models of DriveMoE and Drive-$\pi_0$.

</details>


### [126] [ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay](https://arxiv.org/abs/2505.16282)
*Fanbin Lu,Zhisheng Zhong,Shu Liu,Chi-Wing Fu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了一种名为Agentic Replay Policy Optimization (ARPO)的端到端强化学习方法，用于优化基于视觉语言的GUI代理在复杂任务中的表现。通过结合Group Relative Policy Optimization (GRPO)和重放缓冲区，ARPO能够复用成功经验，并通过任务选择策略稳定训练过程。实验表明，ARPO在OSWorld基准测试中表现优异，为基于强化学习的LLM GUI代理设定了新的性能基准。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）作为交互式代理控制图形用户界面（GUIs）时，面临长时程动作序列优化和多模态反馈的挑战。现有方法在GUI代理中的应用较少，主要因稀疏奖励、延迟反馈和高成本问题。本文旨在通过强化学习优化GUI代理在复杂任务中的表现。

Method: 提出ARPO方法，结合GRPO和重放缓冲区复用成功经验，并设计任务选择策略以稳定训练。同时，比较了ARPO与离线偏好优化方法，突出策略方法在GUI环境中的优势。

Result: 在OSWorld基准测试中，ARPO表现优异，为基于强化学习的LLM GUI代理设定了新的性能基准。

Conclusion: 强化学习在训练多轮、视觉语言GUI代理处理复杂UI交互方面具有显著效果。ARPO为相关研究提供了新的方法和性能参考。

Abstract: Training large language models (LLMs) as interactive agents for controlling
graphical user interfaces (GUIs) presents a unique challenge to optimize
long-horizon action sequences with multimodal feedback from complex
environments. While recent works have advanced multi-turn reinforcement
learning (RL) for reasoning and tool-using capabilities in LLMs, their
application to GUI-based agents remains relatively underexplored due to the
difficulty of sparse rewards, delayed feedback, and high rollout costs. In this
paper, we investigate end-to-end policy optimization for vision-language-based
GUI agents with the aim of improving performance on complex, long-horizon
computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an
end-to-end RL approach that augments Group Relative Policy Optimization (GRPO)
with a replay buffer to reuse the successful experience across training
iterations. To further stabilize the training process, we propose a task
selection strategy that filters tasks based on baseline agent performance,
allowing the agent to focus on learning from informative interactions.
Additionally, we compare ARPO with offline preference optimization approaches,
highlighting the advantages of policy-based methods in GUI environments.
Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive
results, establishing a new performance baseline for LLM-based GUI agents
trained via reinforcement learning. Our findings underscore the effectiveness
of reinforcement learning for training multi-turn, vision-language GUI agents
capable of managing complex real-world UI interactions. Codes and
models:https://github.com/dvlab-research/ARPO.git.

</details>


### [127] [Efficient Prototype Consistency Learning in Medical Image Segmentation via Joint Uncertainty and Data Augmentation](https://arxiv.org/abs/2505.16283)
*Lijian Li,Yuanpeng He,Chi-Man Pun*

Main category: cs.CV

TL;DR: 论文提出了一种基于原型一致性的学习方法（EPCL-JUDA），通过联合不确定性量化和数据增强提升半监督医学图像分割中原型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，标记数据的稀缺性限制了原型的表达能力，可能导致类别嵌入的不完整表示。

Method: 结合Mean-Teacher框架，通过数据增强和不确定性量化生成高质量原型，并设计原型网络以减少内存需求。

Result: 在多个数据集上的实验表明，EPCL-JUDA优于现有方法。

Conclusion: 该方法有效提升了原型表达的语义性，为半监督医学图像分割提供了新思路。

Abstract: Recently, prototype learning has emerged in semi-supervised medical image
segmentation and achieved remarkable performance. However, the scarcity of
labeled data limits the expressiveness of prototypes in previous methods,
potentially hindering the complete representation of prototypes for class
embedding. To overcome this issue, we propose an efficient prototype
consistency learning via joint uncertainty quantification and data augmentation
(EPCL-JUDA) to enhance the semantic expression of prototypes based on the
framework of Mean-Teacher. The concatenation of original and augmented labeled
data is fed into student network to generate expressive prototypes. Then, a
joint uncertainty quantification method is devised to optimize pseudo-labels
and generate reliable prototypes for original and augmented unlabeled data
separately. High-quality global prototypes for each class are formed by fusing
labeled and unlabeled prototypes, which are utilized to generate
prototype-to-features to conduct consistency learning. Notably, a prototype
network is proposed to reduce high memory requirements brought by the
introduction of augmented data. Extensive experiments on Left Atrium,
Pancreas-NIH, Type B Aortic Dissection datasets demonstrate EPCL-JUDA's
superiority over previous state-of-the-art approaches, confirming the
effectiveness of our framework. The code will be released soon.

</details>


### [128] [Self-Classification Enhancement and Correction for Weakly Supervised Object Detection](https://arxiv.org/abs/2505.16294)
*Yufei Yin,Lechao Cheng,Wengang Zhou,Jiajun Deng,Zhou Yu,Houqiang Li*

Main category: cs.CV

TL;DR: 提出了一种新的弱监督目标检测框架，通过自分类增强模块和自分类校正算法解决多类分类任务中的歧义问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督目标检测方法在多类分类任务中存在分类歧义，未能充分利用其独特优势。

Method: 引入自分类增强模块（ICBC）和自分类校正算法，结合多类分类任务提升性能。

Result: 在VOC 2007和2012数据集上表现优异。

Conclusion: 新框架有效解决了分类歧义问题，提升了检测性能。

Abstract: In recent years, weakly supervised object detection (WSOD) has attracted much
attention due to its low labeling cost. The success of recent WSOD models is
often ascribed to the two-stage multi-class classification (MCC) task, i.e.,
multiple instance learning and online classification refinement. Despite
achieving non-trivial progresses, these methods overlook potential
classification ambiguities between these two MCC tasks and fail to leverage
their unique strengths. In this work, we introduce a novel WSOD framework to
ameliorate these two issues. For one thing, we propose a self-classification
enhancement module that integrates intra-class binary classification (ICBC) to
bridge the gap between the two distinct MCC tasks. The ICBC task enhances the
network's discrimination between positive and mis-located samples in a
class-wise manner and forges a mutually reinforcing relationship with the MCC
task. For another, we propose a self-classification correction algorithm during
inference, which combines the results of both MCC tasks to effectively reduce
the mis-classified predictions. Extensive experiments on the prevalent VOC 2007
& 2012 datasets demonstrate the superior performance of our framework.

</details>


### [129] [SAMba-UNet: Synergizing SAM2 and Mamba in UNet with Heterogeneous Aggregation for Cardiac MRI Segmentation](https://arxiv.org/abs/2505.16304)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: cs.CV

TL;DR: 提出了一种名为SAMba-UNet的双编码器架构，用于心脏MRI分割，通过动态特征融合和异构全注意力模块提升复杂病理特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动化心脏MRI分割中复杂病理特征提取的挑战。

Method: 结合SAM2、Mamba和UNet，设计动态特征融合器和异构全注意力模块，实现跨模态特征协作学习。

Result: 在ACDC数据集上，Dice系数达0.9103，HD95边界误差为1.0859 mm，显著优于现有方法。

Conclusion: 为自动化心脏疾病诊断提供了高效可靠的解决方案，代码将开源。

Abstract: To address the challenge of complex pathological feature extraction in
automated cardiac MRI segmentation, this study proposes an innovative
dual-encoder architecture named SAMba-UNet. The framework achieves cross-modal
feature collaborative learning by integrating the vision foundation model SAM2,
the state-space model Mamba, and the classical UNet. To mitigate domain
discrepancies between medical and natural images, a Dynamic Feature Fusion
Refiner is designed, which enhances small lesion feature extraction through
multi-scale pooling and a dual-path calibration mechanism across channel and
spatial dimensions. Furthermore, a Heterogeneous Omni-Attention Convergence
Module (HOACM) is introduced, combining global contextual attention with
branch-selective emphasis mechanisms to effectively fuse SAM2's local
positional semantics and Mamba's long-range dependency modeling capabilities.
Experiments on the ACDC cardiac MRI dataset demonstrate that the proposed model
achieves a Dice coefficient of 0.9103 and an HD95 boundary error of 1.0859 mm,
significantly outperforming existing methods, particularly in boundary
localization for complex pathological structures such as right ventricular
anomalies. This work provides an efficient and reliable solution for automated
cardiac disease diagnosis, and the code will be open-sourced.

</details>


### [130] [Paired and Unpaired Image to Image Translation using Generative Adversarial Networks](https://arxiv.org/abs/2505.16310)
*Gaurav Kumar,Soham Satyadharma,Harpreet Singh*

Main category: cs.CV

TL;DR: 该论文研究了基于GAN的成对和非成对图像翻译任务，使用了条件GAN和循环一致性损失，并通过定量和定性指标评估了实验结果。


<details>
  <summary>Details</summary>
Motivation: 探索图像到图像翻译任务，尤其是成对和非成对翻译，以生成具有不同风格或纹理的新图像。

Method: 使用条件GAN处理成对任务，循环一致性损失处理非成对任务，并尝试不同损失函数、Patch-GAN尺寸和模型架构。

Result: 通过定量指标（精度、召回率、FID分数）和定性分析评估了实验结果。

Conclusion: 论文展示了GAN在图像翻译任务中的有效性，并提供了实验结果的详细分析。

Abstract: Image to image translation is an active area of research in the field of
computer vision, enabling the generation of new images with different styles,
textures, or resolutions while preserving their characteristic properties.
Recent architectures leverage Generative Adversarial Networks (GANs) to
transform input images from one domain to another. In this work, we focus on
the study of both paired and unpaired image translation across multiple image
domains. For the paired task, we used a conditional GAN model, and for the
unpaired task, we trained it using cycle consistency loss. We experimented with
different types of loss functions, multiple Patch-GAN sizes, and model
architectures. New quantitative metrics - precision, recall, and FID score -
were used for analysis. In addition, a qualitative study of the results of
different experiments was conducted.

</details>


### [131] [Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings](https://arxiv.org/abs/2505.16313)
*Arjhun Swaminathan,Mete Akgün*

Main category: cs.CV

TL;DR: 论文提出了一种名为TEA的新型对抗攻击方法，利用目标图像的边缘信息生成对抗样本，显著减少了查询次数，并在低查询场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前黑盒对抗攻击方法主要依赖决策边界的几何特性，而忽略了图像本身的信息。TEA旨在通过利用目标图像的边缘信息，更高效地生成对抗样本。

Method: TEA通过提取目标图像的边缘信息，精心扰动源图像，使其在保持目标分类的同时更接近源图像。

Result: TEA在低查询设置下（查询次数减少近70%）表现优于现有方法，并为几何攻击提供了更好的初始化。

Conclusion: TEA通过结合边缘信息，显著提升了对抗攻击的效率，适用于实际应用中的黑盒场景。

Abstract: Deep neural networks for image classification remain vulnerable to
adversarial examples -- small, imperceptible perturbations that induce
misclassifications. In black-box settings, where only the final prediction is
accessible, crafting targeted attacks that aim to misclassify into a specific
target class is particularly challenging due to narrow decision regions.
Current state-of-the-art methods often exploit the geometric properties of the
decision boundary separating a source image and a target image rather than
incorporating information from the images themselves. In contrast, we propose
Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge
information from the target image to carefully perturb it, thereby producing an
adversarial image that is closer to the source image while still achieving the
desired target classification. Our approach consistently outperforms current
state-of-the-art methods across different models in low query settings (nearly
70\% fewer queries are used), a scenario especially relevant in real-world
applications with limited queries and black-box access. Furthermore, by
efficiently generating a suitable adversarial example, TEA provides an improved
target initialization for established geometry-based attacks.

</details>


### [132] [NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment](https://arxiv.org/abs/2505.16314)
*Shuhao Han,Haotian Fan,Fangyuan Kong,Wenjie Liao,Chunle Guo,Chongyi Li,Radu Timofte,Liang Li,Tao Li,Junhui Cui,Yunqiu Wang,Yang Tai,Jingwei Sun,Jianhui Sun,Xinli Yue,Tianyi Wang,Huan Hou,Junda Lu,Xinyang Huang,Zitang Zhou,Zijian Zhang,Xuhui Zheng,Xuecheng Wu,Chong Peng,Xuezhi Cao,Trong-Hieu Nguyen-Mau,Minh-Hoang Le,Minh-Khoa Le-Phan,Duy-Nam Ly,Hai-Dang Nguyen,Minh-Triet Tran,Yukang Lin,Yan Hong,Chuanbiao Song,Siyuan Li,Jun Lan,Zhichao Zhang,Xinyue Li,Wei Sun,Zicheng Zhang,Yunhao Li,Xiaohong Liu,Guangtao Zhai,Zitong Xu,Huiyu Duan,Jiarui Wang,Guangji Ma,Liu Yang,Lu Liu,Qiang Hu,Xiongkuo Min,Zichuan Wang,Zhenchen Tang,Bo Peng,Jing Dong,Fengbin Guan,Zihao Yu,Yiting Lu,Wei Luo,Xin Li,Minhao Lin,Haofeng Chen,Xuanxuan He,Kele Xu,Qisheng Xu,Zijian Gao,Tianjiao Wan,Bo-Cheng Qiu,Chih-Chung Hsu,Chia-ming Lee,Yu-Fan Lin,Bo Yu,Zehao Wang,Da Mu,Mingxiu Chen,Junkang Fang,Huamei Sun,Wending Zhao,Zhiyu Wang,Wang Liu,Weikang Yu,Puhong Duan,Bin Sun,Xudong Kang,Shutao Li,Shuai He,Lingzhi Fu,Heng Cong,Rongyu Zhang,Jiarong He,Zhishan Qiao,Yongqing Huang,Zewen Chen,Zhe Pang,Juan Wang,Jian Guo,Zhizhuo Shao,Ziyu Feng,Bing Li,Weiming Hu,Hesong Li,Dehua Liu,Zeming Liu,Qingsong Xie,Ruichen Wang,Zhihao Li,Yuqi Liang,Jianqi Bi,Jun Luo,Junfeng Yang,Can Li,Jing Fu,Hongwei Xu,Mingrui Long,Lulin Tang*

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛聚焦文本到图像生成模型的质量评估，分为对齐和结构两个赛道，吸引了大量参与者，获胜方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成模型的细粒度质量评估问题，特别是图像-文本对齐和图像结构失真检测。

Method: 挑战赛分为对齐赛道（使用EvalMuse-40K数据集）和结构赛道（使用EvalMuse-Structure数据集），参与者提交模型进行评估。

Result: 对齐赛道有371名注册者，结构赛道有211名注册者；获胜方法在两项任务中均优于基线方法。

Conclusion: 挑战赛成功推动了文本到图像生成模型质量评估的研究，获胜方法展示了卓越的预测性能。

Abstract: This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)
generation model quality assessment, which will be held in conjunction with the
New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.
The aim of this challenge is to address the fine-grained quality assessment of
text-to-image generation models. This challenge evaluates text-to-image models
from two aspects: image-text alignment and image structural distortion
detection, and is divided into the alignment track and the structural track.
The alignment track uses the EvalMuse-40K, which contains around 40K
AI-Generated Images (AIGIs) generated by 20 popular generative models. The
alignment track has a total of 371 registered participants. A total of 1,883
submissions are received in the development phase, and 507 submissions are
received in the test phase. Finally, 12 participating teams submitted their
models and fact sheets. The structure track uses the EvalMuse-Structure, which
contains 10,000 AI-Generated Images (AIGIs) with corresponding structural
distortion mask. A total of 211 participants have registered in the structure
track. A total of 1155 submissions are received in the development phase, and
487 submissions are received in the test phase. Finally, 8 participating teams
submitted their models and fact sheets. Almost all methods have achieved better
results than baseline methods, and the winning methods in both tracks have
demonstrated superior prediction performance on T2I model quality assessment.

</details>


### [133] [SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models](https://arxiv.org/abs/2505.16318)
*Hossein Khalili,Seongbin Park,Venkat Bollapragada,Nader Sehatbakhsh*

Main category: cs.CV

TL;DR: 论文提出了一种名为SuperPure的新防御策略，用于对抗对抗性补丁攻击，解决了现有方法在分布式补丁攻击和延迟问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着基于视觉的机器学习模型在自主和网络物理系统中的广泛应用，对抗性补丁攻击的威胁日益增加。现有防御方法在分布式补丁攻击和延迟问题上表现不佳。

Method: 提出了一种像素级掩码方案，结合GAN超分辨率技术逐步清除图像中的对抗补丁。

Result: SuperPure在对抗传统局部补丁攻击时平均提升20%的鲁棒性，同时提升10%的清洁准确率；对分布式补丁攻击的鲁棒性达到58%；防御延迟降低98%。

Conclusion: SuperPure在鲁棒性、准确性和延迟方面显著优于现有方法，适用于对延迟敏感的网络物理系统。

Abstract: As vision-based machine learning models are increasingly integrated into
autonomous and cyber-physical systems, concerns about (physical) adversarial
patch attacks are growing. While state-of-the-art defenses can achieve
certified robustness with minimal impact on utility against highly-concentrated
localized patch attacks, they fall short in two important areas: (i)
State-of-the-art methods are vulnerable to low-noise distributed patches where
perturbations are subtly dispersed to evade detection or masking, as shown
recently by the DorPatch attack; (ii) Achieving high robustness with
state-of-the-art methods is extremely time and resource-consuming, rendering
them impractical for latency-sensitive applications in many cyber-physical
systems.
  To address both robustness and latency issues, this paper proposes a new
defense strategy for adversarial patch attacks called SuperPure. The key
novelty is developing a pixel-wise masking scheme that is robust against both
distributed and localized patches. The masking involves leveraging a GAN-based
super-resolution scheme to gradually purify the image from adversarial patches.
Our extensive evaluations using ImageNet and two standard classifiers, ResNet
and EfficientNet, show that SuperPure advances the state-of-the-art in three
major directions: (i) it improves the robustness against conventional localized
patches by more than 20%, on average, while also improving top-1 clean accuracy
by almost 10%; (ii) It achieves 58% robustness against distributed patch
attacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It
decreases the defense end-to-end latency by over 98% compared to PatchCleanser.
Our further analysis shows that SuperPure is robust against white-box attacks
and different patch sizes. Our code is open-source.

</details>


### [134] [Efficient Motion Prompt Learning for Robust Visual Tracking](https://arxiv.org/abs/2505.16321)
*Jie Zhao,Xin Chen,Yongsheng Yuan,Michael Felsberg,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的运动提示跟踪方法，通过结合运动和视觉线索提升跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器主要依赖视觉区分性，忽视了视频数据的时序一致性。

Method: 设计了运动编码器、融合解码器和自适应权重机制，将长期运动轨迹编码到视觉嵌入空间。

Result: 在七个挑战性跟踪基准上显著提升了基于视觉的跟踪器的鲁棒性，训练成本低且速度损失小。

Conclusion: 运动提示模块能有效增强现有跟踪器的性能，具有高效性和易用性。

Abstract: Due to the challenges of processing temporal information, most trackers
depend solely on visual discriminability and overlook the unique temporal
coherence of video data. In this paper, we propose a lightweight and
plug-and-play motion prompt tracking method. It can be easily integrated into
existing vision-based trackers to build a joint tracking framework leveraging
both motion and vision cues, thereby achieving robust tracking through
efficient prompt learning. A motion encoder with three different positional
encodings is proposed to encode the long-term motion trajectory into the visual
embedding space, while a fusion decoder and an adaptive weight mechanism are
designed to dynamically fuse visual and motion features. We integrate our
motion module into three different trackers with five models in total.
Experiments on seven challenging tracking benchmarks demonstrate that the
proposed motion module significantly improves the robustness of vision-based
trackers, with minimal training costs and negligible speed sacrifice. Code is
available at https://github.com/zj5559/Motion-Prompt-Tracking.

</details>


### [135] [TensorAR: Refinement is All You Need in Autoregressive Image Generation](https://arxiv.org/abs/2505.16324)
*Cheng Cheng,Lin Song,Yicheng Xiao,Yuxin Chen,Xuchong Zhang,Hongbin Sun,Ying Shan*

Main category: cs.CV

TL;DR: TensorAR是一种新的自回归范式，通过将图像生成从下一个令牌预测改为下一个张量预测，实现了对先前生成内容的迭代优化。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）图像生成器缺乏修正先前预测的机制，限制了生成质量。TensorAR旨在解决这一问题。

Method: TensorAR通过滑动生成重叠的图像块（张量），并采用离散张量噪声方案防止训练中的信息泄漏。

Result: 实验表明，TensorAR显著提升了自回归模型的生成性能。

Conclusion: TensorAR作为一种即插即用模块，为现有AR模型提供了有效的改进方案。

Abstract: Autoregressive (AR) image generators offer a language-model-friendly approach
to image generation by predicting discrete image tokens in a causal sequence.
However, unlike diffusion models, AR models lack a mechanism to refine previous
predictions, limiting their generation quality. In this paper, we introduce
TensorAR, a new AR paradigm that reformulates image generation from next-token
prediction to next-tensor prediction. By generating overlapping windows of
image patches (tensors) in a sliding fashion, TensorAR enables iterative
refinement of previously generated content. To prevent information leakage
during training, we propose a discrete tensor noising scheme, which perturbs
input tokens via codebook-indexed noise. TensorAR is implemented as a
plug-and-play module compatible with existing AR models. Extensive experiments
on LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly
improves the generation performance of autoregressive models.

</details>


### [136] [Panoptic Captioning: Seeking An Equivalency Bridge for Image and Text](https://arxiv.org/abs/2505.16334)
*Kun-Yu Lin,Hongjun Wang,Weining Ren,Kai Han*

Main category: cs.CV

TL;DR: 论文提出了一种名为全景描述（panoptic captioning）的新任务，旨在生成图像的最小文本等价描述。通过提出PancapEngine数据引擎和PancapChain方法，解决了现有多模态大语言模型（MLLMs）在此任务上的局限性，并在实验中超越了GPT-4o等模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在全景描述任务上表现有限，需要一种更全面的方法来生成包含图像中所有实体、位置、属性和关系的文本描述。

Method: 提出了PancapEngine数据引擎生成高质量数据，以及PancapChain方法将任务分解为多阶段逐步生成描述。

Result: PancapChain-13B模型在实验中超越了InternVL-2.5-78B、GPT-4o和Gemini-2.0-Pro等先进模型。

Conclusion: PancapEngine和PancapChain方法有效提升了全景描述任务的性能，并提供了新的评估指标和测试集。

Abstract: This work introduces panoptic captioning, a novel task striving to seek the
minimum text equivalence of images. We take the first step towards panoptic
captioning by formulating it as a task of generating a comprehensive textual
description for an image, which encapsulates all entities, their respective
locations and attributes, relationships among entities, as well as global image
state.Through an extensive evaluation, our work reveals that state-of-the-art
Multi-modal Large Language Models (MLLMs) have limited performance in solving
panoptic captioning. To address this, we propose an effective data engine named
PancapEngine to produce high-quality data and a novel method named PancapChain
to improve panoptic captioning. Specifically, our PancapEngine first detects
diverse categories of entities in images by an elaborate detection suite, and
then generates required panoptic captions using entity-aware prompts.
Additionally, our PancapChain explicitly decouples the challenging panoptic
captioning task into multiple stages and generates panoptic captions step by
step. More importantly, we contribute a comprehensive metric named PancapScore
and a human-curated test set for reliable model evaluation.Experiments show
that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like
InternVL-2.5-78B and even surpass proprietary models like GPT-4o and
Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method.
Project page: https://visual-ai.github.io/pancap/

</details>


### [137] [FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design](https://arxiv.org/abs/2505.16335)
*Renjie Wei,Songqiang Xu,Qingyu Guo,Meng Li*

Main category: cs.CV

TL;DR: FPQVAR是一种高效的浮点量化框架，通过算法和硬件协同设计，显著降低了VAR模型的内存和计算成本，同时提升了图像生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: VAR模型在图像生成中表现优异，但参数规模和计算成本限制了其在边缘设备上的部署。FPQVAR旨在解决这一问题。

Method: 提出双格式量化、组级Hadamard变换和GHT感知可学习变换，同时设计了低比特FP量化器和乘法器，并开发了基于FPGA的加速器。

Result: 在4位量化下，FID从10.83降至3.58，IS从175.9提升至241.5；6位量化性能接近FP16模型。FPGA加速器吞吐量提升3.1倍，能效优于整数加速器和GPU基线。

Conclusion: FPQVAR通过算法和硬件协同优化，显著提升了VAR模型的效率和性能，适用于边缘设备部署。

Abstract: Visual autoregressive (VAR) modeling has marked a paradigm shift in image
generation from next-token prediction to next-scale prediction. VAR predicts a
set of tokens at each step from coarse to fine scale, leading to better image
quality and faster inference speed compared to existing diffusion models.
However, the large parameter size and computation cost hinder its deployment on
edge devices. To reduce the memory and computation cost, we propose FPQVAR, an
efficient post-training floating-point (FP) quantization framework for VAR
featuring algorithm and hardware co-design. At the algorithm level, we first
identify the challenges of quantizing VAR. To address them, we propose Dual
Format Quantization for the highly imbalanced input activation. We further
propose Group-wise Hadamard Transformation and GHT-Aware Learnable
Transformation to address the time-varying outlier channels. At the hardware
level, we design the first low-bit FP quantizer and multiplier with lookup
tables on FPGA and propose the first FPGA-based VAR accelerator featuring
low-bit FP computation and an elaborate two-level pipeline. Extensive
experiments show that compared to the state-of-the-art quantization method, our
proposed FPQVAR significantly improves Fr\'echet Inception Distance (FID) from
10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit
quantization. FPQVAR also significantly improves the performance of 6-bit
quantized VAR, bringing it on par with the FP16 model. Our accelerator on
AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x
higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x
higher energy efficiency compared to the integer-based accelerator and GPU
baseline, respectively.

</details>


### [138] [Fusion of Foundation and Vision Transformer Model Features for Dermatoscopic Image Classification](https://arxiv.org/abs/2505.16338)
*Amirreza Mahbod,Rupert Ecker,Ramona Woitek*

Main category: cs.CV

TL;DR: 研究比较了皮肤病专用基础模型PanDerm与两种ViT架构在皮肤病变分类任务中的表现，发现PanDerm结合MLP分类器性能与微调后的Swin Transformer相当，融合两者预测可进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变的准确分类对皮肤癌的诊断和治疗至关重要，研究旨在探索基础模型在皮肤病分类中的实用性。

Method: 使用PanDerm提取冻结特征，结合MLP、XGBoost和TabNet分类器进行非线性探测；对ViT架构进行全微调。

Result: 在HAM10000和MSKCC数据集上，PanDerm+MLP表现与微调Swin Transformer相当，融合两者预测可进一步提升性能。

Conclusion: 未来将探索更多基础模型、微调策略和高级融合技术。

Abstract: Accurate classification of skin lesions from dermatoscopic images is
essential for diagnosis and treatment of skin cancer. In this study, we
investigate the utility of a dermatology-specific foundation model, PanDerm, in
comparison with two Vision Transformer (ViT) architectures (ViT base and Swin
Transformer V2 base) for the task of skin lesion classification. Using frozen
features extracted from PanDerm, we apply non-linear probing with three
different classifiers, namely, multi-layer perceptron (MLP), XGBoost, and
TabNet. For the ViT-based models, we perform full fine-tuning to optimize
classification performance. Our experiments on the HAM10000 and MSKCC datasets
demonstrate that the PanDerm-based MLP model performs comparably to the
fine-tuned Swin transformer model, while fusion of PanDerm and Swin Transformer
predictions leads to further performance improvements. Future work will explore
additional foundation models, fine-tuning strategies, and advanced fusion
techniques.

</details>


### [139] [Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation](https://arxiv.org/abs/2505.16360)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Thomas Oberlin*

Main category: cs.CV

TL;DR: 论文提出两种基于扩散模型的语义一致风格迁移方法（CACTI和CACTIF），用于提升合成数据训练的视觉模型在真实图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据训练的语义分割模型在真实图像上表现不佳的问题，尤其是在标注数据稀缺的恶劣条件下。

Method: 提出CACTI（基于语义类的自适应实例归一化和交叉注意力）和CACTIF（进一步通过特征相似性过滤交叉注意力图），实现语义边界和结构一致性的风格迁移。

Result: 在GTA5到Cityscapes/ACDC的实验中，生成图像质量更高（FID分数更低），内容保留更好。

Conclusion: 类感知的扩散风格迁移能有效缩小合成到真实的领域差距，推动鲁棒感知系统的发展。

Abstract: Semantic segmentation models trained on synthetic data often perform poorly
on real-world images due to domain gaps, particularly in adverse conditions
where labeled data is scarce. Yet, recent foundation models enable to generate
realistic images without any training. This paper proposes to leverage such
diffusion models to improve the performance of vision models when learned on
synthetic data. We introduce two novel techniques for semantically consistent
style transfer using diffusion models: Class-wise Adaptive Instance
Normalization and Cross-Attention (CACTI) and its extension with selective
attention Filtering (CACTIF). CACTI applies statistical normalization
selectively based on semantic classes, while CACTIF further filters
cross-attention maps based on feature similarity, preventing artifacts in
regions with weak cross-attention correspondences. Our methods transfer style
characteristics while preserving semantic boundaries and structural coherence,
unlike approaches that apply global transformations or generate content without
constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target
domains show that our approach produces higher quality images with lower FID
scores and better content preservation. Our work demonstrates that class-aware
diffusion-based style transfer effectively bridges the synthetic-to-real domain
gap even with minimal target domain data, advancing robust perception systems
for challenging real-world applications. The source code is available at:
https://github.com/echigot/cactif.

</details>


### [140] [Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition](https://arxiv.org/abs/2505.16372)
*Feng Liu,Bingyu Nan,Xuezhong Qian,Xiaolan Fu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TSFmicro的新框架，通过融合时空特征和多模态技术，显著提高了微表情识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 微表情因其短暂性和局部性难以准确识别，现有方法的准确率仅为50%。

Method: 结合Retention Network和基于transformer的网络，提出并行时空融合方法，在高维特征空间中融合时空信息。

Result: TSFmicro在三个公认的微表情数据集上表现优于现有方法。

Conclusion: TSFmicro框架通过高效的时空特征融合，显著提升了微表情识别的性能。

Abstract: When emotions are repressed, an individual's true feelings may be revealed
through micro-expressions. Consequently, micro-expressions are regarded as a
genuine source of insight into an individual's authentic emotions. However, the
transient and highly localised nature of micro-expressions poses a significant
challenge to their accurate recognition, with the accuracy rate of
micro-expression recognition being as low as 50%, even for professionals. In
order to address these challenges, it is necessary to explore the field of
dynamic micro expression recognition (DMER) using multimodal fusion techniques,
with special attention to the diverse fusion of temporal and spatial modal
features. In this paper, we propose a novel Temporal and Spatial feature Fusion
framework for DMER (TSFmicro). This framework integrates a Retention Network
(RetNet) and a transformer-based DMER network, with the objective of efficient
micro-expression recognition through the capture and fusion of temporal and
spatial relations. Meanwhile, we propose a novel parallel time-space fusion
method from the perspective of modal fusion, which fuses spatio-temporal
information in high-dimensional feature space, resulting in complementary
"where-how" relationships at the semantic level and providing richer semantic
information for the model. The experimental results demonstrate the superior
performance of the TSFmicro method in comparison to other contemporary
state-of-the-art methods. This is evidenced by its effectiveness on three
well-recognised micro-expression datasets.

</details>


### [141] [DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos](https://arxiv.org/abs/2505.16376)
*Zijia Lu,A S M Iftekhar,Gaurav Mittal,Tianjian Meng,Xiawei Wang,Cheng Zhao,Rohith Kukkala,Ehsan Elhamifar,Mei Chen*

Main category: cs.CV

TL;DR: DeCafNet提出了一种高效的长视频时间定位方法，通过“委托-征服”策略减少计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因计算成本高难以扩展，需处理大量视频片段。

Method: 引入轻量级编码器（sidekick encoder）提取特征并生成显著性图，筛选关键片段供专家编码器处理；通过DeCaf-Grounder统一多尺度特征。

Result: 在两个基准数据集上，DeCafNet计算量减少47%，性能优于现有方法。

Conclusion: DeCafNet在效率和性能上均达到新SOTA，为长视频时间定位提供了高效解决方案。

Abstract: Long Video Temporal Grounding (LVTG) aims at identifying specific moments
within lengthy videos based on user-provided text queries for effective content
retrieval. The approach taken by existing methods of dividing video into clips
and processing each clip via a full-scale expert encoder is challenging to
scale due to prohibitive computational costs of processing a large number of
clips in long videos. To address this issue, we introduce DeCafNet, an approach
employing ``delegate-and-conquer'' strategy to achieve computation efficiency
without sacrificing grounding performance. DeCafNet introduces a sidekick
encoder that performs dense feature extraction over all video clips in a
resource-efficient manner, while generating a saliency map to identify the most
relevant clips for full processing by the expert encoder. To effectively
leverage features from sidekick and expert encoders that exist at different
temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines
them via query-aware temporal aggregation and multi-scale temporal refinement
for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate
that DeCafNet reduces computation by up to 47\% while still outperforming
existing methods, establishing a new state-of-the-art for LTVG in terms of both
efficiency and performance. Our code is available at
https://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.

</details>


### [142] [MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module](https://arxiv.org/abs/2505.16384)
*Haoming Huang,Musen Zhang,Jianxin Yang,Zhen Li,Jinkai Li,Yao Guo*

Main category: cs.CV

TL;DR: 论文提出了一种名为MAGE的多任务架构，用于估计6自由度（6-DoF）的视线信息，适用于真实世界的人机交互（HRI）。该方法通过编码面部图像的方向和位置特征，并结合高效的校准模块，解决了现有方法在3D空间中视线分析的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视线估计方法仅能预测视线方向或屏幕上的注视点（PoG），无法提供全面的6自由度视线信息，且因个体眼部形状和结构的差异导致泛化能力受限。

Method: 提出MAGE架构，通过多任务学习编码面部图像的方向和位置特征，并设计Easy-Calibration模块，利用个体特定数据微调模型，无需屏幕即可高效实现。

Result: 在MPIIFaceGaze、EYEDIAP和自建IMRGaze数据集上，MAGE取得了最先进的性能。

Conclusion: MAGE通过多任务架构和高效校准模块，显著提升了6自由度视线估计的准确性和泛化能力，为人机交互提供了更全面的视线分析工具。

Abstract: Eye gaze can provide rich information on human psychological activities, and
has garnered significant attention in the field of Human-Robot Interaction
(HRI). However, existing gaze estimation methods merely predict either the gaze
direction or the Point-of-Gaze (PoG) on the screen, failing to provide
sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze
analysis in 3D space. Moreover, the variations of eye shape and structure among
individuals also impede the generalization capability of these methods. In this
study, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an
efficient calibration module, to predict the 6-DoF gaze information that is
applicable for the real-word HRI. Our basic model encodes both the directional
and positional features from facial images, and predicts gaze results with
dedicated information flow and multiple decoders. To reduce the impact of
individual variations, we propose a novel calibration module, namely
Easy-Calibration, to fine-tune the basic model with subject-specific data,
which is efficient to implement without the need of a screen. Experimental
results demonstrate that our method achieves state-of-the-art performance on
the public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.

</details>


### [143] [Sketchy Bounding-box Supervision for 3D Instance Segmentation](https://arxiv.org/abs/2505.16399)
*Qian Deng,Le Hui,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sketchy-3DIS的弱监督3D实例分割框架，通过联合学习伪标签生成器和分割器，在粗糙边界框监督下提升性能。


<details>
  <summary>Details</summary>
Motivation: 在弱监督3D实例分割中，边界框监督虽减少了点级标注的需求，但实际应用中获取精确边界框仍具挑战性。因此，论文探索了不精确的边界框（称为粗糙边界框），以提升分割性能。

Method: 1. 提出自适应框到点伪标签生成器，为重叠部分的点分配正确实例标签；2. 设计粗到细实例分割器，先预测粗实例，再学习细实例；3. 通过联合训练逐步生成高质量实例。

Result: 在ScanNetV2和S3DIS基准测试中达到最先进性能，甚至优于部分全监督方法。

Conclusion: Sketchy-3DIS框架在粗糙边界框监督下有效提升了3D实例分割性能，展示了弱监督方法的潜力。

Abstract: Bounding box supervision has gained considerable attention in weakly
supervised 3D instance segmentation. While this approach alleviates the need
for extensive point-level annotations, obtaining accurate bounding boxes in
practical applications remains challenging. To this end, we explore the
inaccurate bounding box, named sketchy bounding box, which is imitated through
perturbing ground truth bounding box by adding scaling, translation, and
rotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance
segmentation framework, which jointly learns pseudo labeler and segmentator to
improve the performance under the sketchy bounding-box supervisions.
Specifically, we first propose an adaptive box-to-point pseudo labeler that
adaptively learns to assign points located in the overlapped parts between two
sketchy bounding boxes to the correct instance, resulting in compact and pure
pseudo instance labels. Then, we present a coarse-to-fine instance segmentator
that first predicts coarse instances from the entire point cloud and then
learns fine instances based on the region of coarse instances. Finally, by
using the pseudo instance labels to supervise the instance segmentator, we can
gradually generate high-quality instances through joint training. Extensive
experiments show that our method achieves state-of-the-art performance on both
the ScanNetV2 and S3DIS benchmarks, and even outperforms several fully
supervised methods using sketchy bounding boxes. Code is available at
https://github.com/dengq7/Sketchy-3DIS.

</details>


### [144] [AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems](https://arxiv.org/abs/2505.16402)
*Yuanhao Huang,Yilong Ren,Jinlei Wang,Lujia Huo,Xuesong Bai,Jinchuan Zhang,Haiyan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种统一的对抗训练框架，用于生成2D和3D对抗样本，并通过非刚性表面建模和3D匹配机制增强样本真实性。实验表明，该方法能有效误导目标检测模型，并具有强鲁棒性和迁移性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统易受对抗样本攻击，导致安全事故，如何在物理世界中生成有效的对抗样本并评估检测系统是一大挑战。

Method: 提出统一的联合对抗训练框架，结合非刚性表面建模和3D匹配机制，生成更真实的对抗样本。

Result: 在数字和物理环境中，生成的对抗纹理能有效误导目标检测模型，并在多角度攻击、不同光照和距离下表现出强鲁棒性和迁移性。

Conclusion: 该方法为解决现实场景中的对抗样本问题提供了有效解决方案，并展示了良好的实际应用潜力。

Abstract: Autonomous vehicles are typical complex intelligent systems with artificial
intelligence at their core. However, perception methods based on deep learning
are extremely vulnerable to adversarial samples, resulting in safety accidents.
How to generate effective adversarial examples in the physical world and
evaluate object detection systems is a huge challenge. In this study, we
propose a unified joint adversarial training framework for both 2D and 3D
samples to address the challenges of intra-class diversity and environmental
variations in real-world scenarios. Building upon this framework, we introduce
an adversarial sample reality enhancement approach that incorporates non-rigid
surface modeling and a realistic 3D matching mechanism. We compare with 5
advanced adversarial patches and evaluate their attack performance on 8 object
detecotrs, including single-stage, two-stage, and transformer-based models.
Extensive experiment results in digital and physical environments demonstrate
that the adversarial textures generated by our method can effectively mislead
the target detection model. Moreover, proposed method demonstrates excellent
robustness and transferability under multi-angle attacks, varying lighting
conditions, and different distance in the physical world. The demo video and
code can be obtained at https://github.com/Huangyh98/AdvReal.git.

</details>


### [145] [Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression](https://arxiv.org/abs/2505.16411)
*Sreetama Sarkar,Yue Che,Alex Gavin,Peter A. Beerel,Souvik Kundu*

Main category: cs.CV

TL;DR: SPIN是一种任务无关的注意力引导头抑制策略，可减少大型视觉语言模型（LVLM）的幻觉现象，且不增加计算或延迟开销。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在生成文本时容易出现与视觉内容不符的幻觉现象，现有方法虽能减少幻觉但显著增加延迟。

Method: 通过分析发现幻觉与特定注意力头相关，SPIN选择性地抑制对图像标记注意力低的头，保留前K个头。

Result: 在视觉问答和图像描述任务中，SPIN将幻觉分数降低至2.7倍，F1保持不变，吞吐量提高1.8倍。

Conclusion: SPIN是一种高效且无需额外开销的幻觉抑制方法，适用于多种任务。

Abstract: Despite their remarkable progress in multimodal understanding tasks, large
vision language models (LVLMs) often suffer from "hallucinations", generating
texts misaligned with the visual context. Existing methods aimed at reducing
hallucinations through inference time intervention incur a significant increase
in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided
head suppression strategy that can be seamlessly integrated during inference,
without incurring any significant compute or latency overhead. We investigate
whether hallucination in LVLMs can be linked to specific model components. Our
analysis suggests that hallucinations can be attributed to a dynamic subset of
attention heads in each layer. Leveraging this insight, for each text query
token, we selectively suppress attention heads that exhibit low attention to
image tokens, keeping the top-K attention heads intact. Extensive evaluations
on visual question answering and image description tasks demonstrate the
efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining
F1, and improving throughput by 1.8x compared to existing alternatives. Code is
available at https://github.com/YUECHE77/SPIN.

</details>


### [146] [Pose-invariant face recognition via feature-space pose frontalization](https://arxiv.org/abs/2505.16412)
*Nikolay Stanishev,Yuhang Lu,Touradj Ebrahimi*

Main category: cs.CV

TL;DR: 提出了一种在特征空间中进行人脸正面化和识别的新方法，通过特征空间姿态正面化模块（FSPFM）和新的训练范式，显著提升了姿态不变人脸识别的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI人脸识别系统在姿态不变人脸识别中的挑战，特别是将侧面人脸与正面数据库匹配的问题。

Method: 提出FSPFM模块将任意角度的侧面图像转换为正面特征，并设计包含预训练和注意力引导微调的新训练范式。

Result: 在五个流行的人脸识别基准测试中，该方法不仅优于现有技术，还在其他标准场景中保持优越性能。

Conclusion: 该方法在姿态不变人脸识别任务中表现出色，同时具备广泛适用性。

Abstract: Pose-invariant face recognition has become a challenging problem for modern
AI-based face recognition systems. It aims at matching a profile face captured
in the wild with a frontal face registered in a database. Existing methods
perform face frontalization via either generative models or learning a pose
robust feature representation. In this paper, a new method is presented to
perform face frontalization and recognition within the feature space. First, a
novel feature space pose frontalization module (FSPFM) is proposed to transform
profile images with arbitrary angles into frontal counterparts. Second, a new
training paradigm is proposed to maximize the potential of FSPFM and boost its
performance. The latter consists of a pre-training and an attention-guided
fine-tuning stage. Moreover, extensive experiments have been conducted on five
popular face recognition benchmarks. Results show that not only our method
outperforms the state-of-the-art in the pose-invariant face recognition task
but also maintains superior performance in other standard scenarios.

</details>


### [147] [Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models](https://arxiv.org/abs/2505.16416)
*Chengcheng Wang,Jianyuan Guo,Hongguang Li,Yuchuan Tian,Ying Nie,Chang Xu,Kai Han*

Main category: cs.CV

TL;DR: 论文提出了一种新的位置编码方法Circle-RoPE，用于解决视觉语言模型中跨模态位置偏差问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Rotary Position Embedding (RoPE)在大型语言模型中表现良好，但在视觉语言模型中会引入跨模态位置偏差，导致虚假对齐。

Method: 提出了Per-Token Distance (PTD)度量标准，并设计了Circle-RoPE编码方案，将图像令牌映射到与文本令牌正交的圆形轨迹上，同时采用交错层策略。

Result: 实验表明，Circle-RoPE能有效减少跨模态偏差，同时保留图像的空间信息。

Conclusion: Circle-RoPE为视觉语言模型提供了更鲁棒和灵活的位置编码框架。

Abstract: Rotary Position Embedding (RoPE) is a widely adopted technique for encoding
relative positional information in large language models (LLMs). However, when
extended to large vision-language models (LVLMs), its variants introduce
unintended cross-modal positional biases. Specifically, they enforce relative
positional dependencies between text token indices and image tokens, causing
spurious alignments. This issue arises because image tokens representing the
same content but located at different spatial positions are assigned distinct
positional biases, leading to inconsistent cross-modal associations. To address
this, we propose Per-Token Distance (PTD) - a simple yet effective metric for
quantifying the independence of positional encodings across modalities.
Informed by this analysis, we introduce Circle-RoPE, a novel encoding scheme
that maps image token indices onto a circular trajectory orthogonal to the
linear path of text token indices, forming a cone-like structure. This
configuration ensures that each text token maintains an equal distance to all
image tokens, reducing artificial cross-modal biases while preserving
intra-image spatial information. To further enhance performance, we propose a
staggered layer strategy that applies different RoPE variants across layers.
This design leverages the complementary strengths of each RoPE variant, thereby
enhancing the model's overall performance. Our experimental results demonstrate
that our method effectively preserves spatial information from images while
reducing relative positional bias, offering a more robust and flexible
positional encoding framework for LVLMs. The code is available at
[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).

</details>


### [148] [Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment](https://arxiv.org/abs/2505.16419)
*Soh Takahashi,Masaru Sasaki,Ken Takeda,Masafumi Oizumi*

Main category: cs.CV

TL;DR: 该研究通过无监督对齐方法比较人类与模型的对象表征，发现CLIP模型在细粒度和粗粒度上与人类表征匹配度高，而自监督模型仅能反映粗粒度类别结构。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何获取对象表征，以及不同学习范式（如监督、自监督和CLIP）训练的模型是否能模拟人类表征的细粒度和粗粒度细节。

Method: 采用基于Gromov-Wasserstein最优传输的无监督对齐方法，比较人类与模型的对象表征。

Result: CLIP模型在细粒度和粗粒度上均与人类表征匹配良好，自监督模型仅能反映粗粒度类别结构。

Conclusion: 语言信息对获取精确对象表征至关重要，自监督学习在捕捉粗粒度类别结构方面具有潜力。

Abstract: The learning mechanisms by which humans acquire internal representations of
objects are not fully understood. Deep neural networks (DNNs) have emerged as a
useful tool for investigating this question, as they have internal
representations similar to those of humans as a byproduct of optimizing their
objective functions. While previous studies have shown that models trained with
various learning paradigms - such as supervised, self-supervised, and CLIP -
acquire human-like representations, it remains unclear whether their similarity
to human representations is primarily at a coarse category level or extends to
finer details. Here, we employ an unsupervised alignment method based on
Gromov-Wasserstein Optimal Transport to compare human and model object
representations at both fine-grained and coarse-grained levels. The unique
feature of this method compared to conventional representational similarity
analysis is that it estimates optimal fine-grained mappings between the
representation of each object in human and model representations. We used this
unsupervised alignment method to assess the extent to which the representation
of each object in humans is correctly mapped to the corresponding
representation of the same object in models. Using human similarity judgments
of 1,854 objects from the THINGS dataset, we find that models trained with CLIP
consistently achieve strong fine- and coarse-grained matching with human object
representations. In contrast, self-supervised models showed limited matching at
both fine- and coarse-grained levels, but still formed object clusters that
reflected human coarse category structure. Our results offer new insights into
the role of linguistic information in acquiring precise object representations
and the potential of self-supervised learning to capture coarse categorical
structures.

</details>


### [149] [Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach](https://arxiv.org/abs/2505.16422)
*Xiaoran Yin,Xu Luo,Hao Wu,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: FPWC框架通过自然语言理解和结构化推理，利用任务导向的世界模型进行前瞻性规划，显著提升了移动设备自动控制的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因依赖即时观察而导致的决策次优问题，提升全局环境理解能力。

Method: 提出FPWC框架，结合自然语言理解和世界模型，通过迭代规划生成可执行代码的前瞻性动作。

Result: 在模拟环境和真实设备上实验，任务成功率相对提升44.4%。

Conclusion: FPWC通过世界模型和前瞻性规划，显著优于现有方法，适用于复杂任务控制。

Abstract: The automatic control of mobile devices is essential for efficiently
performing complex tasks that involve multiple sequential steps. However, these
tasks pose significant challenges due to the limited environmental information
available at each step, primarily through visual observations. As a result,
current approaches, which typically rely on reactive policies, focus solely on
immediate observations and often lead to suboptimal decision-making. To address
this problem, we propose \textbf{Foresighted Planning with World Model-Driven
Code Execution (FPWC)},a framework that prioritizes natural language
understanding and structured reasoning to enhance the agent's global
understanding of the environment by developing a task-oriented, refinable
\emph{world model} at the outset of the task. Foresighted actions are
subsequently generated through iterative planning within this world model,
executed in the form of executable code. Extensive experiments conducted in
simulated environments and on real mobile devices demonstrate that our method
outperforms previous approaches, particularly achieving a 44.4\% relative
improvement in task success rate compared to the state-of-the-art in the
simulated environment. Code and demo are provided in the supplementary
material.

</details>


### [150] [Ranked Entropy Minimization for Continual Test-Time Adaptation](https://arxiv.org/abs/2505.16441)
*Jisu Han,Jaemin Na,Wonjun Hwang*

Main category: cs.CV

TL;DR: 提出了一种基于排名熵最小化的方法，用于解决持续测试时间适应中的稳定性问题，并通过渐进掩码策略优化模型预测。


<details>
  <summary>Details</summary>
Motivation: 熵最小化方法在持续测试时间适应中易导致模型崩溃，需要一种更稳定的解决方案。

Method: 采用排名熵最小化和渐进掩码策略，逐步对齐不同预测难度下的概率分布。

Result: 在多个基准测试中验证了方法的有效性。

Conclusion: 排名熵最小化方法显著提升了持续测试时间适应的稳定性。

Abstract: Test-time adaptation aims to adapt to realistic environments in an online
manner by learning during test time. Entropy minimization has emerged as a
principal strategy for test-time adaptation due to its efficiency and
adaptability. Nevertheless, it remains underexplored in continual test-time
adaptation, where stability is more important. We observe that the entropy
minimization method often suffers from model collapse, where the model
converges to predicting a single class for all images due to a trivial
solution. We propose ranked entropy minimization to mitigate the stability
problem of the entropy minimization method and extend its applicability to
continuous scenarios. Our approach explicitly structures the prediction
difficulty through a progressive masking strategy. Specifically, it gradually
aligns the model's probability distributions across different levels of
prediction difficulty while preserving the rank order of entropy. The proposed
method is extensively evaluated across various benchmarks, demonstrating its
effectiveness through empirical results. Our code is available at
https://github.com/pilsHan/rem

</details>


### [151] [MAFE R-CNN: Selecting More Samples to Learn Category-aware Features for Small Object Detection](https://arxiv.org/abs/2505.16442)
*Yichen Li,Qiankun Liu,Zhenchao Jin,Jiuzhe Wei,Jing Nie,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MAFE R-CNN的方法，通过多线索样本选择和类别感知特征增强机制，解决了小目标检测中的特征学习和样本选择问题。


<details>
  <summary>Details</summary>
Motivation: 小目标检测在复杂环境中一直是一个挑战，主要由于检测器难以有效学习小目标的区分性特征，以及训练过程中高质量小目标样本的选择困难。

Method: MAFE R-CNN包含两个关键组件：多线索样本选择策略（MCSS）和类别感知特征增强机制（CFEM）。MCSS利用IoU距离、预测类别置信度和真实区域大小作为样本选择的线索；CFEM通过类别感知记忆模块增强目标特征表示。

Result: 在大规模小目标数据集SODA上的实验验证了该方法的有效性。

Conclusion: MAFE R-CNN通过改进样本选择和特征增强，显著提升了小目标检测的性能。

Abstract: Small object detection in intricate environments has consistently represented
a major challenge in the field of object detection. In this paper, we identify
that this difficulty stems from the detectors' inability to effectively learn
discriminative features for objects of small size, compounded by the complexity
of selecting high-quality small object samples during training, which motivates
the proposal of the Multi-Clue Assignment and Feature Enhancement
R-CNN.Specifically, MAFE R-CNN integrates two pivotal components.The first is
the Multi-Clue Sample Selection (MCSS) strategy, in which the Intersection over
Union (IoU) distance, predicted category confidence, and ground truth region
sizes are leveraged as informative clues in the sample selection process. This
methodology facilitates the selection of diverse positive samples and ensures a
balanced distribution of object sizes during training, thereby promoting
effective model learning.The second is the Category-aware Feature Enhancement
Mechanism (CFEM), where we propose a simple yet effective category-aware memory
module to explore the relationships among object features. Subsequently, we
enhance the object feature representation by facilitating the interaction
between category-aware features and candidate box features.Comprehensive
experiments conducted on the large-scale small object dataset SODA validate the
effectiveness of the proposed method. The code will be made publicly available.

</details>


### [152] [TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition](https://arxiv.org/abs/2505.16447)
*Oliver Grainge,Michael Milford,Indu Bodala,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: TAT-VPR是一种三元量化Transformer模型，通过动态权衡精度与效率，为视觉SLAM闭环提供灵活的计算控制。


<details>
  <summary>Details</summary>
Motivation: 解决视觉SLAM闭环中精度与计算效率之间的动态权衡问题，适用于微型无人机和嵌入式SLAM系统。

Method: 融合三元权重与学习的激活稀疏门，实现运行时计算量控制；采用两阶段蒸馏流程保持描述符质量。

Result: 模型运行时计算量可减少40%且不降低性能（Recall@1），匹配当前最优的定位精度。

Conclusion: TAT-VPR在保持高精度的同时，显著提升了计算效率，适用于资源受限的应用场景。

Abstract: TAT-VPR is a ternary-quantized transformer that brings dynamic
accuracy-efficiency trade-offs to visual SLAM loop-closure. By fusing ternary
weights with a learned activation-sparsity gate, the model can control
computation by up to 40% at run-time without degrading performance (Recall@1).
The proposed two-stage distillation pipeline preserves descriptor quality,
letting it run on micro-UAV and embedded SLAM stacks while matching
state-of-the-art localization accuracy.

</details>


### [153] [CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI](https://arxiv.org/abs/2505.16452)
*Mohamed S. Elmahdy,Marius Staring,Patrick J. H. de Koning,Samer Alabed,Mahan Salehi,Faisal Alandejani,Michael Sharkey,Ziad Aldabbagh,Andrew J. Swift,Rob J. van der Geest*

Main category: cs.CV

TL;DR: 提出了一种端到端深度学习模型，联合估计心脏cine-MRI图像的群组配准和分割，以提高心脏功能评估的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 左心室射血分数（LVEF）和心肌应变是评估心脏功能的重要指标，但现有方法通常分开处理配准和分割任务，限制了评估效果。

Method: 提出了一种解剖引导的深度群组配准网络（Deep GW），在374名受试者的四腔视图cine-MRI图像上训练和验证。

Result: 与传统的elastix配准和两种基于深度学习的方法相比，所提模型性能提升且计算时间大幅减少。

Conclusion: 该模型为心脏功能的全面评估提供了一种高效且准确的解决方案。

Abstract: Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.

</details>


### [154] [MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM](https://arxiv.org/abs/2505.16456)
*Siwei Meng,Yawei Luo,Ping Liu*

Main category: cs.CV

TL;DR: MAGIC是一个无需训练的动态3D生成框架，通过结合预训练的图像到视频扩散模型和LLM推理，从静态图像生成物理合理的动态内容。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型注重视觉真实感但忽略物理合理性，且依赖大规模标注数据或模型微调，计算和数据负担重。

Method: MAGIC整合预训练扩散模型和LLM推理，通过置信驱动的反馈循环生成物理合理的动态视频，并引入可微分MPM模拟器。

Result: MAGIC在推理准确性和时间一致性上优于现有物理感知生成方法和视频扩散模型。

Conclusion: MAGIC提供了一种无需训练的高效方法，填补了视觉与物理动态之间的差距。

Abstract: Recent advances in static 3D generation have intensified the demand for
physically consistent dynamic 3D content. However, existing video generation
models, including diffusion-based methods, often prioritize visual realism
while neglecting physical plausibility, resulting in implausible object
dynamics. Prior approaches for physics-aware dynamic generation typically rely
on large-scale annotated datasets or extensive model fine-tuning, which imposes
significant computational and data collection burdens and limits scalability
across scenarios. To address these challenges, we present MAGIC, a
training-free framework for single-image physical property inference and
dynamic generation, integrating pretrained image-to-video diffusion models with
iterative LLM-based reasoning. Our framework generates motion-rich videos from
a static image and closes the visual-to-physical gap through a
confidence-driven LLM feedback loop that adaptively steers the diffusion model
toward physics-relevant motion. To translate visual dynamics into controllable
physical behavior, we further introduce a differentiable MPM simulator
operating directly on 3D Gaussians reconstructed from the single image,
enabling physically grounded, simulation-ready outputs without any supervision
or model tuning. Experiments show that MAGIC outperforms existing physics-aware
generative methods in inference accuracy and achieves greater temporal
coherence than state-of-the-art video diffusion models.

</details>


### [155] [AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer](https://arxiv.org/abs/2505.16463)
*Jiquan Shan,Junxiao Wang,Lifeng Zhao,Liang Cai,Hongyuan Zhang,Ioannis Liritzis*

Main category: cs.CV

TL;DR: AnchorFormer通过引入锚点标记减少计算复杂度，从O(n²)降至O(mn)，同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的全局自注意力计算复杂度高且部分标记对任务无帮助。

Method: 使用锚点标记学习关键信息，通过二分注意力降低复杂度，并扩展至分类、检测和分割任务。

Result: 在ImageNet分类上准确率提升9.0%或FLOPs减少46.7%，COCO检测mAP提升81.3%。

Conclusion: AnchorFormer在高效性和性能上均优于现有基线。

Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.

</details>


### [156] [Consistent World Models via Foresight Diffusion](https://arxiv.org/abs/2505.16474)
*Yu Zhang,Xingzhuo Guo,Haoran Xu,Mingsheng Long*

Main category: cs.CV

TL;DR: ForeDiff提出了一种基于扩散的世界建模框架，通过解耦条件理解和目标去噪，提高了样本一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但在世界建模中因样本一致性问题受限，主要原因是条件理解和目标去噪的耦合。

Method: ForeDiff引入独立的确定性预测流处理条件输入，并利用预训练预测器提取信息表示以指导生成。

Result: 在机器人视频预测和科学时空预测实验中，ForeDiff在预测准确性和样本一致性上优于基线方法。

Conclusion: ForeDiff为基于扩散的世界模型提供了一个有前景的方向。

Abstract: Diffusion and flow-based models have enabled significant progress in
generation tasks across various modalities and have recently found applications
in world modeling. However, unlike typical generation tasks that encourage
sample diversity, world models entail different sources of uncertainty and
require consistent samples aligned with the ground-truth trajectory, which is a
limitation we empirically observe in diffusion models. We argue that a key
bottleneck in learning consistent diffusion-based world models lies in the
suboptimal predictive ability, which we attribute to the entanglement of
condition understanding and target denoising within shared architectures and
co-training schemes. To address this, we propose Foresight Diffusion
(ForeDiff), a diffusion-based world modeling framework that enhances
consistency by decoupling condition understanding from target denoising.
ForeDiff incorporates a separate deterministic predictive stream to process
conditioning inputs independently of the denoising stream, and further
leverages a pretrained predictor to extract informative representations that
guide generation. Extensive experiments on robot video prediction and
scientific spatiotemporal forecasting show that ForeDiff improves both
predictive accuracy and sample consistency over strong baselines, offering a
promising direction for diffusion-based world models.

</details>


### [157] [Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration](https://arxiv.org/abs/2505.16479)
*Yuetong Liu,Yunqiu Xu,Yang Wei,Xiuli Bi,Bin Xiao*

Main category: cs.CV

TL;DR: 论文提出了一种解决夜间多天气条件下图像恢复的方法，并贡献了一个新数据集AllWeatherNight和框架ClearNight，通过Retinex先验和动态天气感知方法实现高效恢复。


<details>
  <summary>Details</summary>
Motivation: 夜间多天气条件下的图像恢复是一个实际但研究不足的问题，因为多种天气和光照效应常同时存在。

Method: 提出ClearNight框架，结合Retinex先验和动态天气感知方法，通过双先验提取和自适应候选单元选择实现恢复。

Result: ClearNight在合成和真实图像上均达到最先进性能，实验验证了数据集和方法的有效性。

Conclusion: 论文成功解决了夜间多天气图像恢复问题，并展示了数据集和框架的实用价值。

Abstract: Restoring nighttime images affected by multiple adverse weather conditions is
a practical yet under-explored research problem, as multiple weather conditions
often coexist in the real world alongside various lighting effects at night.
This paper first explores the challenging multi-weather nighttime image
restoration task, where various types of weather degradations are intertwined
with flare effects. To support the research, we contribute the AllWeatherNight
dataset, featuring large-scale high-quality nighttime images with diverse
compositional degradations, synthesized using our introduced illumination-aware
degradation generation. Moreover, we present ClearNight, a unified nighttime
image restoration framework, which effectively removes complex degradations in
one go. Specifically, ClearNight extracts Retinex-based dual priors and
explicitly guides the network to focus on uneven illumination regions and
intrinsic texture contents respectively, thereby enhancing restoration
effectiveness in nighttime scenarios. In order to better represent the common
and unique characters of multiple weather degradations, we introduce a
weather-aware dynamic specific-commonality collaboration method, which
identifies weather degradations and adaptively selects optimal candidate units
associated with specific weather types. Our ClearNight achieves
state-of-the-art performance on both synthetic and real-world images.
Comprehensive ablation experiments validate the necessity of AllWeatherNight
dataset as well as the effectiveness of ClearNight. Project page:
https://henlyta.github.io/ClearNight/mainpage.html

</details>


### [158] [InspectionV3: Enhancing Tobacco Quality Assessment with Deep Convolutional Neural Networks for Automated Workshop Management](https://arxiv.org/abs/2505.16485)
*Yao Wei,Muhammad Usman,Hazrat Bilal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度卷积神经网络的自动化烟草分级系统InspectionV3，解决了传统方法成本高、效率低的问题，实现了高精度分级。


<details>
  <summary>Details</summary>
Motivation: 烟草加工中存在分级效率低、成本高和质量不稳定等问题，传统人工方法不可靠。

Method: 采用定制化的深度卷积神经网络架构，利用21,113张标记图像训练模型，结合预处理和批量归一化技术，分析烟草的颜色、成熟度和干燥度等特征。

Result: 系统在准确性（97%）、精确度（95%）和召回率（95%）等指标上表现优异，验证了其实际应用的可行性。

Conclusion: InspectionV3通过自动化分级和实时分析，显著提升了烟草加工的效率和决策优化能力。

Abstract: The problems that tobacco workshops encounter include poor curing,
inconsistencies in supplies, irregular scheduling, and a lack of oversight, all
of which drive up expenses and worse quality. Large quantities make manual
examination costly, sluggish, and unreliable. Deep convolutional neural
networks have recently made strides in capabilities that transcend those of
conventional methods. To effectively enhance them, nevertheless, extensive
customization is needed to account for subtle variations in tobacco grade. This
study introduces InspectionV3, an integrated solution for automated flue-cured
tobacco grading that makes use of a customized deep convolutional neural
network architecture. A scope that covers color, maturity, and curing
subtleties is established via a labelled dataset consisting of 21,113 images
spanning 20 quality classes. Expert annotators performed preprocessing on the
tobacco leaf images, including cleaning, labelling, and augmentation.
Multi-layer CNN factors use batch normalization to describe domain properties
like as permeability and moisture spots, and so account for the subtleties of
the workshop. Its expertise lies in converting visual patterns into useful
information for enhancing workflow. Fast notifications are made possible by
real-time, on-the-spot grading that matches human expertise. Images-powered
analytics dashboards facilitate the tracking of yield projections, inventories,
bottlenecks, and the optimization of data-driven choices. More labelled images
are assimilated after further retraining, improving representational capacities
and enabling adaptations for seasonal variability. Metrics demonstrate 97%
accuracy, 95% precision and recall, 96% F1-score and AUC, 95% specificity;
validating real-world viability.

</details>


### [159] [ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation](https://arxiv.org/abs/2505.16495)
*Lingfeng Wang,Hualing Lin,Senda Chen,Tao Wang,Changxu Cheng,Yangyang Zhong,Dong Zheng,Wuyue Zhao*

Main category: cs.CV

TL;DR: ALTo是一种自适应长度分词器，用于改进多模态大语言模型（MLLM）的掩码生成，通过动态调整注意力分配提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM因固定分词表示而受限，无法像人类一样根据对象复杂度自适应分配注意力。

Method: 设计了新的分词长度预测器、长度正则化项和可微分分词块策略，并集成到ALToLLM模型中，通过GRPO优化掩码质量与效率的权衡。

Result: ALToLLM在主流分割基准测试中实现了最佳性能，且具有自适应分词成本。

Conclusion: ALToLLM通过自适应分词策略显著提升了MLLM的掩码生成能力，代码和模型已开源。

Abstract: While humans effortlessly draw visual objects and shapes by adaptively
allocating attention based on their complexity, existing multimodal large
language models (MLLMs) remain constrained by rigid token representations.
Bridging this gap, we propose ALTo, an adaptive length tokenizer for
autoregressive mask generation. To achieve this, a novel token length predictor
is designed, along with a length regularization term and a differentiable token
chunking strategy. We further build ALToLLM that seamlessly integrates ALTo
into MLLM. Preferences on the trade-offs between mask quality and efficiency is
implemented by group relative policy optimization (GRPO). Experiments
demonstrate that ALToLLM achieves state-of-the-art performance with adaptive
token cost on popular segmentation benchmarks. Code and models are released at
https://github.com/yayafengzi/ALToLLM.

</details>


### [160] [Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection](https://arxiv.org/abs/2505.16512)
*Jiaxin Liu,Jia Wang,Saihui Hou,Min Ren,Huijia Wu,Zhaofeng He*

Main category: cs.CV

TL;DR: 论文介绍了DigiFakeAV数据集和DigiShield检测方法，以应对扩散模型生成的高真实性数字人视频带来的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的高真实性数字人视频对公共安全构成威胁，现有检测方法难以应对。

Method: 提出DigiFakeAV数据集，包含60,000个视频，并基于时空和跨模态融合提出DigiShield检测方法。

Result: 用户研究表明伪造视频与真实视频混淆率达68%，DigiShield在多个数据集上表现优异。

Conclusion: DigiShield通过细粒度分析有效识别合成视频的隐蔽伪影，为检测高真实性伪造视频提供了新方法。

Abstract: In recent years, the rapid development of deepfake technology has given rise
to an emerging and serious threat to public security: diffusion model-based
digital human generation. Unlike traditional face manipulation methods, such
models can generate highly realistic videos with consistency through multimodal
control signals. Their flexibility and covertness pose severe challenges to
existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the
first large-scale multimodal digital human forgery dataset based on diffusion
models. Employing five latest digital human generation methods (Sonic, Hallo,
etc.) and voice cloning method, we systematically produce a dataset comprising
60,000 videos (8.4 million frames), covering multiple nationalities, skin
tones, genders, and real-world scenarios, significantly enhancing data
diversity and realism. User studies show that the confusion rate between forged
and real videos reaches 68%, and existing state-of-the-art (SOTA) detection
models exhibit large drops in AUC values on DigiFakeAV, highlighting the
challenge of the dataset. To address this problem, we further propose
DigiShield, a detection baseline based on spatiotemporal and cross-modal
fusion. By jointly modeling the 3D spatiotemporal features of videos and the
semantic-acoustic features of audio, DigiShield achieves SOTA performance on
both the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method
effectively identifies covert artifacts through fine-grained analysis of the
temporal evolution of facial features in synthetic videos.

</details>


### [161] [Detailed Evaluation of Modern Machine Learning Approaches for Optic Plastics Sorting](https://arxiv.org/abs/2505.16513)
*Vaishali Maheshkar,Aadarsh Anantha Ramakrishnan,Charuvahan Adhivarahan,Karthik Dantu*

Main category: cs.CV

TL;DR: 论文探讨了光学识别技术在塑料回收自动分拣中的局限性，指出其依赖颜色和形状等物理特性导致实际效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前塑料回收率低（仅8%），主要因污染、经济激励不足和技术难题。自动化分拣是关键，但现有光学识别方法在现实场景中效果有限。

Method: 研究通过构建20,000+图像数据集，使用公共和自定义机器学习管道评估光学识别能力，并利用Grad-CAM、显著性图和混淆矩阵分析模型行为。

Result: 光学识别方法在实际塑料分拣中效果有限，因其依赖颜色和形状等物理特性。

Conclusion: 光学识别技术需进一步改进以应对现实分拣挑战，可能需结合其他技术手段。

Abstract: According to the EPA, only 25% of waste is recycled, and just 60% of U.S.
municipalities offer curbside recycling. Plastics fare worse, with a recycling
rate of only 8%; an additional 16% is incinerated, while the remaining 76% ends
up in landfills. The low plastic recycling rate stems from contamination, poor
economic incentives, and technical difficulties, making efficient recycling a
challenge. To improve recovery, automated sorting plays a critical role.
Companies like AMP Robotics and Greyparrot utilize optical systems for sorting,
while Materials Recovery Facilities (MRFs) employ Near-Infrared (NIR) sensors
to detect plastic types.
  Modern optical sorting uses advances in computer vision such as object
recognition and instance segmentation, powered by machine learning. Two-stage
detectors like Mask R-CNN use region proposals and classification with deep
backbones like ResNet. Single-stage detectors like YOLO handle detection in one
pass, trading some accuracy for speed. While such methods excel under ideal
conditions with a large volume of labeled training data, challenges arise in
realistic scenarios, emphasizing the need to further examine the efficacy of
optic detection for automated sorting.
  In this study, we compiled novel datasets totaling 20,000+ images from varied
sources. Using both public and custom machine learning pipelines, we assessed
the capabilities and limitations of optical recognition for sorting. Grad-CAM,
saliency maps, and confusion matrices were employed to interpret model
behavior. We perform this analysis on our custom trained models from the
compiled datasets. To conclude, our findings are that optic recognition methods
have limited success in accurate sorting of real-world plastics at MRFs,
primarily because they rely on physical properties such as color and shape.

</details>


### [162] [CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving](https://arxiv.org/abs/2505.16524)
*Huitong Yang,Zhuoxiao Chen,Fengyi Zhang,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: CodeMerge是一种轻量级模型合并框架，通过在紧凑的潜在空间中操作，解决了现有测试时适应方法在高方差任务中的不稳定性和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 动态和不可预测的测试条件下，保持稳健的3D感知是自动驾驶系统的关键挑战，现有方法因不稳定优化和高计算成本而表现不佳。

Method: CodeMerge利用低维指纹和键值码本表示模型检查点，使用岭杠杆得分计算合并系数，实现高效模型组合。

Result: 在nuScenes-C和nuScenes-to-KITTI基准测试中，分别提升了14.9% NDS和7.6% mAP，并支持下游任务。

Conclusion: CodeMerge提供了一种轻量且高效的模型合并方案，显著提升了3D检测性能，适用于自动驾驶系统。

Abstract: Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.

</details>


### [163] [Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction](https://arxiv.org/abs/2505.16533)
*Jiacong Chen,Qingyu Mao,Youneng Bao,Xiandong Meng,Fanyang Meng,Ronggang Wang,Yongsheng Liang*

Main category: cs.CV

TL;DR: 提出了一种名为ComGS的新型紧凑高斯流框架，通过关键点驱动运动表示显著减少存储需求，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有在线自由视点视频重建方法因逐点建模导致存储需求过高，未能充分利用运动特性。

Method: 利用运动局部性和一致性，通过关键点驱动运动表示建模高斯点运动，采用自适应运动驱动机制和误差感知校正策略。

Result: ComGS实现了存储需求显著降低（比3DGStream减少159倍，比QUEEN减少14倍），同时保持视觉保真度和渲染速度。

Conclusion: ComGS为动态场景的高效存储和高质量重建提供了创新解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient
paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers
rapid responsiveness and immersive experiences. However, existing online
methods face challenge in prohibitive storage requirements primarily due to
point-wise modeling that fails to exploit the motion properties. To address
this limitation, we propose a novel Compact Gaussian Streaming (ComGS)
framework, leveraging the locality and consistency of motion in dynamic scene,
that models object-consistent Gaussian point motion through keypoint-driven
motion representation. By transmitting only the keypoint attributes, this
framework provides a more storage-efficient solution. Specifically, we first
identify a sparse set of motion-sensitive keypoints localized within motion
regions using a viewspace gradient difference strategy. Equipped with these
keypoints, we propose an adaptive motion-driven mechanism that predicts a
spatial influence field for propagating keypoint motion to neighboring Gaussian
points with similar motion. Moreover, ComGS adopts an error-aware correction
strategy for key frame reconstruction that selectively refines erroneous
regions and mitigates error accumulation without unnecessary overhead. Overall,
ComGS achieves a remarkable storage reduction of over 159 X compared to
3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining
competitive visual fidelity and rendering speed. Our code will be released.

</details>


### [164] [SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion](https://arxiv.org/abs/2505.16535)
*Asrar Alruwayqi*

Main category: cs.CV

TL;DR: 提出了一种动态3D场景重建的新框架，结合了显式三平面变形场、基于球谐函数的视图条件化辐射场和时序感知潜在扩散先验，实现了高效时空表示。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖MLP建模运动，效率低且泛化能力不足，需改进动态场景重建的效率和鲁棒性。

Method: 使用三平面变形场编码4D场景，通过SH注意力机制改进渲染，引入潜在扩散模块增强时空一致性。

Result: 在合成基准测试中表现优异，视觉质量、时序一致性和稀疏视图输入鲁棒性均超越现有方法。

Conclusion: 新框架通过显式变形和潜在扩散显著提升了动态场景重建的性能和泛化能力。

Abstract: We present a novel framework for dynamic 3D scene reconstruction that
integrates three key components: an explicit tri-plane deformation field, a
view-conditioned canonical radiance field with spherical harmonics (SH)
attention, and a temporally-aware latent diffusion prior. Our method encodes 4D
scenes using three orthogonal 2D feature planes that evolve over time, enabling
efficient and compact spatiotemporal representation. These features are
explicitly warped into a canonical space via a deformation offset field,
eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured
SH-based rendering head that synthesizes view-dependent color via attention
over learned frequency bands improving both interpretability and rendering
efficiency. To further enhance fidelity and temporal consistency, we introduce
a transformer-guided latent diffusion module that refines the tri-plane and
deformation features in a compressed latent space. This generative module
denoises scene representations under ambiguous or out-of-distribution (OOD)
motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained
independently, and then fine-tuned jointly with the full pipeline using a
combination of image reconstruction, diffusion denoising, and temporal
consistency losses. We demonstrate state-of-the-art results on synthetic
benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian
Splatting in visual quality, temporal coherence, and robustness to sparse-view
dynamic inputs.

</details>


### [165] [TextureSAM: Towards a Texture Aware Foundation Model for Segmentation](https://arxiv.org/abs/2505.16540)
*Inbal Cohen,Boaz Meivar,Peihan Tu,Shai Avidan,Gal Oren*

Main category: cs.CV

TL;DR: TextureSAM是一种新的纹理感知基础模型，通过纹理增强技术改进SAM模型，在纹理主导的场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: SAM模型在对象分割任务中表现优异，但主要基于形状而非纹理特征，限制了其在纹理定义边界的领域（如医学影像、材料分类等）的应用。

Method: 采用纹理增强技术对ADE20K数据集进行纹理修改，通过微调训练TextureSAM，使其更关注纹理特征。

Result: TextureSAM在自然和合成纹理数据集上的分割性能显著优于SAM-2（分别提升0.2和0.18 mIoU）。

Conclusion: TextureSAM有效解决了SAM模型的形状偏差问题，为纹理主导的分割任务提供了更优解决方案。

Abstract: Segment Anything Models (SAM) have achieved remarkable success in object
segmentation tasks across diverse datasets. However, these models are
predominantly trained on large-scale semantic segmentation datasets, which
introduce a bias toward object shape rather than texture cues in the image.
This limitation is critical in domains such as medical imaging, material
classification, and remote sensing, where texture changes define object
boundaries. In this study, we investigate SAM's bias toward semantics over
textures and introduce a new texture-aware foundation model, TextureSAM, which
performs superior segmentation in texture-dominant scenarios. To achieve this,
we employ a novel fine-tuning approach that incorporates texture augmentation
techniques, incrementally modifying training images to emphasize texture
features. By leveraging a novel texture-alternation of the ADE20K dataset, we
guide TextureSAM to prioritize texture-defined regions, thereby mitigating the
inherent shape bias present in the original SAM model. Our extensive
experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both
natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation
datasets. The code and texture-augmented dataset will be publicly available.

</details>


### [166] [Auto-nnU-Net: Towards Automated Medical Image Segmentation](https://arxiv.org/abs/2505.16561)
*Jannis Becktepe,Leona Hennig,Steffen Oeltze-Jafra,Marius Lindauer*

Main category: cs.CV

TL;DR: Auto-nnU-Net是一个基于nnU-Net的全自动化医学图像分割框架，通过超参数优化（HPO）、神经架构搜索（NAS）和分层NAS（HNAS）提升性能，同时提出Regularized PriorBand平衡计算资源与模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有nnU-Net框架受限于固定超参数和启发式设计，无法充分优化模型性能，且医学场景中计算资源有限。

Method: 提出Auto-nnU-Net，结合HPO、NAS和HNAS，并引入Regularized PriorBand优化资源分配。

Result: 在Medical Segmentation Decathlon的10个数据集中，6个性能显著提升，其余持平，同时保持计算资源可控。

Conclusion: Auto-nnU-Net在提升分割性能的同时，解决了医学场景中的资源限制问题，具有实际应用价值。

Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ
segmentation, each with its own challenges in finding the best segmentation
model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many
aspects of model configuration but remains constrained by fixed hyperparameters
and heuristic design choices. As a full-AutoML framework for MIS, we propose
Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization
(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).
Additionally, we propose Regularized PriorBand to balance model accuracy with
the computational resources required for training, addressing the resource
constraints often faced in real-world medical settings that limit the
feasibility of extensive training procedures. We evaluate our approach across
diverse MIS datasets from the well-established Medical Segmentation Decathlon,
analyzing the impact of AutoML techniques on segmentation performance,
computational efficiency, and model design choices. The results demonstrate
that our AutoML approach substantially improves the segmentation performance of
nnU-Net on 6 out of 10 datasets and is on par on the other datasets while
maintaining practical resource requirements. Our code is available at
https://github.com/LUH-AI/AutonnUNet.

</details>


### [167] [M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion](https://arxiv.org/abs/2505.16565)
*Nina Shvetsova,Goutam Bhat,Prune Truong,Hilde Kuehne,Federico Tombari*

Main category: cs.CV

TL;DR: 提出了一种用于单目到立体视频转换的新架构，通过改进的SVD模型生成高质量右视图，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单目到立体视频转换问题，提升右视图生成质量。

Method: 扩展SVD模型，利用左视图、变形右视图和遮挡掩码作为输入，改进注意力层以利用邻帧信息进行修复。

Result: 在用户研究中平均排名1.43，速度比第二名快6倍。

Conclusion: 该方法在质量和效率上均优于现有技术。

Abstract: We tackle the problem of monocular-to-stereo video conversion and propose a
novel architecture for inpainting and refinement of the warped right view
obtained by depth-based reprojection of the input left view. We extend the
Stable Video Diffusion (SVD) model to utilize the input left video, the warped
right video, and the disocclusion masks as conditioning input to generate a
high-quality right camera view. In order to effectively exploit information
from neighboring frames for inpainting, we modify the attention layers in SVD
to compute full attention for discoccluded pixels. Our model is trained to
generate the right view video in an end-to-end manner by minimizing image space
losses to ensure high-quality generation. Our approach outperforms previous
state-of-the-art methods, obtaining an average rank of 1.43 among the 4
compared methods in a user study, while being 6x faster than the second placed
method.

</details>


### [168] [Temporal Object Captioning for Street Scene Videos from LiDAR Tracks](https://arxiv.org/abs/2505.16594)
*Vignesh Gopinathan,Urs Zimmermann,Michael Arnold,Matthias Rottmann*

Main category: cs.CV

TL;DR: 论文提出了一种基于LiDAR的自动视频字幕生成方法，专注于交通参与者的时间动态，通过规则系统和模板字幕提升模型对时间语义的理解。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕模型在时间语义捕获方面存在不足，尤其是在高级驾驶辅助系统（ADAS）中，缺乏对时间动态的深入理解。

Method: 使用基于规则的系统从目标轨迹中提取车道位置和相对运动等细节，结合模板生成字幕，并训练SwinBERT模型。

Result: 实验表明，该方法在三个数据集上均显著提升了模型对时间语义的理解能力。

Conclusion: 结合LiDAR的字幕监督能有效减少当前先进模型中的视觉/静态偏差，增强时间理解能力。

Abstract: Video captioning models have seen notable advancements in recent years,
especially with regard to their ability to capture temporal information. While
many research efforts have focused on architectural advancements, such as
temporal attention mechanisms, there remains a notable gap in understanding how
models capture and utilize temporal semantics for effective temporal feature
extraction, especially in the context of Advanced Driver Assistance Systems. We
propose an automated LiDAR-based captioning procedure that focuses on the
temporal dynamics of traffic participants. Our approach uses a rule-based
system to extract essential details such as lane position and relative motion
from object tracks, followed by a template-based caption generation. Our
findings show that training SwinBERT, a video captioning model, using only
front camera images and supervised with our template-based captions,
specifically designed to encapsulate fine-grained temporal behavior, leads to
improved temporal understanding consistently across three datasets. In
conclusion, our results clearly demonstrate that integrating LiDAR-based
caption supervision significantly enhances temporal understanding, effectively
addressing and reducing the inherent visual/static biases prevalent in current
state-of-the-art model architectures.

</details>


### [169] [Decoupled Geometric Parameterization and its Application in Deep Homography Estimation](https://arxiv.org/abs/2505.16599)
*Yao Huang,Si-Yuan Cao,Yaqing Ding,Hao Yin,Shibin Xie,Shuting Wang,Zhijun Fang,Jiachun Wang,Shen Cai,Junchi Yan,Shuhan Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于相似-核-相似（SKS）分解的平面单应性几何参数化方法，取代了传统的四角点位置偏移参数化，提高了几何可解释性并简化了计算。


<details>
  <summary>Details</summary>
Motivation: 传统的四角点位置偏移参数化缺乏几何可解释性，且需要解线性系统来计算单应矩阵。

Method: 利用SKS分解将单应性参数化为两组独立的几何参数（相似变换和核变换），并推导了核变换参数与角度偏移的线性关系。

Result: 提出的参数化方法通过矩阵乘法直接估计单应性，无需解线性系统，性能与四角点偏移方法相当。

Conclusion: 新方法在保持性能的同时，提高了几何可解释性和计算效率。

Abstract: Planar homography, with eight degrees of freedom (DOFs), is fundamental in
numerous computer vision tasks. While the positional offsets of four corners
are widely adopted (especially in neural network predictions), this
parameterization lacks geometric interpretability and typically requires
solving a linear system to compute the homography matrix. This paper presents a
novel geometric parameterization of homographies, leveraging the
similarity-kernel-similarity (SKS) decomposition for projective
transformations. Two independent sets of four geometric parameters are
decoupled: one for a similarity transformation and the other for the kernel
transformation. Additionally, the geometric interpretation linearly relating
the four kernel transformation parameters to angular offsets is derived. Our
proposed parameterization allows for direct homography estimation through
matrix multiplication, eliminating the need for solving a linear system, and
achieves performance comparable to the four-corner positional offsets in deep
homography estimation.

</details>


### [170] [MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation](https://arxiv.org/abs/2505.16602)
*Bohan Zhou,Yi Zhan,Zhongbin Zhang,Zongqing Lu*

Main category: cs.CV

TL;DR: MEgoHand是一个多模态框架，通过结合视觉语言模型和深度估计，生成物理合理的手-物体交互动作，显著降低了误差并提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视角不稳定、遮挡和噪声运动下的局限性，以及预定义3D物体先验和文本提示模糊的问题。

Method: 采用双层架构：高层使用视觉语言模型和深度估计器推断运动先验，低层通过DiT流匹配策略生成细粒度轨迹。

Result: 在多个数据集上验证，手腕平移误差降低86.9%，关节旋转误差降低34.1%。

Conclusion: MEgoHand能准确建模手部关节结构，并在多样化场景中表现出强泛化能力。

Abstract: Egocentric hand-object motion generation is crucial for immersive AR/VR and
robotic imitation but remains challenging due to unstable viewpoints,
self-occlusions, perspective distortion, and noisy ego-motion. Existing methods
rely on predefined 3D object priors, limiting generalization to novel objects,
which restricts their generalizability to novel objects. Meanwhile, recent
multimodal approaches suffer from ambiguous generation from abstract textual
cues, intricate pipelines for modeling 3D hand-object correlation, and
compounding errors in open-loop prediction. We propose MEgoHand, a multimodal
framework that synthesizes physically plausible hand-object interactions from
egocentric RGB, text, and initial hand pose. MEgoHand introduces a bi-level
architecture: a high-level "cerebrum" leverages a vision language model (VLM)
to infer motion priors from visual-textual context and a monocular depth
estimator for object-agnostic spatial reasoning, while a low-level DiT-based
flow-matching policy generates fine-grained trajectories with temporal
orthogonal filtering to enhance stability. To address dataset inconsistency, we
design a dataset curation paradigm with an Inverse MANO Retargeting Network and
Virtual RGB-D Renderer, curating a unified dataset of 3.35M RGB-D frames, 24K
interactions, and 1.2K objects. Extensive experiments across five in-domain and
two cross-domain datasets demonstrate the effectiveness of MEgoHand, achieving
substantial reductions in wrist translation error (86.9%) and joint rotation
error (34.1%), highlighting its capacity to accurately model fine-grained hand
joint structures and generalize robustly across diverse scenarios.

</details>


### [171] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
*Francesco Dalla Serra,Patrick Schrempf,Chaoyang Wang,Zaiqiao Meng,Fani Deligianni,Alison Q. O'Neil*

Main category: cs.CV

TL;DR: 提出了一种新的胸部X光视觉问答方法，整合放射报告提升性能，支持单图像和图像差异问题，并采用两步模型（报告生成和答案生成）实现最佳效果。


<details>
  <summary>Details</summary>
Motivation: 解决胸部X光视觉问答中单图像和图像差异问题的挑战，探索如何利用放射报告进一步提升模型性能。

Method: 提出统一方法处理两类问题，采用自回归生成答案，并引入两步模型（报告生成和答案生成），利用放射报告作为额外输入。

Result: 在Medical-Diff-VQA数据集上实现最佳性能，证明放射报告能显著提升答案生成效果。

Conclusion: 整合放射报告的两步模型显著提升了胸部X光视觉问答任务的性能，为未来研究提供了新方向。

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [172] [Background Matters: A Cross-view Bidirectional Modeling Framework for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2505.16625)
*Luyang Cao,Jianwei Li,Yinghuan Shi*

Main category: cs.CV

TL;DR: 论文提出了一种名为CVBM的半监督医学图像分割框架，通过显式建模背景区域提升前景分割性能，并在多个数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前半监督医学图像分割方法主要关注前景建模，忽略了背景建模的潜在优势。研究表明，背景建模的高置信度预测能提升前景建模的置信度。

Method: 提出CVBM框架，引入背景建模作为辅助视角，并通过双向一致性机制确保前景预测与背景引导预测的对齐。

Result: 在LA、Pancreas、ACDC和HRF数据集上表现优异，Pancreas数据集上仅用20%标注数据即超越全监督方法（DSC: 84.57% vs. 83.89%）。

Conclusion: CVBM通过背景建模和双向一致性机制显著提升了半监督医学图像分割的性能，证明了背景建模的重要性。

Abstract: Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data
to reduce reliance on manually annotated images. However, current SOTA
approaches predominantly focus on foreground-oriented modeling (i.e.,
segmenting only the foreground region) and have largely overlooked the
potential benefits of explicitly modeling the background region. Our study
theoretically and empirically demonstrates that highly certain predictions in
background modeling enhance the confidence of corresponding foreground
modeling. Building on this insight, we propose the Cross-view Bidirectional
Modeling (CVBM) framework, which introduces a novel perspective by
incorporating background modeling to improve foreground modeling performance.
Within CVBM, background modeling serves as an auxiliary perspective, providing
complementary supervisory signals to enhance the confidence of the foreground
model. Additionally, CVBM introduces an innovative bidirectional consistency
mechanism, which ensures mutual alignment between foreground predictions and
background-guided predictions. Extensive experiments demonstrate that our
approach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets.
Notably, on the Pancreas dataset, CVBM outperforms fully supervised methods
(i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data.
Our code is publicly available at https://github.com/caoluyang0830/CVBM.git.

</details>


### [173] [SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding](https://arxiv.org/abs/2505.16630)
*Sushant Gautam,Cise Midoglu,Vajira Thambawita,Michael A. Riegler,Pål Halvorsen,Mubarak Shah*

Main category: cs.CV

TL;DR: SoccerChat是一个多模态对话AI框架，整合视觉和文本数据以提升足球视频理解能力，在动作分类和裁判决策任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统足球分析方法依赖孤立数据流，无法全面捕捉比赛动态，SoccerChat旨在通过多模态整合解决这一问题。

Method: 利用SoccerNet数据集（含球衣颜色注释和ASR转录），通过结构化视频指令数据集微调SoccerChat，实现比赛理解、事件分类和裁判决策。

Result: SoccerChat在动作分类和裁判决策任务中表现优异，展示了多模态整合在足球分析中的重要性。

Conclusion: 多模态整合推动了足球分析的进步，为交互式和可解释的AI驱动体育分析奠定了基础。

Abstract: The integration of artificial intelligence in sports analytics has
transformed soccer video understanding, enabling real-time, automated insights
into complex game dynamics. Traditional approaches rely on isolated data
streams, limiting their effectiveness in capturing the full context of a match.
To address this, we introduce SoccerChat, a multimodal conversational AI
framework that integrates visual and textual data for enhanced soccer video
comprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey
color annotations and automatic speech recognition (ASR) transcripts,
SoccerChat is fine-tuned on a structured video instruction dataset to
facilitate accurate game understanding, event classification, and referee
decision making. We benchmark SoccerChat on action classification and referee
decision-making tasks, demonstrating its performance in general soccer event
comprehension while maintaining competitive accuracy in referee decision
making. Our findings highlight the importance of multimodal integration in
advancing soccer analytics, paving the way for more interactive and explainable
AI-driven sports analysis. https://github.com/simula/SoccerChat

</details>


### [174] [Towards Texture- And Shape-Independent 3D Keypoint Estimation in Birds](https://arxiv.org/abs/2505.16633)
*Valentin Schmuker,Alex Hoi Hang Chan,Bastian Goldluecke,Urs Waldmann*

Main category: cs.CV

TL;DR: 提出了一种纹理无关的方法来估计和跟踪多只鸽子的3D关节位置，扩展了3D-MuPPET框架，通过分割生成个体轮廓并估计2D关键点，最终实现3D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有的3D-MuPPET框架依赖纹理信息，限制了其适用性。本文旨在开发一种纹理无关的方法，提高通用性。

Method: 使用分割方法生成个体轮廓，估计2D关键点，通过三角测量推断3D姿态，并在后续帧中跟踪2D身份。

Result: 纹理无关方法达到与纹理依赖方法相当的精度，并初步验证了在其他鸟类上的适用性。

Conclusion: 该方法为开发更鲁棒和准确的纹理无关姿态估计框架奠定了基础。

Abstract: In this paper, we present a texture-independent approach to estimate and
track 3D joint positions of multiple pigeons. For this purpose, we build upon
the existing 3D-MuPPET framework, which estimates and tracks the 3D poses of up
to 10 pigeons using a multi-view camera setup. We extend this framework by
using a segmentation method that generates silhouettes of the individuals,
which are then used to estimate 2D keypoints. Following 3D-MuPPET, these 2D
keypoints are triangulated to infer 3D poses, and identities are matched in the
first frame and tracked in 2D across subsequent frames. Our proposed
texture-independent approach achieves comparable accuracy to the original
texture-dependent 3D-MuPPET framework. Additionally, we explore our approach's
applicability to other bird species. To do that, we infer the 2D joint
positions of four bird species without additional fine-tuning the model trained
on pigeons and obtain preliminary promising results. Thus, we think that our
approach serves as a solid foundation and inspires the development of more
robust and accurate texture-independent pose estimation frameworks.

</details>


### [175] [From Evaluation to Defense: Advancing Safety in Video Large Language Models](https://arxiv.org/abs/2505.16643)
*Yiwei Sun,Peiqi Jiang,Chuanbin Liu,Luohao Lin,Zhiying Lu,Hongtao Xie*

Main category: cs.CV

TL;DR: 论文提出了首个大规模、文化多样的视频大语言模型安全基准VideoSafetyBench（VSB-77k），并揭示了视频模态会降低安全性能42.3%。为解决这一问题，作者提出了双阶段框架VideoSafety-R1，通过创新方法显著提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型的安全风险尚未被充分研究，而现有研究主要集中在图像模态上。作者旨在填补这一空白，并揭示多模态攻击的系统性风险。

Method: 提出了VideoSafety-R1框架，包含两个创新：Alarm Token-Guided Safety Fine-Tuning（AT-SFT）和Safety-Guided GRPO，通过多任务目标和动态策略优化提升安全性。

Result: 框架在VSB-Eval-HH上提升了65.1%，在其他图像安全数据集上也表现优异。

Conclusion: 视频模态显著增加了安全风险，但通过主动推理和多模态验证，可以显著提升模型的安全性。

Abstract: While the safety risks of image-based large language models have been
extensively studied, their video-based counterparts (Video LLMs) remain
critically under-examined. To systematically study this problem, we introduce
\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse
benchmark for Video LLM safety}, which compromises 77,646 video-query pairs and
spans 19 principal risk categories across 10 language communities. \textit{We
reveal that integrating video modality degrades safety performance by an
average of 42.3\%, exposing systemic risks in multimodal attack exploitation.}
To address this vulnerability, we propose \textbf{VideoSafety-R1}, a dual-stage
framework achieving unprecedented safety gains through two innovations: (1)
Alarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens
into visual and textual sequences, enabling explicit harm perception across
modalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances
defensive reasoning through dynamic policy optimization with rule-based rewards
derived from dual-modality verification. These components synergize to shift
safety alignment from passive harm recognition to active reasoning. The
resulting framework achieves a 65.1\% improvement on VSB-Eval-HH, and improves
by 59.1\%, 44.3\%, and 15.0\% on the image safety datasets MMBench, VLGuard,
and FigStep, respectively. \textit{Our codes are available in the supplementary
materials.} \textcolor{red}{Warning: This paper contains examples of harmful
language and videos, and reader discretion is recommended.}

</details>


### [176] [Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models](https://arxiv.org/abs/2505.16647)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 研究通过指令调优的视觉语言模型（VLM）在医学图像多任务理解中的表现，包括检测、定位和计数，并验证其是否能提升诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过指令调优的VLM同时改进医学图像的多任务理解，以提升诊断效率和准确性。

Method: 使用MedMultiPoints数据集，将任务转化为基于指令的提示，并通过LoRA方法微调Qwen2.5-VL-7B-Instruct模型。

Result: 多任务训练提高了模型的鲁棒性和准确性，例如减少了计数平均绝对误差（MAE），但边缘案例的可靠性有所下降。

Conclusion: 研究表明，通过提示驱动的微调，通用VLM可以适应专业医学任务，并生成可解释的结构化输出，为可解释和多功能医学AI提供了可能。

Abstract: We investigate fine-tuning Vision-Language Models (VLMs) for multi-task
medical image understanding, focusing on detection, localization, and counting
of findings in medical images. Our objective is to evaluate whether
instruction-tuned VLMs can simultaneously improve these tasks, with the goal of
enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a
multimodal dataset with annotations from endoscopy (polyps and instruments) and
microscopy (sperm cells), we reformulate each task into instruction-based
prompts suitable for vision-language reasoning. We fine-tune
Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task
combinations. Results show that multi-task training improves robustness and
accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and
increases Matching Accuracy in the Counting + Pointing task. However,
trade-offs emerge, such as more zero-case point predictions, indicating reduced
reliability in edge cases despite overall performance gains. Our study
highlights the potential of adapting general-purpose VLMs to specialized
medical tasks via prompt-driven fine-tuning. This approach mirrors clinical
workflows, where radiologists simultaneously localize, count, and describe
findings - demonstrating how VLMs can learn composite diagnostic reasoning
patterns. The model produces interpretable, structured outputs, offering a
promising step toward explainable and versatile medical AI. Code, model
weights, and scripts will be released for reproducibility at
https://github.com/simula/PointDetectCount.

</details>


### [177] [Unsupervised Network Anomaly Detection with Autoencoders and Traffic Images](https://arxiv.org/abs/2505.16650)
*Michael Neri,Sara Baldoni*

Main category: cs.CV

TL;DR: 提出了一种基于图像的网络流量表示方法，用于快速检测安全异常，并通过无监督学习实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 随着连接设备数量的增加，检测安全问题的需求日益迫切，且设备异构性要求简化数据处理架构。

Method: 采用基于图像的流量表示方法，以1秒时间窗口压缩网络状态，并通过无监督学习检测异常。

Result: 该方法有效减少了复杂处理架构的需求，并能高效检测异常。

Conclusion: 提出的方法为网络流量异常检测提供了一种高效且简洁的解决方案。

Abstract: Due to the recent increase in the number of connected devices, the need to
promptly detect security issues is emerging. Moreover, the high number of
communication flows creates the necessity of processing huge amounts of data.
Furthermore, the connected devices are heterogeneous in nature, having
different computational capacities. For this reason, in this work we propose an
image-based representation of network traffic which allows to realize a compact
summary of the current network conditions with 1-second time windows. The
proposed representation highlights the presence of anomalies thus reducing the
need for complex processing architectures. Finally, we present an unsupervised
learning approach which effectively detects the presence of anomalies. The code
and the dataset are available at
https://github.com/michaelneri/image-based-network-traffic-anomaly-detection.

</details>


### [178] [Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding](https://arxiv.org/abs/2505.16652)
*Feilong Tang,Chengzhi Liu,Zhongxing Xu,Ming Hu,Zelin Peng,Zhiwei Yang,Jionglong Su,Minquan Lin,Yifan Peng,Xuelian Cheng,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: FarSight是一种解码策略，通过优化因果掩码减少异常标记的注意力干扰，从而缓解多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉问答中常出现幻觉问题，分为初始幻觉和雪球幻觉。作者认为通过标记交互过程可以提取足够的上下文信息。

Method: 利用因果掩码建立多模态标记间的信息传播，设计注意力寄存器结构和位置感知编码方法，动态分配注意力以减少异常标记的干扰。

Result: FarSight在图像和视频基准测试中显著减少了幻觉现象，证明了其有效性。

Conclusion: FarSight是一种即插即用的解码策略，通过优化标记传播过程，有效缓解了多模态大语言模型中的幻觉问题。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
significantly improved performance in visual question answering. However, they
often suffer from hallucinations. In this work, hallucinations are categorized
into two main types: initial hallucinations and snowball hallucinations. We
argue that adequate contextual information can be extracted directly from the
token interaction process. Inspired by causal inference in the decoding
strategy, we propose to leverage causal masks to establish information
propagation between multimodal tokens. The hypothesis is that insufficient
interaction between those tokens may lead the model to rely on outlier tokens,
overlooking dense and rich contextual cues. Therefore, we propose to intervene
in the propagation process by tackling outlier tokens to enhance in-context
inference. With this goal, we present FarSight, a versatile plug-and-play
decoding strategy to reduce attention interference from outlier tokens merely
by optimizing the causal mask. The heart of our method is effective token
propagation. We design an attention register structure within the upper
triangular matrix of the causal mask, dynamically allocating attention to
capture attention diverted to outlier tokens. Moreover, a positional awareness
encoding method with a diminishing masking rate is proposed, allowing the model
to attend to further preceding tokens, especially for video sequence tasks.
With extensive experiments, FarSight demonstrates significant
hallucination-mitigating performance across different MLLMs on both image and
video benchmarks, proving its effectiveness.

</details>


### [179] [Zero-Shot Hyperspectral Pansharpening Using Hysteresis-Based Tuning for Spectral Quality Control](https://arxiv.org/abs/2505.16658)
*Giuseppe Guarino,Matteo Ciotola,Gemine Vivone,Giovanni Poggi,Giuseppe Scarpa*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级神经网络方法，用于高光谱图像融合，解决了传统方法在光谱一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像融合面临多波段、噪声、光谱不匹配和高分辨率比等独特挑战，现有方法难以保证所有波段的一致性质量。

Method: 使用自适应权重的轻量级神经网络，动态调整空间损失，并重新定义空间损失以考虑非线性依赖关系。

Result: 实验表明，该方法在所有波段上均能保持优异的一致性质量，性能与现有最优方法相当。

Conclusion: 该方法无需外部数据训练，灵活且复杂度低，为高光谱图像融合提供了可靠解决方案。

Abstract: Hyperspectral pansharpening has received much attention in recent years due
to technological and methodological advances that open the door to new
application scenarios. However, research on this topic is only now gaining
momentum. The most popular methods are still borrowed from the more mature
field of multispectral pansharpening and often overlook the unique challenges
posed by hyperspectral data fusion, such as i) the very large number of bands,
ii) the overwhelming noise in selected spectral ranges, iii) the significant
spectral mismatch between panchromatic and hyperspectral components, iv) a
typically high resolution ratio. Imprecise data modeling especially affects
spectral fidelity. Even state-of-the-art methods perform well in certain
spectral ranges and much worse in others, failing to ensure consistent quality
across all bands, with the risk of generating unreliable results. Here, we
propose a hyperspectral pansharpening method that explicitly addresses this
problem and ensures uniform spectral quality. To this end, a single lightweight
neural network is used, with weights that adapt on the fly to each band. During
fine-tuning, the spatial loss is turned on and off to ensure a fast convergence
of the spectral loss to the desired level, according to a hysteresis-like
dynamic. Furthermore, the spatial loss itself is appropriately redefined to
account for nonlinear dependencies between panchromatic and spectral bands.
Overall, the proposed method is fully unsupervised, with no prior training on
external data, flexible, and low-complexity. Experiments on a recently
published benchmarking toolbox show that it ensures excellent sharpening
quality, competitive with the state-of-the-art, consistently across all bands.
The software code and the full set of results are shared online on
https://github.com/giu-guarino/rho-PNN.

</details>


### [180] [SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images](https://arxiv.org/abs/2505.16659)
*Kaiyu Guo,Tan Pan,Chen Jiang,Zijian Wang,Brian C. Lovell,Limei Han,Yuan Cheng,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: 提出了一种针对少样本医学异常检测的框架SD-MAD，通过结合大语言模型生成的多样化文本描述，区分多类异常，并引入两阶段策略优化检测性能。


<details>
  <summary>Details</summary>
Motivation: 医学异常检测因数据隐私和孤岛问题受限，少样本学习成为解决方案，但现有方法多忽略多类异常的区别。

Method: SD-MAD框架分两阶段：1) 通过放大异常间差异对齐放射学特征；2) 自动选择特征以减少数据不足的影响。

Result: 实验验证了方法的有效性。

Conclusion: SD-MAD在多类医学异常检测中表现优异，为少样本场景提供了新思路。

Abstract: Medical anomaly detection (AD) is crucial for early clinical intervention,
yet it faces challenges due to limited access to high-quality medical imaging
data, caused by privacy concerns and data silos. Few-shot learning has emerged
as a promising approach to alleviate these limitations by leveraging the
large-scale prior knowledge embedded in vision-language models (VLMs). Recent
advancements in few-shot medical AD have treated normal and abnormal cases as a
one-class classification problem, often overlooking the distinction among
multiple anomaly categories. Thus, in this paper, we propose a framework
tailored for few-shot medical anomaly detection in the scenario where the
identification of multiple anomaly categories is required. To capture the
detailed radiological signs of medical anomaly categories, our framework
incorporates diverse textual descriptions for each category generated by a
Large-Language model, under the assumption that different anomalies in medical
images may share common radiological signs in each category. Specifically, we
introduce SD-MAD, a two-stage Sign-Driven few-shot Multi-Anomaly Detection
framework: (i) Radiological signs are aligned with anomaly categories by
amplifying inter-anomaly discrepancy; (ii) Aligned signs are selected further
to mitigate the effect of the under-fitting and uncertain-sample issue caused
by limited medical data, employing an automatic sign selection strategy at
inference. Moreover, we propose three protocols to comprehensively quantify the
performance of multi-anomaly detection. Extensive experiments illustrate the
effectiveness of our method.

</details>


### [181] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
*Huanjin Yao,Qixiang Yin,Jingyi Zhang,Min Yang,Yibo Wang,Wenhao Wu,Fei Su,Li Shen,Minghui Qiu,Dacheng Tao,Jiaxing Huang*

Main category: cs.CV

TL;DR: Share-GRPO是一种通过强化学习提升多模态大语言模型推理能力的新方法，通过扩展问题空间和共享推理轨迹解决稀疏奖励和优势消失问题。


<details>
  <summary>Details</summary>
Motivation: 激励多模态大语言模型的推理能力，并解决强化学习中的稀疏奖励和优势消失问题。

Method: 提出Share-GRPO方法，扩展问题空间并共享推理轨迹和奖励信息，以更准确地估计优势并稳定策略训练。

Result: 在六个广泛使用的推理基准测试中表现出优越性能。

Conclusion: Share-GRPO有效提升了模型的推理能力，并通过共享机制解决了强化学习中的关键问题。

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


### [182] [Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual Question Answering with Prior Knowledge](https://arxiv.org/abs/2505.16674)
*Marcella Astrid,Abdelrahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉问答（VQA）模型的零样本异常检测方法，用于电池热图像中的异常检测，避免了传统深度学习方法需要大量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 电池安全与效率至关重要，但传统方法需要大量标注数据且难以获取，尤其是异常数据。

Method: 利用预训练的VQA模型（如ChatGPT-4o、LLaVa-13b和BLIP-2），结合电池正常行为的先验知识设计文本提示，实现零样本异常检测。

Result: 尽管未针对电池数据进行微调，该方法在性能上与传统基于电池数据训练的先进模型相当。

Conclusion: VQA模型的零样本学习在电池异常检测中具有潜力，未来可进一步优化其效果。

Abstract: Batteries are essential for various applications, including electric vehicles
and renewable energy storage, making safety and efficiency critical concerns.
Anomaly detection in battery thermal images helps identify failures early, but
traditional deep learning methods require extensive labeled data, which is
difficult to obtain, especially for anomalies due to safety risks and high data
collection costs. To overcome this, we explore zero-shot anomaly detection
using Visual Question Answering (VQA) models, which leverage pretrained
knowledge and textbased prompts to generalize across vision tasks. By
incorporating prior knowledge of normal battery thermal behavior, we design
prompts to detect anomalies without battery-specific training data. We evaluate
three VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness
to prompt variations, repeated trials, and qualitative outputs. Despite the
lack of finetuning on battery data, our approach demonstrates competitive
performance compared to state-of-the-art models that are trained with the
battery data. Our findings highlight the potential of VQA-based zero-shot
learning for battery anomaly detection and suggest future directions for
improving its effectiveness.

</details>


### [183] [Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds](https://arxiv.org/abs/2505.16679)
*Jordan Dotzel,Tony Montes,Mohamed S. Abdelfattah,Zhiru Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义的3D对象压缩方法，与传统方法相比，能在高压缩率下保持质量。


<details>
  <summary>Details</summary>
Motivation: 传统3D对象压缩方法在高压缩率下表现不佳，而语义压缩通过忽略结构信息直接处理核心概念，实现了更高的压缩率。

Method: 利用公开生成模型构建3D语义压缩流程，通过自然语言存储格式和深度生成模型预测缺失信息。

Result: 在Objaverse数据集上实现了高达105倍的压缩率，并在100倍压缩率附近优于传统方法。

Conclusion: 语义压缩在高压缩率下具有优势，适合大规模协作项目。

Abstract: Traditional methods for 3D object compression operate only on structural
information within the object vertices, polygons, and textures. These methods
are effective at compression rates up to 10x for standard object sizes but
quickly deteriorate at higher compression rates with texture artifacts,
low-polygon counts, and mesh gaps. In contrast, semantic compression ignores
structural information and operates directly on the core concepts to push to
extreme levels of compression. In addition, it uses natural language as its
storage format, which makes it natively human-readable and a natural fit for
emerging applications built around large-scale, collaborative projects within
augmented and virtual reality. It deprioritizes structural information like
location, size, and orientation and predicts the missing information with
state-of-the-art deep generative models. In this work, we construct a pipeline
for 3D semantic compression from public generative models and explore the
quality-compression frontier for 3D object compression. We apply this pipeline
to achieve rates as high as 105x for 3D objects taken from the Objaverse
dataset and show that semantic compression can outperform traditional methods
in the important quality-preserving region around 100x compression.

</details>


### [184] [On the use of Graphs for Satellite Image Time Series](https://arxiv.org/abs/2505.16685)
*Corentin Dufourg,Charlotte Pelletier,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 论文探讨了基于图的方法在时空遥感分析中的应用，提出了一种通用的图处理流程，并展示了其在土地覆盖制图和水资源预测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 地球表面动态过程复杂，卫星图像时间序列（SITS）提供了全局监测能力，但数据量大且复杂。图方法能突破欧几里得结构的限制，建模时空交互，为模式检测和分类任务提供支持。

Method: 提出了一种通用的图处理流程，包括从SITS构建时空图，并将其应用于下游任务。论文还包含综述和两个案例研究。

Result: 案例研究表明，图方法在土地覆盖制图和水资源预测中具有潜力。

Conclusion: 论文总结了图方法的优势，并讨论了当前局限性和未来发展方向。

Abstract: The Earth's surface is subject to complex and dynamic processes, ranging from
large-scale phenomena such as tectonic plate movements to localized changes
associated with ecosystems, agriculture, or human activity. Satellite images
enable global monitoring of these processes with extensive spatial and temporal
coverage, offering advantages over in-situ methods. In particular, resulting
satellite image time series (SITS) datasets contain valuable information. To
handle their large volume and complexity, some recent works focus on the use of
graph-based techniques that abandon the regular Euclidean structure of
satellite data to work at an object level. Besides, graphs enable modelling
spatial and temporal interactions between identified objects, which are crucial
for pattern detection, classification and regression tasks. This paper is an
effort to examine the integration of graph-based methods in spatio-temporal
remote-sensing analysis. In particular, it aims to present a versatile
graph-based pipeline to tackle SITS analysis. It focuses on the construction of
spatio-temporal graphs from SITS and their application to downstream tasks. The
paper includes a comprehensive review and two case studies, which highlight the
potential of graph-based approaches for land cover mapping and water resource
forecasting. It also discusses numerous perspectives to resolve current
limitations and encourage future developments.

</details>


### [185] [One-Step Diffusion-Based Image Compression with Semantic Distillation](https://arxiv.org/abs/2505.16687)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: OneDC是一种基于一步扩散的生成图像编解码器，通过结合潜在压缩模块和一步扩散生成器，显著减少了传统扩散模型的多步采样延迟，同时保持了高感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型的多步采样过程导致高延迟，影响了生成图像编解码器的实用性。

Method: 提出OneDC，结合潜在压缩模块和一步扩散生成器，利用超先验作为语义信号，并引入语义蒸馏机制和混合优化策略。

Result: OneDC在一步生成下实现SOTA感知质量，比特率降低40%，解码速度提升20倍。

Conclusion: OneDC证明了生成压缩中多步采样并非必要，为高效生成图像编解码提供了新思路。

Abstract: While recent diffusion-based generative image codecs have shown impressive
performance, their iterative sampling process introduces unpleasing latency. In
this work, we revisit the design of a diffusion-based codec and argue that
multi-step sampling is not necessary for generative compression. Based on this
insight, we propose OneDC, a One-step Diffusion-based generative image Codec --
that integrates a latent compression module with a one-step diffusion
generator. Recognizing the critical role of semantic guidance in one-step
diffusion, we propose using the hyperprior as a semantic signal, overcoming the
limitations of text prompts in representing complex visual content. To further
enhance the semantic capability of the hyperprior, we introduce a semantic
distillation mechanism that transfers knowledge from a pretrained generative
tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and
latent-domain optimization to jointly enhance both reconstruction fidelity and
perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA
perceptual quality even with one-step generation, offering over 40% bitrate
reduction and 20x faster decoding compared to prior multi-step diffusion-based
codecs. Code will be released later.

</details>


### [186] [KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models](https://arxiv.org/abs/2505.16707)
*Yongliang Wu,Zonghui Li,Xinting Hu,Xinyu Ye,Xianfang Zeng,Gang Yu,Wenbo Zhu,Bernt Schiele,Ming-Hsuan Yang,Xu Yang*

Main category: cs.CV

TL;DR: KRIS-Bench是一个基于知识推理的图像编辑诊断基准，旨在评估模型在事实、概念和程序性知识任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在知识推理编辑任务中的能力尚未充分探索，需要知识为中心的基准来推动智能图像编辑系统的发展。

Method: 基于教育理论，设计了22个任务和1,267个标注实例，并提出包含知识合理性指标的综合评估协议。

Result: 对10个先进模型的实证结果显示推理性能存在显著差距。

Conclusion: KRIS-Bench强调了知识中心化基准的重要性，为智能图像编辑系统的进步提供了方向。

Abstract: Recent advances in multi-modal generative models have enabled significant
progress in instruction-based image editing. However, while these models
produce visually plausible outputs, their capacity for knowledge-based
reasoning editing tasks remains under-explored. In this paper, we introduce
KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a
diagnostic benchmark designed to assess models through a cognitively informed
lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks
across three foundational knowledge types: Factual, Conceptual, and Procedural.
Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning
dimensions and release 1,267 high-quality annotated editing instances. To
support fine-grained evaluation, we propose a comprehensive protocol that
incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints
and calibrated through human studies. Empirical results on 10 state-of-the-art
models reveal significant gaps in reasoning performance, highlighting the need
for knowledge-centric benchmarks to advance the development of intelligent
image editing systems.

</details>


### [187] [SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned Point Cloud Compression](https://arxiv.org/abs/2505.16709)
*Kai Hsiang Hsieh,Monyneath Yim,Jui Chiu Chiang*

Main category: cs.CV

TL;DR: SEDD-PCC提出了一种端到端的学习框架，联合压缩点云的几何和属性，通过单一编码器和双解码器设计，结合知识蒸馏提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将几何和属性编码分离，导致计算复杂且未能充分利用共享特征。

Method: 使用单一编码器提取共享特征至统一潜在空间，双解码器顺序重建几何和属性，结合知识蒸馏优化特征学习。

Result: 在规则和基于学习的对比中表现优异，验证了其高效性和实用性。

Conclusion: SEDD-PCC是一种高效且实用的AI驱动点云压缩方法。

Abstract: To encode point clouds containing both geometry and attributes, most
learning-based compression schemes treat geometry and attribute coding
separately, employing distinct encoders and decoders. This not only increases
computational complexity but also fails to fully exploit shared features
between geometry and attributes. To address this limitation, we propose
SEDD-PCC, an end-to-end learning-based framework for lossy point cloud
compression that jointly compresses geometry and attributes. SEDD-PCC employs a
single encoder to extract shared geometric and attribute features into a
unified latent space, followed by dual specialized decoders that sequentially
reconstruct geometry and attributes. Additionally, we incorporate knowledge
distillation to enhance feature representation learning from a teacher model,
further improving coding efficiency. With its simple yet effective design,
SEDD-PCC provides an efficient and practical solution for point cloud
compression. Comparative evaluations against both rule-based and learning-based
methods demonstrate its competitive performance, highlighting SEDD-PCC as a
promising AI-driven compression approach.

</details>


### [188] [Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP](https://arxiv.org/abs/2505.16740)
*Alya Zouzou,Léo andéol,Mélanie Ducoffe,Ryma Boumazouza*

Main category: cs.CV

TL;DR: 使用共形预测为基于视觉的着陆系统中的跑道检测提供统计不确定性保证，提出新指标C-mAP，提升检测可靠性。


<details>
  <summary>Details</summary>
Motivation: 提高跑道检测的可靠性，为机器学习系统在航空航天领域的认证提供支持。

Method: 使用微调的YOLOv5和YOLOv6模型，结合共形预测量化定位可靠性。

Result: 共形预测能通过统计方法量化不确定性，提升跑道检测的可靠性。

Conclusion: 共形预测为跑道检测提供了统计上的可靠性保证，有助于提升安全性和系统认证。

Abstract: We explore the use of conformal prediction to provide statistical uncertainty
guarantees for runway detection in vision-based landing systems (VLS). Using
fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal
prediction to quantify localization reliability under user-defined risk levels.
We also introduce Conformal mean Average Precision (C-mAP), a novel metric
aligning object detection performance with conformal guarantees. Our results
show that conformal prediction can improve the reliability of runway detection
by quantifying uncertainty in a statistically sound way, increasing safety
on-board and paving the way for certification of ML system in the aerospace
domain.

</details>


### [189] [Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16761)
*Jian Liu,Jing Xu,Song Guo,Jing Li,Jingfeng Guo,Jiaao Yu,Haohan Weng,Biwen Lei,Xianghui Yang,Zhuo Chen,Fangqi Zhu,Tao Han,Chunchao Guo*

Main category: cs.CV

TL;DR: Mesh-RFT提出了一种细粒度强化学习微调框架，通过Masked Direct Preference Optimization（M-DPO）实现局部优化，解决了现有预训练模型在3D网格生成中的偏差和低质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型存在数据偏差和低质量生成问题，而全局强化学习方法难以捕捉局部结构细节。

Method: 采用M-DPO进行局部优化，引入边界边缘比（BER）和拓扑分数（TS）评估几何完整性和拓扑规则性。

Result: 实验显示，Mesh-RFT显著降低了Hausdorff Distance（HD）并提高了TS，优于预训练模型和全局DPO方法。

Conclusion: Mesh-RFT在几何完整性和拓扑规则性上取得突破，成为生产级网格生成的新标杆。

Abstract: Existing pretrained models for 3D mesh generation often suffer from data
biases and produce low-quality results, while global reinforcement learning
(RL) methods rely on object-level rewards that struggle to capture local
structure details. To address these challenges, we present \textbf{Mesh-RFT}, a
novel fine-grained reinforcement fine-tuning framework that employs Masked
Direct Preference Optimization (M-DPO) to enable localized refinement via
quality-aware face masking. To facilitate efficient quality evaluation, we
introduce an objective topology-aware scoring system to evaluate geometric
integrity and topological regularity at both object and face levels through two
metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating
these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first
method to optimize mesh quality at the granularity of individual faces,
resolving localized errors while preserving global coherence. Experiment
results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\%
and improves Topology Score (TS) by 3.8\% over pre-trained models, while
outperforming global DPO methods with a 17.4\% HD reduction and 4.9\% TS gain.
These results demonstrate Mesh-RFT's ability to improve geometric integrity and
topological regularity, achieving new state-of-the-art performance in
production-ready mesh generation. Project Page:
\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.

</details>


### [190] [Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation](https://arxiv.org/abs/2505.16763)
*Hongji Yang,Yucheng Zhou,Wencheng Han,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉语言模型（LVLM）的提示优化框架，通过AI反馈而非人工标注数据优化文本到图像模型的提示。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量人工标注数据和训练模型，存在数据依赖和偏差问题，需更高效且无偏的提示优化方法。

Method: 利用LVLM作为提示重写器和奖励模型，通过强化学习实现自我改进，统一优化和评分功能。

Result: 在两个流行数据集上表现优于其他强竞争方法。

Conclusion: 该框架有效减少对人工标注的依赖，利用LVLM先验知识实现高效提示优化。

Abstract: Text-to-image models are powerful for producing high-quality images based on
given text prompts, but crafting these prompts often requires specialized
vocabulary. To address this, existing methods train rewriting models with
supervision from large amounts of manually annotated data and trained aesthetic
assessment models. To alleviate the dependence on data scale for model training
and the biases introduced by trained models, we propose a novel prompt
optimization framework, designed to rephrase a simple user prompt into a
sophisticated prompt to a text-to-image model. Specifically, we employ the
large vision language models (LVLMs) as the solver to rewrite the user prompt,
and concurrently, employ LVLMs as a reward model to score the aesthetics and
alignment of the images generated by the optimized prompt. Instead of laborious
human feedback, we exploit the prior knowledge of the LVLM to provide rewards,
i.e., AI feedback. Simultaneously, the solver and the reward model are unified
into one model and iterated in reinforcement learning to achieve
self-improvement by giving a solution and judging itself. Results on two
popular datasets demonstrate that our method outperforms other strong
competitors.

</details>


### [191] [RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/abs/2505.16770)
*Meng-Hao Guo,Xuanyu Chu,Qianrui Yang,Zhe-Han Mo,Yiqing Shen,Pei-lin Li,Xinjie Lin,Jinnian Zhang,Xin-Sheng Chen,Yi Zhang,Kiyohiro Nakayama,Zhengyang Geng,Houwen Peng,Han Hu,Shi-Nin Hu*

Main category: cs.CV

TL;DR: 论文提出了一个名为RBench-V的基准测试，用于评估多模态模型在视觉推理任务中的表现，发现当前模型在多模态输出推理能力上仍有显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注多模态输入和纯文本推理，忽视了多模态输出在推理过程中的重要性，因此需要一个新的评估工具。

Method: 通过精心挑选803个涵盖数学、物理、计数和游戏的问题，构建RBench-V基准测试，专注于多模态输出（如图像生成和辅助线构建）的推理能力。

Result: 评估多个开源和闭源模型后，表现最佳的o3模型准确率仅为25.8%，远低于人类的82.3%，表明当前模型在多模态推理上存在困难。

Conclusion: RBench-V揭示了当前多模态模型在视觉推理任务中的局限性，为未来研究提供了改进方向。

Abstract: The rapid advancement of native multi-modal models and omni-models,
exemplified by GPT-4o, Gemini, and o3, with their capability to process and
generate content across modalities such as text and images, marks a significant
milestone in the evolution of intelligence. Systematic evaluation of their
multi-modal output capabilities in visual thinking processes (also known as
multi-modal chain of thought, M-CoT) becomes critically important. However,
existing benchmarks for evaluating multi-modal models primarily focus on
assessing multi-modal inputs and text-only reasoning while neglecting the
importance of reasoning through multi-modal outputs. In this paper, we present
a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable
reasoning abilities. To construct RBench-V, we carefully hand-pick 803
questions covering math, physics, counting, and games. Unlike previous
benchmarks that typically specify certain input modalities, RBench-V presents
problems centered on multi-modal outputs, which require image manipulation such
as generating novel images and constructing auxiliary lines to support the
reasoning process. We evaluate numerous open- and closed-source models on
RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the
best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below
the human score of 82.3%, highlighting that current models struggle to leverage
multi-modal reasoning. Data and code are available at
https://evalmodels.github.io/rbenchv

</details>


### [192] [Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis](https://arxiv.org/abs/2505.16773)
*Iván Matas,Carmen Serrano,Miguel Nogales,David Moreno,Lara Ferrándiz,Teresa Ojeda,Begoña Acha*

Main category: cs.CV

TL;DR: 本文提出了一种无监督学习框架，用于提取皮肤病学特征，避免了依赖ImageNet预训练模型的局限性。结果表明，自监督学习在泛化能力和适应性上优于ImageNet预训练。


<details>
  <summary>Details</summary>
Motivation: 解决ImageNet预训练模型在医学影像领域可能无法捕捉特定领域特征的问题。

Method: 使用变分自编码器（VAE）在皮肤病学数据集上从头训练，构建结构化且临床相关的潜在空间。

Result: 自监督模型的验证损失降低33.33%，泛化能力更强；ImageNet预训练模型虽收敛快，但过拟合严重。

Conclusion: 领域特定的特征提取在医学影像中至关重要，自监督学习表现更优。

Abstract: Deep learning has transformed computer vision but relies heavily on large
labeled datasets and computational resources. Transfer learning, particularly
fine-tuning pretrained models, offers a practical alternative; however, models
pretrained on natural image datasets such as ImageNet may fail to capture
domain-specific characteristics in medical imaging. This study introduces an
unsupervised learning framework that extracts high-value dermatological
features instead of relying solely on ImageNet-based pretraining. We employ a
Variational Autoencoder (VAE) trained from scratch on a proprietary
dermatological dataset, allowing the model to learn a structured and clinically
relevant latent space. This self-supervised feature extractor is then compared
to an ImageNet-pretrained backbone under identical classification conditions,
highlighting the trade-offs between general-purpose and domain-specific
pretraining. Our results reveal distinct learning patterns. The self-supervised
model achieves a final validation loss of 0.110 (-33.33%), while the
ImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.
Accuracy trends confirm this: the self-supervised model improves from 45% to
65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained
model reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting
gap increasing to +0.060. These findings suggest that while ImageNet
pretraining accelerates convergence, it also amplifies overfitting on
non-clinically relevant features. In contrast, self-supervised learning
achieves steady improvements, stronger generalization, and superior
adaptability, underscoring the importance of domain-specific feature extraction
in medical imaging.

</details>


### [193] [Single Domain Generalization for Few-Shot Counting via Universal Representation Matching](https://arxiv.org/abs/2505.16778)
*Xianing Chen,Si Huo,Borui Jiang,Hailin Hu,Xinghao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为URM的通用表示匹配方法，用于解决少样本计数中的领域泛化问题，通过结合大规模预训练的视觉-语言表示，显著提升了模型对领域偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有少样本计数方法在领域偏移下泛化能力不足，且缺乏对通用原型学习的研究。

Method: 提出URM模型，利用大规模预训练的视觉-语言表示改进相关性图的构建过程。

Result: URM在领域内和新引入的领域泛化设置中均达到最优性能。

Conclusion: URM通过结合通用表示，显著提升了少样本计数模型的鲁棒性和泛化能力。

Abstract: Few-shot counting estimates the number of target objects in an image using
only a few annotated exemplars. However, domain shift severely hinders existing
methods to generalize to unseen scenarios. This falls into the realm of single
domain generalization that remains unexplored in few-shot counting. To solve
this problem, we begin by analyzing the main limitations of current methods,
which typically follow a standard pipeline that extract the object prototypes
from exemplars and then match them with image feature to construct the
correlation map. We argue that existing methods overlook the significance of
learning highly generalized prototypes. Building on this insight, we propose
the first single domain generalization few-shot counting model, Universal
Representation Matching, termed URM. Our primary contribution is the discovery
that incorporating universal vision-language representations distilled from a
large scale pretrained vision-language model into the correlation construction
process substantially improves robustness to domain shifts without compromising
in domain performance. As a result, URM achieves state-of-the-art performance
on both in domain and the newly introduced domain generalization setting.

</details>


### [194] [Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles](https://arxiv.org/abs/2505.16784)
*Jun Xie,Xiongjun Guan,Yingjian Zhu,Zhaoran Zhao,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文介绍了CVPR 2025 Ego4D EgoSchema挑战赛的亚军解决方案，通过少样本学习和模型集成策略，利用多模态大模型提升视频理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 受大模型成功的启发，探索如何利用多模态大模型解决视频理解任务，并充分发挥其泛化和适应能力。

Method: 采用少样本学习和模型集成策略，系统探索多样化的提示风格和处理范式，以引导大模型的注意力。

Result: 实验表明，单个多模态模型已超越现有SOTA方法，进一步通过结果集成实现显著性能提升。

Conclusion: 本文为大模型的实践应用提供了参考，并启发了未来研究方向。

Abstract: In this paper, we present the runner-up solution for the Ego4D EgoSchema
Challenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of
large models, we evaluate and leverage leading accessible multimodal large
models and adapt them to video understanding tasks via few-shot learning and
model ensemble strategies. Specifically, diversified prompt styles and process
paradigms are systematically explored and evaluated to effectively guide the
attention of large models, fully unleashing their powerful generalization and
adaptability abilities. Experimental results demonstrate that, with our
carefully designed approach, directly utilizing an individual multimodal model
already outperforms the previous state-of-the-art (SOTA) method which includes
several additional processes. Besides, an additional stage is further
introduced that facilitates the cooperation and ensemble of periodic results,
which achieves impressive performance improvements. We hope this work serves as
a valuable reference for the practical application of large models and inspires
future research in the field.

</details>


### [195] [REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training](https://arxiv.org/abs/2505.16792)
*Ziqiao Wang,Wangbo Zhao,Yuhao Zhou,Zekai Li,Zhiyuan Liang,Mingjia Shi,Xuanlei Zhao,Pengfei Zhou,Kaipeng Zhang,Zhangyang Wang,Kai Wang,Yang You*

Main category: cs.CV

TL;DR: HASTE通过两阶段训练策略（先对齐后终止）加速DiTs训练，显著减少优化步骤，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决DiTs训练速度慢的问题，尤其是REPA方法在后期性能下降的问题。

Method: 提出HASTE，分两阶段：第一阶段通过对齐损失快速收敛，第二阶段终止对齐损失以释放生成能力。

Result: 在ImageNet 256X256上，HASTE仅需50epochs达到基线性能，500epochs匹配REPA最佳性能，优化步骤减少28倍。

Conclusion: HASTE是一种简单有效的扩散训练方法，适用于多种任务。

Abstract: Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet
their training remains notoriously slow. A recent remedy -- representation
alignment (REPA) that matches DiT hidden features to those of a non-generative
teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus
or even degrades performance later. We trace this failure to a capacity
mismatch: once the generative student begins modelling the joint data
distribution, the teacher's lower-dimensional embeddings and attention patterns
become a straitjacket rather than a guide. We then introduce HASTE (Holistic
Alignment with Stage-wise Termination for Efficient training), a two-phase
schedule that keeps the help and drops the hindrance. Phase I applies a
holistic alignment loss that simultaneously distills attention maps (relational
priors) and feature projections (semantic anchors) from the teacher into
mid-level layers of the DiT, yielding rapid convergence. Phase II then performs
one-shot termination that deactivates the alignment loss, once a simple trigger
such as a fixed iteration is hit, freeing the DiT to focus on denoising and
exploit its generative capacity. HASTE speeds up training of diverse DiTs
without architecture changes. On ImageNet 256X256, it reaches the vanilla
SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,
amounting to a 28X reduction in optimization steps. HASTE also improves
text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled
recipe for efficient diffusion training across various tasks. Our code is
available at https://github.com/NUS-HPC-AI-Lab/HASTE .

</details>


### [196] [REOBench: Benchmarking Robustness of Earth Observation Foundation Models](https://arxiv.org/abs/2505.16793)
*Xiang Li,Yong Tao,Siyuan Zhang,Siwei Liu,Zhitong Xiong,Chunbo Luo,Lu Liu,Mykola Pechenizkiy,Xiao Xiang Zhu,Tianjin Huang*

Main category: cs.CV

TL;DR: REOBench是首个评估地球观测基础模型在真实世界扰动下鲁棒性的综合基准，涵盖六项任务和十二种图像扰动。结果显示现有模型在输入扰动下性能显著下降，且不同模型和任务表现差异大，视觉语言模型在多模态任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 地球观测基础模型在真实世界扰动下的鲁棒性尚未充分研究，REOBench旨在填补这一空白，提供更全面的评估。

Method: 通过高分辨率光学遥感图像，系统评估了基于掩码图像建模、对比学习和视觉语言预训练的多种模型。

Result: 现有模型在输入扰动下性能显著下降（1%-20%），视觉语言模型在多模态任务中表现更优。

Conclusion: REOBench揭示了当前模型的脆弱性，并为开发更鲁棒的模型提供了实用建议。

Abstract: Earth observation foundation models have shown strong generalization across
multiple Earth observation tasks, but their robustness under real-world
perturbations remains underexplored. To bridge this gap, we introduce REOBench,
the first comprehensive benchmark for evaluating the robustness of Earth
observation foundation models across six tasks and twelve types of image
corruptions, including both appearance-based and geometric perturbations. To
ensure realistic and fine-grained evaluation, our benchmark focuses on
high-resolution optical remote sensing images, which are widely used in
critical applications such as urban planning and disaster response. We conduct
a systematic evaluation of a broad range of models trained using masked image
modeling, contrastive learning, and vision-language pre-training paradigms. Our
results reveal that (1) existing Earth observation foundation models experience
significant performance degradation when exposed to input corruptions. (2) The
severity of degradation varies across tasks, model architectures, backbone
sizes, and types of corruption, with performance drop varying from less than 1%
to over 20%. (3) Vision-language models show enhanced robustness, particularly
in multimodal tasks. REOBench underscores the vulnerability of current Earth
observation foundation models to real-world corruptions and provides actionable
insights for developing more robust and reliable models.

</details>


### [197] [V2V: Scaling Event-Based Vision through Efficient Video-to-Voxel Simulation](https://arxiv.org/abs/2505.16797)
*Hanyue Lou,Jinxiu Liang,Minggui Teng,Yi Wang,Boxin Shi*

Main category: cs.CV

TL;DR: 论文提出Video-to-Voxel (V2V)方法，将传统视频帧直接转换为事件体素网格表示，显著减少存储需求并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机数据存储需求大、真实数据稀缺的问题，以支持事件视觉模型的规模化训练。

Method: 通过V2V方法将视频帧转换为事件体素网格，避免存储密集型事件流生成。

Result: 存储需求减少150倍，训练数据集规模扩大10倍，模型性能显著提升。

Conclusion: V2V方法为事件视觉模型的规模化训练提供了高效解决方案。

Abstract: Event-based cameras offer unique advantages such as high temporal resolution,
high dynamic range, and low power consumption. However, the massive storage
requirements and I/O burdens of existing synthetic data generation pipelines
and the scarcity of real data prevent event-based training datasets from
scaling up, limiting the development and generalization capabilities of event
vision models. To address this challenge, we introduce Video-to-Voxel (V2V), an
approach that directly converts conventional video frames into event-based
voxel grid representations, bypassing the storage-intensive event stream
generation entirely. V2V enables a 150 times reduction in storage requirements
while supporting on-the-fly parameter randomization for enhanced model
robustness. Leveraging this efficiency, we train several video reconstruction
and optical flow estimation model architectures on 10,000 diverse videos
totaling 52 hours--an order of magnitude larger than existing event datasets,
yielding substantial improvements.

</details>


### [198] [SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving](https://arxiv.org/abs/2505.16805)
*Xuesong Chen,Linjiang Huang,Tao Ma,Rongyao Fang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: SOLVE框架通过结合视觉语言模型（VLM）和端到端（E2E）模型，提升自动驾驶规划能力，采用轨迹链式思维（T-CoT）和特征共享策略，显著提高轨迹预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高效集成和实时决策方面存在不足，SOLVE旨在解决这些问题，提升自动驾驶系统的鲁棒性和可靠性。

Method: 提出SOLVE框架，通过共享视觉编码器实现VLM与E2E模型的交互，采用T-CoT逐步优化轨迹预测，并利用时间解耦策略平衡性能与实时性。

Result: 在nuScenes数据集上验证，轨迹预测准确性显著提升。

Conclusion: SOLVE为自动驾驶系统提供了一种更高效、可靠的规划方法，具有实际应用潜力。

Abstract: The integration of Vision-Language Models (VLMs) into autonomous driving
systems has shown promise in addressing key challenges such as learning
complexity, interpretability, and common-sense reasoning. However, existing
approaches often struggle with efficient integration and realtime
decision-making due to computational demands. In this paper, we introduce
SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E)
models to enhance autonomous vehicle planning. Our approach emphasizes
knowledge sharing at the feature level through a shared visual encoder,
enabling comprehensive interaction between VLM and E2E components. We propose a
Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines
trajectory predictions, reducing uncertainty and improving accuracy. By
employing a temporal decoupling strategy, SOLVE achieves efficient cooperation
by aligning high-quality VLM outputs with E2E real-time performance. Evaluated
on the nuScenes dataset, our method demonstrates significant improvements in
trajectory prediction accuracy, paving the way for more robust and reliable
autonomous driving systems.

</details>


### [199] [Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining](https://arxiv.org/abs/2505.16811)
*Shangquan Sun,Wenqi Ren,Juxiang Zhou,Shu Wang,Jianhou Gan,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种双分支时空状态空间模型，用于视频雨线去除，通过半监督学习和动态堆叠滤波器提升性能，并引入真实世界基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基于配对数据的方法在真实场景中泛化能力不足，因合成与真实雨效差异。

Method: 设计时空状态空间模型层提取特征，动态堆叠滤波器优化多帧融合，半监督学习生成伪干净补丁。

Result: 在合成和真实雨视频基准测试中表现优越，定量指标、视觉质量和效率均领先。

Conclusion: 方法在雨线去除及下游任务中具有显著优势，为真实场景应用提供支持。

Abstract: Significant progress has been made in video restoration under rainy
conditions over the past decade, largely propelled by advancements in deep
learning. Nevertheless, existing methods that depend on paired data struggle to
generalize effectively to real-world scenarios, primarily due to the disparity
between synthetic and authentic rain effects. To address these limitations, we
propose a dual-branch spatio-temporal state-space model to enhance rain streak
removal in video sequences. Specifically, we design spatial and temporal
state-space model layers to extract spatial features and incorporate temporal
dependencies across frames, respectively. To improve multi-frame feature
fusion, we derive a dynamic stacking filter, which adaptively approximates
statistical filters for superior pixel-wise feature refinement. Moreover, we
develop a median stacking loss to enable semi-supervised learning by generating
pseudo-clean patches based on the sparsity prior of rain. To further explore
the capacity of deraining models in supporting other vision-based tasks in
rainy environments, we introduce a novel real-world benchmark focused on object
detection and tracking in rainy conditions. Our method is extensively evaluated
across multiple benchmarks containing numerous synthetic and real-world rainy
videos, consistently demonstrating its superiority in quantitative metrics,
visual quality, efficiency, and its utility for downstream tasks.

</details>


### [200] [Perceptual Quality Assessment for Embodied AI](https://arxiv.org/abs/2505.16815)
*Chunyi Li,Jiaohao Xiao,Jianbo Zhang,Farong Wen,Zicheng Zhang,Yuan Tian,Xiangyang Zhu,Xiaohong Liu,Zhengxue Cheng,Weisi Lin,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了一种面向具身AI的图像质量评估方法（Embodied-IQA），旨在解决传统IQA方法在具身任务中的不足，并建立了包含36k图像对和5m标注的数据库。


<details>
  <summary>Details</summary>
Motivation: 具身AI在现实世界中的应用受限于图像质量评估方法的不足，传统IQA方法无法评估图像对机器人的感知质量。

Method: 基于Mertonian系统和元认知理论，构建了感知-认知-决策-执行流程，并建立了Embodied-IQA数据库，验证了主流IQA方法的性能。

Result: 建立了包含36k图像对和5m标注的数据库，验证了现有IQA方法在具身任务中的不足。

Conclusion: 通过评估具身AI的图像质量，有望推动其在复杂失真环境下的应用。

Abstract: Embodied AI has developed rapidly in recent years, but it is still mainly
deployed in laboratories, with various distortions in the Real-world limiting
its application. Traditionally, Image Quality Assessment (IQA) methods are
applied to predict human preferences for distorted images; however, there is no
IQA method to assess the usability of an image in embodied tasks, namely, the
perceptual quality for robots. To provide accurate and reliable quality
indicators for future embodied scenarios, we first propose the topic: IQA for
Embodied AI. Specifically, we (1) based on the Mertonian system and
meta-cognitive theory, constructed a perception-cognition-decision-execution
pipeline and defined a comprehensive subjective score collection process; (2)
established the Embodied-IQA database, containing over 36k reference/distorted
image pairs, with more than 5m fine-grained annotations provided by Vision
Language Models/Vision Language Action-models/Real-world robots; (3) trained
and validated the performance of mainstream IQA methods on Embodied-IQA,
demonstrating the need to develop more accurate quality indicators for Embodied
AI. We sincerely hope that through evaluation, we can promote the application
of Embodied AI under complex distortions in the Real-world. Project page:
https://github.com/lcysyzxdxc/EmbodiedIQA

</details>


### [201] [Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts](https://arxiv.org/abs/2505.16819)
*Taewon Kang,Ming C. Lin*

Main category: cs.CV

TL;DR: 提出了一种模块化流程，将动作级提示转化为视觉和听觉基础叙事对话，丰富视觉叙事。


<details>
  <summary>Details</summary>
Motivation: 当前场景视频生成系统在角色驱动对话和语音方面研究不足，需填补这一空白。

Method: 结合预训练视觉语言编码器提取语义特征，引导大语言模型生成自然对话，并引入递归叙事库保持上下文一致性。

Result: 生成角色一致、上下文连贯的对话，并渲染为语音，适用于多种故事场景。

Conclusion: 无需额外训练，框架适用于多样化故事设置，增强了视频叙事的表达能力。

Abstract: Recent advances in scene-based video generation have enabled systems to
synthesize coherent visual narratives from structured prompts. However, a
crucial dimension of storytelling -- character-driven dialogue and speech --
remains underexplored. In this paper, we present a modular pipeline that
transforms action-level prompts into visually and auditorily grounded narrative
dialogue, enriching visual storytelling with natural voice and character
expression. Our method takes as input a pair of prompts per scene, where the
first defines the setting and the second specifies a character's behavior.
While a story generation model such as Text2Story generates the corresponding
visual scene, we focus on generating expressive character utterances from these
prompts and the scene image. We apply a pretrained vision-language encoder to
extract a high-level semantic feature from the representative frame, capturing
salient visual context. This feature is then combined with the structured
prompts and used to guide a large language model in synthesizing natural,
character-consistent dialogue. To ensure contextual consistency across scenes,
we introduce a Recursive Narrative Bank that conditions each dialogue
generation on the accumulated dialogue history from prior scenes. This approach
enables characters to speak in ways that reflect their evolving goals and
interactions throughout a story. Finally, we render each utterance as
expressive, character-consistent speech, resulting in fully-voiced video
narratives. Our framework requires no additional training and demonstrates
applicability across a variety of story settings, from fantasy adventures to
slice-of-life episodes.

</details>


### [202] [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/abs/2505.16836)
*Fanrui Zhang,Dian Li,Qiang Zhang,Chenjun,sinbadliu,Junxiong Lin,Jiahong Yan,Jiawei Liu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 论文提出FakeVV数据集和Fact-R1框架，用于视频虚假信息检测，结合深度推理和规则强化学习，显著提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态虚假信息快速传播，现有方法因数据不足和缺乏深度推理而受限。

Method: 提出Fact-R1框架，分三阶段训练：长链思维指令调优、偏好对齐和基于可验证奖励的优化。

Result: Fact-R1在多模态虚假信息检测中展现出与先进文本强化学习系统相当的推理能力。

Conclusion: 研究为虚假信息检测提供了新范式，结合大规模视频理解、推理对齐和可解释验证。

Abstract: The rapid spread of multimodal misinformation on social media has raised
growing concerns, while research on video misinformation detection remains
limited due to the lack of large-scale, diverse datasets. Existing methods
often overfit to rigid templates and lack deep reasoning over deceptive
content. To address these challenges, we introduce FakeVV, a large-scale
benchmark comprising over 100,000 video-text pairs with fine-grained,
interpretable annotations. In addition, we further propose Fact-R1, a novel
framework that integrates deep reasoning with collaborative rule-based
reinforcement learning. Fact-R1 is trained through a three-stage process: (1)
misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference
alignment via Direct Preference Optimization (DPO), and (3) Group Relative
Policy Optimization (GRPO) using a novel verifiable reward function. This
enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those
observed in advanced text-based reinforcement learning systems, but in the more
complex multimodal misinformation setting. Our work establishes a new paradigm
for misinformation detection, bridging large-scale video understanding,
reasoning-guided alignment, and interpretable verification.

</details>


### [203] [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/abs/2505.16839)
*Shufan Li,Konstantinos Kallidromitis,Hritik Bansal,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Jason Kuen,Zhe Lin,Kai-Wei Chang,Aditya Grover*

Main category: cs.CV

TL;DR: LaViDa是一种基于离散扩散模型（DMs）的视觉语言模型（VLM），通过并行解码和双向推理，在推理速度和可控生成方面优于现有自回归模型（如LLaVA）。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视觉语言模型（如LLaVA）在推理速度和可控生成方面表现不佳，离散扩散模型（DMs）在语言任务中表现出色，但在多模态任务中潜力未充分挖掘。

Method: LaViDa通过为DMs配备视觉编码器，并结合互补掩码、前缀KV缓存和时间步偏移等技术，实现高效训练和推理。

Result: LaViDa在多模态基准测试（如MMMU）中表现优于自回归模型，在COCO字幕任务中CIDEr得分提升4.1，推理速度提升1.92倍。

Conclusion: LaViDa展示了离散扩散模型在多模态任务中的潜力，是自回归模型的有力替代方案。

Abstract: Modern Vision-Language Models (VLMs) can solve a wide range of tasks
requiring visual reasoning. In real-world scenarios, desirable properties for
VLMs include fast inference and controllable generation (e.g., constraining
outputs to adhere to a desired format). However, existing autoregressive (AR)
VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)
offer a promising alternative, enabling parallel decoding for faster inference
and bidirectional context for controllable generation through text-infilling.
While effective in language-only settings, DMs' potential for multimodal tasks
is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build
LaViDa by equipping DMs with a vision encoder and jointly fine-tune the
combined parts for multimodal instruction following. To address challenges
encountered, LaViDa incorporates novel techniques such as complementary masking
for effective training, prefix KV cache for efficient inference, and timestep
shifting for high-quality sampling. Experiments show that LaViDa achieves
competitive or superior performance to AR VLMs on multi-modal benchmarks such
as MMMU, while offering unique advantages of DMs, including flexible
speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO
captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x
speedup. On bidirectional tasks, it achieves +59% improvement on Constrained
Poem Completion. These results demonstrate LaViDa as a strong alternative to AR
VLMs. Code and models will be released in the camera-ready version.

</details>


### [204] [Conditional Panoramic Image Generation via Masked Autoregressive Modeling](https://arxiv.org/abs/2505.16862)
*Chaoyang Wang,Xiangtai Li,Lu Qi,Xiaofan Lin,Jinbin Bai,Qianyu Zhou,Yunhai Tong*

Main category: cs.CV

TL;DR: 提出了一种统一的框架PAR，通过掩码自回归建模解决全景图像生成中的i.i.d.假设问题和任务分离问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于扩散模型，不适用于ERP全景图，且将文本生成和图像生成视为独立任务。

Method: 使用掩码自回归建模避免i.i.d.假设，结合文本和图像条件，引入循环填充和一致性对齐策略。

Result: 在文本到图像生成和全景图外绘任务中表现优异，具有良好扩展性和泛化能力。

Conclusion: PAR框架有效解决了现有方法的局限性，实现了高质量全景图像生成。

Abstract: Recent progress in panoramic image generation has underscored two critical
limitations in existing approaches. First, most methods are built upon
diffusion models, which are inherently ill-suited for equirectangular
projection (ERP) panoramas due to the violation of the identically and
independently distributed (i.i.d.) Gaussian noise assumption caused by their
spherical mapping. Second, these methods often treat text-conditioned
generation (text-to-panorama) and image-conditioned generation (panorama
outpainting) as separate tasks, relying on distinct architectures and
task-specific data. In this work, we propose a unified framework, Panoramic
AutoRegressive model (PAR), which leverages masked autoregressive modeling to
address these challenges. PAR avoids the i.i.d. assumption constraint and
integrates text and image conditioning into a cohesive architecture, enabling
seamless generation across tasks. To address the inherent discontinuity in
existing generative models, we introduce circular padding to enhance spatial
coherence and propose a consistency alignment strategy to improve generation
quality. Extensive experiments demonstrate competitive performance in
text-to-image generation and panorama outpainting tasks while showcasing
promising scalability and generalization capabilities.

</details>


### [205] [Training-Free Efficient Video Generation via Dynamic Token Carving](https://arxiv.org/abs/2505.16864)
*Yuechen Zhang,Jinbo Xing,Bin Xia,Shaoteng Liu,Bohao Peng,Xin Tao,Pengfei Wan,Eric Lo,Jiaya Jia*

Main category: cs.CV

TL;DR: Jenga是一种新型推理管道，通过动态注意力分割和渐进分辨率生成，显著降低了视频扩散变换器（DiT）的计算需求，实现了8.83倍加速且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 视频DiT模型的计算需求过高，阻碍了实际部署，主要问题包括自注意力的二次复杂性和扩散模型的多步特性。

Method: Jenga结合了动态注意力分割（通过3D空间填充曲线选择相关token交互）和渐进分辨率生成（逐步增加潜在分辨率）。

Result: 实验表明，Jenga在多个先进视频扩散模型中实现了显著加速（8.83倍），同时保持生成质量（VBench上仅0.01%性能下降）。

Conclusion: Jenga是一种即插即用的解决方案，无需重新训练模型，即可将推理时间从分钟级降至秒级，实现高效、高质量的视频生成。

Abstract: Despite the remarkable generation quality of video Diffusion Transformer
(DiT) models, their practical deployment is severely hindered by extensive
computational requirements. This inefficiency stems from two key challenges:
the quadratic complexity of self-attention with respect to token length and the
multi-step nature of diffusion models. To address these limitations, we present
Jenga, a novel inference pipeline that combines dynamic attention carving with
progressive resolution generation. Our approach leverages two key insights: (1)
early denoising steps do not require high-resolution latents, and (2) later
steps do not require dense attention. Jenga introduces a block-wise attention
mechanism that dynamically selects relevant token interactions using 3D
space-filling curves, alongside a progressive resolution strategy that
gradually increases latent resolution during generation. Experimental results
demonstrate that Jenga achieves substantial speedups across multiple
state-of-the-art video diffusion models while maintaining comparable generation
quality (8.83$\times$ speedup with 0.01\% performance drop on VBench). As a
plug-and-play solution, Jenga enables practical, high-quality video generation
on modern hardware by reducing inference time from minutes to seconds --
without requiring model retraining. Code:
https://github.com/dvlab-research/Jenga

</details>


### [206] [T2I-ConBench: Text-to-Image Benchmark for Continual Post-training](https://arxiv.org/abs/2505.16875)
*Zhehao Huang,Yuhang Liu,Yixin Lou,Zhengbao He,Mingzhen He,Wenxing Zhou,Tao Li,Kehan Li,Zeyi Huang,Xiaolin Huang*

Main category: cs.CV

TL;DR: 论文提出了T2I-ConBench，一个用于文本到图像模型持续后训练的统一基准，分析了四种维度，并评估了十种方法，发现现有方法均未在所有方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 持续后训练可以避免单独模型的成本，但缺乏标准化评估协议阻碍了相关研究。

Method: 引入T2I-ConBench基准，结合自动化指标、人类偏好建模和视觉语言QA，评估十种方法在三种任务序列中的表现。

Result: 发现现有方法无法在所有维度上表现优异，跨任务泛化问题仍未解决。

Conclusion: 发布数据集、代码和评估工具以加速文本到图像模型持续后训练的研究。

Abstract: Continual post-training adapts a single text-to-image diffusion model to
learn new tasks without incurring the cost of separate models, but naive
post-training causes forgetting of pretrained knowledge and undermines
zero-shot compositionality. We observe that the absence of a standardized
evaluation protocol hampers related research for continual post-training. To
address this, we introduce T2I-ConBench, a unified benchmark for continual
post-training of text-to-image models. T2I-ConBench focuses on two practical
scenarios, item customization and domain enhancement, and analyzes four
dimensions: (1) retention of generality, (2) target-task performance, (3)
catastrophic forgetting, and (4) cross-task generalization. It combines
automated metrics, human-preference modeling, and vision-language QA for
comprehensive assessment. We benchmark ten representative methods across three
realistic task sequences and find that no approach excels on all fronts. Even
joint "oracle" training does not succeed for every task, and cross-task
generalization remains unsolved. We release all datasets, code, and evaluation
tools to accelerate research in continual post-training for text-to-image
models.

</details>


### [207] [Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)](https://arxiv.org/abs/2505.16882)
*Isla Duporge,Sofia Minano,Nikoloz Sirmpilatze,Igor Tatarnikov,Scott Wolf,Adam L. Tyson,Daniel Rubenstein*

Main category: cs.CV

TL;DR: 论文评估了三种方法（生物成像配准、SfM管道和混合插值）用于从无人机视频中分离动物运动，并在斑马逃逸事件中验证其效果。最佳方法揭示了群体行为模式。


<details>
  <summary>Details</summary>
Motivation: 无人机技术为动物行为研究提供了高分辨率数据，但分离动物运动与无人机运动是技术挑战，需要高效的开源工具。

Method: 研究比较了生物成像配准、SfM管道和混合插值三种方法，应用于44匹斑马的逃逸事件视频。

Result: 最佳方法成功提取个体轨迹，发现逃逸时群体对齐增强、停止前间距短暂扩大、中心区域协调更紧密等行为模式。

Conclusion: 该方法高效且可扩展，有助于大规模动物集体行为研究。

Abstract: Ethological research increasingly benefits from the growing affordability and
accessibility of drones, which enable the capture of high-resolution footage of
animal movement at fine spatial and temporal scales. However, analyzing such
footage presents the technical challenge of separating animal movement from
drone motion. While non-trivial, computer vision techniques such as image
registration and Structure-from-Motion (SfM) offer practical solutions. For
conservationists, open-source tools that are user-friendly, require minimal
setup, and deliver timely results are especially valuable for efficient data
interpretation. This study evaluates three approaches: a bioimaging-based
registration technique, an SfM pipeline, and a hybrid interpolation method. We
apply these to a recorded escape event involving 44 plains zebras, captured in
a single drone video. Using the best-performing method, we extract individual
trajectories and identify key behavioral patterns: increased alignment
(polarization) during escape, a brief widening of spacing just before stopping,
and tighter coordination near the group's center. These insights highlight the
method's effectiveness and its potential to scale to larger datasets,
contributing to broader investigations of collective animal behavior.

</details>


### [208] [RealEngine: Simulating Autonomous Driving in Realistic Context](https://arxiv.org/abs/2505.16902)
*Junzhe Jiang,Nan Song,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: RealEngine是一个新型驾驶模拟框架，通过3D场景重建和新视角合成技术实现逼真闭环模拟，满足多模态感知、多样交通场景和多代理交互需求。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器和基准测试未能全面满足高质量驾驶模拟的关键要求，如多模态感知、闭环评估和多样性场景。

Method: 利用真实世界多模态传感器数据，分别重建背景场景和前景交通参与者，通过灵活场景组合实现逼真渲染。

Result: RealEngine支持非反应式模拟、安全测试和多代理交互，形成全面可靠的驾驶代理评估基准。

Conclusion: RealEngine填补了现有模拟器的不足，为驾驶代理的评估提供了逼真且灵活的闭环模拟环境。

Abstract: Driving simulation plays a crucial role in developing reliable driving agents
by providing controlled, evaluative environments. To enable meaningful
assessments, a high-quality driving simulator must satisfy several key
requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with
realistic scene rendering to minimize observational discrepancies; closed-loop
evaluation to support free-form trajectory behaviors; highly diverse traffic
scenarios for thorough evaluation; multi-agent cooperation to capture
interaction dynamics; and high computational efficiency to ensure affordability
and scalability. However, existing simulators and benchmarks fail to
comprehensively meet these fundamental criteria. To bridge this gap, this paper
introduces RealEngine, a novel driving simulation framework that holistically
integrates 3D scene reconstruction and novel view synthesis techniques to
achieve realistic and flexible closed-loop simulation in the driving context.
By leveraging real-world multi-modal sensor data, RealEngine reconstructs
background scenes and foreground traffic participants separately, allowing for
highly diverse and realistic traffic scenarios through flexible scene
composition. This synergistic fusion of scene reconstruction and view synthesis
enables photorealistic rendering across multiple sensor modalities, ensuring
both perceptual fidelity and geometric accuracy. Building upon this
environment, RealEngine supports three essential driving simulation categories:
non-reactive simulation, safety testing, and multi-agent interaction,
collectively forming a reliable and comprehensive benchmark for evaluating the
real-world performance of driving agents.

</details>


### [209] [DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?](https://arxiv.org/abs/2505.16915)
*Qirui Jiao,Daoyuan Chen,Yilun Huang,Xika Lin,Ying Shen,Yaliang Li*

Main category: cs.CV

TL;DR: DetailMaster是一个专门评估文本到图像（T2I）模型处理长文本提示能力的基准测试，揭示了当前模型在复杂细节和结构理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在简短描述下表现良好，但在长且细节密集的提示下性能显著下降，缺乏专业应用所需的系统评估工具。

Method: 提出DetailMaster基准，包含四个关键评估维度（如属性绑定和空间关系），使用平均284.89个token的长提示，并通过专家验证。

Result: 评估7个通用和5个优化模型，发现关键维度准确率仅约50%，且性能随提示长度增加而下降。

Conclusion: 揭示了模型在结构理解和细节处理上的系统性缺陷，呼吁未来研究改进组合推理架构，并开源数据集和工具。

Abstract: While recent text-to-image (T2I) models show impressive capabilities in
synthesizing images from brief descriptions, their performance significantly
degrades when confronted with long, detail-intensive prompts required in
professional applications. We present DetailMaster, the first comprehensive
benchmark specifically designed to evaluate T2I models' systematical abilities
to handle extended textual inputs that contain complex compositional
requirements. Our benchmark introduces four critical evaluation dimensions:
Character Attributes, Structured Character Locations, Multi-Dimensional Scene
Attributes, and Explicit Spatial/Interactive Relationships. The benchmark
comprises long and detail-rich prompts averaging 284.89 tokens, with high
quality validated by expert annotators. Evaluation on 7 general-purpose and 5
long-prompt-optimized T2I models reveals critical performance limitations:
state-of-the-art models achieve merely ~50% accuracy in key dimensions like
attribute binding and spatial reasoning, while all models showing progressive
performance degradation as prompt length increases. Our analysis highlights
systemic failures in structural comprehension and detail overload handling,
motivating future research into architectures with enhanced compositional
reasoning. We open-source the dataset, data curation code, and evaluation tools
to advance detail-rich T2I generation and enable broad applications that would
otherwise be infeasible due to the lack of a dedicated benchmark.

</details>


### [210] [Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation](https://arxiv.org/abs/2505.16942)
*Karlis Martins Briedis,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出了一种更高效的光流估计方法，通过改进全对相关体积采样实现，显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高分辨率图像时存在计算和内存复杂度高的问题，导致细节丢失。

Method: 提出了一种高效的全对相关体积采样实现，与RAFT的数学操作一致，但更节省资源。

Result: 新方法在内存使用上降低95%，速度提升90%，并在高分辨率数据集上达到最优性能。

Conclusion: 该方法在内存受限环境下显著提升了光流估计的效率和精度。

Abstract: Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.

</details>


### [211] [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
*Suhao Yu,Haojin Wang,Juncheng Wu,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: MedFrameQA是首个专注于医学视觉问答（VQA）中多图像推理的基准数据集，旨在模拟临床医生的工作流程。通过自动化流程和多阶段过滤策略构建高质量数据集，评估发现现有多模态大语言模型在多图像推理任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有医学VQA基准多关注单图像分析，而临床诊断通常需要比较多张图像。为更贴近实际工作流程，需开发多图像推理的评估基准。

Method: 开发自动化流程从医学视频中提取时序连贯的帧，并构建逻辑连贯的VQA项目；采用多阶段过滤策略确保数据质量。

Result: 构建了包含2,851个VQA对的数据集，覆盖9个人体系统和43个器官。评估显示所有模型表现不佳，准确率大多低于50%，且随图像数量增加波动。

Conclusion: MedFrameQA揭示了现有模型在多图像推理中的不足，希望推动临床多图像推理研究和更强大的诊断AI系统发展。

Abstract: Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.

</details>


### [212] [UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation](https://arxiv.org/abs/2505.16971)
*Himangi Mittal,Peiye Zhuang,Hsin-Ying Lee,Shubham Tulsiani*

Main category: cs.CV

TL;DR: UniPhy是一种通用的潜在条件神经本构模型，能够编码多种材料的物理特性，并通过可微模拟推断材料属性，无需用户指定材料类型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖用户指定的材料类型信息，而UniPhy旨在通过共享训练提高估计的鲁棒性和准确性。

Method: UniPhy通过模拟不同几何形状和材料的轨迹进行训练，利用潜在优化推断未知材料的属性。

Result: UniPhy在推断材料属性时比现有方法更准确，并能支持新条件下的重模拟。

Conclusion: UniPhy在无需材料类型信息的情况下，实现了更准确的逆向模拟和重模拟能力。

Abstract: We propose UniPhy, a common latent-conditioned neural constitutive model that
can encode the physical properties of diverse materials. At inference UniPhy
allows `inverse simulation' i.e. inferring material properties by optimizing
the scene-specific latent to match the available observations via
differentiable simulation. In contrast to existing methods that treat such
inference as system identification, UniPhy does not rely on user-specified
material type information. Compared to prior neural constitutive modeling
approaches which learn instance specific networks, the shared training across
materials improves both, robustness and accuracy of the estimates. We train
UniPhy using simulated trajectories across diverse geometries and materials --
elastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At
inference, given an object with unknown material properties, UniPhy can infer
the material properties via latent optimization to match the motion
observations, and can then allow re-simulating the object under diverse
scenarios. We compare UniPhy against prior inverse simulation methods, and show
that the inference from UniPhy enables more accurate replay and re-simulation
under novel conditions.

</details>


### [213] [OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning](https://arxiv.org/abs/2505.16974)
*Zongyan Han,Jiale Cao,Shuo Chen,Tong Wang,Jorma Laaksonen,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: OpenSeg-R提出了一种逐步视觉推理框架，用于开放词汇分割，显著提升了分割精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇分割方法缺乏显式推理和上下文理解，难以区分相似类别。

Method: 利用大型多模态模型进行分层视觉推理，生成结构化三元组，并基于推理步骤构建详细描述提示。

Result: 在五个基准数据集上显著优于现有方法，并在开放词汇全景分割中实现一致提升。

Conclusion: OpenSeg-R首次将逐步视觉推理引入开放词汇分割，提升了分割精度和可解释性。

Abstract: Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its
capacity to generalize segmentation beyond predefined categories. However,
existing methods typically predict segmentation masks with simple forward
inference, lacking explicit reasoning and interpretability. This makes it
challenging for OVS model to distinguish similar categories in open-world
settings due to the lack of contextual understanding and discriminative visual
cues. To address this limitation, we propose a step-by-step visual reasoning
framework for open-vocabulary segmentation, named OpenSeg-R. The proposed
OpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical
visual reasoning before segmentation. Specifically, we generate both generic
and image-specific reasoning for each image, forming structured triplets that
explain the visual reason for objects in a coarse-to-fine manner. Based on
these reasoning steps, we can compose detailed description prompts, and feed
them to the segmentor to produce more accurate segmentation masks. To the best
of our knowledge, OpenSeg-R is the first framework to introduce explicit
step-by-step visual reasoning into OVS. Experimental results demonstrate that
OpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary
semantic segmentation across five benchmark datasets. Moreover, it achieves
consistent gains across all metrics on open-vocabulary panoptic segmentation.
Qualitative results further highlight the effectiveness of our reasoning-guided
framework in improving both segmentation precision and interpretability. Our
code is publicly available at https://github.com/Hanzy1996/OpenSeg-R.

</details>


### [214] [Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation](https://arxiv.org/abs/2505.16985)
*Moru Liu,Hao Dong,Jessica Kelly,Olga Fink,Mario Trapp*

Main category: cs.CV

TL;DR: 提出了一种名为Feature Mixing的快速多模态异常合成方法，用于提升OOD检测性能，并发布了CARLA-OOD数据集。


<details>
  <summary>Details</summary>
Motivation: 现实应用多为多模态，但现有研究集中于单模态图像数据，且缺乏对未知数据的监督信号。

Method: 提出Feature Mixing方法，通过多模态特征混合生成异常样本，理论支持且模态无关。

Result: 在多个数据集上实现SOTA性能，速度提升10至370倍。

Conclusion: Feature Mixing在多模态OOD检测中高效且性能优越，CARLA-OOD数据集填补了多模态OOD分割的空白。

Abstract: Out-of-distribution (OOD) detection and segmentation are crucial for
deploying machine learning models in safety-critical applications such as
autonomous driving and robot-assisted surgery. While prior research has
primarily focused on unimodal image data, real-world applications are
inherently multimodal, requiring the integration of multiple modalities for
improved OOD detection. A key challenge is the lack of supervision signals from
unknown data, leading to overconfident predictions on OOD samples. To address
this challenge, we propose Feature Mixing, an extremely simple and fast method
for multimodal outlier synthesis with theoretical support, which can be further
optimized to help the model better distinguish between in-distribution (ID) and
OOD data. Feature Mixing is modality-agnostic and applicable to various
modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal
dataset for OOD segmentation, featuring synthetic OOD objects across diverse
scenes and weather conditions. Extensive experiments on SemanticKITTI,
nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that
Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370
\times$ speedup. Our source code and dataset will be available at
https://github.com/mona4399/FeatureMixing.

</details>


### [215] [Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding](https://arxiv.org/abs/2505.16990)
*Runpeng Yu,Xinyin Ma,Xinchao Wang*

Main category: cs.CV

TL;DR: Dimple是首个离散扩散多模态大语言模型（DMLLM），通过结合自回归和扩散训练阶段，性能超越LLaVA-NEXT 3.9%，并提出自信解码策略提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决纯离散扩散方法导致的训练不稳定、性能不佳和长度偏差问题。

Method: 设计结合自回归和扩散阶段的训练范式，提出自信解码策略和预填充技术。

Result: Dimple-7B性能提升3.9%，推理效率显著提高（迭代次数减少至响应长度的1/3）。

Conclusion: 验证了DMLLM的可行性和优势，提升了推理效率和可控性。

Abstract: In this work, we propose Dimple, the first Discrete Diffusion Multimodal
Large Language Model (DMLLM). We observe that training with a purely discrete
diffusion approach leads to significant training instability, suboptimal
performance, and severe length bias issues. To address these challenges, we
design a novel training paradigm that combines an initial autoregressive phase
with a subsequent diffusion phase. This approach yields the Dimple-7B model,
trained on the same dataset and using a similar training pipeline as
LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,
demonstrating that DMLLM can achieve performance comparable to that of
autoregressive models. To improve inference efficiency, we propose a decoding
strategy termed confident decoding, which dynamically adjusts the number of
tokens generated at each step, significantly reducing the number of generation
iterations. In autoregressive models, the number of forward iterations during
generation equals the response length. With confident decoding, however, the
number of iterations needed by Dimple is even only $\frac{\text{response
length}}{3}$. We also re-implement the prefilling technique in autoregressive
models and demonstrate that it does not significantly impact performance on
most benchmark evaluations, while offering a speedup of 1.5x to 7x.
Additionally, we explore Dimple's capability to precisely control its response
using structure priors. These priors enable structured responses in a manner
distinct from instruction-based or chain-of-thought prompting, and allow
fine-grained control over response format and length, which is difficult to
achieve in autoregressive models. Overall, this work validates the feasibility
and advantages of DMLLM and enhances its inference efficiency and
controllability. Code and models are available at
https://github.com/yu-rp/Dimple.

</details>


### [216] [An Effective Training Framework for Light-Weight Automatic Speech Recognition Models](https://arxiv.org/abs/2505.16991)
*Abdul Hannan,Alessio Brutti,Shah Nawaz,Mubashir Noman*

Main category: cs.CV

TL;DR: 提出了一种基于两步表示学习的方法，从单一大型模型中生成多个小型模型，显著提升性能并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将大型ASR模型压缩为小型模型时，常导致性能显著下降或需要长时间训练。本文旨在解决这一问题。

Method: 采用两步表示学习方法，从单一大型模型中生成多个小型模型，确保在有限训练周期内获得更好性能。

Result: 在ASR基准测试中，实现了三倍训练速度提升和高达12.54%的词错误率改善。

Conclusion: 该方法有效解决了大型ASR模型在低资源设备上部署的难题，性能显著优于现有方法。

Abstract: Recent advancement in deep learning encouraged developing large automatic
speech recognition (ASR) models that achieve promising results while ignoring
computational and memory constraints. However, deploying such models on low
resource devices is impractical despite of their favorable performance.
Existing approaches (pruning, distillation, layer skip etc.) transform the
large models into smaller ones at the cost of significant performance
degradation or require prolonged training of smaller models for better
performance. To address these issues, we introduce an efficacious two-step
representation learning based approach capable of producing several small sized
models from a single large model ensuring considerably better performance in
limited number of epochs. Comprehensive experimentation on ASR benchmarks
reveals the efficacy of our approach, achieving three-fold training speed-up
and up to 12.54% word error rate improvement.

</details>


### [217] [Native Segmentation Vision Transformers](https://arxiv.org/abs/2505.16993)
*Guillem Brasó,Aljoša Ošep,Laura Leal-Taixé*

Main category: cs.CV

TL;DR: 提出了一种基于内容感知空间分组层的视觉Transformer架构，实现无需额外分割头的原生分割。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀下采样方法在视觉主干网络中仍占主导地位，但缺乏对图像内容和边界的动态适应能力。

Method: 通过动态分配token到基于图像边界和语义内容的减少集合，构建层次化分割，形成Native Segmentation Vision Transformer。

Result: 架构设计使得仅通过分组层即可生成强分割掩码，无需额外分割头，支持零样本分割和高效下游任务设计。

Conclusion: 为原生、主干级分割提供了新范式，支持无监督分割和高效模型设计。

Abstract: Uniform downsampling remains the de facto standard for reducing spatial
resolution in vision backbones. In this work, we propose an alternative design
built around a content-aware spatial grouping layer, that dynamically assigns
tokens to a reduced set based on image boundaries and their semantic content.
Stacking our grouping layer across consecutive backbone stages results in
hierarchical segmentation that arises natively in the feature extraction
process, resulting in our coined Native Segmentation Vision Transformer. We
show that a careful design of our architecture enables the emergence of strong
segmentation masks solely from grouping layers, that is, without additional
segmentation-specific heads. This sets the foundation for a new paradigm of
native, backbone-level segmentation, which enables strong zero-shot results
without mask supervision, as well as a minimal and efficient standalone model
design for downstream segmentation tasks. Our project page is
https://research.nvidia.com/labs/dvl/projects/native-segmentation.

</details>


### [218] [Seeing through Satellite Images at Street Views](https://arxiv.org/abs/2505.17001)
*Ming Qian,Bin Tan,Qiuyu Wang,Xianwei Zheng,Hanjiang Xiong,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: 本文提出Sat2Density++方法，通过从卫星和街景图像对中学习神经辐射场，实现逼真的街景全景图像和视频合成。


<details>
  <summary>Details</summary>
Motivation: 解决卫星与街景图像间视角变化大且数据稀疏的挑战，实现街景特定元素（如天空和光照效果）的逼真渲染。

Method: 提出Sat2Density++方法，建模街景特定元素于神经网络中，解决稀疏视角和大视角变化问题。

Result: 在城乡场景数据集上验证，Sat2Density++能生成多视角一致且忠实于卫星图像的逼真街景全景。

Conclusion: Sat2Density++成功实现了从卫星图像到街景全景的逼真渲染，解决了视角变化和稀疏数据的挑战。

Abstract: This paper studies the task of SatStreet-view synthesis, which aims to render
photorealistic street-view panorama images and videos given any satellite image
and specified camera positions or trajectories. We formulate to learn neural
radiance field from paired images captured from satellite and street
viewpoints, which comes to be a challenging learning problem due to the
sparse-view natural and the extremely-large viewpoint changes between satellite
and street-view images. We tackle the challenges based on a task-specific
observation that street-view specific elements, including the sky and
illumination effects are only visible in street-view panoramas, and present a
novel approach Sat2Density++ to accomplish the goal of photo-realistic
street-view panoramas rendering by modeling these street-view specific in
neural networks. In the experiments, our method is testified on both urban and
suburban scene datasets, demonstrating that Sat2Density++ is capable of
rendering photorealistic street-view panoramas that are consistent across
multiple views and faithful to the satellite image.

</details>


### [219] [PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association](https://arxiv.org/abs/2505.17002)
*Abdul Hannan,Muhammad Arslan Manzoor,Shah Nawaz,Muhammad Irzam Liaqat,Markus Schedl,Mubashir Noman*

Main category: cs.CV

TL;DR: 论文提出了一种改进的面部-声音关联学习方法，通过对齐嵌入空间和增强门控融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前面部-声音关联学习方法存在负样本挖掘和依赖边缘参数的问题，需改进。

Method: 提出对齐嵌入空间并应用增强门控融合的方法。

Result: 在VoxCeleb数据集上验证了方法的有效性。

Conclusion: 该方法显著提升了面部-声音关联的性能。

Abstract: We study the task of learning association between faces and voices, which is
gaining interest in the multimodal community lately. These methods suffer from
the deliberate crafting of negative mining procedures as well as the reliance
on the distant margin parameter. These issues are addressed by learning a joint
embedding space in which orthogonality constraints are applied to the fused
embeddings of faces and voices. However, embedding spaces of faces and voices
possess different characteristics and require spaces to be aligned before
fusing them. To this end, we propose a method that accurately aligns the
embedding spaces and fuses them with an enhanced gated fusion thereby improving
the performance of face-voice association. Extensive experiments on the
VoxCeleb dataset reveals the merits of the proposed approach.

</details>


### [220] [CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning](https://arxiv.org/abs/2505.17006)
*Jiange Yang,Yansong Shi,Haoyi Zhu,Mingyu Liu,Kaijing Ma,Yating Wang,Gangshan Wu,Tong He,Limin Wang*

Main category: cs.CV

TL;DR: CoMo提出了一种从互联网视频中学习连续运动表示的方法，解决了离散潜在动作方法的信息丢失问题，并通过新指标和零样本泛化能力提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散潜在动作方法存在信息丢失问题，难以处理复杂和细粒度的动态。

Method: CoMo采用早期时间特征差异机制防止模型崩溃，并通过信息瓶颈原则约束潜在运动嵌入维度。引入两个新指标评估运动学习。

Result: CoMo表现出强大的零样本泛化能力，能够在未见过的视频域中生成连续伪动作，提升策略性能。

Conclusion: CoMo通过连续运动表示和新指标，显著提升了运动学习的效果，适用于多种视频数据集和实际场景。

Abstract: Learning latent motion from Internet videos is crucial for building
generalist robots. However, existing discrete latent action methods suffer from
information loss and struggle with complex and fine-grained dynamics. We
propose CoMo, which aims to learn more informative continuous motion
representations from diverse, internet-scale videos. CoMo employs a early
temporal feature difference mechanism to prevent model collapse and suppress
static appearance noise, effectively discouraging shortcut learning problem.
Furthermore, guided by the information bottleneck principle, we constrain the
latent motion embedding dimensionality to achieve a better balance between
retaining sufficient action-relevant information and minimizing the inclusion
of action-irrelevant appearance noise. Additionally, we also introduce two new
metrics for more robustly and affordably evaluating motion and guiding motion
learning methods development: (i) the linear probing MSE of action prediction,
and (ii) the cosine similarity between past-to-current and future-to-current
motion embeddings. Critically, CoMo exhibits strong zero-shot generalization,
enabling it to generate continuous pseudo actions for previously unseen video
domains. This capability facilitates unified policy joint learning using pseudo
actions derived from various action-less video datasets (such as
cross-embodiment videos and, notably, human demonstration videos), potentially
augmented with limited labeled robot data. Extensive experiments show that
policies co-trained with CoMo pseudo actions achieve superior performance with
both diffusion and autoregressive architectures in simulated and real-world
settings.

</details>


### [221] [Deep mineralogical segmentation of thin section images based on QEMSCAN maps](https://arxiv.org/abs/2505.17008)
*Jean Pablo Vieira de Mello,Matheus Augusto Alves Cuglieri,Leandro P. de Figueiredo,Fernando Bordignon,Marcelo Ramalho Albuquerque,Rodrigo Surmas,Bruno Cavalcanti de Paula*

Main category: cs.CV

TL;DR: 该论文提出了一种基于卷积神经网络（U-Net）的自动矿物分割模型，用于碳酸盐岩薄片图像的矿物学分析，以低成本、高效的方式模拟QEMSCAN技术。


<details>
  <summary>Details</summary>
Motivation: 传统矿物学分析依赖人工或昂贵设备（如QEMSCAN），存在主观性和高成本问题。

Method: 使用U-Net架构，以平面和交叉偏振薄片图像为输入，QEMSCAN矿物分布图为目标进行训练，并验证模型在训练集内外的泛化能力。

Result: 模型在矿物边界划分和分布预测上表现良好，R^2值在训练集内为0.97，训练集外为0.88。

Conclusion: 该模型为矿物学分析提供了一种低成本、高效的替代方案，但分割质量受图像分辨率和岩石纹理多样性影响。

Abstract: Interpreting the mineralogical aspects of rock thin sections is an important
task for oil and gas reservoirs evaluation. However, human analysis tend to be
subjective and laborious. Technologies like QEMSCAN(R) are designed to automate
the mineralogical mapping process, but also suffer from limitations like high
monetary costs and time-consuming analysis. This work proposes a Convolutional
Neural Network model for automatic mineralogical segmentation of thin section
images of carbonate rocks. The model is able to mimic the QEMSCAN mapping
itself in a low-cost, generalized and efficient manner. For this, the U-Net
semantic segmentation architecture is trained on plane and cross polarized thin
section images using the corresponding QEMSCAN maps as target, which is an
approach not widely explored. The model was instructed to differentiate
occurrences of Calcite, Dolomite, Mg-Clay Minerals, Quartz, Pores and the
remaining mineral phases as an unique class named "Others", while it was
validated on rock facies both seen and unseen during training, in order to
address its generalization capability. Since the images and maps are provided
in different resolutions, image registration was applied to align then
spatially. The study reveals that the quality of the segmentation is very much
dependent on these resolution differences and on the variety of learnable rock
textures. However, it shows promising results, especially with regard to the
proper delineation of minerals boundaries on solid textures and precise
estimation of the minerals distributions, describing a nearly linear
relationship between expected and predicted distributions, with coefficient of
determination (R^2) superior to 0.97 for seen facies and 0.88 for unseen.

</details>


### [222] [Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space](https://arxiv.org/abs/2505.17011)
*Yan Li,Changyao Tian,Renqiu Xia,Ning Liao,Weiwei Guo,Junchi Yan,Hongsheng Li,Jifeng Dai,Hao Li,Xue Yang*

Main category: cs.CV

TL;DR: AdapTok是一种自适应视频标记化方法，通过动态分配标记提升视频重建和生成效率。


<details>
  <summary>Details</summary>
Motivation: 视频内容复杂且动态变化，固定标记分配效率低下，需要一种自适应方法。

Method: 采用块掩码策略和块因果评分器，结合整数线性规划动态分配标记。

Result: 在UCF-101和Kinetics-600上显著提升重建质量和生成性能。

Conclusion: AdapTok在可控预算下实现高效、可扩展的视频建模。

Abstract: We propose AdapTok, an adaptive temporal causal video tokenizer that can
flexibly allocate tokens for different frames based on video content. AdapTok
is equipped with a block-wise masking strategy that randomly drops tail tokens
of each block during training, and a block causal scorer to predict the
reconstruction quality of video frames using different numbers of tokens.
During inference, an adaptive token allocation strategy based on integer linear
programming is further proposed to adjust token usage given predicted scores.
Such design allows for sample-wise, content-aware, and temporally dynamic token
allocation under a controllable overall budget. Extensive experiments for video
reconstruction and generation on UCF-101 and Kinetics-600 demonstrate the
effectiveness of our approach. Without additional image data, AdapTok
consistently improves reconstruction quality and generation performance under
different token budgets, allowing for more scalable and token-efficient
generative video modeling.

</details>


### [223] [SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding](https://arxiv.org/abs/2505.17012)
*Haoning Wu,Xiao Huang,Yaohui Chen,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）的3D空间感知能力，提出了VGBench和SpatialScore两个基准测试，并开发了SpatialAgent系统进行评测。


<details>
  <summary>Details</summary>
Motivation: 探索现有MLLMs是否具备3D空间感知和理解能力。

Method: 提出VGBench和SpatialScore基准测试，开发SpatialAgent多智能体系统，支持多种推理范式。

Result: 评测揭示了空间推理的挑战，同时证明了SpatialAgent的有效性。

Conclusion: SpatialScore将为MLLMs的进一步发展提供有价值的基准和见解。

Abstract: Multimodal large language models (MLLMs) have achieved impressive success in
question-answering tasks, yet their capabilities for spatial understanding are
less explored. This work investigates a critical question: do existing MLLMs
possess 3D spatial perception and understanding abilities? Concretely, we make
the following contributions in this paper: (i) we introduce VGBench, a
benchmark specifically designed to assess MLLMs for visual geometry perception,
e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most
comprehensive and diverse multimodal spatial understanding benchmark to date,
integrating VGBench with relevant data from the other 11 existing datasets.
This benchmark comprises 28K samples across various spatial understanding
tasks, modalities, and QA formats, along with a carefully curated challenging
subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent
system incorporating 9 specialized tools for spatial understanding, supporting
both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive
evaluations to reveal persistent challenges in spatial reasoning while
demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will
offer valuable insights and serve as a rigorous benchmark for the next
evolution of MLLMs.

</details>


### [224] [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
*Runsen Xu,Weiyao Wang,Hao Tang,Xingyu Chen,Xiaodong Wang,Fu-Jen Chu,Dahua Lin,Matt Feiszli,Kevin J. Liang*

Main category: cs.CV

TL;DR: 论文提出了一种框架，通过整合深度感知、视觉对应和动态感知，赋予多模态大语言模型（MLLMs）多帧空间理解能力，并引入MultiSPA数据集和基准测试。模型Multi-SpatialMLLM在性能上显著优于基线模型和专有系统。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在空间理解上局限于单图像，无法满足机器人等需要多帧推理的实际应用需求。

Method: 提出框架整合深度感知、视觉对应和动态感知，并基于MultiSPA数据集（2700万样本）训练模型。

Result: Multi-SpatialMLLM在基准测试中表现优异，展示了可扩展和通用的多帧推理能力，并在机器人任务中表现出多任务优势和新兴能力。

Conclusion: 该框架显著提升了MLLMs的多帧空间理解能力，为实际应用提供了潜力。

Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.

</details>


### [225] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 论文探讨了强化学习（RL）在提升大型语言模型（LLMs）的链式思维（CoT）推理能力中的作用，重点比较了DPO和GRPO两种算法在自回归图像生成中的表现和挑战。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成作为CoT推理过程面临独特挑战，如文本-图像一致性、图像美学质量等，而现有研究缺乏对这些挑战及不同RL策略特性的深入分析。

Method: 通过全面研究GRPO和DPO算法在自回归图像生成中的应用，评估其域内性能和域外泛化能力，并分析不同奖励模型的影响。

Result: 发现GRPO和DPO各有优势，且奖励模型的内在泛化能力可能增强RL算法的泛化潜力；同时探索了三种扩展策略以提升性能。

Conclusion: 研究为开发更有效的RL算法以实现自回归图像生成中的稳健CoT推理提供了新思路。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [226] [SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://arxiv.org/abs/2505.17018)
*Kaixuan Fan,Kaituo Feng,Haoming Lyu,Dongzhan Zhou,Xiangyu Yue*

Main category: cs.CV

TL;DR: 论文提出SophiaVL-R1，通过为思维过程添加奖励信号，结合信任权重和退火训练策略，提升多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则强化学习的多模态大语言模型缺乏对思维过程的监督，可能导致次优推理策略，影响泛化能力。

Method: 训练思维奖励模型评估思维过程质量，提出Trust-GRPO方法分配信任权重，并设计退火训练策略逐步减少思维奖励。

Result: SophiaVL-R1在多个基准测试中超越现有推理模型，SophiaVL-R1-7B甚至优于参数多10倍的LLaVA-OneVision-72B。

Conclusion: 通过监督思维过程和动态调整奖励，SophiaVL-R1显著提升了模型的推理和泛化能力。

Abstract: Recent advances have shown success in eliciting strong reasoning abilities in
multimodal large language models (MLLMs) through rule-based reinforcement
learning (RL) with outcome rewards. However, this paradigm typically lacks
supervision over the thinking process leading to the final outcome.As a result,
the model may learn sub-optimal reasoning strategies, which can hinder its
generalization ability. In light of this, we propose SophiaVL-R1, as an attempt
to add reward signals for the thinking process in this paradigm. To achieve
this, we first train a thinking reward model that evaluates the quality of the
entire thinking process. Given that the thinking reward may be unreliable for
certain samples due to reward hacking, we propose the Trust-GRPO method, which
assigns a trustworthiness weight to the thinking reward during training. This
weight is computed based on the thinking reward comparison of responses leading
to correct answers versus incorrect answers, helping to mitigate the impact of
potentially unreliable thinking rewards. Moreover, we design an annealing
training strategy that gradually reduces the thinking reward over time,
allowing the model to rely more on the accurate rule-based outcome reward in
later training stages. Experiments show that our SophiaVL-R1 surpasses a series
of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),
demonstrating strong reasoning and generalization capabilities. Notably, our
SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite
the latter having 10 times more parameters. All code, models, and datasets are
made publicly available at https://github.com/kxfan2002/SophiaVL-R1.

</details>


### [227] [CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/abs/2505.17020)
*Shilin Yan,Jiaming Han,Joey Tsai,Hongwei Xue,Rongyao Fang,Lingyi Hong,Ziyu Guo,Ray Zhang*

Main category: cs.CV

TL;DR: CrossLMM通过双交叉注意力机制解耦长视频序列与LMM，显著减少视觉令牌数量且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 随着输入复杂性增加（尤其是长视频序列），令牌数量激增导致计算成本二次增长，亟需高效压缩视频令牌的方法。

Method: 1. 通过池化方法从预训练视觉编码器中显著减少令牌；2. 在LLM层中使用视觉到视觉交叉注意力机制，以池化令牌为查询；3. 引入文本到视觉交叉注意力机制，增强文本令牌的视觉理解。

Result: 在多种视频LMM基准测试中表现相当或更优，同时大幅减少计算资源。

Conclusion: CrossLMM通过高效令牌利用和交叉注意力机制，解决了长视频序列处理中的计算效率问题。

Abstract: The advent of Large Multimodal Models (LMMs) has significantly enhanced Large
Language Models (LLMs) to process and interpret diverse data modalities (e.g.,
image and video). However, as input complexity increases, particularly with
long video sequences, the number of required tokens has grown significantly,
leading to quadratically computational costs. This has made the efficient
compression of video tokens in LMMs, while maintaining performance integrity, a
pressing research challenge. In this paper, we introduce CrossLMM, decoupling
long video sequences from LMMs via a dual cross-attention mechanism, which
substantially reduces visual token quantity with minimal performance
degradation. Specifically, we first implement a significant token reduction
from pretrained visual encoders through a pooling methodology. Then, within LLM
layers, we employ a visual-to-visual cross-attention mechanism, wherein the
pooled visual tokens function as queries against the original visual token set.
This module enables more efficient token utilization while retaining
fine-grained informational fidelity. In addition, we introduce a text-to-visual
cross-attention mechanism, for which the text tokens are enhanced through
interaction with the original visual tokens, enriching the visual comprehension
of the text tokens. Comprehensive empirical evaluation demonstrates that our
approach achieves comparable or superior performance across diverse video-based
LMM benchmarks, despite utilizing substantially fewer computational resources.

</details>


### [228] [ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark](https://arxiv.org/abs/2505.17021)
*Sara Ghaboura,Ketan More,Wafa Alghallabi,Omkar Thawakar,Jorma Laaksonen,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: 论文介绍了首个针对阿拉伯语的多模态推理基准（ARB），用于评估大模型在文本和视觉模态中的逐步推理能力，覆盖11个领域，发现现有模型在连贯性、忠实性和文化背景理解上存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注英语，忽略了阿拉伯语等语言的多模态推理评估，因此需要填补这一空白。

Method: 设计了包含1,356个多模态样本和5,119个人工标注推理步骤的ARB基准，评估了12种先进的大模型。

Result: 发现模型在连贯性、忠实性和文化背景理解上表现不佳。

Conclusion: ARB为诊断多模态推理提供了结构化框架，推动了包容性、透明性和文化感知AI的发展。

Abstract: As Large Multimodal Models (LMMs) become more capable, there is growing
interest in evaluating their reasoning processes alongside their final outputs.
However, most benchmarks remain focused on English, overlooking languages with
rich linguistic and cultural contexts, such as Arabic. To address this gap, we
introduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the
first benchmark designed to evaluate step-by-step reasoning in Arabic across
both textual and visual modalities. ARB spans 11 diverse domains, including
visual reasoning, document understanding, OCR, scientific analysis, and
cultural interpretation. It comprises 1,356 multimodal samples paired with
5,119 human-curated reasoning steps and corresponding actions. We evaluated 12
state-of-the-art open- and closed-source LMMs and found persistent challenges
in coherence, faithfulness, and cultural grounding. ARB offers a structured
framework for diagnosing multimodal reasoning in underrepresented languages and
marks a critical step toward inclusive, transparent, and culturally aware AI
systems. We release the benchmark, rubric, and evaluation suit to support
future research and reproducibility. Code available at:
https://github.com/mbzuai-oryx/ARB

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [229] [Adaptive Tokenization: On the Hop-Overpriority Problem in Tokenized Graph Learning Models](https://arxiv.org/abs/2505.15845)
*Zhibiao Wang,Yunlong Zhou,Ziwei Zhang,Mengmei Zhang,Shirui Pan,Chunming Hu,Xiao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的图令牌列表（LGTL）模块，解决了现有Tokenized Graph Learning Models（TGLMs）中手工设计令牌列表导致的局部节点过度优先问题。


<details>
  <summary>Details</summary>
Motivation: 现有TGLMs依赖手工设计的令牌列表，导致在异质图中无法平衡局部和全局信号，限制了模型的适应性。

Method: 提出LGTL模块，通过图注意力门模块和选择模块自适应调整跳数权重并优先选择信息节点。

Result: 实验验证了LGTL在Graph Transformers和Graph LLM中的有效性。

Conclusion: LGTL解决了跳数过度优先问题，提升了模型在异质图和同质图中的表现。

Abstract: Graph Transformers, leveraging the global attention to capture long-range
dependencies in graph structures, have significantly advanced graph machine
learning, but face prohibitive computational complexity. Tokenized Graph
Learning Models (TGLMs) address this issue by converting graphs into ordered
token lists for scalable processing. Besides, TGLMs also empower Large Language
Models (LLMs) to handle text-attributed graphs more effectively and thus are
also employed in Graph LLMs. However, existing TGLMs rely on hand-designed
token lists and their adaptability to diverse graph learning scenarios remains
unexplored. In this paper, we first conduct extensive empirical and theoretical
preliminary studies for hand-designed token lists. Surprisingly, we identify an
unexplored hop-overpriority problem: the common pre-defined token lists
overemphasize nearby nodes and overwhelm the ability of TGLMs to balance local
and global signals. This phenomenon is especially harmful for heterophilic
graphs. To address this problem, we propose the Learnable Graph Token List
(LGTL), a plug-and-play module to replace hand-designed token lists in TGLMs.
Specifically, LGTL adaptively adjusts the weights across hops and prioritizes
informative nodes within hops through a graph attention gate module and a
selection module, respectively. In this way, contextually informative nodes can
be adaptively emphasized for both homophilic and heterophilic graphs. Besides,
we theoretically show that LGTL can address the hop-overpriority problem.
Extensive experiments on benchmarks validate the efficacy of LGTL across both
Graph Transformers and Graph LLM backbones.

</details>


### [230] [Last Layer Empirical Bayes](https://arxiv.org/abs/2505.15888)
*Valentin Villecroze,Yixin Wang,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: LLEB是一种基于经验贝叶斯的方法，通过可学习的先验分布（归一化流）在最后一层实现不确定性量化，性能与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络预测的不确定性是人工智能的关键挑战，BNN和深度集成是主要方法，但各有局限性。

Method: 提出LLEB，使用归一化流作为可学习的先验分布，仅应用于最后一层，通过最大化证据下界训练。

Result: LLEB在性能上与现有方法相当，展示了经验贝叶斯在不确定性量化中的潜力。

Conclusion: LLEB是BNN和深度集成的折中方案，经验贝叶斯是未来研究的有前景方向。

Abstract: The task of quantifying the inherent uncertainty associated with neural
network predictions is a key challenge in artificial intelligence. Bayesian
neural networks (BNNs) and deep ensembles are among the most prominent
approaches to tackle this task. Both approaches produce predictions by
computing an expectation of neural network outputs over some distribution on
the corresponding weights; this distribution is given by the posterior in the
case of BNNs, and by a mixture of point masses for ensembles. Inspired by
recent work showing that the distribution used by ensembles can be understood
as a posterior corresponding to a learned data-dependent prior, we propose last
layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a
normalizing flow, which is then trained to maximize the evidence lower bound;
to retain tractability we use the flow only on the last layer. We show why LLEB
is well motivated, and how it interpolates between standard BNNs and ensembles
in terms of the strength of the prior that they use. LLEB performs on par with
existing approaches, highlighting that empirical Bayes is a promising direction
for future research in uncertainty quantification.

</details>


### [231] [Is (Selective) Round-To-Nearest Quantization All You Need?](https://arxiv.org/abs/2505.15909)
*Alex Kogan*

Main category: cs.LG

TL;DR: RTN（Round-to-Nearest）是一种简单且低成本的量化方法，尽管被更先进的方法忽视，但其在生成吞吐量和准确性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 反驳RTN在量化LLMs中被低估的观点，证明其在实际应用中的高效性和可行性。

Method: 基于Marlin内核实现RTN，并通过选择性提高某些层和模块的数据精度来逐步提升准确性。

Result: RTN的生成吞吐量优于其他方法，准确性接近更先进的替代方案。

Conclusion: RTN是量化LLMs的实用选择，具有低成本和高性能的优势。

Abstract: Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
quantization technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced quantization methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.

</details>


### [232] [AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning](https://arxiv.org/abs/2505.15931)
*Morteza Alizadeh,Mehrdad Oveisi,Sonya Falahati,Ghazal Mousavi,Mohsen Alambardar Meybodi,Somayeh Sadat Mehrnia,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.LG

TL;DR: AllMetrics是一个开源的Python库，旨在标准化机器学习任务中的性能评估，解决现有库的碎片化和不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习库在性能评估中存在碎片化、实现不一致和数据验证不足的问题，导致结果不可靠。

Method: 开发了AllMetrics库，通过模块化API和输入验证机制，支持多种机器学习任务，并与其他语言组件进行比较验证。

Result: AllMetrics能够减少评估错误，提高机器学习工作流的可信度。

Conclusion: AllMetrics为机器学习性能评估提供了统一、可靠的解决方案。

Abstract: Machine learning (ML) models rely heavily on consistent and accurate
performance metrics to evaluate and compare their effectiveness. However,
existing libraries often suffer from fragmentation, inconsistent
implementations, and insufficient data validation protocols, leading to
unreliable results. Existing libraries have often been developed independently
and without adherence to a unified standard, particularly concerning the
specific tasks they aim to support. As a result, each library tends to adopt
its conventions for metric computation, input/output formatting, error
handling, and data validation protocols. This lack of standardization leads to
both implementation differences (ID) and reporting differences (RD), making it
difficult to compare results across frameworks or ensure reliable evaluations.
To address these issues, we introduce AllMetrics, an open-source unified Python
library designed to standardize metric evaluation across diverse ML tasks,
including regression, classification, clustering, segmentation, and
image-to-image translation. The library implements class-specific reporting for
multi-class tasks through configurable parameters to cover all use cases, while
incorporating task-specific parameters to resolve metric computation
discrepancies across implementations. Various datasets from domains like
healthcare, finance, and real estate were applied to our library and compared
with Python, Matlab, and R components to identify which yield similar results.
AllMetrics combines a modular Application Programming Interface (API) with
robust input validation mechanisms to ensure reproducibility and reliability in
model evaluation. This paper presents the design principles, architectural
components, and empirical analyses demonstrating the ability to mitigate
evaluation errors and to enhance the trustworthiness of ML workflows.

</details>


### [233] [MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding](https://arxiv.org/abs/2505.15946)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Main category: cs.LG

TL;DR: MoRE-Brain是一个基于大脑网络原理的混合专家框架，用于高保真、可适应且可解释的视觉重建，通过动态路由机制提升解码效果。


<details>
  <summary>Details</summary>
Motivation: 当前fMRI视觉解码研究过于注重重建保真度而忽视可解释性，MoRE-Brain旨在填补这一空白，提供更通用的神经解码方法。

Method: 采用分层混合专家架构，专家网络处理功能相关的fMRI信号，通过CLIP空间编码和扩散模型合成图像，动态路由机制优化专家贡献。

Result: 实验验证了MoRE-Brain的高重建保真度，并有效利用fMRI信号，避免过度依赖生成先验。

Conclusion: MoRE-Brain在通用性和可解释性方面取得显著进展，为fMRI视觉解码提供了新方向。

Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand
human perception and develop advanced brain-computer interfaces. However,
current progress often prioritizes maximizing reconstruction fidelity while
overlooking interpretability, an essential aspect for deriving neuroscientific
insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework
designed for high-fidelity, adaptable, and interpretable visual reconstruction.
MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture
where distinct experts process fMRI signals from functionally related voxel
groups, mimicking specialized brain networks. The experts are first trained to
encode fMRI into the frozen CLIP space. A finetuned diffusion model then
synthesizes images, guided by expert outputs through a novel dual-stage routing
mechanism that dynamically weighs expert contributions across the diffusion
process. MoRE-Brain offers three main advancements: First, it introduces a
novel Mixture-of-Experts architecture grounded in brain network principles for
neuro-decoding. Second, it achieves efficient cross-subject generalization by
sharing core expert networks while adapting only subject-specific routers.
Third, it provides enhanced mechanistic insight, as the explicit routing
reveals precisely how different modeled brain regions shape the semantic and
spatial attributes of the reconstructed image. Extensive experiments validate
MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further
demonstrating its effective utilization of fMRI signals, distinguishing genuine
neural decoding from over-reliance on generative priors. Consequently,
MoRE-Brain marks a substantial advance towards more generalizable and
interpretable fMRI-based visual decoding. Code will be publicly available soon:
https://github.com/yuxiangwei0808/MoRE-Brain.

</details>


### [234] [Towards Identifiability of Interventional Stochastic Differential Equations](https://arxiv.org/abs/2505.15987)
*Aaron Zweig,Zaikang Lin,Elham Azizi,David Knowles*

Main category: cs.LG

TL;DR: 论文研究了在多重干预下随机微分方程（SDE）模型的可识别性，首次给出了从平稳分布样本中唯一恢复SDE参数的证明界限。


<details>
  <summary>Details</summary>
Motivation: 研究SDE模型在多重干预下的可识别性，填补了从平稳分布样本中恢复参数的证明空白。

Method: 针对线性SDE给出了紧致的干预次数界限，对非线性SDE在小噪声情况下给出了上限。

Result: 通过合成数据实验验证了真实参数的恢复，并展示了可学习激活函数参数化的优势。

Conclusion: 论文为SDE模型的可识别性提供了理论支持，并通过实验验证了方法的有效性。

Abstract: We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.

</details>


### [235] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
*Aaron J. Li,Suraj Srinivas,Usha Bhalla,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: SAEs用于解释LLMs的内部激活，但现有评估忽略了概念表示的鲁棒性。本文提出鲁棒性量化方法，发现SAE表示易受微小对抗扰动影响，可能不适合模型监控。


<details>
  <summary>Details</summary>
Motivation: 现有SAE评估忽视了概念表示对输入扰动的鲁棒性，而鲁棒性是概念标签真实性的关键。

Method: 通过输入空间优化问题量化鲁棒性，并开发评估框架，模拟对抗扰动操纵SAE表示的场景。

Result: 实验表明，微小对抗扰动能有效操纵SAE概念表示，但对基础LLM输出影响不大。

Conclusion: SAE概念表示脆弱，可能不适合模型监控和监管应用。

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [236] [GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2505.16017)
*Mariia Seleznova,Hung-Hsu Chou,Claudio Mayrink Verdun,Gitta Kutyniok*

Main category: cs.LG

TL;DR: GradPCA是一种基于梯度主成分分析的OOD检测方法，利用NTK对齐的低秩结构，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法性能不一致，GradPCA旨在通过梯度分析和NTK对齐提供更稳定的解决方案。

Method: 对梯度类均值应用PCA，结合NTK对齐的理论支持，分析特征空间特性以实现有效检测。

Result: 实验验证GradPCA性能优越，特征质量（如预训练表示）对检测效果至关重要。

Conclusion: GradPCA为设计更原则化的光谱OOD检测器提供了理论和实践指导。

Abstract: We introduce GradPCA, an Out-of-Distribution (OOD) detection method that
exploits the low-rank structure of neural network gradients induced by Neural
Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis
(PCA) to gradient class-means, achieving more consistent performance than
existing methods across standard image classification benchmarks. We provide a
theoretical perspective on spectral OOD detection in neural networks to support
GradPCA, highlighting feature-space properties that enable effective detection
and naturally emerge from NTK alignment. Our analysis further reveals that
feature quality -- particularly the use of pretrained versus non-pretrained
representations -- plays a crucial role in determining which detectors will
succeed. Extensive experiments validate the strong performance of GradPCA, and
our theoretical framework offers guidance for designing more principled
spectral OOD detectors.

</details>


### [237] [Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging](https://arxiv.org/abs/2505.16024)
*Weiguo Gao,Ming Li*

Main category: cs.LG

TL;DR: 论文研究了扩散轨迹蒸馏方法，通过将多步去噪过程压缩为单步生成，提升采样速度，并探讨了不同蒸馏策略与生成质量之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但采样速度慢，现有蒸馏方法缺乏理论指导，优化和选择策略复杂。

Method: 将轨迹蒸馏重新解释为线性机制下的算子合并问题，提出动态规划算法计算最优合并策略。

Result: 发现信号收缩现象及数据协方差结构主导的相变现象，提出最优合并策略。

Conclusion: 研究增强了扩散轨迹蒸馏的理论理解，并为优化蒸馏策略提供了实用指导。

Abstract: Diffusion trajectory distillation methods aim to accelerate sampling in
diffusion models, which produce high-quality outputs but suffer from slow
sampling speeds. These methods train a student model to approximate the
multi-step denoising process of a pretrained teacher model in a single step,
enabling one-shot generation. However, theoretical insights into the trade-off
between different distillation strategies and generative quality remain
limited, complicating their optimization and selection. In this work, we take a
first step toward addressing this gap. Specifically, we reinterpret trajectory
distillation as an operator merging problem in the linear regime, where each
step of the teacher model is represented as a linear operator acting on noisy
data. These operators admit a clear geometric interpretation as projections and
rescalings corresponding to the noise schedule. During merging, signal
shrinkage occurs as a convex combination of operators, arising from both
discretization and limited optimization time of the student model. We propose a
dynamic programming algorithm to compute the optimal merging strategy that
maximally preserves signal fidelity. Additionally, we demonstrate the existence
of a sharp phase transition in the optimal strategy, governed by data
covariance structures. Our findings enhance the theoretical understanding of
diffusion trajectory distillation and offer practical insights for improving
distillation strategies.

</details>


### [238] [Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces](https://arxiv.org/abs/2505.16035)
*Alejandro García-Castellanos,David R. Wessels,Nicky J. van den Berg,Remco Duits,Daniël M. Pelt,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出了一种结合等变神经场和神经Eikonal求解器的新框架，通过共享权重和几何基础实现高效、可控的Eikonal解建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统Eikonal求解器在多样性和几何适应性上的不足，提供更高效、可控的解决方案。

Method: 使用单一神经场，通过Lie群中的点云条件化共享主干网络，结合物理信息神经网络（PINNs）建模Eikonal解。

Result: 在2D和3D地震旅行时建模中表现优异，具有高效性、可扩展性和用户可控性。

Conclusion: 该框架为Eikonal问题提供了高效、几何基础强且可控的解决方案，适用于多种黎曼流形。

Abstract: We introduce Equivariant Neural Eikonal Solvers, a novel framework that
integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our
approach employs a single neural field where a unified shared backbone is
conditioned on signal-specific latent variables - represented as point clouds
in a Lie group - to model diverse Eikonal solutions. The ENF integration
ensures equivariant mapping from these latent representations to the solution
field, delivering three key benefits: enhanced representation efficiency
through weight-sharing, robust geometric grounding, and solution steerability.
This steerability allows transformations applied to the latent point cloud to
induce predictable, geometrically meaningful modifications in the resulting
Eikonal solution. By coupling these steerable representations with
Physics-Informed Neural Networks (PINNs), our framework accurately models
Eikonal travel-time solutions while generalizing to arbitrary Riemannian
manifolds with regular group actions. This includes homogeneous spaces such as
Euclidean, position-orientation, spherical, and hyperbolic manifolds. We
validate our approach through applications in seismic travel-time modeling of
2D and 3D benchmark datasets. Experimental results demonstrate superior
performance, scalability, adaptability, and user controllability compared to
existing Neural Operator-based Eikonal solver methods.

</details>


### [239] [Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs](https://arxiv.org/abs/2505.16053)
*Jan Tönshoff,Martin Grohe*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的SAT求解器分支启发式方法（RLAF），利用图神经网络（GNN）学习指导分支策略，显著提升了求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统的SAT求解器依赖手工设计的启发式方法，性能受限。本文旨在通过数据驱动的方式优化分支启发式策略。

Method: 采用GNN在单次前向传递中为变量分配权重和极性，并将其注入现有SAT求解器的分支启发式中。通过强化学习（如GRPO）训练GNN，以求解器的计算成本作为奖励信号。

Result: RLAF训练的策略显著降低了不同SAT求解器的平均求解时间，某些情况下提速超过2倍，并能有效泛化到更大更难的SAT问题。

Conclusion: RLAF在组合优化中提供了一种数据驱动的启发式设计方法，性能优于基于手工权重启发式的专家监督方法。

Abstract: Boolean Satisfiability (SAT) solvers are foundational to computer science,
yet their performance typically hinges on hand-crafted heuristics. This work
introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm
for learning to guide SAT solver branching heuristics with Graph Neural
Networks (GNNs). Central to our approach is a novel and generic mechanism for
injecting inferred variable weights and polarities into the branching
heuristics of existing SAT solvers. In a single forward pass, a GNN assigns
these parameters to all variables. Casting this one-shot guidance as a
reinforcement learning problem lets us train the GNN with off-the-shelf
policy-gradient methods, such as GRPO, directly using the solver's
computational cost as the sole reward signal. Extensive evaluations demonstrate
that RLAF-trained policies significantly reduce the mean solve times of
different base solvers across diverse SAT problem distributions, achieving more
than a 2x speedup in some cases, while generalizing effectively to larger and
harder problems after training. Notably, these policies consistently outperform
expert-supervised approaches based on learning handcrafted weighting
heuristics, offering a promising path towards data-driven heuristic design in
combinatorial optimization.

</details>


### [240] [Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models](https://arxiv.org/abs/2505.16056)
*Jingcong Liang,Siyuan Wang,Miren Tian,Yitong Li,Duyu Tang,Zhongyu Wei*

Main category: cs.LG

TL;DR: 论文提出了两种衡量MoE模型局部路由一致性的指标（SRP和SCH），分析了20种MoE LLMs，发现某些模型设计特征（如每层应用MoE、不共享专家）能提高一致性，并指出领域专家对一致性的贡献更大。


<details>
  <summary>Details</summary>
Motivation: 研究MoE模型在内存受限设备上的高效部署，尤其是局部路由一致性的影响。

Method: 提出SRP和SCH两种指标，分析20种不同规模和架构的MoE LLMs。

Result: 发现某些模型设计特征能显著提高局部路由一致性，领域专家贡献更大，缓存大小约为活跃专家的2倍时效果最佳。

Conclusion: 研究结果为高效MoE设计和部署提供了指导，平衡内存效率和推理速度。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with sparsely activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .

</details>


### [241] [Mesh-free sparse identification of nonlinear dynamics](https://arxiv.org/abs/2505.16058)
*Mars Liyao Gao,J. Nathan Kutz,Bernat Font*

Main category: cs.LG

TL;DR: 提出了一种名为mesh-free SINDy的新算法，利用神经网络和自动微分从非均匀采样的数据中识别动态系统的控制方程，具有高噪声鲁棒性和计算高效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要高质量、均匀采样的时空数据，而实际应用中数据往往非均匀且噪声高，因此需要一种更灵活的方法。

Method: 结合神经网络近似和自动微分技术，直接从任意传感器布局和非均匀时间采样数据中识别控制方程。

Result: 在高噪声（75%）和低数据量（100样本）情况下仍能成功识别PDE，训练时间短于1分钟。

Conclusion: mesh-free SINDy是一种高效、鲁棒且易于应用的方法，适用于科学和工程中的多种问题。

Abstract: Identifying the governing equations of a dynamical system is one of the most
important tasks for scientific modeling. However, this procedure often requires
high-quality spatio-temporal data uniformly sampled on structured grids. In
this paper, we propose mesh-free SINDy, a novel algorithm which leverages the
power of neural network approximation as well as auto-differentiation to
identify governing equations from arbitrary sensor placements and non-uniform
temporal data sampling. We show that mesh-free SINDy is robust to high noise
levels and limited data while remaining computationally efficient. In our
implementation, the training procedure is straight-forward and nearly free of
hyperparameter tuning, making mesh-free SINDy widely applicable to many
scientific and engineering problems. In the experiments, we demonstrate its
effectiveness on a series of PDEs including the Burgers' equation, the heat
equation, the Korteweg-De Vries equation and the 2D advection-diffusion
equation. We conduct detailed numerical experiments on all datasets, varying
the noise levels and number of samples, and we also compare our approach to
previous state-of-the-art methods. It is noteworthy that, even in high-noise
and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,
achieving successful identification with up to 75% noise for the Burgers'
equation using 5,000 samples and with as few as 100 samples and 1% noise. All
of this is achieved within a training time of under one minute.

</details>


### [242] [Scalable Graph Generative Modeling via Substructure Sequences](https://arxiv.org/abs/2505.16130)
*Zehong Wang,Zheyuan Zhang,Tianyi Ma,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 论文提出了一种基于生成Transformer的图神经网络框架G²PM，解决了传统消息传递方法的局限性，并在多个任务中表现出优异的可扩展性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）的消息传递方法存在表达力受限、过平滑、过压缩以及长距离依赖建模能力不足等问题，限制了其作为图基础模型的潜力。

Method: 引入G²PM框架，将图实例表示为子结构序列，并通过生成式预训练学习通用且可迁移的表示。

Result: 在ogbn-arxiv基准测试中，G²PM在模型规模达到6000万参数时仍能持续提升性能，优于其他生成方法。此外，在节点分类、图分类和迁移学习等任务中表现优异。

Conclusion: G²PM为可扩展的图学习提供了一个强有力的基础框架，解决了传统方法的局限性。

Abstract: Graph neural networks (GNNs) has been predominantly driven by
message-passing, where node representations are iteratively updated via local
neighborhood aggregation. Despite their success, message-passing suffers from
fundamental limitations -- including constrained expressiveness,
over-smoothing, over-squashing, and limited capacity to model long-range
dependencies. These issues hinder scalability: increasing data size or model
size often fails to yield improved performance, limiting the viability of GNNs
as backbones for graph foundation models. In this work, we explore pathways
beyond message-passing and introduce Generative Graph Pattern Machine
(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM
represents graph instances (nodes, edges, or entire graphs) as sequences of
substructures, and employs generative pre-training over the sequences to learn
generalizable, transferable representations. Empirically, G$^2$PM demonstrates
strong scalability: on the ogbn-arxiv benchmark, it continues to improve with
model sizes up to 60M parameters, outperforming prior generative approaches
that plateau at significantly smaller scales (e.g., 3M). In addition, we
systematically analyze the model design space, highlighting key architectural
choices that contribute to its scalability and generalization. Across diverse
tasks -- including node classification, graph classification, and transfer
learning -- G$^2$PM consistently outperforms strong baselines, establishing a
compelling foundation for scalable graph learning. The code and dataset are
available at https://github.com/Zehong-Wang/G2PM.

</details>


### [243] [Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond](https://arxiv.org/abs/2505.16060)
*Shangding Gu,Donghao Ying,Ming Jin,Yu Joe Lu,Jun Wang,Javad Lavaei,Costas Spanos*

Main category: cs.LG

TL;DR: Model Feedback Learning (MFL) 是一种新颖的测试时优化框架，无需重新训练模型或修改硬件，通过轻量级反向模型迭代搜索最优输入，适用于半导体制造等实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖调整模型参数，而MFL旨在无需修改部署系统的情况下高效适应新目标，特别适用于无法或成本过高修改硬件的场景。

Method: MFL利用轻量级反向模型迭代优化输入，结合稳定性感知优化提升鲁棒性。

Result: 在半导体等离子蚀刻任务中，MFL仅需5次迭代即可生成目标配方，优于贝叶斯优化和人类专家，并在化学过程和电子系统中表现优异。

Conclusion: MFL为实际环境中的智能控制提供了可扩展且高效的范式，支持少样本适应。

Abstract: We introduce Model Feedback Learning (MFL), a novel test-time optimization
framework for optimizing inputs to pre-trained AI models or deployed hardware
systems without requiring any retraining of the models or modifications to the
hardware. In contrast to existing methods that rely on adjusting model
parameters, MFL leverages a lightweight reverse model to iteratively search for
optimal inputs, enabling efficient adaptation to new objectives under
deployment constraints. This framework is particularly advantageous in
real-world settings, such as semiconductor manufacturing recipe generation,
where modifying deployed systems is often infeasible or cost-prohibitive. We
validate MFL on semiconductor plasma etching tasks, where it achieves target
recipe generation in just five iterations, significantly outperforming both
Bayesian optimization and human experts. Beyond semiconductor applications, MFL
also demonstrates strong performance in chemical processes (e.g., chemical
vapor deposition) and electronic systems (e.g., wire bonding), highlighting its
broad applicability. Additionally, MFL incorporates stability-aware
optimization, enhancing robustness to process variations and surpassing
conventional supervised learning and random search methods in high-dimensional
control settings. By enabling few-shot adaptation, MFL provides a scalable and
efficient paradigm for deploying intelligent control in real-world
environments.

</details>


### [244] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
*Zhixu Silvia Tao,Kasper Vinken,Hao-Wei Yeh,Avi Cooper,Xavier Boix*

Main category: cs.LG

TL;DR: 提出了一种名为Merge to Mix的新方法，通过模型合并加速数据集混合的构建，避免了多次微调的需求。


<details>
  <summary>Details</summary>
Motivation: 传统的数据集混合方法依赖启发式和试错，效率低下，需要多次微调才能达到理想效果。

Method: 利用模型合并技术，将单独微调后的模型通过简单算术操作合并，作为整个数据集混合的替代。

Result: 实验表明，Merge to Mix在数据集选择方面优于现有方法。

Conclusion: Merge to Mix提供了一种高效的数据集混合构建方法，显著提升了性能。

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [245] [Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity](https://arxiv.org/abs/2505.16638)
*Benedikt Höltgen,Nuria Oliver*

Main category: cs.LG

TL;DR: 论文证明公平性通过无知（FtU）可以减少算法歧视且不降低准确性，结合模型多重性理论，并通过实证支持其在现实应用中的可行性。


<details>
  <summary>Details</summary>
Motivation: 探讨FtU是否能在不牺牲准确性的情况下减少算法歧视，挑战传统观点认为FtU对公平性和准确性均不利。

Method: 通过理论和实证分析，结合模型多重性理论，验证FtU的效果。

Result: FtU能减少歧视且不降低准确性，适用于高风险场景，需谨慎使用受保护属性。

Conclusion: FtU在实际应用中值得考虑，特别是高风险场景，使用受保护属性需明确理由。

Abstract: Fairness through Unawareness (FtU) describes the idea that discrimination
against demographic groups can be avoided by not considering group membership
in the decisions or predictions. This idea has long been criticized in the
machine learning literature as not being sufficient to ensure fairness. In
addition, the use of additional features is typically thought to increase the
accuracy of the predictions for all groups, so that FtU is sometimes thought to
be detrimental to all groups. In this paper, we show both theoretically and
empirically that FtU can reduce algorithmic discrimination without necessarily
reducing accuracy. We connect this insight with the literature on Model
Multiplicity, to which we contribute with novel theoretical and empirical
results. Furthermore, we illustrate how, in a real-life application, FtU can
contribute to the deployment of more equitable policies without losing
efficacy. Our findings suggest that FtU is worth considering in practical
applications, particularly in high-risk scenarios, and that the use of
protected attributes such as gender in predictive models should be accompanied
by a clear and well-founded justification.

</details>


### [246] [Bidirectional Variational Autoencoders](https://arxiv.org/abs/2505.16074)
*Bart Kosko,Olaoluwa Adigun*

Main category: cs.LG

TL;DR: 提出双向变分自编码器（BVAE），通过单一神经网络实现编码和解码，参数减少近50%，性能略优于传统VAE。


<details>
  <summary>Details</summary>
Motivation: 传统VAE需要独立的编码器和解码器网络，参数较多。BVAE旨在通过双向结构减少参数并保持性能。

Method: BVAE使用单一神经网络，正向编码，反向解码。在图像重建、分类、插值和生成任务上与传统VAE对比。

Result: BVAE参数减少近50%，在MNIST、Fashion-MNIST、CIFAR-10和CelebA-64数据集上性能略优于传统VAE。

Conclusion: BVAE通过双向结构高效减少参数，同时保持或提升性能，适用于多种图像任务。

Abstract: We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.

</details>


### [247] [Ensembling Sparse Autoencoders](https://arxiv.org/abs/2505.16077)
*Soham Gadgil,Chris Lin,Su-In Lee*

Main category: cs.LG

TL;DR: 通过集成多个稀疏自编码器（SAE）提升特征提取能力，改进语言模型激活重建和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 单个SAE只能提取激活空间中的有限特征子集，不同初始权重的SAE学习不同特征，因此需要集成方法以提升性能。

Method: 采用朴素装袋（bagging）和提升（boosting）方法集成多个SAE，分别通过不同初始权重训练和逐步最小化残差实现。

Result: 集成SAE在语言模型激活重建、特征多样性和稳定性上表现更优，且在下游任务（如概念检测和虚假相关移除）中优于单个SAE。

Conclusion: 集成多个SAE能显著提升特征提取的全面性和实用性，为语言模型分析提供更优工具。

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into human-interpretable features. Typically, features learned by a single SAE
are used for downstream applications. However, it has recently been shown that
SAEs trained with different initial weights can learn different features,
demonstrating that a single SAE captures only a limited subset of features that
can be extracted from the activation space. Motivated by this limitation, we
propose to ensemble multiple SAEs through naive bagging and boosting.
Specifically, SAEs trained with different weight initializations are ensembled
in naive bagging, whereas SAEs sequentially trained to minimize the residual
error are ensembled in boosting. We evaluate our ensemble approaches with three
settings of language models and SAE architectures. Our empirical results
demonstrate that ensembling SAEs can improve the reconstruction of language
model activations, diversity of features, and SAE stability. Furthermore,
ensembling SAEs performs better than applying a single SAE on downstream tasks
such as concept detection and spurious correlation removal, showing improved
practical utility.

</details>


### [248] [FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model](https://arxiv.org/abs/2505.16083)
*Jiahuan Long,Wenzhe Zhang,Ning Wang,Tingsong Jiang,Wen Yao*

Main category: cs.LG

TL;DR: FR-Mamba是一种基于状态空间建模的时空流场重建框架，结合了FNO和SSM，显著提升了物理场重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以捕捉长时程时间依赖性，导致在时变物理系统中表现不佳。

Method: 设计了混合神经网络架构，结合FNO和SSM，分别捕捉全局空间特征和长时程时间依赖性，并融合时空表征进行重建。

Result: 实验表明，FR-Mamba在流场重建任务中显著优于现有方法，尤其在长序列上表现优异。

Conclusion: FR-Mamba通过结合FNO和SSM，有效解决了长时程依赖性问题，为物理场重建提供了高效且准确的解决方案。

Abstract: Physical field reconstruction (PFR) aims to predict the state distribution of
physical quantities (e.g., velocity, pressure, and temperature) based on
limited sensor measurements. It plays a critical role in domains such as fluid
dynamics and thermodynamics. However, existing deep learning methods often fail
to capture long-range temporal dependencies, resulting in suboptimal
performance on time-evolving physical systems. To address this, we propose
FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on
state space modeling. Specifically, we design a hybrid neural network
architecture that combines Fourier Neural Operator (FNO) and State Space Model
(SSM) to capture both global spatial features and long-range temporal
dependencies. We adopt Mamba, a recently proposed efficient SSM architecture,
to model long-range temporal dependencies with linear time complexity. In
parallel, the FNO is employed to capture non-local spatial features by
leveraging frequency-domain transformations. The spatiotemporal representations
extracted by these two components are then fused to reconstruct the full-field
distribution of the physical system. Extensive experiments demonstrate that our
approach significantly outperforms existing PFR methods in flow field
reconstruction tasks, achieving high-accuracy performance on long sequences.

</details>


### [249] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
*Ziqing Wang,Kexin Zhang,Zihan Zhao,Yibo Wen,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: 这篇综述探讨了大型语言模型（LLMs）在分子发现中的应用，重点关注分子生成和分子优化任务，并提出了分类法、分析代表性技术，以及讨论了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: LLMs为分子发现带来了新的范式，通过自然语言和符号表示与化学空间交互，需要系统化的综述来推动这一新兴领域的发展。

Method: 提出了分子生成和分子优化的分类法，分析了代表性技术，总结了常用数据集和评估协议。

Result: 综述提供了LLMs在分子发现中的最新进展，并指出了关键挑战和未来研究方向。

Conclusion: LLMs在分子发现中具有巨大潜力，但仍需解决多模态输入等挑战，该综述为研究者提供了资源。

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [250] [Reinforcement Learning for Stock Transactions](https://arxiv.org/abs/2505.16099)
*Ziyi,Zhou,Nicholas Stern,Julien Laasri*

Main category: cs.LG

TL;DR: 该研究应用强化学习（RL）确定股票最佳买入时机，并通过Q-Learning等方法训练代理，比较其策略以最大化利润。


<details>
  <summary>Details</summary>
Motivation: 股票市场存在潜在模式，识别这些模式可带来巨大利润。

Method: 定义马尔可夫决策过程（MDP），使用Q-Learning、线性函数逼近和深度Q-Learning训练代理，并结合机器学习回归和分类模型预测股价。

Result: 比较不同代理的策略收敛情况，评估其利润最大化效果。

Conclusion: 强化学习可用于股票市场策略优化，不同方法表现各异。

Abstract: Much research has been done to analyze the stock market. After all, if one
can determine a pattern in the chaotic frenzy of transactions, then they could
make a hefty profit from capitalizing on these insights. As such, the goal of
our project was to apply reinforcement learning (RL) to determine the best time
to buy a stock within a given time frame. With only a few adjustments, our
model can be extended to identify the best time to sell a stock as well. In
order to use the format of free, real-world data to train the model, we define
our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped
us in formulating the state space and the reward system of our MDP problem. We
train a series of agents using Q-Learning, Q-Learning with linear function
approximation, and deep Q-Learning. In addition, we try to predict the stock
prices using machine learning regression and classification models. We then
compare our agents to see if they converge on a policy, and if so, which one
learned the best policy to maximize profit on the stock market.

</details>


### [251] [Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI](https://arxiv.org/abs/2505.16103)
*Monirul Islam Mahmud*

Main category: cs.LG

TL;DR: 该研究通过传统机器学习模型和集成方法检测键盘记录器，结合特征选择和可解释AI技术，最终AdaBoost模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 键盘记录器对系统安全构成威胁，研究旨在通过机器学习方法高效检测异常行为。

Method: 使用SVC、随机森林等传统模型及Stacking等集成方法，结合信息增益等特征选择技术，并应用SHAP和LIME进行模型解释。

Result: AdaBoost模型表现最优，准确率达99.76%，F1分数0.99，特异性1.0，AUC 0.99。

Conclusion: 研究证明AdaBoost结合Fisher Score特征选择能实现近乎完美的键盘记录器检测。

Abstract: Keylogger detection involves monitoring for unusual system behaviors such as
delays between typing and character display, analyzing network traffic patterns
for data exfiltration. In this study, we provide a comprehensive analysis for
keylogger detection with traditional machine learning models - SVC, Random
Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes
and advanced ensemble methods including Stacking, Blending and Voting.
Moreover, feature selection approaches such as Information gain, Lasso L1 and
Fisher Score are thoroughly assessed to improve predictive performance and
lower computational complexity. The Keylogger Detection dataset from publicly
available Kaggle website is used in this project. In addition to accuracy-based
classification, this study implements the approach for model interpretation
using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to
deliver finer explanations for how much each feature contributes in assisting
or hindering the detection process. To evaluate the models result, we have used
AUC score, sensitivity, Specificity, Accuracy and F1 score. The best
performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,
100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is
near-perfect classification with Fisher Score.

</details>


### [252] [Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools](https://arxiv.org/abs/2505.16113)
*Panagiotis Lymperopoulos,Vasanth Sarathy*

Main category: cs.LG

TL;DR: 提出了一种新框架，用于量化LLM与外部工具结合时的整体不确定性，增强高风险应用中的信任。


<details>
  <summary>Details</summary>
Motivation: 在LLM与外部工具结合的场景中，现有方法无法量化整体不确定性，影响高风险应用中的可靠性。

Method: 扩展了不确定性量化方法，联合考虑LLM和外部工具的预测不确定性，并提出高效近似计算。

Result: 在合成QA数据集和RAG系统中验证了框架的有效性，增强了LLM系统的可信度。

Conclusion: 该框架显著提升了LLM与外部工具结合时的可靠性，尤其适用于内部知识不足的场景。

Abstract: Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.

</details>


### [253] [A Generic Framework for Conformal Fairness](https://arxiv.org/abs/2505.16115)
*Aditya T. Vadlamani,Anutam Srinivasan,Pranav Maneriker,Ali Payani,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 本文提出了一种基于共形预测的公平性概念（Conformal Fairness），并设计了一种算法和框架，以控制不同敏感群体之间的覆盖率差距。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）虽然能提供关于真实标签覆盖率的概率保证，但这些保证忽略了数据中敏感属性的存在。本文旨在填补这一空白，确保公平性。

Method: 通过利用共形预测的交换性假设（而非传统的IID假设），设计了一种算法和框架，适用于非IID数据（如图数据）。

Result: 在图表和表格数据集上的实验表明，该算法不仅能控制覆盖率，还能有效减少公平性相关的差距。

Conclusion: 本文提出的Conformal Fairness框架为共形预测提供了公平性保证，适用于更广泛的数据类型和任务。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
with machine learning models. While conformal prediction provides probabilistic
guarantees regarding the coverage of the true label, these guarantees are
agnostic to the presence of sensitive attributes within the dataset. In this
work, we formalize \textit{Conformal Fairness}, a notion of fairness using
conformal predictors, and provide a theoretically well-founded algorithm and
associated framework to control for the gaps in coverage between different
sensitive groups. Our framework leverages the exchangeability assumption
(implicit to CP) rather than the typical IID assumption, allowing us to apply
the notion of Conformal Fairness to data types and tasks that are not IID, such
as graph data. Experiments were conducted on graph and tabular datasets to
demonstrate that the algorithm can control fairness-related gaps in addition to
coverage aligned with theoretical expectations.

</details>


### [254] [Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning](https://arxiv.org/abs/2505.16122)
*Junhong Lin,Xinyue Zeng,Jie Zhu,Song Wang,Julian Shun,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文提出Plan-and-Budget框架，通过分解复杂问题并动态分配计算资源，显著提升LLMs的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂推理任务中表现出色，但其推理过程计算效率低下，存在“过度思考”问题。

Method: 提出BBAM理论模型和$E^3$指标，并开发Plan-and-Budget框架，动态分配计算资源。

Result: Plan-and-Budget显著提升效率，最高实现+70%准确率提升、-39%令牌减少和+187.5% $E^3$改进。

Conclusion: Plan-and-Budget框架无需重新训练即可显著提升LLMs效率，缩小模型间性能差距。

Abstract: Large Language Models (LLMs) have achieved remarkable success in complex
reasoning tasks, but their inference remains computationally inefficient. We
observe a common failure mode in many prevalent LLMs, overthinking, where
models generate verbose and tangential reasoning traces even for simple
queries. Recent works have tried to mitigate this by enforcing fixed token
budgets, however, this can lead to underthinking, especially on harder
problems. Through empirical analysis, we identify that this inefficiency often
stems from unclear problem-solving strategies. To formalize this, we develop a
theoretical model, BBAM (Bayesian Budget Allocation Model), which models
reasoning as a sequence of sub-questions with varying uncertainty, and
introduce the $E^3$ metric to capture the trade-off between correctness and
computation efficiency. Building on theoretical results from BBAM, we propose
Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex
queries into sub-questions and allocates token budgets based on estimated
complexity using adaptive scheduling. Plan-and-Budget improves reasoning
efficiency across a range of tasks and models, achieving up to +70% accuracy
gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it
elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger
model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close
performance gaps without retraining. Our code is available at
anonymous.4open.science/r/P-and-B-6513/.

</details>


### [255] [Robust Invariant Representation Learning by Distribution Extrapolation](https://arxiv.org/abs/2505.16126)
*Kotaro Yoshida,Slavakis Konstantinos*

Main category: cs.LG

TL;DR: 论文提出了一种基于外推的新框架，通过增强环境多样性改进IRM，解决了现有IRM方法对有限环境多样性和过参数化的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法（如IRMv1）因对有限环境多样性和过参数化敏感，导致性能下降，无法超越经验风险最小化（ERM）。

Method: 提出一种外推框架，通过合成分布偏移增强IRM惩罚项的环境多样性。

Result: 实验表明，该方法在多种场景下均优于现有IRM变体，验证了其有效性和鲁棒性。

Conclusion: 该方法显著提升了IRM的鲁棒性，为OOD泛化提供了更可靠的解决方案。

Abstract: Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.

</details>


### [256] [Multimodal Online Federated Learning with Modality Missing in Internet of Things](https://arxiv.org/abs/2505.16138)
*Heqiang Wang,Xiang Liu,Xiaoxiong Zhong,Lixing Chen,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为MMO-FL的新型框架，用于物联网环境中的动态分布式多模态学习，并针对模态缺失问题提出了PMM算法。


<details>
  <summary>Details</summary>
Motivation: 物联网设备从简单数据收集单元发展为能执行复杂计算任务的节点，需要分布式学习策略处理多模态数据，同时实时数据收集和有限存储要求在线学习范式。

Method: 提出了MMO-FL框架，并针对模态缺失问题设计了PMM算法，利用原型学习补偿缺失模态。

Result: 理论分析和实验结果表明，PMM算法在模态缺失情况下表现优于基准方法。

Conclusion: MMO-FL框架和PMM算法有效解决了物联网环境中多模态学习的动态性和模态缺失问题。

Abstract: The Internet of Things (IoT) ecosystem generates vast amounts of multimodal
data from heterogeneous sources such as sensors, cameras, and microphones. As
edge intelligence continues to evolve, IoT devices have progressed from simple
data collection units to nodes capable of executing complex computational
tasks. This evolution necessitates the adoption of distributed learning
strategies to effectively handle multimodal data in an IoT environment.
Furthermore, the real-time nature of data collection and limited local storage
on edge devices in IoT call for an online learning paradigm. To address these
challenges, we introduce the concept of Multimodal Online Federated Learning
(MMO-FL), a novel framework designed for dynamic and decentralized multimodal
learning in IoT environments. Building on this framework, we further account
for the inherent instability of edge devices, which frequently results in
missing modalities during the learning process. We conduct a comprehensive
theoretical analysis under both complete and missing modality scenarios,
providing insights into the performance degradation caused by missing
modalities. To mitigate the impact of modality missing, we propose the
Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype
learning to effectively compensate for missing modalities. Experimental results
on two multimodal datasets further demonstrate the superior performance of PMM
compared to benchmarks.

</details>


### [257] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
*Chongjie Si,Kangtao Lv,Jingjing Jiang,Yadao Wang,Yongwei Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 本文提出了一种基于最小二乘优化的模型合并方法NAN，通过参数范数的逆估计合并系数，无需训练且通用性强。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法依赖启发式规则确定合并系数，限制了其扩展性和通用性。

Method: 通过最小二乘优化重新审视模型合并，提出NAN方法，利用参数范数的逆估计合并系数。

Result: 实验表明，NAN能持续提升基线方法的性能。

Conclusion: NAN是一种简单有效的训练免费方法，适用于多种合并策略。

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [258] [Why Can Accurate Models Be Learned from Inaccurate Annotations?](https://arxiv.org/abs/2505.16159)
*Chongjie Si,Yidan Cui,Fuchao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.LG

TL;DR: 论文探讨了模型为何能从错误标注中学习到正确信息，发现标签不准确性主要影响权重矩阵的低奇异值分量，而主成分子空间保持稳定。基于此，提出了LIP插件以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究模型在错误标注下仍能准确预测的现象，探索其背后的原理。

Method: 通过理论和实证分析权重矩阵，发现标签噪声主要影响低奇异值分量，主成分子空间保持稳定。提出LIP插件以保留主成分信息并减少噪声影响。

Result: 实验表明，LIP能有效提升模型在多种错误标注条件下的性能。

Conclusion: 标签不准确性对模型主成分子空间影响有限，LIP插件能显著提升模型鲁棒性。

Abstract: Learning from inaccurate annotations has gained significant attention due to
the high cost of precise labeling. However, despite the presence of erroneous
labels, models trained on noisy data often retain the ability to make accurate
predictions. This intriguing phenomenon raises a fundamental yet largely
unexplored question: why models can still extract correct label information
from inaccurate annotations remains unexplored. In this paper, we conduct a
comprehensive investigation into this issue. By analyzing weight matrices from
both empirical and theoretical perspectives, we find that label inaccuracy
primarily accumulates noise in lower singular components and subtly perturbs
the principal subspace. Within a certain range, the principal subspaces of
weights trained on inaccurate labels remain largely aligned with those learned
from clean labels, preserving essential task-relevant information. We formally
prove that the angles of principal subspaces exhibit minimal deviation under
moderate label inaccuracy, explaining why models can still generalize
effectively. Building on these insights, we propose LIP, a lightweight plug-in
designed to help classifiers retain principal subspace information while
mitigating noise induced by label inaccuracy. Extensive experiments on tasks
with various inaccuracy conditions demonstrate that LIP consistently enhances
the performance of existing algorithms. We hope our findings can offer valuable
theoretical and practical insights to understand of model robustness under
inaccurate supervision.

</details>


### [259] [Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare](https://arxiv.org/abs/2505.16190)
*Navid Seidi,Satyaki Roy,Sajal Das*

Main category: cs.LG

TL;DR: 提出了一种基于声誉机制的联邦学习框架，用于医疗健康领域，通过混合通信模型和差分隐私保护数据隐私，同时动态调整信任分数以应对数据异质性和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗健康领域潜力巨大，但机构间的异质性、声誉缺失和不可靠贡献是主要挑战。

Method: 采用混合通信模型结合去中心化同行反馈和基于聚类的噪声处理，同时通过差分隐私保护模型更新。

Result: 在合成数据和SEER数据集上的实验表明，该方法能稳定获得高C-index值，优于无声誉系统的联邦学习方法。

Conclusion: 提出的框架有效解决了数据异质性和声誉问题，同时保护了隐私，适用于医疗健康领域的联邦学习。

Abstract: Federated Learning (FL) holds great promise for digital health by enabling
collaborative model training without compromising patient data privacy.
However, heterogeneity across institutions, lack of sustained reputation, and
unreliable contributions remain major challenges. In this paper, we propose a
robust, peer-driven reputation mechanism for federated healthcare that employs
a hybrid communication model to integrate decentralized peer feedback with
clustering-based noise handling to enhance model aggregation. Crucially, our
approach decouples the federated aggregation and reputation mechanisms by
applying differential privacy to client-side model updates before sharing them
for peer evaluation. This ensures sensitive information remains protected
during reputation computation, while unaltered updates are sent to the server
for global model training. Using the Cox Proportional Hazards model for
survival analysis across multiple federated nodes, our framework addresses both
data heterogeneity and reputation deficit by dynamically adjusting trust scores
based on local performance improvements measured via the concordance index.
Experimental evaluations on both synthetic datasets and the SEER dataset
demonstrate that our method consistently achieves high and stable C-index
values, effectively down-weighing noisy client updates and outperforming FL
methods that lack a reputation system.

</details>


### [260] [Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks](https://arxiv.org/abs/2505.16204)
*Ichiro Hashimoto*

Main category: cs.LG

TL;DR: 论文证明了固定宽度的Leaky ReLU双层神经网络在梯度下降优化下的参数方向收敛性，并分析了其收敛方向的条件，揭示了良性过拟合的新相变现象。


<details>
  <summary>Details</summary>
Motivation: 此前仅知梯度流下的参数收敛性，研究梯度下降下的收敛性及其对良性过拟合的影响。

Method: 通过分析收敛方向，建立良性过拟合的充分条件，并研究其在非正交数据中的表现。

Result: 发现了测试误差边界的新相变，并证明在高斯混合模型中良性过拟合高概率发生。

Conclusion: 研究扩展了梯度下降下的收敛性理论，并揭示了良性过拟合的新现象。

Abstract: In this paper, we prove directional convergence of network parameters of
fixed width leaky ReLU two-layer neural networks optimized by gradient descent
with exponential loss, which was previously only known for gradient flow. By a
careful analysis of the convergent direction, we establish sufficient
conditions of benign overfitting and discover a new phase transition in the
test error bound. All of these results hold beyond the nearly orthogonal data
setting which was studied in prior works. As an application, we demonstrate
that benign overfitting occurs with high probability in sub-Gaussian mixture
models.

</details>


### [261] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
*Zhihang Cai,Xingjun Zhang,Zhendong Tan,Zheng Wei*

Main category: cs.LG

TL;DR: NQKV算法通过分块量化KV缓存，显著减少内存消耗，提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: LLMs推理时KV缓存的内存消耗成为瓶颈，现有量化方法难以在低比特下保持精度。

Method: 分析KV缓存的元素分布，设计NQKV算法，采用分块量化方法。

Result: NQKV使OPT模型推理时批次大小翻倍或上下文长度增加4倍，吞吐量提升9.3倍。

Conclusion: NQKV在低比特量化下显著优化内存和性能，为LLM部署提供高效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [262] [Reward-Aware Proto-Representations in Reinforcement Learning](https://arxiv.org/abs/2505.16217)
*Hon Tik Tse,Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文提出了一种考虑奖励动态的默认表示（DR），与传统的后继表示（SR）相比，DR在奖励感知行为和性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 后继表示（SR）在强化学习中广泛应用，但它忽略了奖励动态。本文旨在研究一种类似但考虑奖励动态的表示方法（DR），以解决SR的局限性。

Method: 通过动态规划和时间差分方法学习DR，分析其向量空间基础，并将其扩展到函数近似情况。

Result: 实验表明，DR在奖励塑造、选项发现、探索和迁移学习等任务中表现优于SR。

Conclusion: DR提供了一种奖励感知的表示方法，在多个任务中优于SR，为强化学习提供了新的理论工具。

Abstract: In recent years, the successor representation (SR) has attracted increasing
attention in reinforcement learning (RL), and it has been used to address some
of its key challenges, such as exploration, credit assignment, and
generalization. The SR can be seen as representing the underlying credit
assignment structure of the environment by implicitly encoding its induced
transition dynamics. However, the SR is reward-agnostic. In this paper, we
discuss a similar representation that also takes into account the reward
dynamics of the problem. We study the default representation (DR), a recently
proposed representation with limited theoretical (and empirical) analysis.
Here, we lay some of the theoretical foundation underlying the DR in the
tabular case by (1) deriving dynamic programming and (2) temporal-difference
methods to learn the DR, (3) characterizing the basis for the vector space of
the DR, and (4) formally extending the DR to the function approximation case
through default features. Empirically, we analyze the benefits of the DR in
many of the settings in which the SR has been applied, including (1) reward
shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our
results show that, compared to the SR, the DR gives rise to qualitatively
different, reward-aware behaviour and quantitatively better performance in
several settings.

</details>


### [263] [Realistic Evaluation of TabPFN v2 in Open Environments](https://arxiv.org/abs/2505.16226)
*Zi-Jian Cheng,Zi-Yi Jia,Zhi Zhou,Yu-Feng Li,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: TabPFN v2在开放环境中表现有限，适合小规模、协变量偏移和类别平衡任务；树模型仍是开放环境中表格任务的最佳选择。


<details>
  <summary>Details</summary>
Motivation: 研究TabPFN v2在开放环境中的适应性，填补现有研究忽略开放环境挑战的空白。

Method: 构建统一评估框架，覆盖多种现实挑战，评估TabPFN v2在开放环境中的鲁棒性。

Result: TabPFN v2在开放环境中表现不佳，但适合特定任务；树模型在开放环境中表现更优。

Conclusion: 呼吁建立开放环境表格基准、多指标评估和通用模块以增强模型鲁棒性，并公开评估框架。

Abstract: Tabular data, owing to its ubiquitous presence in real-world domains, has
garnered significant attention in machine learning research. While tree-based
models have long dominated tabular machine learning tasks, the recently
proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled
performance and scalability potential. Although extensive research has been
conducted on TabPFN v2 to further improve performance, the majority of this
research remains confined to closed environments, neglecting the challenges
that frequently arise in open environments. This raises the question: Can
TabPFN v2 maintain good performance in open environments? To this end, we
conduct the first comprehensive evaluation of TabPFN v2's adaptability in open
environments. We construct a unified evaluation framework covering various
real-world challenges and assess the robustness of TabPFN v2 under open
environments scenarios using this framework. Empirical results demonstrate that
TabPFN v2 shows significant limitations in open environments but is suitable
for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models
remain the optimal choice for general tabular tasks in open environments. To
facilitate future research on open environments challenges, we advocate for
open environments tabular benchmarks, multi-metric evaluation, and universal
modules to strengthen model robustness. We publicly release our evaluation
framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.

</details>


### [264] [Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies](https://arxiv.org/abs/2505.16242)
*Runze Yan,Xun Shen,Akifumi Wachi,Sebastien Gros,Anni Zhao,Xiao Hu*

Main category: cs.LG

TL;DR: 论文提出了一种基于模型的离线强化学习框架（OGSRL），通过双重约束机制解决医疗场景中的分布外问题，确保策略的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，离线强化学习的分布外问题可能导致有害建议，现有方法仅约束动作选择，无法调节状态轨迹，限制了长期治疗策略的改进。

Method: OGSRL引入双重约束机制：1）OOD守护者限定临床验证区域；2）安全成本约束编码生理安全边界。

Result: OGSRL在理论和实践中确保策略在安全可靠区域内，并接近数据支持的最佳性能。

Conclusion: OGSRL通过双重约束机制，在医疗场景中安全改进策略，优于现有方法。

Abstract: When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.

</details>


### [265] [Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems](https://arxiv.org/abs/2505.16248)
*Wenxuan Zhu,Qiyuan Wu,Tengda Tang,Renzi Meng,Sheng Chai,Xuehui Quan*

Main category: cs.LG

TL;DR: 提出了一种基于GNN的多节点协同感知机制，解决了分布式系统中多节点感知和调度响应延迟的问题，通过图结构和多层GNN实现高效信息聚合与动态状态推断。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中多节点感知能力不足和调度响应延迟的问题。

Method: 将系统建模为图结构，引入消息传递和状态更新模块，构建多层GNN，设计感知表示方法融合局部状态与全局特征。

Result: 在定制实验框架中验证，性能优于主流算法，尤其在带宽有限和动态结构变化下表现突出。

Conclusion: 该方法显著提升了节点的全局感知能力和协作调度性能，实现了快速收敛和对复杂系统状态的高效响应。

Abstract: This paper addresses the limitations of multi-node perception and delayed
scheduling response in distributed systems by proposing a GNN-based multi-node
collaborative perception mechanism. The system is modeled as a graph structure.
Message-passing and state-update modules are introduced. A multi-layer graph
neural network is constructed to enable efficient information aggregation and
dynamic state inference among nodes. In addition, a perception representation
method is designed by fusing local states with global features. This improves
each node's ability to perceive the overall system status. The proposed method
is evaluated within a customized experimental framework. A dataset featuring
heterogeneous task loads and dynamic communication topologies is used.
Performance is measured in terms of task completion rate, average latency, load
balancing, and transmission efficiency. Experimental results show that the
proposed method outperforms mainstream algorithms under various conditions,
including limited bandwidth and dynamic structural changes. It demonstrates
superior perception capabilities and cooperative scheduling performance. The
model achieves rapid convergence and efficient responses to complex system
states.

</details>


### [266] [Small-to-Large Generalization: Data Influences Models Consistently Across Scale](https://arxiv.org/abs/2505.16260)
*Alaa Khaddaj,Logan Engstrom,Aleksander Madry*

Main category: cs.LG

TL;DR: 研究发现，训练数据分布对小规模和大规模语言模型的预测行为高度相关，为数据归因和数据集选择提供了指导。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对模型行为的影响，尤其是在大规模模型中难以直接实验的情况下，如何通过小规模代理模型推断其效果。

Method: 通过比较小规模和大规模语言模型在不同训练数据分布下的预测行为，分析其相关性。

Result: 小规模和大规模语言模型的预测行为高度相关，验证了代理模型的有效性。

Conclusion: 研究支持在小规模代理模型上进行数据分布实验的可行性，为数据归因和数据集选择提供了实用指导。

Abstract: Choice of training data distribution greatly influences model behavior. Yet,
in large-scale settings, precisely characterizing how changes in training data
affects predictions is often difficult due to model training costs. Current
practice is to instead extrapolate from scaled down, inexpensive-to-train proxy
models. However, changes in data do not influence smaller and larger models
identically. Therefore, understanding how choice of data affects large-scale
models raises the question: how does training data distribution influence model
behavior across compute scale? We find that small- and large-scale language
model predictions (generally) do highly correlate across choice of training
data. Equipped with these findings, we characterize how proxy scale affects
effectiveness in two downstream proxy model applications: data attribution and
dataset selection.

</details>


### [267] [Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models](https://arxiv.org/abs/2505.16265)
*Ilgee Hong,Changlong Yu,Liang Qiu,Weixiang Yan,Zhenghao Xu,Haoming Jiang,Qingru Zhang,Qin Lu,Xin Liu,Chao Zhang,Tuo Zhao*

Main category: cs.LG

TL;DR: 论文提出Think-RM框架，通过长时程推理改进生成式奖励模型（GenRM），并设计新的成对RLHF流程，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统Bradley-Terry奖励模型（BT RM）对数据规模和覆盖敏感且易受奖励攻击，而现有生成式奖励模型（GenRM）推理能力有限且与标准RLHF算法不兼容。

Method: 提出Think-RM框架，通过监督微调和规则强化学习实现长时程推理，并设计成对RLHF流程直接优化策略。

Result: Think-RM在RM-Bench上性能提升8%，结合成对RLHF流程后优于传统方法。

Conclusion: Think-RM框架显著提升了奖励模型的推理能力和RLHF效果。

Abstract: Reinforcement learning from human feedback (RLHF) has become a powerful
post-training paradigm for aligning large language models with human
preferences. A core challenge in RLHF is constructing accurate reward signals,
where the conventional Bradley-Terry reward models (BT RMs) often suffer from
sensitivity to data size and coverage, as well as vulnerability to reward
hacking. Generative reward models (GenRMs) offer a more robust alternative by
generating chain-of-thought (CoT) rationales followed by a final reward.
However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting
their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.
Moreover, their pairwise preference outputs are incompatible with standard RLHF
algorithms that require pointwise reward signals. In this work, we introduce
Think-RM, a training framework that enables long-horizon reasoning in GenRMs by
modeling an internal thinking process. Rather than producing structured,
externally provided rationales, Think-RM generates flexible, self-guided
reasoning traces that support advanced capabilities such as self-reflection,
hypothetical reasoning, and divergent reasoning. To elicit these reasoning
abilities, we first warm-up the models by supervised fine-tuning (SFT) over
long CoT data. We then further improve the model's long-horizon abilities by
rule-based reinforcement learning (RL). In addition, we propose a novel
pairwise RLHF pipeline that directly optimizes policies using pairwise
preference rewards, eliminating the need for pointwise reward conversion and
enabling more effective use of Think-RM outputs. Experiments show that Think-RM
achieves state-of-the-art results on RM-Bench, outperforming both BT RM and
vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,
it demonstrates superior end-policy performance compared to traditional
approaches.

</details>


### [268] [Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse](https://arxiv.org/abs/2505.16284)
*Josh Alman,Zhao Song*

Main category: cs.LG

TL;DR: 论文证明，在注意力机制中，除非权重较大，否则无法避免层坍塌（layer collapse），导致网络表达能力受限。因此，二次时间复杂度的注意力计算是不可避免的。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制中权重大小对网络表达能力的影响，揭示层坍塌现象及其与权重的关系。

Method: 通过理论分析，探讨权重大小与层坍塌的关系，并与现有研究（如秩坍塌）进行对比。

Result: 发现即使使用跳跃连接，小权重仍会导致层坍塌，只有大权重能避免这一问题。

Conclusion: 大权重是避免层坍塌和保持网络表达能力的关键，二次时间复杂度的注意力计算是必要的。

Abstract: Attention mechanisms lie at the heart of modern large language models (LLMs).
Straightforward algorithms for forward and backward (gradient) computation take
quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]
and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary
unless the model weights are small, in which case almost linear time algorithms
are possible. In this paper, we show that large weights are necessary to avoid
a strong preclusion to representational strength we call layer collapse, which
means that the entire network can be approximated well by a network with only a
single layer. Thus, the quadratic running time of attention is unavoidable for
expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of
rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They
showed that in Self Attention Networks with small weights and with skip
connections, rank collapse must occur. This is typically interpreted as
justifying the necessity of skip connections in expressive networks. However,
our result shows that even with skip connections, if the weights are small,
then layer collapse still occurs. Thus, only large weights, and not skip
connections, can prevent these representational weaknesses.

</details>


### [269] [Fairness under Competition](https://arxiv.org/abs/2505.16291)
*Ronen Gradwohl,Eilam Shapira,Moshe Tennenholtz*

Main category: cs.LG

TL;DR: 研究探讨了公平分类器在竞争性企业生态系统中的效果，发现即使单个分类器公平，整体生态系统仍可能不公平。


<details>
  <summary>Details</summary>
Motivation: 算法公平性成为ML核心问题，但公平分类器对生态系统公平性的影响尚未充分研究。

Method: 引入竞争性企业的公平性研究模型，分析分类器相关性和数据重叠对公平性的影响。

Result: 即使单个分类器公平，生态系统仍可能不公平；调整偏见的算法可能降低整体公平性。

Conclusion: 研究呼吁重新审视公平分类器对生态系统的影响，提出进一步行动的必要性。

Abstract: Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.

</details>


### [270] [Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution](https://arxiv.org/abs/2505.16305)
*Bingyang Cheng,Zhongtao Chen,Yichen Jin,Hao Zhang,Chen Zhang,Edmud Y. Lam,Yik-Chung Wu*

Main category: cs.LG

TL;DR: CP-GAMP是一种可扩展的贝叶斯张量分解算法，通过避免矩阵求逆和联合推断张量秩与噪声功率，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯张量分解方法因高维矩阵求逆问题难以扩展至大规模张量，因此需要一种更高效的算法。

Method: 利用广义近似消息传递（GAMP）避免矩阵求逆，并结合期望最大化（EM）方法联合推断张量秩和噪声功率。

Result: 在100x100x100秩20的张量实验中，仅需20%观测元素，CP-GAMP比现有方法减少82.7%运行时间，同时保持重构精度。

Conclusion: CP-GAMP是一种高效且准确的贝叶斯张量分解方法，适用于大规模张量处理。

Abstract: Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for
tensor reconstruction. Although the Bayesian framework allows for principled
uncertainty quantification and automatic hyperparameter learning, existing
methods do not scale well for large tensors because of high-dimensional matrix
inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD
algorithm. This algorithm leverages generalized approximate message passing
(GAMP) to avoid matrix inversions and incorporates an expectation-maximization
routine to jointly infer the tensor rank and noise power. Through multiple
experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements
observed, the proposed algorithm reduces runtime by 82.7% compared to the
state-of-the-art variational Bayesian CPD method, while maintaining comparable
reconstruction accuracy.

</details>


### [271] [CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.16308)
*Xingyu Zhang,Wenwen Qiang,Siyu Zhao,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种新的多变量时间序列预测范式（all-to-one），通过构建结构因果模型区分变量角色，并设计CAIFormer模型处理因果相关子段，排除虚假相关。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用all-to-all范式，未能区分变量角色，导致因果信息与虚假相关混杂。

Method: 构建结构因果模型，将历史序列分为内生、直接因果、碰撞因果和虚假相关四类子段，仅使用前三类进行预测，并设计CAIFormer模型处理这些子段。

Result: 在多个基准数据集上的实验验证了CAIFormer的有效性。

Conclusion: 提出的all-to-one范式和CAIFormer模型能有效区分因果信息，提升预测性能。

Abstract: Most existing multivariate time series forecasting methods adopt an
all-to-all paradigm that feeds all variable histories into a unified model to
predict their future values without distinguishing their individual roles.
However, this undifferentiated paradigm makes it difficult to identify
variable-specific causal influences and often entangles causally relevant
information with spurious correlations. To address this limitation, we propose
an all-to-one forecasting paradigm that predicts each target variable
separately. Specifically, we first construct a Structural Causal Model from
observational data and then, for each target variable, we partition the
historical sequence into four sub-segments according to the inferred causal
structure: endogenous, direct causal, collider causal, and spurious
correlation. The prediction relies solely on the first three causally relevant
sub-segments, while the spurious correlation sub-segment is excluded.
Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel
forecasting model comprising three components: Endogenous Sub-segment
Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider
Causal Sub-segment Prediction Block, which process the endogenous, direct
causal, and collider causal sub-segments, respectively. Their outputs are then
combined to produce the final prediction. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of the CAIFormer.

</details>


### [272] [FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail](https://arxiv.org/abs/2505.16319)
*Yangyang Wang,Jiawei Gu,Li Long,Xin Li,Li Shen,Zhouyu Fu,Xiangjun Zhou,Xu Jiang*

Main category: cs.LG

TL;DR: FreshRetailNet-50K是一个大规模基准数据集，用于解决零售业中因缺货导致的需求估计偏差问题，通过小时级销售数据和详细标注，提升了需求预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 零售业中，缺货期间的需求数据被截断，导致需求估计偏差，现有数据集缺乏解决这一问题的时空分辨率和标注。

Method: 提出了FreshRetailNet-50K数据集，包含50,000个时间序列数据，结合小时级销售数据和丰富的上下文变量，采用两阶段需求建模方法：先重构缺货期间的潜在需求，再训练鲁棒的需求预测模型。

Result: 实验结果显示，该方法将预测准确性提高了2.73%，并将系统性需求低估从7.37%降至接近零偏差。

Conclusion: FreshRetailNet-50K为需求填补、易腐品库存优化和零售因果分析提供了新研究方向，解决了零售AI中的长期限制。

Abstract: Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73\%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37\% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.

</details>


### [273] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
*Woosung Koh,Wonbeen Oh,Jaein Jang,MinHyung Lee,Hyeongjin Kim,Ah Yeon Kim,Joonkee Kim,Junghyun Lee,Taehyeon Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: AdaSTaR通过自适应采样优化训练平衡与难度，显著提升模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统随机采样导致的训练不平衡问题，提升语言模型的自学习效率。

Method: 引入两种自适应采样原则：多样性采样和课程采样，动态调整数据分布。

Result: 在六个基准测试中均取得最佳准确率，训练FLOPs平均减少58.6%。

Conclusion: AdaSTaR为高效自学习语言模型提供了新方向。

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [274] [ChemMLLM: Chemical Multimodal Large Language Model](https://arxiv.org/abs/2505.16326)
*Qian Tan,Dongzhan Zhou,Peng Xia,Wanhao Liu,Wanli Ouyang,Lei Bai,Yuqiang Li,Tianfan Fu*

Main category: cs.LG

TL;DR: ChemMLLM是一种多模态化学大语言模型，用于分子理解和生成，在多项任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的化学多模态大语言模型在跨模态理解和生成方面研究不足，ChemMLLM旨在填补这一空白。

Method: 设计了五种跨模态任务（文本、分子SMILES字符串、图像），并构建了数据集，通过实验对比了ChemMLLM与其他领先模型的性能。

Result: ChemMLLM在所有任务中表现优异，例如在分子图像优化任务中，性能提升118.9%。

Conclusion: ChemMLLM为化学多模态任务提供了高效解决方案，代码已开源。

Abstract: Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large
language model for molecule understanding and generation. Also, we design five
multimodal tasks across text, molecular SMILES strings, and image, and curate
the datasets. We benchmark ChemMLLM against a range of general leading MLLMs
and Chemical LLMs on these tasks. Experimental results show that ChemMLLM
achieves superior performance across all evaluated tasks. For example, in
molecule image optimization task, ChemMLLM outperforms the best baseline
(GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly
available at https://github.com/bbsbz/ChemMLLM.git.

</details>


### [275] [Understanding Differential Transformer Unchains Pretrained Self-Attentions](https://arxiv.org/abs/2505.16333)
*Chaerin Kong,Jiho Jang,Nojun Kwak*

Main category: cs.LG

TL;DR: Differential Transformer的性能优势源于负注意力、注意力头冗余减少和学习动态改善。DEX方法通过轻量级操作将这些优势融入预训练模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究Differential Transformer的成功机制，并提出一种高效方法将其优势融入预训练模型。

Method: 提出DEX方法，通过复用softmax注意力分数并添加轻量级差分操作，将差分注意力的优势融入预训练模型。

Result: DEX显著提升了预训练模型在多个基准测试中的性能，且仅需极少适应数据（<0.01%）。

Conclusion: DEX成功将差分注意力的优势高效融入预训练模型，实现了显著的性能提升。

Abstract: Differential Transformer has recently gained significant attention for its
impressive empirical performance, often attributed to its ability to perform
noise canceled attention. However, precisely how differential attention
achieves its empirical benefits remains poorly understood. Moreover,
Differential Transformer architecture demands large-scale training from
scratch, hindering utilization of open pretrained weights. In this work, we
conduct an in-depth investigation of Differential Transformer, uncovering three
key factors behind its success: (1) enhanced expressivity via negative
attention, (2) reduced redundancy among attention heads, and (3) improved
learning dynamics. Based on these findings, we propose DEX, a novel method to
efficiently integrate the advantages of differential attention into pretrained
language models. By reusing the softmax attention scores and adding a
lightweight differential operation on the output value matrix, DEX effectively
incorporates the key advantages of differential attention while remaining
lightweight in both training and inference. Evaluations confirm that DEX
substantially improves the pretrained LLMs across diverse benchmarks, achieving
significant performance gains with minimal adaptation data (< 0.01\%).

</details>


### [276] [Improving Chemical Understanding of LLMs via SMILES Parsing](https://arxiv.org/abs/2505.16340)
*Yunhui Jang,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.LG

TL;DR: CLEANMOL框架通过结构化任务提升LLMs对SMILES分子结构的理解能力，并在Mol-Instructions基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在解析SMILES分子结构时表现不佳，无法完成基本任务（如计数分子环），限制了其在分子科学中的应用。

Method: 提出CLEANMOL框架，将SMILES解析转化为一系列确定性任务（如子图匹配和全局图匹配），并构建自适应难度评分的预训练数据集。

Result: CLEANMOL显著提升了LLMs对分子结构的理解能力，并在Mol-Instructions基准测试中达到或超越基线水平。

Conclusion: CLEANMOL为LLMs在分子科学中的应用提供了有效的解决方案，提升了模型对复杂分子结构的解析能力。

Abstract: Large language models (LLMs) are increasingly recognized as powerful tools
for scientific discovery, particularly in molecular science. A fundamental
requirement for these models is the ability to accurately understand molecular
structures, commonly encoded in the SMILES representation. However, current
LLMs struggle to interpret SMILES, even failing to carry out basic tasks such
as counting molecular rings. To address this limitation, we introduce CLEANMOL,
a novel framework that formulates SMILES parsing into a suite of clean and
deterministic tasks explicitly designed to promote graph-level molecular
comprehension. These tasks span from subgraph matching to global graph
matching, providing structured supervision aligned with molecular structural
properties. We construct a molecular pretraining dataset with adaptive
difficulty scoring and pre-train open-source LLMs on these tasks. Our results
show that CLEANMOL not only enhances structural comprehension but also achieves
the best or competes with the baseline on the Mol-Instructions benchmark.

</details>


### [277] [A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2505.16341)
*Yaxin Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 论文提出了一种动态专家分配模块和多深度特征融合模块，用于解决长尾半监督学习中的分布不匹配问题，通过充分利用不同专家的优势和多深度特征，提升了伪标签质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究长尾半监督学习中标记数据与未标记数据分布不匹配的问题，现有方法未能充分利用不同专家的优势。

Method: 提出动态专家分配模块，根据样本类别动态选择合适专家生成伪标签；提出多深度特征融合模块，结合不同深度特征以减少模型偏差。

Result: 在CIFAR-10-LT、STL-10-LT和SVHN-LT数据集上验证了方法的有效性。

Conclusion: 动态专家分配和多深度特征融合显著提升了长尾半监督学习的性能。

Abstract: This paper studies the long-tailed semi-supervised learning (LTSSL) with
distribution mismatch, where the class distribution of the labeled training
data follows a long-tailed distribution and mismatches with that of the
unlabeled training data. Most existing methods introduce auxiliary classifiers
(experts) to model various unlabeled data distributions and produce
pseudo-labels, but the expertises of various experts are not fully utilized. We
observe that different experts are good at predicting different intervals of
samples, e.g., long-tailed expert is skilled in samples located in the head
interval and uniform expert excels in samples located in the medium interval.
Therefore, we propose a dynamic expert assignment module that can estimate the
class membership (i.e., head, medium, or tail class) of samples, and
dynamically assigns suitable expert to each sample based on the estimated
membership to produce high-quality pseudo-label in the training phase and
produce prediction in the testing phase. We also theoretically reveal that
integrating different experts' strengths will lead to a smaller generalization
error bound. Moreover, we find that the deeper features are more biased toward
the head class but with more discriminative ability, while the shallower
features are less biased but also with less discriminative ability. We,
therefore, propose a multi-depth feature fusion module to utilize different
depth features to mitigate the model bias. Our method demonstrates its
effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,
and SVHN-LT datasets across various settings. The code is available at
https://github.com/yaxinhou/Meta-Expert.

</details>


### [278] [Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning](https://arxiv.org/abs/2505.16353)
*Céline Comte,Pascal Moyal*

Main category: cs.LG

TL;DR: 本文提出了一种优化准可逆排队系统到达率的通用方案，重新定义了准可逆性，并引入平衡到达控制策略，证明了其能保持准可逆性。


<details>
  <summary>Details</summary>
Motivation: 研究准可逆排队系统的到达率优化问题，扩展了平衡到达率的概念，并探索其在优化和强化学习中的应用。

Method: 提出新的准可逆性定义，引入平衡到达控制策略，并通过理论证明其有效性。

Result: 证明了平衡到达控制策略能保持准可逆性，并给出了稳态测度的形式。

Conclusion: 该方法为准可逆排队系统的优化和强化学习提供了新的理论支持。

Abstract: In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.

</details>


### [279] [AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training](https://arxiv.org/abs/2505.16363)
*Huishuai Zhang,Bohan Wang,Luoxin Chen*

Main category: cs.LG

TL;DR: AdamS是一种替代Adam的优化器，通过新颖的分母设计（动量与当前梯度的平方加权和的根）简化计算，无需二阶矩估计，高效且易用。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer目标的局部平滑性特性，利用动量近似梯度幅度，简化优化过程。

Method: 采用动量与当前梯度的平方加权和的根作为分母，消除二阶矩估计需求，保持与SGD相似的计算和内存开销。

Result: 在GPT-2和Llama2等模型上表现优异，支持大规模预训练和强化学习任务。

Conclusion: AdamS以其高效性、简单性和理论支持，成为现有优化器的有力替代。

Abstract: We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.

</details>


### [280] [A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules](https://arxiv.org/abs/2505.16365)
*Manuel Ruiz-Botella,Marta Sales-Pardo,Roger Guimerà*

Main category: cs.LG

TL;DR: CoCoGraph是一种协作约束图扩散模型，用于生成化学有效的分子，性能优于现有方法，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决分子空间探索的挑战，开发高效生成化学有效分子的方法。

Method: 采用协作约束图扩散模型（CoCoGraph），确保生成的分子化学有效。

Result: 在标准基准测试中表现优于现有方法，参数更少；生成的分子分布更接近真实分子。

Conclusion: CoCoGraph高效且实用，生成了820万合成分子数据库，并通过专家测试验证其可行性。

Abstract: Developing new molecular compounds is crucial to address pressing challenges,
from health to environmental sustainability. However, exploring the molecular
space to discover new molecules is difficult due to the vastness of the space.
Here we introduce CoCoGraph, a collaborative and constrained graph diffusion
model capable of generating molecules that are guaranteed to be chemically
valid. Thanks to the constraints built into the model and to the collaborative
mechanism, CoCoGraph outperforms state-of-the-art approaches on standard
benchmarks while requiring up to an order of magnitude fewer parameters.
Analysis of 36 chemical properties also demonstrates that CoCoGraph generates
molecules with distributions more closely matching real molecules than current
models. Leveraging the model's efficiency, we created a database of 8.2M
million synthetically generated molecules and conducted a Turing-like test with
organic chemistry experts to further assess the plausibility of the generated
molecules, and potential biases and limitations of CoCoGraph.

</details>


### [281] [SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning](https://arxiv.org/abs/2505.16368)
*Huanyu Liu,Jia Li,Hao Zhu,Kechi Zhang,Yihong Dong,Ge Li*

Main category: cs.LG

TL;DR: Saturn是一个基于SAT问题的强化学习框架，用于训练和评估大型语言模型的推理能力，解决了现有任务的可扩展性、可验证性和难度控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL任务在可扩展性、可验证性和难度控制方面存在不足，限制了大型语言模型推理能力的发挥。

Method: Saturn利用布尔可满足性问题（SAT）构建任务，支持基于规则的验证和精确难度控制，并设计了渐进式学习流程。

Result: Saturn在SAT问题、数学和编程任务上显著提升了模型性能，并在基准测试中优于现有方法。

Conclusion: Saturn为训练和评估LLM推理能力提供了一种高效且可控的方法，推动了相关研究的进展。

Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.

</details>


### [282] [Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space](https://arxiv.org/abs/2505.16386)
*Ahmed K. Kadhim,Lei Jiao,Rishad Shafik,Ole-Christoffer Granmo*

Main category: cs.LG

TL;DR: Omni TM-AE是一种新型嵌入模型，结合了Tsetlin Machine的可解释性和可扩展性，通过单一训练阶段生成可重用、可解释的嵌入，性能优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型的可解释性和可重用性问题，避免传统嵌入模型的黑盒特性。

Method: 利用Tsetlin Machine的状态矩阵信息，包括之前被排除在子句形成之外的文字，构建Omni TM-AE。

Result: 在语义相似性、情感分类和文档聚类任务中表现优于主流嵌入模型。

Conclusion: Omni TM-AE证明可以在现代NLP系统中平衡性能、可扩展性和可解释性。

Abstract: The increasing complexity of large-scale language models has amplified
concerns regarding their interpretability and reusability. While traditional
embedding models like Word2Vec and GloVe offer scalability, they lack
transparency and often behave as black boxes. Conversely, interpretable models
such as the Tsetlin Machine (TM) have shown promise in constructing explainable
learning systems, though they previously faced limitations in scalability and
reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni
TM-AE), a novel embedding model that fully exploits the information contained
in the TM's state matrix, including literals previously excluded from clause
formation. This method enables the construction of reusable, interpretable
embeddings through a single training phase. Extensive experiments across
semantic similarity, sentiment classification, and document clustering tasks
show that Omni TM-AE performs competitively with and often surpasses mainstream
embedding models. These results demonstrate that it is possible to balance
performance, scalability, and interpretability in modern Natural Language
Processing (NLP) systems without resorting to opaque architectures.

</details>


### [283] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
*Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.LG

TL;DR: 论文提出了一种通过大规模强化学习（RL）提升中小型模型推理能力的方法，通过数学和代码提示的分阶段训练，显著超越蒸馏模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模强化学习在推理任务中取得进展，但高性能推理模型的训练方法仍不明确，且前沿模型的实现细节常被忽略。本文旨在探索RL如何提升中小型模型的推理能力。

Method: 采用分阶段RL训练：先数学提示训练，后代码提示训练；开发数据筛选流程收集高质量提示和测试用例；研究课程学习和策略参数更新的稳定效果。

Result: 数学RL显著提升数学和代码推理性能（如AIME 2025提升14.6%/17.2%）；代码RL进一步优化代码任务表现且不影响数学结果。

Conclusion: RL不仅能激发模型的预训练和微调能力，还能突破其推理极限，解决此前无法解决的问题。

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [284] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/abs/2505.16401)
*Xiaoqing Zhang,Huabin Zheng,Ang Lv,Yuhan Liu,Zirui Song,Flood Sung,Xiuying Chen,Rui Yan*

Main category: cs.LG

TL;DR: 论文提出Divide-Fuse-Conquer框架，通过分组训练和参数融合提升多场景强化学习的泛化能力，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多场景游戏中，规则和环境的多样性导致策略难以泛化，传统方法存在训练不稳定和性能差的问题。

Method: Divide-Fuse-Conquer框架：1）分组游戏；2）为每组训练专用模型；3）融合参数并继续训练。

Result: 在18个TextArena游戏中，Qwen2.5-32B-Align达到与Claude3.5相当的性能，7胜4平。

Conclusion: 该方法为提升LLMs在多场景强化学习中的泛化能力提供了新思路。

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [285] [Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach](https://arxiv.org/abs/2505.16403)
*Huazi Pan,Yanjun Zhang,Leo Yu Zhang,Scott Adams,Abbas Kouzani,Suiyang Khoo*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedSA的新型联邦学习攻击方法，通过滑动模式控制理论精确控制投毒程度，实现预定义目标（如降低全局模型准确率10%）。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的协作性使其容易受到投毒攻击，现有攻击多导致拒绝服务问题，缺乏精确控制。

Method: 结合非线性滑动模式控制（SMC）理论与模型投毒攻击，通过恶意客户端更新驱动全局模型至受损状态，且控制投毒速率。

Result: 实验表明，FedSA能以较少恶意客户端精确实现预定义全局准确率，同时保持高隐蔽性和可调学习率。

Conclusion: FedSA为联邦学习投毒攻击提供了精确控制手段，展示了其高效性和隐蔽性。

Abstract: Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.

</details>


### [286] [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/abs/2505.16446)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一种新型隐式越狱框架IJA，通过最低有效位隐写术将恶意指令嵌入图像，并结合看似无害的文本提示，成功攻击多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的跨模态推理能力强大，但也引入了新的攻击面。传统显式攻击容易被检测和拦截，因此需要更隐蔽的攻击方法。

Method: 使用最低有效位隐写术将恶意指令嵌入图像，结合无害文本提示；通过对抗后缀和模板优化模块提升攻击效果。

Result: 在GPT-4o和Gemini-1.5 Pro等商业模型上，平均仅需3次查询即可实现90%以上的攻击成功率。

Conclusion: IJA框架展示了隐式攻击对MLLMs的高效性，揭示了跨模态一致性和对齐机制的潜在漏洞。

Abstract: Multimodal large language models (MLLMs) enable powerful cross-modal
reasoning capabilities. However, the expanded input space introduces new attack
surfaces. Previous jailbreak attacks often inject malicious instructions from
text into less aligned modalities, such as vision. As MLLMs increasingly
incorporate cross-modal consistency and alignment mechanisms, such explicit
attacks become easier to detect and block. In this work, we propose a novel
implicit jailbreak framework termed IJA that stealthily embeds malicious
instructions into images via least significant bit steganography and couples
them with seemingly benign, image-related textual prompts. To further enhance
attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes
generated by a surrogate model and introduce a template optimization module
that iteratively refines both the prompt and embedding based on model feedback.
On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack
success rates of over 90% using an average of only 3 queries.

</details>


### [287] [Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling](https://arxiv.org/abs/2505.16481)
*Xinxing Shi,Xiaoyu Jiang,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 提出了一种基于邻居驱动的近似策略，用于扩展高斯过程变分自编码器（GPVAE），以解决大规模计算问题。


<details>
  <summary>Details</summary>
Motivation: 标准GPVAE在大规模数据上计算成本过高，现有方法通常依赖限制性核假设或大量诱导点。

Method: 利用潜在空间中的局部邻接关系，通过限制计算为每个数据点的最近邻，实现可扩展的GPVAE推断。

Result: 在表示学习、数据填补和条件生成等任务中，该方法在预测性能和计算效率上优于其他GPVAE变体。

Conclusion: 邻居驱动的近似策略为GPVAE提供了更灵活的选择，同时减少了计算负担。

Abstract: Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by
replacing the fully factorised Gaussian prior with a GP prior, thereby
capturing richer correlations among latent variables. However, performing exact
GP inference in large-scale GPVAEs is computationally prohibitive, often
forcing existing approaches to rely on restrictive kernel assumptions or large
sets of inducing points. In this work, we propose a neighbour-driven
approximation strategy that exploits local adjacencies in the latent space to
achieve scalable GPVAE inference. By confining computations to the nearest
neighbours of each data point, our method preserves essential latent
dependencies, allowing more flexible kernel choices and mitigating the need for
numerous inducing points. Through extensive experiments on tasks including
representation learning, data imputation, and conditional generation, we
demonstrate that our approach outperforms other GPVAE variants in both
predictive performance and computational efficiency.

</details>


### [288] [Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics](https://arxiv.org/abs/2505.16493)
*Seyedeh Fatemeh Ebrahimi,Jaakko Peltonen*

Main category: cs.LG

TL;DR: 论文提出了一种通过约束非负矩阵分解（NMF）的方法，解决主题模型难以捕捉低流行度、领域关键主题（如在线评论中的心理健康主题）的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合领域知识时可能要求过于详细的预期主题，限制了主题划分和变异的发现。

Method: 采用约束NMF方法，结合种子词列表表征感兴趣的少数主题内容，并通过流行度约束和种子词内容学习数据驱动的少数和多数主题。

Result: 在合成数据上，模型在主题纯度、归一化互信息和Jensen-Shannon散度（JSD）方面优于基线方法。案例研究成功识别了YouTube评论中的心理健康内容。

Conclusion: 该方法有效解决了主题模型对少数主题的捕捉问题，并在实际应用中展示了其价值。

Abstract: Topic models often fail to capture low-prevalence, domain-critical themes,
so-called minority topics, such as mental health themes in online comments.
While some existing methods can incorporate domain knowledge, such as expected
topical content, methods allowing guidance may require overly detailed expected
topics, hindering the discovery of topic divisions and variation. We propose a
topic modeling solution via a specially constrained NMF. We incorporate a seed
word list characterizing minority content of interest, but we do not require
experts to pre-specify their division across minority topics. Through
prevalence constraints on minority topics and seed word content across topics,
we learn distinct data-driven minority topics as well as majority topics. The
constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with
multiplicative updates. We outperform several baselines on synthetic data in
terms of topic purity, normalized mutual information, and also evaluate topic
quality using Jensen-Shannon divergence (JSD). We conduct a case study on
YouTube vlog comments, analyzing viewer discussion of mental health content;
our model successfully identifies and reveals this domain-relevant minority
content.

</details>


### [289] [Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility](https://arxiv.org/abs/2505.16494)
*Noga Amit,Omer Reingold,Guy N. Rothblum*

Main category: cs.LG

TL;DR: 该论文研究了在训练数据包含更丰富标签（如个体类型、排名或风险估计）时公平性与效用和效率的关系，提出了实现更强证据公平性的算法，并展示了在某些情况下同时实现准确分类率和最优损失最小化的计算不可行性。


<details>
  <summary>Details</summary>
Motivation: 探讨在更复杂标签数据下公平性与效用、效率的交互关系，提出比传统监督学习更强的公平性算法。

Method: 提出支持分类和排名技术的算法，确保子群体分类率的准确性，并实现损失最小化。

Result: 展示了在某些情况下同时实现准确分类率和最优损失最小化的计算不可行性，但两者可分别高效实现。

Conclusion: 论文揭示了公平性目标之间的计算权衡，提出了两种可实现的准确性概念，为公平性研究提供了新视角。

Abstract: We revisit the foundations of fairness and its interplay with utility and
efficiency in settings where the training data contain richer labels, such as
individual types, rankings, or risk estimates, rather than just binary
outcomes. In this context, we propose algorithms that achieve stronger notions
of evidence-based fairness than are possible in standard supervised learning.
Our methods support classification and ranking techniques that preserve
accurate subpopulation classification rates, as suggested by the underlying
data distributions, across a broad class of classification rules and downstream
applications. Furthermore, our predictors enable loss minimization, whether
aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results
demonstrating that simultaneously achieving accurate classification rates and
optimal loss minimization is, in some cases, computationally infeasible. Unlike
prior impossibility results, our notions are not inherently in conflict and are
simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show
that each notion can be satisfied individually via efficient learning. Our
separation thus stems from the computational hardness of learning a
sufficiently good approximation of the Bayes-optimal predictor. These
computational impossibilities present a choice between two natural and
attainable notions of accuracy that could both be motivated by fairness.

</details>


### [290] [Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods](https://arxiv.org/abs/2505.16516)
*Majid Mohammadi,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: PKeX-Shapley是一种新算法，利用乘积核的乘法结构在多项式时间内精确计算Shapley值，提升核方法的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 核方法的黑盒特性限制了其在高风险应用中的使用，需要可解释性更强的特征归因技术。

Method: 通过乘积核的功能分解，递归计算Shapley值，实现多项式时间内的精确计算。

Result: PKeX-Shapley不仅提高了计算效率，还增强了核方法学习的可解释性，并扩展至解释核统计差异。

Conclusion: 该算法为核方法的可解释性和统计推断提供了新工具。

Abstract: Kernel methods are widely used in machine learning due to their flexibility
and expressive power. However, their black-box nature poses significant
challenges to interpretability, limiting their adoption in high-stakes
applications. Shapley value-based feature attribution techniques, such as SHAP
and kernel-specific variants like RKHS-SHAP, offer a promising path toward
explainability. Yet, computing exact Shapley values remains computationally
intractable in general, motivating the development of various approximation
schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that
utilizes the multiplicative structure of product kernels to enable the exact
computation of Shapley values in polynomial time. We show that product-kernel
models admit a functional decomposition that allows for a recursive formulation
of Shapley values. This decomposition not only yields computational efficiency
but also enhances interpretability in kernel-based learning. We also
demonstrate how our framework can be generalized to explain kernel-based
statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the
Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for
interpretable statistical inference.

</details>


### [291] [Joint Relational Database Generation via Graph-Conditional Diffusion Models](https://arxiv.org/abs/2505.16527)
*Mohamed Amine Ketata,David Lüdke,Leo Schwinn,Stephan Günnemann*

Main category: cs.LG

TL;DR: 论文提出了一种新的关系数据库生成模型GRDM，通过图神经网络联合建模所有表，避免了传统方法的顺序生成限制和误差累积。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单表生成或顺序生成多表，限制了并行性和灵活性，且误差累积。

Method: 使用图表示关系数据库，提出Graph-Conditional Relational Diffusion Model (GRDM)，通过图神经网络联合去噪行属性并捕获表间依赖。

Result: 在六个真实RDB上实验表明，GRDM在多跳表间相关性建模上显著优于基线，单表保真度达到SOTA。

Conclusion: GRDM是一种更灵活、高效的关系数据库生成方法，适用于隐私保护数据发布等应用。

Abstract: Building generative models for relational databases (RDBs) is important for
applications like privacy-preserving data release and augmenting real datasets.
However, most prior work either focuses on single-table generation or relies on
autoregressive factorizations that impose a fixed table order and generate
tables sequentially. This approach limits parallelism, restricts flexibility in
downstream applications like missing value imputation, and compounds errors due
to commonly made conditional independence assumptions. We propose a
fundamentally different approach: jointly modeling all tables in an RDB without
imposing any order. By using a natural graph representation of RDBs, we propose
the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph
neural network to jointly denoise row attributes and capture complex
inter-table dependencies. Extensive experiments on six real-world RDBs
demonstrate that our approach substantially outperforms autoregressive
baselines in modeling multi-hop inter-table correlations and achieves
state-of-the-art performance on single-table fidelity metrics.

</details>


### [292] [HOFT: Householder Orthogonal Fine-tuning](https://arxiv.org/abs/2505.16531)
*Alejandro Moreno Arcas,Albert Sanchis,Jorge Civera,Alfons Juan*

Main category: cs.LG

TL;DR: 提出了两种新的正交微调方法（HOFT和SHOFT），旨在降低时间和空间复杂度，并在多个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有正交微调方法虽然泛化性好，但时间和内存效率低。

Method: 提出HOFT和SHOFT，探索正交微调的理论特性，并在多个任务中验证。

Result: HOFT和SHOFT在多个任务中表现优于或与现有方法相当。

Conclusion: HOFT和SHOFT是高效且性能优异的正交微调方法。

Abstract: Adaptation of foundation models using low-rank methods is a widespread
approach. Another way to adapt these models is to employ orthogonal fine-tuning
methods, which are less time and memory efficient despite their good
generalization properties. In this work, we propose Householder Orthogonal
Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to
alleviate time and space complexity. Moreover, some theoretical properties of
the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled
Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are
evaluated in downstream tasks, namely commonsense reasoning, machine
translation, subject-driven generation and mathematical reasoning. Compared
with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or
better results.

</details>


### [293] [Incremental Sequence Classification with Temporal Consistency](https://arxiv.org/abs/2505.16548)
*Lucas Maystre,Gabriel Barello,Tudor Berariu,Aleix Cambray,Rares Dolga,Alvaro Ortega Gonzalez,Andrei Nica,David Barber*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间一致性条件的增量序列分类方法，通过新损失函数提升数据效率和预测准确性，并在文本分类和数学问题验证任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决增量序列分类问题，即在序列元素逐步揭示时更新预测，需要满足时间一致性条件。

Method: 借鉴强化学习中的时间差分学习，提出时间一致性条件，并基于此设计新的损失函数训练增量序列分类器。

Result: 在文本分类任务和验证大语言模型生成的数学问题正确性任务中，该方法显著提高了数据效率和预测准确性。

Conclusion: 提出的方法在增量序列分类中表现优异，尤其在早期观察少量标记时能更好区分有效和无效生成。

Abstract: We address the problem of incremental sequence classification, where
predictions are updated as new elements in the sequence are revealed. Drawing
on temporal-difference learning from reinforcement learning, we identify a
temporal-consistency condition that successive predictions should satisfy. We
leverage this condition to develop a novel loss function for training
incremental sequence classifiers. Through a concrete example, we demonstrate
that optimizing this loss can offer substantial gains in data efficiency. We
apply our method to text classification tasks and show that it improves
predictive accuracy over competing approaches on several benchmark datasets. We
further evaluate our approach on the task of verifying large language model
generations for correctness in grade-school math problems. Our results show
that models trained with our method are better able to distinguish promising
generations from unpromising ones after observing only a few tokens.

</details>


### [294] [Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations](https://arxiv.org/abs/2505.16549)
*Trung V. Phan,George A. Kevrekidis,Soledad Villar,Yannis G. Kevrekidis,Juan M. Bello-Rivas*

Main category: cs.LG

TL;DR: 论文提出了一种坐标和维度无关的机器学习方法，用于学习偏微分方程（PDEs），从而实现“空间解放”的PDE学习。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的PDE学习方法依赖于特定空间维度和坐标系，限制了其泛化能力。

Method: 采用外微积分形式主义，通过机器学习预测标量场系统的演化，该方法具有坐标无关性并天然支持任意维度。

Result: 在多个模型（FitzHugh-Nagumo、Barkley反应扩散模型等）中验证了方法的有效性，能够跨不同空间维度、坐标系和边界条件进行预测。

Conclusion: 该方法实现了PDE学习的空间无关性，为跨空间场景的泛化提供了新途径。

Abstract: The machine learning methods for data-driven identification of partial
differential equations (PDEs) are typically defined for a given number of
spatial dimensions and a choice of coordinates the data have been collected in.
This dependence prevents the learned evolution equation from generalizing to
other spaces. In this work, we reformulate the problem in terms of coordinate-
and dimension-independent representations, paving the way toward what we call
``spatially liberated" PDE learning. To this end, we employ a machine learning
approach to predict the evolution of scalar field systems expressed in the
formalism of exterior calculus, which is coordinate-free and immediately
generalizes to arbitrary dimensions by construction. We demonstrate the
performance of this approach in the FitzHugh-Nagumo and Barkley
reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by
in-situ chemotactic bacteria observations. We provide extensive numerical
experiments that demonstrate that our approach allows for seamless transitions
across various spatial contexts. We show that the field dynamics learned in one
space can be used to make accurate predictions in other spaces with different
dimensions, coordinate systems, boundary conditions, and curvatures.

</details>


### [295] [A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices](https://arxiv.org/abs/2505.16563)
*Chen Gong,Rui Xing,Zhenzhe Zheng,Fan Wu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Titan的两阶段数据选择框架，旨在提高边缘设备上机器学习模型训练的数据利用率，通过优化数据选择和训练效率，显著减少训练时间并提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的机器学习模型训练需求增加，但现有方法因数据利用率低、存储有限和数据重要性多样而受限。

Method: Titan采用两阶段数据选择：粗粒度筛选候选数据集，细粒度选择最优数据批次，并通过流水线技术提升效率。

Result: 实验表明，Titan在真实边缘设备上减少了43%的训练时间，提高了6.2%的最终准确率，且系统开销较小。

Conclusion: Titan有效解决了边缘设备训练中的数据利用率问题，显著提升了训练效率和模型性能。

Abstract: The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.

</details>


### [296] [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567)
*Thibaud Gloaguen,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 论文提出了一种名为FAB的攻击方法，通过元学习技术毒害LLM，使其在微调后触发恶意行为，挑战了微调安全性的假设。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示微调LLM的安全隐患，证明攻击者可以制造看似无害但微调后表现出恶意行为的模型。

Method: 方法是通过FAB攻击，利用元学习模拟下游微调，优化模型在微调后展现恶意行为，同时保持微调前的正常表现。

Result: 实验证明FAB在多种LLM和目标行为（如广告推送、拒绝服务和越狱）上有效，且对用户微调选择具有鲁棒性。

Conclusion: 结论指出FAB攻击暴露了微调LLM的新安全漏洞，强调了模型安全性的复杂性。

Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.

</details>


### [297] [Large Language Model-Empowered Interactive Load Forecasting](https://arxiv.org/abs/2505.16577)
*Yu Zuo,Dalin Qin,Yi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于大语言模型（LLM）的多智能体协作框架，旨在解决电力系统负荷预测中人与模型交互不足的问题，提高非专家用户的使用体验和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前负荷预测方法缺乏人机交互机制，系统操作员难以理解和应用这些高级模型，也无法将自身经验融入预测过程。LLM的自然语言理解和推理能力为解决这一问题提供了新机会。

Method: 设计了一个基于LLM的多智能体协作框架，包含多个专用智能体，通过专用通信机制协作完成预测任务，嵌入交互机制以降低技术门槛并支持人类经验整合。

Result: 实验表明，用户在关键阶段提供适当见解时，交互式负荷预测准确性显著提升；成本分析显示该框架经济可行，适合实际部署。

Conclusion: 该框架有效解决了负荷预测中的人机交互问题，提升了预测精度和用户体验，具有实际应用潜力。

Abstract: The growing complexity of power systems has made accurate load forecasting
more important than ever. An increasing number of advanced load forecasting
methods have been developed. However, the static design of current methods
offers no mechanism for human-model interaction. As the primary users of
forecasting models, system operators often find it difficult to understand and
apply these advanced models, which typically requires expertise in artificial
intelligence (AI). This also prevents them from incorporating their experience
and real-world contextual understanding into the forecasting process. Recent
breakthroughs in large language models (LLMs) offer a new opportunity to
address this issue. By leveraging their natural language understanding and
reasoning capabilities, we propose an LLM-based multi-agent collaboration
framework to bridge the gap between human operators and forecasting models. A
set of specialized agents is designed to perform different tasks in the
forecasting workflow and collaborate via a dedicated communication mechanism.
This framework embeds interactive mechanisms throughout the load forecasting
pipeline, reducing the technical threshold for non-expert users and enabling
the integration of human experience. Our experiments demonstrate that the
interactive load forecasting accuracy can be significantly improved when users
provide proper insight in key stages. Our cost analysis shows that the
framework remains affordable, making it practical for real-world deployment.

</details>


### [298] [How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning](https://arxiv.org/abs/2505.16581)
*Max Weltevrede,Moritz A. Zanger,Matthijs T. J. Spaan,Wendelin Böhmer*

Main category: cs.LG

TL;DR: 论文研究了零样本策略迁移中策略蒸馏的作用，证明了在特定假设下的泛化边界，并提出两点实用建议：训练蒸馏策略集成和在更多训练数据上蒸馏。


<details>
  <summary>Details</summary>
Motivation: 探索为何策略蒸馏能提升泛化性能，以及如何选择蒸馏数据。

Method: 在理论假设下证明策略蒸馏的泛化边界，并通过实验验证建议的普适性。

Result: 实验表明，基于多样化数据的策略集成能显著优于原始策略。

Conclusion: 策略蒸馏和多样化数据集成是提升零样本策略迁移性能的有效方法。

Abstract: In the zero-shot policy transfer setting in reinforcement learning, the goal
is to train an agent on a fixed set of training environments so that it can
generalise to similar, but unseen, testing environments. Previous work has
shown that policy distillation after training can sometimes produce a policy
that outperforms the original in the testing environments. However, it is not
yet entirely clear why that is, or what data should be used to distil the
policy. In this paper, we prove, under certain assumptions, a generalisation
bound for policy distillation after training. The theory provides two practical
insights: for improved generalisation, you should 1) train an ensemble of
distilled policies, and 2) distil it on as much data from the training
environments as possible. We empirically verify that these insights hold in
more general settings, when the assumptions required for the theory no longer
hold. Finally, we demonstrate that an ensemble of policies distilled on a
diverse dataset can generalise significantly better than the original agent.

</details>


### [299] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/abs/2505.16583)
*Shpresim Sadiku,Kartikeya Chitranshi,Hiroshi Kera,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 论文研究了通过训练分类器使用合理的反事实解释（p-CFEs）标记错误目标类，以分类未扰动输入。结果显示，这种方法不仅提高了分类准确性，还减少了虚假相关性带来的偏差。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用合理的反事实解释（p-CFEs）来改进分类器的性能，尤其是在减少虚假相关性方面。

Method: 训练分类器使用p-CFEs标记的错误目标类，以分类未扰动输入。

Result: 分类器不仅保持了高准确性，还显著减少了虚假相关性带来的偏差。

Conclusion: 学习p-CFEs比对抗扰动更有效，能够同时提升分类性能和减少偏差。

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [300] [CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models](https://arxiv.org/abs/2505.16620)
*Benjamin Herdeanu,Juan Nathaniel,Carla Roesch,Jatan Buch,Gregor Ramien,Johannes Haux,Pierre Gentine*

Main category: cs.LG

TL;DR: CausalDynamics是一个用于动态因果模型结构发现的大规模基准和可扩展数据生成框架，旨在解决现有方法在确定性、低维和弱非线性时间序列数据上的局限性。


<details>
  <summary>Details</summary>
Motivation: 动态系统的因果发现在无法进行主动干预的领域中具有挑战性，现有方法多针对确定性、低维和弱非线性数据，缺乏普适性。

Method: 通过构建包含数千个耦合常微分和随机微分方程以及理想化气候模型的真实因果图，评估现有因果发现算法在噪声、混杂和滞后动态系统中的表现。

Result: CausalDynamics提供了一个即插即用的耦合工作流，支持构建物理系统的层次结构，并提供了用户友好的实现和文档。

Conclusion: 该框架有望促进鲁棒因果发现算法的开发，适用于跨领域应用，并解决其独特挑战。

Abstract: Causal discovery for dynamical systems poses a major challenge in fields
where active interventions are infeasible. Most methods used to investigate
these systems and their associated benchmarks are tailored to deterministic,
low-dimensional and weakly nonlinear time-series data. To address these
limitations, we present CausalDynamics, a large-scale benchmark and extensible
data generation framework to advance the structural discovery of dynamical
causal models. Our benchmark consists of true causal graphs derived from
thousands of coupled ordinary and stochastic differential equations as well as
two idealized climate models. We perform a comprehensive evaluation of
state-of-the-art causal discovery algorithms for graph reconstruction on
systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of
a plug-and-play, build-your-own coupling workflow that enables the construction
of a hierarchy of physical systems. We anticipate that our framework will
facilitate the development of robust causal discovery algorithms that are
broadly applicable across domains while addressing their unique challenges. We
provide a user-friendly implementation and documentation on
https://kausable.github.io/CausalDynamics.

</details>


### [301] [Multivariate Latent Recalibration for Conditional Normalizing Flows](https://arxiv.org/abs/2505.16636)
*Victor Dheur,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 论文提出了一种新的潜在校准概念和潜在重新校准（LR）方法，用于改进多元响应变量的条件分布建模，确保概率密度函数的完整性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多元模型可能因误设或校准不良导致联合分布近似不准确，而标准校准方法仅限于单变量，且现有技术无法提供完整的概率密度函数。

Method: 引入潜在校准概念，并提出潜在重新校准（LR）方法，通过学习潜在空间的变换，确保有限样本下的潜在校准。

Result: 实验表明，LR显著降低了潜在校准误差和负对数似然，提升了模型性能。

Conclusion: LR是一种高效且有效的多元模型校准方法，能够生成具有明确概率密度函数的校准分布。

Abstract: Reliably characterizing the full conditional distribution of a multivariate
response variable given a set of covariates is crucial for trustworthy
decision-making. However, misspecified or miscalibrated multivariate models may
yield a poor approximation of the joint distribution of the response variables,
leading to unreliable predictions and suboptimal decisions. Furthermore,
standard recalibration methods are primarily limited to univariate settings,
while conformal prediction techniques, despite generating multivariate
prediction regions with coverage guarantees, do not provide a full probability
density function. We address this gap by first introducing a novel notion of
latent calibration, which assesses probabilistic calibration in the latent
space of a conditional normalizing flow. Second, we propose latent
recalibration (LR), a novel post-hoc model recalibration method that learns a
transformation of the latent space with finite-sample bounds on latent
calibration. Unlike existing methods, LR produces a recalibrated distribution
with an explicit multivariate density function while remaining computationally
efficient. Extensive experiments on both tabular and image datasets show that
LR consistently improves latent calibration error and the negative
log-likelihood of the recalibrated models.

</details>


### [302] [Stochastic Forward-Forward Learning through Representational Dimensionality Compression](https://arxiv.org/abs/2505.16649)
*Zhichao Zhu,Yang Qi,Hengyuan Ma,Wenlian Lu,Jianfeng Feng*

Main category: cs.LG

TL;DR: 论文提出了一种基于维度压缩的新型“goodness”函数，用于替代反向传播训练神经网络，通过有效维度（ED）捕捉神经元响应的二阶统计结构，并在噪声环境下优化表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于能量学习的“goodness”函数忽略了神经元间的相关性，限制了性能。本文旨在通过引入有效维度（ED）改进这一缺陷。

Method: 提出了一种称为“维度压缩”的新“goodness”函数，利用神经元响应的有效维度（ED）来整合二阶统计结构，并在噪声条件下优化输入。

Result: 该方法在非反向传播方法中表现优异，噪声还被证明能提升泛化能力和推理性能。

Conclusion: 该研究为生物合理性学习算法和神经形态计算提供了新思路，噪声被证明是一种计算资源而非干扰。

Abstract: The Forward-Forward (FF) algorithm provides a bottom-up alternative to
backpropagation (BP) for training neural networks, relying on a layer-wise
"goodness" function to guide learning. Existing goodness functions, inspired by
energy-based learning (EBL), are typically defined as the sum of squared
post-synaptic activations, neglecting the correlations between neurons. In this
work, we propose a novel goodness function termed dimensionality compression
that uses the effective dimensionality (ED) of fluctuating neural responses to
incorporate second-order statistical structure. Our objective minimizes ED for
clamped inputs when noise is considered while maximizing it across the sample
distribution, promoting structured representations without the need to prepare
negative samples. We demonstrate that this formulation achieves competitive
performance compared to other non-BP methods. Moreover, we show that noise
plays a constructive role that can enhance generalization and improve inference
when predictions are derived from the mean of squared outputs, which is
equivalent to making predictions based on the energy term. Our findings
contribute to the development of more biologically plausible learning
algorithms and suggest a natural fit for neuromorphic computing, where
stochasticity is a computational resource rather than a nuisance. The code is
available at https://github.com/ZhichaoZhu/StochasticForwardForward

</details>


### [303] [End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries](https://arxiv.org/abs/2505.16664)
*Khoa Tran,Tri Le,Bao Huynh,Hung-Cuong Trinh,Vy-Rin Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合信号处理和深度学习的方法，用于预测锂离子电池的剩余使用寿命（RUL），并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测锂离子电池的RUL对及时维护至关重要，影响依赖电池的电动应用的运行效率。

Method: 提出了一种信号预处理管道和混合深度学习模型（1D CNN、A-LSTM和ODE-LSTM），用于捕捉信号特征和长期依赖关系。

Result: 模型在公开数据集上表现优异，RMSE为101.59，优于基线深度学习和机器学习方法。

Conclusion: 该方法在有限目标数据下仍保持稳健性能，具有实际应用潜力。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for
enabling timely maintenance of lithium-ion batteries, impacting the operational
efficiency of electric applications that rely on them. This paper proposes a
RUL prediction approach that leverages data from recent charge-discharge cycles
to estimate the number of remaining usable cycles. The approach introduces both
a novel signal processing pipeline and a deep learning prediction model. In the
signal preprocessing pipeline, a derived capacity feature is computed based on
current and capacity signals. Alongside original capacity, voltage and current,
these features are denoised and enhanced using statistical metrics and a
delta-based method to capture differences between the current and previous
cycles. In the prediction model, the processed features are then fed into a
hybrid deep learning architecture composed of 1D Convolutional Neural Networks
(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential
Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to
capture both local signal characteristics and long-range temporal dependencies
while modeling the continuous-time dynamics of battery degradation. The model
is further evaluated using transfer learning across different learning
strategies and target data partitioning scenarios. Results indicate that the
model maintains robust performance, even when fine-tuned on limited target
data. Experimental results on two publicly available large-scale datasets
demonstrate that the proposed method outperforms a baseline deep learning
approach and machine learning techniques, achieving an RMSE of 101.59,
highlighting its strong potential for real-world RUL prediction applications.

</details>


### [304] [Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data](https://arxiv.org/abs/2505.16672)
*Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 比较了三种区块链交易数据聚类方法，发现浅层量子电路能有效提取非线性特征，显著提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 区块链交易数据具有高维度、噪声和复杂特征纠缠，传统聚类算法面临挑战。

Method: 比较了经典K均值聚类、混合聚类（结合量子随机特征）和全量子聚类（自监督QNN训练）。

Result: 浅层量子电路能有效提取非线性特征，显著改善聚类效果。

Conclusion: 量子方法在区块链数据聚类中具有潜力，尤其是浅层量子电路的表现优异。

Abstract: Blockchain transaction data exhibits high dimensionality, noise, and
intricate feature entanglement, presenting significant challenges for
traditional clustering algorithms. In this study, we conduct a comparative
analysis of three clustering approaches: (1) Classical K-Means Clustering,
applied to pre-processed feature representations; (2) Hybrid Clustering,
wherein classical features are enhanced with quantum random features extracted
using randomly initialized quantum neural networks (QNNs); and (3) Fully
Quantum Clustering, where a QNN is trained in a self-supervised manner
leveraging a SwAV-based loss function to optimize the feature space for
clustering directly. The proposed experimental framework systematically
investigates the impact of quantum circuit depth and the number of learned
prototypes, demonstrating that even shallow quantum circuits can effectively
extract meaningful non-linear representations, significantly improving
clustering performance.

</details>


### [305] [On the Out-of-Distribution Generalization of Self-Supervised Learning](https://arxiv.org/abs/2505.16675)
*Wenwen Qiang,Jingyao Wang,Zeen Song,Jiangmeng Li,Changwen Zheng*

Main category: cs.LG

TL;DR: 论文研究了自监督学习（SSL）在分布外（OOD）泛化中的表现，提出了一种基于因果推断的批量采样策略以提升OOD性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在训练过程中可能学习到虚假相关性，导致OOD泛化能力下降，因此需要一种方法来解决这一问题。

Method: 通过结构因果模型提出后干预分布（PID），并设计了一种批量采样策略，确保训练过程中满足PID条件。

Result: 理论分析和实验表明，该采样策略能有效提升SSL模型在OOD任务中的表现。

Conclusion: 提出的方法通过消除虚假相关性，显著提升了自监督学习的OOD泛化能力。

Abstract: In this paper, we focus on the out-of-distribution (OOD) generalization of
self-supervised learning (SSL). By analyzing the mini-batch construction during
the SSL training phase, we first give one plausible explanation for SSL having
OOD generalization. Then, from the perspective of data generation and causal
inference, we analyze and conclude that SSL learns spurious correlations during
the training process, which leads to a reduction in OOD generalization. To
address this issue, we propose a post-intervention distribution (PID) grounded
in the Structural Causal Model. PID offers a scenario where the spurious
variable and label variable is mutually independent. Besides, we demonstrate
that if each mini-batch during SSL training satisfies PID, the resulting SSL
model can achieve optimal worst-case OOD performance. This motivates us to
develop a batch sampling strategy that enforces PID constraints through the
learning of a latent variable model. Through theoretical analysis, we
demonstrate the identifiability of the latent variable model and validate the
effectiveness of the proposed sampling strategy. Experiments conducted on
various downstream OOD tasks demonstrate the effectiveness of the proposed
sampling strategy.

</details>


### [306] [Learning Genomic Structure from $k$-mers](https://arxiv.org/abs/2505.16680)
*Filip Thor,Carl Nettelblad*

Main category: cs.LG

TL;DR: 提出了一种基于对比学习的方法，用于分析基因组测序数据，通过训练编码器模型生成嵌入，保留基因组区域的序列信息，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 基因组测序产生大量短序列（reads），需要重组以重建完整基因组。传统方法依赖专业算法，而本文旨在提供一种通用表示方法，简化分析流程。

Method: 使用对比学习训练编码器模型，生成能够聚类同一基因组区域序列的嵌入。嵌入空间保留序列轨迹，支持无监督和监督学习，并可结合噪声模型增强鲁棒性。

Result: 在E. coli基因组和模拟古DNA（aDNA）数据上验证了方法的有效性，性能与当前黄金标准BWA-aln相当，且适用于宏基因组物种鉴定。

Conclusion: 该方法无需依赖完整基因组组装，具有通用性和扩展性，适用于宏基因组和大基因组（如人类基因组）分析。

Abstract: Sequencing a genome to determine an individual's DNA produces an enormous
number of short nucleotide subsequences known as reads, which must be
reassembled to reconstruct the full genome. We present a method for analyzing
this type of data using contrastive learning, in which an encoder model is
trained to produce embeddings that cluster together sequences from the same
genomic region. The sequential nature of genomic regions is preserved in the
form of trajectories through this embedding space. Trained solely to reflect
the structure of the genome, the resulting model provides a general
representation of $k$-mer sequences, suitable for a range of downstream tasks
involving read data. We apply our framework to learn the structure of the $E.\
coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read
mapping and identification of structural variations. Furthermore, we illustrate
the potential of using this type of model for metagenomic species
identification. We show how incorporating a domain-specific noise model can
enhance embedding robustness, and how a supervised contrastive learning setting
can be adopted when a linear reference genome is available, by introducing a
distance thresholding parameter $\Gamma$. The model can also be trained fully
self-supervised on read data, enabling analysis without the need to construct a
full genome assembly using specialized algorithms. Small prediction heads based
on a pre-trained embedding are shown to perform on par with BWA-aln, the
current gold standard approach for aDNA mapping, in terms of accuracy and
runtime for short genomes. Given the method's favorable scaling properties with
respect to total genome size, inference using our approach is highly promising
for metagenomic applications and for mapping to genomes comparable in size to
the human genome.

</details>


### [307] [Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator](https://arxiv.org/abs/2505.16690)
*Beier Luo,Shuoyuan Wang,Yixuan Li,Hongxin Wei*

Main category: cs.LG

TL;DR: 论文提出了一种名为DACA的无监督方法，用于解决后训练语言模型（PoLM）的过度自信问题，通过选择性使用一致样本进行校准，显著提升了模型的校准性能。


<details>
  <summary>Details</summary>
Motivation: 后训练语言模型（PoLM）常因过度自信而影响可靠性，尤其是在缺乏标注数据的情况下。PLM与PoLM的预测不一致导致校准困难，需要一种新方法解决。

Method: 提出DACA方法，通过选择性使用PLM与PoLM预测一致的样本进行温度缩放校准，避免不一致样本对校准参数的影响。

Result: 实验表明，DACA显著提升了校准性能，将开源和API-based LLM（如GPT-4o）的平均ECE降低了15.08%。

Conclusion: DACA通过解决PLM与PoLM预测不一致导致的校准问题，有效提升了后训练语言模型的可靠性。

Abstract: Post-training of large language models is essential for adapting pre-trained
language models (PLMs) to align with human preferences and downstream tasks.
While PLMs typically exhibit well-calibrated confidence, post-trained language
models (PoLMs) often suffer from over-confidence, assigning high confidence to
both correct and incorrect outputs, which can undermine reliability in critical
applications. A major obstacle in calibrating PoLMs is the scarcity of labeled
data for individual downstream tasks. To address this, we propose
Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to
optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence
calibration. Our method is motivated by the under-confidence issue caused by
prediction disagreement between the PLM and PoLM while aligning their
confidence via temperature scaling. Theoretically, the PLM's confidence
underestimates PoLM's prediction accuracy on disagreement examples, causing a
larger $\tau$ and producing under-confident predictions. DACA mitigates this by
selectively using only agreement examples for calibration, effectively
decoupling the influence of disagreement. In this manner, our method avoids an
overly large $\tau$ in temperature scaling caused by disagreement examples,
improving calibration performance. Extensive experiments demonstrate the
effectiveness of our method, improving the average ECE of open-sourced and
API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.

</details>


### [308] [An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations](https://arxiv.org/abs/2505.16705)
*Seonghwan Park,Jueun Mun,Donghyun Oh,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文研究了概念瓶颈模型（CBMs）中噪声对性能、可解释性和干预效果的影响，并提出了一种两阶段框架来缓解噪声问题。


<details>
  <summary>Details</summary>
Motivation: CBMs通过分解预测为可解释概念来确保透明性，但训练中的噪声标注会影响其性能，目前对此影响的研究不足。

Method: 提出两阶段框架：训练阶段使用锐度感知最小化稳定噪声敏感概念的学习；推理阶段通过预测熵排名并校正最不确定的概念。

Result: 研究发现某些概念对噪声特别敏感，其准确性下降远超平均水平，且噪声导致大部分性能损失。提出的框架能有效提升鲁棒性。

Conclusion: 通过理论分析和实验验证，该框架在保持可解释性的同时增强了模型对噪声的鲁棒性。

Abstract: Concept bottleneck models (CBMs) ensure interpretability by decomposing
predictions into human interpretable concepts. Yet the annotations used for
training CBMs that enable this transparency are often noisy, and the impact of
such corruption is not well understood. In this study, we present the first
systematic study of noise in CBMs and show that even moderate corruption
simultaneously impairs prediction performance, interpretability, and the
intervention effectiveness. Our analysis identifies a susceptible subset of
concepts whose accuracy declines far more than the average gap between noisy
and clean supervision and whose corruption accounts for most performance loss.
To mitigate this vulnerability we propose a two-stage framework. During
training, sharpness-aware minimization stabilizes the learning of
noise-sensitive concepts. During inference, where clean labels are unavailable,
we rank concepts by predictive entropy and correct only the most uncertain
ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and
extensive ablations elucidate why sharpness-aware training confers robustness
and why uncertainty reliably identifies susceptible concepts, providing a
principled basis that preserves both interpretability and resilience in the
presence of noise.

</details>


### [309] [Training Long-Context LLMs Efficiently via Chunk-wise Optimization](https://arxiv.org/abs/2505.16710)
*Wenhao Li,Yuxin Zhang,Gen Luo,Daohai Yu,Rongrong Ji*

Main category: cs.LG

TL;DR: 论文提出了SeCO和SpaCO两种内存高效的训练方法，用于优化长上下文大语言模型的训练成本。SeCO通过分块处理输入，SpaCO进一步选择性地传播梯度，显著提升了训练效率和序列长度。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型的训练成本过高，限制了其定制化应用。

Method: SeCO将长输入分块处理，每块独立构建计算图并局部反向传播；SpaCO在此基础上选择性传播梯度并引入补偿因子。

Result: SeCO将最大序列长度从1K扩展到16K，SpaCO在相同实验条件下训练速度比SeCO快3倍。

Conclusion: SeCO和SpaCO为优化长上下文模型提供了新思路，使其更适用于实际应用。

Abstract: While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.

</details>


### [310] [Advancing Brainwave Modeling with a Codebook-Based Foundation Model](https://arxiv.org/abs/2505.16724)
*Konstantinos Barmpas,Na Lee,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: LaBraM++是一种改进的大型脑电波基础模型，通过优化架构设计提升了性能，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型未能充分捕捉神经振荡的丰富信息，限制了其在BCI任务中的性能和泛化能力。

Method: 引入LaBraM++，基于稳健的信号处理基础进行改进。

Result: LaBraM++在多种任务中表现优异，优于原架构，并与其他开源模型竞争。

Conclusion: LaBraM++性能优越且训练高效，有望成为未来脑电波基础模型的重要基础。

Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models
have shown great promise, driving progress in Brain-Computer Interfaces (BCIs)
and healthcare applications. However, despite their success, many existing
pre-trained models have struggled to fully capture the rich information content
of neural oscillations, a limitation that fundamentally constrains their
performance and generalizability across diverse BCI tasks. This limitation is
frequently rooted in suboptimal architectural design choices which constrain
their representational capacity. In this work, we introduce LaBraM++, an
enhanced Large Brainwave Foundation Model (LBM) that incorporates principled
improvements grounded in robust signal processing foundations. LaBraM++
demonstrates substantial gains across a variety of tasks, consistently
outperforming its originally-based architecture and achieving competitive
results when compared to other open-source LBMs. Its superior performance and
training efficiency highlight its potential as a strong foundation for future
advancements in LBMs.

</details>


### [311] [Masked Conditioning for Deep Generative Models](https://arxiv.org/abs/2505.16725)
*Phillip Mueller,Jannik Wiese,Sebastian Mueller,Lars Mikelsons*

Main category: cs.LG

TL;DR: 提出了一种新的掩码条件方法，用于处理稀疏、混合类型数据，适用于小规模工程数据集，并结合预训练模型提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 工程领域数据集通常规模小、标签稀疏且包含数值和分类条件，计算资源有限，限制了生成模型的应用。

Method: 采用掩码条件训练模拟稀疏条件，探索多种稀疏调度方案，并设计灵活嵌入处理分类和数值条件。

Result: 方法在2D点云和图像数据集上验证有效，小模型结合预训练模型可提升生成质量。

Conclusion: 掩码条件方法适用于工程领域，结合预训练模型可提高生成质量并保持可控性。

Abstract: Datasets in engineering domains are often small, sparsely labeled, and
contain numerical as well as categorical conditions. Additionally.
computational resources are typically limited in practical applications which
hinders the adoption of generative models for engineering tasks. We introduce a
novel masked-conditioning approach, that enables generative models to work with
sparse, mixed-type data. We mask conditions during training to simulate sparse
conditions at inference time. For this purpose, we explore the use of various
sparsity schedules that show different strengths and weaknesses. In addition,
we introduce a flexible embedding that deals with categorical as well as
numerical conditions. We integrate our method into an efficient variational
autoencoder as well as a latent diffusion model and demonstrate the
applicability of our approach on two engineering-related datasets of 2D point
clouds and images. Finally, we show that small models trained on limited data
can be coupled with large pretrained foundation models to improve generation
quality while retaining the controllability induced by our conditioning scheme.

</details>


### [312] [Sequential Monte Carlo for Policy Optimization in Continuous POMDPs](https://arxiv.org/abs/2505.16732)
*Hany Abdulsamad,Sahel Iqbal,Simo Särkkä*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略优化框架，用于连续部分可观测马尔可夫决策过程（POMDP），通过概率推断方法平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，智能体需平衡减少不确定性（探索）与追求即时目标（利用）。现有方法难以有效应对这一挑战。

Method: 将策略学习建模为非马尔可夫Feynman-Kac模型中的概率推断，无需外部探索奖励或启发式设计。采用嵌套顺序蒙特卡洛（SMC）算法优化策略。

Result: 在标准连续POMDP基准测试中，该方法显著优于现有方法，尤其在不确定性下表现优异。

Conclusion: 该框架通过概率推断和嵌套SMC算法，有效解决了POMDP中的探索与利用问题，具有广泛适用性。

Abstract: Optimal decision-making under partial observability requires agents to
balance reducing uncertainty (exploration) against pursuing immediate
objectives (exploitation). In this paper, we introduce a novel policy
optimization framework for continuous partially observable Markov decision
processes (POMDPs) that explicitly addresses this challenge. Our method casts
policy learning as probabilistic inference in a non-Markovian Feynman--Kac
model that inherently captures the value of information gathering by
anticipating future observations, without requiring extrinsic exploration
bonuses or handcrafted heuristics. To optimize policies under this model, we
develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently
estimates a history-dependent policy gradient under samples from the optimal
trajectory distribution induced by the POMDP. We demonstrate the effectiveness
of our algorithm across standard continuous POMDP benchmarks, where existing
methods struggle to act under uncertainty.

</details>


### [313] [Forward-only Diffusion Probabilistic Models](https://arxiv.org/abs/2505.16733)
*Ziwei Luo,Fredrik K. Gustafsson,Jens Sjölund,Thomas B. Schön*

Main category: cs.LG

TL;DR: 本文提出了一种仅前向扩散（FoD）的生成建模方法，通过单一前向扩散过程直接学习数据生成，简化了传统扩散模型的前后耦合过程。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖前后耦合的扩散方案，复杂且效率低。FoD旨在通过单一前向过程简化生成框架，同时保持高效性。

Method: FoD基于状态依赖的线性随机微分方程，包含漂移和扩散函数中的均值回归项，确保收敛到干净数据。训练采用随机流匹配目标，支持非马尔可夫链采样。

Result: FoD在图像修复和无条件生成任务中表现优异，证明了其高效性。

Conclusion: FoD是一种简单且高效的生成建模方法，具有竞争性能，适用于多种任务。

Abstract: This work presents a forward-only diffusion (FoD) approach for generative
modelling. In contrast to traditional diffusion models that rely on a coupled
forward-backward diffusion scheme, FoD directly learns data generation through
a single forward diffusion process, yielding a simple yet efficient generative
framework. The core of FoD is a state-dependent linear stochastic differential
equation that involves a mean-reverting term in both the drift and diffusion
functions. This mean-reversion property guarantees the convergence to clean
data, naturally simulating a stochastic interpolation between source and target
distributions. More importantly, FoD is analytically tractable and is trained
using a simple stochastic flow matching objective, enabling a few-step
non-Markov chain sampling during inference. The proposed FoD model, despite its
simplicity, achieves competitive performance on various image-conditioned
(e.g., image restoration) and unconditional generation tasks, demonstrating its
effectiveness in generative modelling. Our code is available at
https://github.com/Algolzw/FoD.

</details>


### [314] [Maximum Total Correlation Reinforcement Learning](https://arxiv.org/abs/2505.16734)
*Bang You,Puze Liu,Huaping Liu,Jan Peters,Oleg Arenz*

Main category: cs.LG

TL;DR: 论文探讨了通过最大化轨迹内的总相关性来促进简单行为的方法，提出了一种优化策略和状态表示的算法，在模拟机器人环境中表现出更强的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 通过关注本质内容（如简单策略、表示和目标）来提高强化学习的泛化性和鲁棒性。

Method: 修改强化学习问题，最大化轨迹内的总相关性，并提出一种基于下界近似的算法优化策略和状态表示。

Result: 在模拟机器人环境中，该方法生成的策略具有周期性和可压缩轨迹，对噪声和动态变化表现出更强的鲁棒性，同时提升了原始任务的性能。

Conclusion: 通过促进简单行为，可以显著提升强化学习模型的鲁棒性和性能。

Abstract: Simplicity is a powerful inductive bias. In reinforcement learning,
regularization is used for simpler policies, data augmentation for simpler
representations, and sparse reward functions for simpler objectives, all that,
with the underlying motivation to increase generalizability and robustness by
focusing on the essentials. Supplementary to these techniques, we investigate
how to promote simple behavior throughout the episode. To that end, we
introduce a modification of the reinforcement learning problem that
additionally maximizes the total correlation within the induced trajectories.
We propose a practical algorithm that optimizes all models, including policy
and state representation, based on a lower-bound approximation. In simulated
robot environments, our method naturally generates policies that induce
periodic and compressible trajectories, and that exhibit superior robustness to
noise and changes in dynamics compared to baseline methods, while also
improving performance in the original tasks.

</details>


### [315] [Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?](https://arxiv.org/abs/2505.16736)
*Nicolas Keriven*

Main category: cs.LG

TL;DR: 论文分析了图神经网络（GNN）中的过度平滑问题，特别是从优化角度探讨了反向传播中的过度平滑现象及其对梯度的影响。


<details>
  <summary>Details</summary>
Motivation: GNN在实践中难以避免过度平滑问题，即使理论上可以通过权重调整避免。本文旨在从优化角度揭示这一现象的根本原因。

Method: 通过分析反向传播中的误差平滑现象，研究了非线性激活函数下前向与反向平滑的相互作用，并证明了GNN中存在大量虚假驻点。

Result: 研究表明，反向传播中的线性平滑导致梯度趋近于零而损失仍高，这种现象在深层GNN中尤为显著。

Conclusion: 本文揭示了GNN优化景观中的特定问题，为理解其优化行为提供了新视角。

Abstract: Oversmoothing has long been identified as a major limitation of Graph Neural
Networks (GNNs): input node features are smoothed at each layer and converge to
a non-informative representation, if the weights of the GNN are sufficiently
bounded. This assumption is crucial: if, on the contrary, the weights are
sufficiently large, then oversmoothing may not happen. Theoretically, GNN could
thus learn to not oversmooth. However it does not really happen in practice,
which prompts us to examine oversmoothing from an optimization point of view.
In this paper, we analyze backward oversmoothing, that is, the notion that
backpropagated errors used to compute gradients are also subject to
oversmoothing from output to input. With non-linear activation functions, we
outline the key role of the interaction between forward and backward smoothing.
Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit
many spurious stationary points: as soon as the last layer is trained, the
whole GNN is at a stationary point. As a result, we can exhibit regions where
gradients are near-zero while the loss stays high. The proof relies on the fact
that, unlike forward oversmoothing, backward errors are subjected to a linear
oversmoothing even in the presence of non-linear activation function, such that
the average of the output error plays a key role. Additionally, we show that
this phenomenon is specific to deep GNNs, and exhibit counter-example
Multi-Layer Perceptron. This paper is a step toward a more complete
comprehension of the optimization landscape specific to GNNs.

</details>


### [316] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
*Chengcan Wu,Zhixin Zhang,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 本文探讨了微调大型语言模型（LLMs）时安全性能下降的问题，并提出了一种安全感知探测（SAP）优化框架，以在保持任务性能的同时减少有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 尽管在预训练阶段采用了安全对齐技术，但微调LLMs时仍可能因数据问题导致安全性能下降，本文旨在解决这一问题。

Method: 提出SAP框架，通过在梯度传播过程中引入安全感知探测，识别潜在风险方向，从而减少安全性能下降。

Result: 实验表明，SAP能有效降低有害内容生成，同时保持与标准微调方法相当的测试损失。

Conclusion: SAP是一种有效的框架，可在微调LLMs时兼顾任务性能与安全性。

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [317] [Meta-reinforcement learning with minimum attention](https://arxiv.org/abs/2505.16741)
*Pilhwa Lee,Shashank Gupta*

Main category: cs.LG

TL;DR: 论文提出将最小注意力原则应用于强化学习中，结合元学习和稳定性分析，在高维非线性动态系统中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索最小注意力原则在强化学习中的应用，以模拟生物控制（如运动学习）并提升学习效率。

Method: 采用基于模型的元学习方法，结合集成模型学习和梯度元策略学习，交替进行。

Result: 最小注意力在少样本快速适应和模型与环境扰动下的方差减少方面优于现有算法，同时提高了能量效率。

Conclusion: 最小注意力原则在强化学习中具有显著优势，尤其在快速适应和稳定性方面表现突出。

Abstract: Minimum attention applies the least action principle in the changes of
control concerning state and time, first proposed by Brockett. The involved
regularization is highly relevant in emulating biological control, such as
motor learning. We apply minimum attention in reinforcement learning (RL) as
part of the rewards and investigate its connection to meta-learning and
stabilization. Specifically, model-based meta-learning with minimum attention
is explored in high-dimensional nonlinear dynamics. Ensemble-based model
learning and gradient-based meta-policy learning are alternately performed.
Empirically, we show that the minimum attention does show outperforming
competence in comparison to the state-of-the-art algorithms in model-free and
model-based RL, i.e., fast adaptation in few shots and variance reduction from
the perturbations of the model and environment. Furthermore, the minimum
attention demonstrates the improvement in energy efficiency.

</details>


### [318] [Revenue Optimization with Price-Sensitive and Interdependent Demand](https://arxiv.org/abs/2505.16748)
*Julien Laasri,Marc Revol*

Main category: cs.LG

TL;DR: 本文聚焦于航空机票定价和数量的优化问题，旨在通过预定义价格选项最大化直飞航班的收入。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决航空公司如何在预定义价格和需求数据下优化定价和数量决策，以实现收入最大化。

Method: 方法基于预定义的价格选项和需求数据，通过优化算法选择最佳价格组合。

Result: 预期结果是找到能够最大化直飞航班收入的最优定价策略。

Conclusion: 结论是通过优化定价和数量决策，航空公司可以有效提升直飞航班的收入。

Abstract: As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],
Revenue Management aims to maximize an organization's revenue by considering
three types of decision categories: structural, pricing, and quantity. In this
document, our primary focus will be on decisions related to pricing and
quantity for the sale of airline tickets on a direct flight over a certain
number of time periods. More specifically, we will only focus on the
optimization aspect of this problem. We will assume the demand data to be
given, since Air France estimates it beforehand using real data. Similarly, we
assume all price options to be predetermined by Air France's algorithms and
verified by their analysts. Our objective will be to maximize the revenue of a
direct flight by choosing the prices for each product from the predefined set
of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur
ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un
organisme \`a partir de trois types de cat\'egories de d\'ecision :
structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons
principalement aux d\'ecisions de type prix et quantit\'e pour la vente de
billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.
Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du
probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car
elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees
r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on
nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se
basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es
par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct
en choisissant les prix de chaque produit parmi ceux impos\'es.

</details>


### [319] [PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects](https://arxiv.org/abs/2505.16754)
*Hannah Markgraf,Michael Eichelbeck,Daria Cappey,Selin Demirtürk,Yara Schattschneider,Matthias Althoff*

Main category: cs.LG

TL;DR: PyTupli是一个Python工具，旨在简化离线强化学习（RL）中基准环境和数据集的创建、存储与共享。


<details>
  <summary>Details</summary>
Motivation: 离线RL需要高效管理数据集，但现有工具缺乏标准化和可扩展性，PyTupli填补了这一空白。

Method: PyTupli提供轻量级客户端库和容器化服务器，支持数据上传、检索和精细过滤。

Result: PyTupli实现了数据集的高效管理，支持协作和可复现的离线RL研究。

Conclusion: PyTupli通过优化数据集基础设施，推动了离线RL研究的协作性和可扩展性。

Abstract: Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.

</details>


### [320] [Multi-Output Gaussian Processes for Graph-Structured Data](https://arxiv.org/abs/2505.16755)
*Ayano Nakai-Kasai,Tadashi Wadayama*

Main category: cs.LG

TL;DR: 本文提出了一种基于多输出高斯过程（MOGP）的图结构数据回归方法，能够捕捉顶点间及数据间的相关性，具有广泛的适用性和高表达能力。


<details>
  <summary>Details</summary>
Motivation: 图结构数据中顶点和边的相关性需要有效建模，现有方法在数据配置、模型选择和推断场景上存在限制。

Method: 基于MOGP定义构建回归方法，利用灵活的核设计提高表达能力，涵盖现有高斯过程方法并扩展其应用范围。

Result: 通过合成和真实数据的计算机实验验证了方法的性能，展示了其扩展能力。

Conclusion: 该方法不仅涵盖现有方法，还能突破其限制，适用于多种数据配置和场景。

Abstract: Graph-structured data is a type of data to be obtained associated with a
graph structure where vertices and edges describe some kind of data
correlation. This paper proposes a regression method on graph-structured data,
which is based on multi-output Gaussian processes (MOGP), to capture both the
correlation between vertices and the correlation between associated data. The
proposed formulation is built on the definition of MOGP. This allows it to be
applied to a wide range of data configurations and scenarios. Moreover, it has
high expressive capability due to its flexibility in kernel design. It includes
existing methods of Gaussian processes for graph-structured data as special
cases and is possible to remove restrictions on data configurations, model
selection, and inference scenarios in the existing methods. The performance of
extensions achievable by the proposed formulation is evaluated through computer
experiments with synthetic and real data.

</details>


### [321] [FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting](https://arxiv.org/abs/2505.16786)
*Fares B. Mehouachi,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: FlowMixer是一种通过约束矩阵操作建模时空模式的神经网络架构，结合非负矩阵混合层和可逆映射框架，提供可解释的模式和直接代数操作预测的能力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过架构约束提升神经预测系统的预测性能和数学可解释性，尤其是在时空模式建模中。

Method: 采用非负矩阵混合层和可逆映射框架，结合Kronecker-Koopman特征模态框架，实现形状保持设计。

Result: 在多个领域的实验中表现出强大的长期预测能力，并能有效建模混沌吸引子和湍流等物理现象。

Conclusion: 架构约束可以同时提升神经预测系统的预测性能和数学可解释性。

Abstract: We introduce FlowMixer, a neural architecture that leverages constrained
matrix operations to model structured spatiotemporal patterns. At its core,
FlowMixer incorporates non-negative matrix mixing layers within a reversible
mapping framework-applying transforms before mixing and their inverses
afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode
framework that bridges statistical learning with dynamical systems theory,
providing interpretable spatiotemporal patterns and facilitating direct
algebraic manipulation of prediction horizons without retraining. Extensive
experiments across diverse domains demonstrate FlowMixer's robust long-horizon
forecasting capabilities while effectively modeling physical phenomena such as
chaotic attractors and turbulent flows. These results suggest that
architectural constraints can simultaneously enhance predictive performance and
mathematical interpretability in neural forecasting systems.

</details>


### [322] [Learning Flexible Forward Trajectories for Masked Molecular Diffusion](https://arxiv.org/abs/2505.16790)
*Hyunjin Seo,Taewon Kim,Sihyun Yu,SungSoo Ahn*

Main category: cs.LG

TL;DR: MDMs在分子生成中表现不佳，提出MELD方法通过元素级噪声调度解决状态冲突问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索MDMs在分子生成中的潜力，发现标准方法性能下降，需解决状态冲突问题。

Method: 提出MELD方法，通过参数化噪声调度网络为每个分子元素分配不同噪声率。

Result: MELD将化学有效性从15%提升至93%，在条件生成任务中达到SOTA性能。

Conclusion: MELD有效解决了MDMs在分子生成中的状态冲突问题，显著提升了生成质量。

Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling
discrete data, while their potential in molecular generation remains
underexplored. In this work, we explore their potential and introduce the
surprising result that naively applying standards MDMs severely degrades the
performance. We identify the critical cause of this issue as a state-clashing
problem-where the forward diffusion of distinct molecules collapse into a
common state, resulting in a mixture of reconstruction targets that cannot be
learned using typical reverse diffusion process with unimodal predictions. To
mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that
orchestrates per-element corruption trajectories to avoid collision between
distinct molecular graphs. This is achieved through a parameterized noise
scheduling network that assigns distinct corruption rates to individual graph
elements, i.e., atoms and bonds. Extensive experiments on diverse molecular
benchmarks reveal that MELD markedly enhances overall generation quality
compared to element-agnostic noise scheduling, increasing the chemical validity
of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves
state-of-the-art property alignment in conditional generation tasks.

</details>


### [323] [Cohort-Based Active Modality Acquisition](https://arxiv.org/abs/2505.16791)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 论文提出了一种基于队列的主动模态获取方法（CAMA），用于在资源有限的情况下选择需要额外模态的样本，通过生成式插补和判别式建模优化获取策略。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态机器学习应用中，样本的模态可能不完整，且获取额外模态成本高，因此需要一种有效的方法在资源有限时优先获取关键模态。

Method: 提出CAMA框架，结合生成式插补和判别式建模，估计缺失模态的预期收益，并引入性能上限启发式方法作为基准。

Result: 实验表明，基于插补的策略比单模态信息、熵引导或随机选择更有效。

Conclusion: CAMA为队列级别的模态获取提供了有效解决方案，优化了资源利用。

Abstract: Real-world machine learning applications often involve data from multiple
modalities that must be integrated effectively to make robust predictions.
However, in many practical settings, not all modalities are available for every
sample, and acquiring additional modalities can be costly. This raises the
question: which samples should be prioritized for additional modality
acquisition when resources are limited? While prior work has explored
individual-level acquisition strategies and training-time active learning
paradigms, test-time and cohort-based acquisition remain underexplored despite
their importance in many real-world settings. We introduce Cohort-based Active
Modality Acquisition (CAMA), a novel test-time setting to formalize the
challenge of selecting which samples should receive additional modalities. We
derive acquisition strategies that leverage a combination of generative
imputation and discriminative modeling to estimate the expected benefit of
acquiring missing modalities based on common evaluation metrics. We also
introduce upper-bound heuristics that provide performance ceilings to benchmark
acquisition strategies. Experiments on common multimodal datasets demonstrate
that our proposed imputation-based strategies can more effectively guide the
acquisition of new samples in comparison to those relying solely on unimodal
information, entropy guidance, and random selections. Our work provides an
effective solution for optimizing modality acquisition at the cohort level,
enabling better utilization of resources in constrained settings.

</details>


### [324] [A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents](https://arxiv.org/abs/2505.16801)
*Eleftherios Kalafatis,Konstantinos Mitsis,Konstantia Zarkogianni,Maria Athanasiou,Konstantina Nikita*

Main category: cs.LG

TL;DR: 该研究提出了一种自动化评估PCG（程序化内容生成）在严肃游戏中集成效果的方法，结合深度强化学习（DRL）游戏测试代理，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前严肃游戏开发中，PCG技术的集成缺乏有效的评估框架，难以量化其对玩家体验的影响。

Method: 研究采用DRL游戏测试代理，对比了三种不同PCG版本（随机生成和遗传算法生成NPC）在游戏中的表现。

Result: 实验结果显示，使用遗传算法的PCG版本（2和3）在胜率和训练时间上显著优于随机生成版本（1），胜率峰值达97%。

Conclusion: 提出的框架能够为PCG在严肃游戏中的评估提供有意义的数据支持。

Abstract: Serious Games (SGs) are nowadays shifting focus to include procedural content
generation (PCG) in the development process as a means of offering personalized
and enhanced player experience. However, the development of a framework to
assess the impact of PCG techniques when integrated into SGs remains
particularly challenging. This study proposes a methodology for automated
evaluation of PCG integration in SGs, incorporating deep reinforcement learning
(DRL) game testing agents. To validate the proposed framework, a previously
introduced SG featuring card game mechanics and incorporating three different
versions of PCG for nonplayer character (NPC) creation has been deployed.
Version 1 features random NPC creation, while versions 2 and 3 utilize a
genetic algorithm approach. These versions are used to test the impact of
different dynamic SG environments on the proposed framework's agents. The
obtained results highlight the superiority of the DRL game testing agents
trained on Versions 2 and 3 over those trained on Version 1 in terms of win
rate (i.e. number of wins per played games) and training time. More
specifically, within the execution of a test emulating regular gameplay, both
Versions 2 and 3 peaked at a 97% win rate and achieved statistically
significant higher (p=0009) win rates compared to those achieved in Version 1
that peaked at 94%. Overall, results advocate towards the proposed framework's
capability to produce meaningful data for the evaluation of procedurally
generated content in SGs.

</details>


### [325] [Contextual Learning for Stochastic Optimization](https://arxiv.org/abs/2505.16829)
*Anna Heuser,Thomas Kesselheim*

Main category: cs.LG

TL;DR: 论文研究了从上下文价值分布样本中学习的问题，通过最小化凸替代损失，学习每个上下文的经验分布，并应用于随机优化问题的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 受随机优化的启发，研究如何从上下文价值分布的样本中学习，以解决未知分布下的随机优化问题。

Method: 通过最小化凸替代损失，学习每个上下文的经验分布，确保其与真实分布的L\'evy距离较小。

Result: 为随机优化问题提供了样本复杂度界限，证明在强单调和稳定优化问题中样本复杂度为多项式级别。

Conclusion: 该方法适用于多种随机优化问题，如单物品收益最大化、潘多拉盒和最优停止问题。

Abstract: Motivated by stochastic optimization, we introduce the problem of learning
from samples of contextual value distributions. A contextual value distribution
can be understood as a family of real-valued distributions, where each sample
consists of a context $x$ and a random variable drawn from the corresponding
real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn
an empirical distribution $D'_x$ for each context, ensuring a small L\'evy
distance to $D_x$. We apply this result to obtain the sample complexity bounds
for the learning of an $\epsilon$-optimal policy for stochastic optimization
problems defined on an unknown contextual value distribution. The sample
complexity is shown to be polynomial for the general case of strongly monotone
and stable optimization problems, including Single-item Revenue Maximization,
Pandora's Box and Optimal Stopping.

</details>


### [326] [Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning](https://arxiv.org/abs/2505.16833)
*Alihan Hüyük,Finale Doshi-Velez*

Main category: cs.LG

TL;DR: 论文提出了一种量化计划行动间依赖关系的方法——战略链接分数，并展示了其在解释黑盒RL代理、改进决策支持系统以及分析非RL代理规划过程中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究长期规划中行动间的依赖关系，以揭示策略性行动如何通过牺牲短期利益换取更大的长期回报。

Method: 引入战略链接分数，衡量一个决策在后续决策不可用时的可能性下降程度。

Result: 通过三个应用验证了战略链接分数的实用性：解释RL代理、改进决策支持系统、分析非RL代理的规划过程。

Conclusion: 战略链接分数是一种有效的工具，可用于量化行动间的策略依赖关系，并在多种实际场景中发挥作用。

Abstract: Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.

</details>


### [327] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
*Tajamul Ashraf,Mohammed Mohsen Peerzada,Moloud Abdar,Yutong Xie,Yuyin Zhou,Xiaofeng Liu,Iqra Altaf Gillani,Janibul Bashir*

Main category: cs.LG

TL;DR: ATR-Bench是一个统一框架，用于通过适应性、信任和推理三个维度分析联邦学习，填补了标准化评估的空白。


<details>
  <summary>Details</summary>
Motivation: 联邦学习缺乏标准化的评估方法，阻碍了系统进展和公平比较，ATR-Bench旨在解决这一问题。

Method: 提出ATR-Bench框架，从适应性、信任和推理三个维度分析联邦学习，并进行了代表性方法和数据集的基准测试。

Result: 在适应性和信任方面提供了详细分析，推理方面因缺乏可靠指标仅提供文献驱动的见解。

Conclusion: ATR-Bench为联邦学习的系统化评估奠定了基础，并计划公开代码库和持续更新的研究资源。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


### [328] [Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only](https://arxiv.org/abs/2505.16856)
*Wei Xiao,Jiacheng Liu,Zifeng Zhuang,Runze Suo,Shangke Lyu,Donglin Wang*

Main category: cs.LG

TL;DR: 论文提出PORL方法，仅利用离线预训练策略进行在线强化学习微调，避免了对预训练Q函数的依赖，解决了现有方法因保守性导致的探索受限问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习方法依赖离线预训练的Q函数，限制了其在仅有预训练策略（如模仿学习）场景下的应用，且因保守性阻碍探索。

Method: 提出PORL方法，在线阶段从零初始化Q函数，避免保守性，仅依赖离线预训练策略进行微调。

Result: PORL性能与先进离线到在线强化学习算法相当，且首次实现了直接微调行为克隆策略。

Conclusion: PORL为仅依赖预训练策略的在线强化学习微调提供了新思路，扩展了应用场景。

Abstract: Improving the performance of pre-trained policies through online
reinforcement learning (RL) is a critical yet challenging topic. Existing
online RL fine-tuning methods require continued training with offline
pretrained Q-functions for stability and performance. However, these offline
pretrained Q-functions commonly underestimate state-action pairs beyond the
offline dataset due to the conservatism in most offline RL methods, which
hinders further exploration when transitioning from the offline to the online
setting. Additionally, this requirement limits their applicability in scenarios
where only pre-trained policies are available but pre-trained Q-functions are
absent, such as in imitation learning (IL) pre-training. To address these
challenges, we propose a method for efficient online RL fine-tuning using
solely the offline pre-trained policy, eliminating reliance on pre-trained
Q-functions. We introduce PORL (Policy-Only Reinforcement Learning
Fine-Tuning), which rapidly initializes the Q-function from scratch during the
online phase to avoid detrimental pessimism. Our method not only achieves
competitive performance with advanced offline-to-online RL algorithms and
online RL approaches that leverage data or policies prior, but also pioneers a
new path for directly fine-tuning behavior cloning (BC) policies.

</details>


### [329] [Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft](https://arxiv.org/abs/2505.16857)
*Ertuğrul Keçeci,Müjde Güzelkaya,Tufan Kumbasar*

Main category: cs.LG

TL;DR: 论文提出了一种基于增量聚类的联邦学习方法IC-SYSID，用于解决多数据源下的系统辨识问题，无需先验知识。通过动态聚类和合并优化，结合正则化和初始化策略，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习框架下多数据源系统辨识问题，避免依赖先验知识。

Method: 提出IC-SYSID算法，结合增量聚类ClusterCraft和ClusterMerge优化，加入正则化项和Glorot初始化，采用小批量深度学习。

Result: 实验表明IC-SYSID在车辆动力学学习中表现优异，避免了不稳定聚类。

Conclusion: IC-SYSID有效解决了多数据源系统辨识问题，性能优越且稳定。

Abstract: This paper addresses the System Identification (SYSID) problem within the
framework of federated learning. We introduce a novel algorithm, Incremental
Clustering-based federated learning method for SYSID (IC-SYSID), designed to
tackle SYSID challenges across multiple data sources without prior knowledge.
IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to
eliminate the dependency on the prior knowledge of the dataset. CC starts with
a single cluster model and assigns similar local workers to the same clusters
by dynamically increasing the number of clusters. To reduce the number of
clusters generated by CC, we introduce ClusterMerge, where similar cluster
models are merged. We also introduce enhanced ClusterCraft to reduce the
generation of similar cluster models during the training. Moreover, IC-SYSID
addresses cluster model instability by integrating a regularization term into
the loss function and initializing cluster models with scaled Glorot
initialization. It also utilizes a mini-batch deep learning approach to manage
large SYSID datasets during local training. Through the experiments conducted
on a real-world representing SYSID problem, where a fleet of vehicles
collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high
SYSID performance while preventing the learning of unstable clusters.

</details>


### [330] [GCAL: Adapting Graph Models to Evolving Domain Shifts](https://arxiv.org/abs/2505.16860)
*Ziyue Qiao,Qianyi Cai,Hao Dong,Jiawei Gu,Pengyang Wang,Meng Xiao,Xiao Luo,Hui Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种图持续自适应学习（GCAL）方法，用于解决多分布外（OOD）图数据的持续域适应问题，显著提升了模型的适应性和知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 传统图域适应方法局限于单步适应，无法处理连续域偏移且容易发生灾难性遗忘。

Method: GCAL采用双层优化策略，包括基于信息最大化的适应阶段和基于信息瓶颈理论的记忆生成阶段。

Result: 实验表明，GCAL在适应性和知识保留方面显著优于现有方法。

Conclusion: GCAL为图数据的持续域适应提供了一种有效解决方案。

Abstract: This paper addresses the challenge of graph domain adaptation on evolving,
multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation
methods are confined to single-step adaptation, making them ineffective in
handling continuous domain shifts and prone to catastrophic forgetting. This
paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed
to enhance model sustainability and adaptability across various graph domains.
GCAL employs a bilevel optimization strategy. The "adapt" phase uses an
information maximization approach to fine-tune the model with new graph domains
while re-adapting past memories to mitigate forgetting. Concurrently, the
"generate memory" phase, guided by a theoretical lower bound derived from
information bottleneck theory, involves a variational memory graph generation
module to condense original graphs into memories. Extensive experimental
evaluations demonstrate that GCAL substantially outperforms existing methods in
terms of adaptability and knowledge retention.

</details>


### [331] [A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams](https://arxiv.org/abs/2505.16872)
*Mohammed Al-Qudah,Fadi AlMahamid*

Main category: cs.LG

TL;DR: 本文提出了一种多步骤评估框架，分析预处理步骤（如归一化、变换和特征选择）与不同机器学习模型（RNN-LSTM、自动编码器和梯度提升）的交互作用，以提升IoT异常检测性能。实验表明，梯度提升在多数预处理配置中表现最佳，而RNN-LSTM和自动编码器在特定场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: IoT设备的快速扩展带来了安全挑战，现有研究缺乏对预处理步骤与模型架构交互作用的系统性分析，因此本文旨在填补这一空白。

Method: 提出多步骤评估框架，结合三种预处理步骤（归一化、变换、特征选择）和三种机器学习算法（RNN-LSTM、自动编码器、梯度提升），在IoTID20数据集上进行实验。

Result: 梯度提升在多数预处理配置中表现最优；RNN-LSTM在z-score归一化下表现显著提升；自动编码器在召回率上表现突出，适合无监督场景。

Conclusion: 通过系统分析预处理选择与机器学习技术的交互作用，该框架为提升IoT异常检测性能提供了实用指导。

Abstract: The rapid expansion of Internet of Things (IoT) devices has introduced
critical security challenges, underscoring the need for accurate anomaly
detection. Although numerous studies have proposed machine learning (ML)
methods for this purpose, limited research systematically examines how
different preprocessing steps--normalization, transformation, and feature
selection--interact with distinct model architectures. To address this gap,
this paper presents a multi-step evaluation framework assessing the combined
impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder
neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the
IoTID20 dataset shows that GBoosting consistently delivers superior accuracy
across preprocessing configurations, while RNN-LSTM shows notable gains with
z-score normalization and autoencoders excel in recall, making them well-suited
for unsupervised scenarios. By offering a structured analysis of preprocessing
decisions and their interplay with various ML techniques, the proposed
framework provides actionable guidance to enhance anomaly detection performance
in IoT environments.

</details>


### [332] [Structure-Aligned Protein Language Model](https://arxiv.org/abs/2505.16896)
*Can Chen,David Heurtel-Depeiges,Robert M. Vernon,Christopher James Langmead,Yoshua Bengio,Quentin Fournier*

Main category: cs.LG

TL;DR: 通过对比学习和结构预测任务，将蛋白质结构知识融入蛋白质语言模型（pLM），显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有pLM缺乏结构知识，限制了其在生物应用中的表现。

Method: 提出双任务框架：1）对比学习对齐pLM和pGNN的残基表示；2）结构预测任务优化pLM。引入残基损失选择模块处理PDB数据质量差异。

Result: 在ESM2和AMPLIFY上应用后，性能显著提升，如ESM2接触预测提高12.7%。

Conclusion: 该方法成功将结构知识融入pLM，为生物应用提供了更强大的工具。

Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases
excel at various downstream tasks but lack the structural knowledge essential
for many biological applications. To address this, we integrate structural
insights from pre-trained protein graph neural networks (pGNNs) into pLMs
through a latent-level contrastive learning task. This task aligns residue
representations from pLMs with those from pGNNs across multiple proteins,
enriching pLMs with inter-protein structural knowledge. Additionally, we
incorporate a physical-level task that infuses intra-protein structural
knowledge by optimizing pLMs to predict structural tokens. The proposed
dual-task framework effectively incorporates both inter-protein and
intra-protein structural knowledge into pLMs. Given the variability in the
quality of protein structures in PDB, we further introduce a residue loss
selection module, which uses a small model trained on high-quality structures
to select reliable yet challenging residue losses for the pLM to learn.
Applying our structure alignment method to the state-of-the-art ESM2 and
AMPLIFY results in notable performance gains across a wide range of tasks,
including a 12.7% increase in ESM2 contact prediction. The data, code, and
resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.

</details>


### [333] [Unsupervised Prompting for Graph Neural Networks](https://arxiv.org/abs/2505.16903)
*Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 本文提出了一种完全无监督的GNN提示方法，通过伪标签的一致性正则化，解决了现有GNN提示方法依赖标签数据的问题，并在无标签数据下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN提示方法依赖标签数据且需轻量级微调，而LLMs的上下文学习方法展示了无参数更新和少量标签数据的潜力，因此本文旨在探索无监督GNN提示方法。

Method: 提出基于一致性正则化和伪标签的无监督提示方法，使用两种正则化技术对齐分布并减少预测偏差。

Result: 实验表明，该方法在无标签数据下优于现有依赖标签的提示方法。

Conclusion: 本文提出的无监督GNN提示方法在无标签数据和参数不更新的情况下，显著提升了预训练GNN的泛化能力。

Abstract: Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to
address the semantic gap between pre-training and fine-tuning steps. However,
existing GNN prompting methods rely on labeled data and involve lightweight
fine-tuning for downstream tasks. Meanwhile, in-context learning methods for
Large Language Models (LLMs) have shown promising performance with no parameter
updating and no or minimal labeled data. Inspired by these approaches, in this
work, we first introduce a challenging problem setup to evaluate GNN prompting
methods. This setup encourages a prompting function to enhance a pre-trained
GNN's generalization to a target dataset under covariate shift without updating
the GNN's parameters and with no labeled data. Next, we propose a fully
unsupervised prompting method based on consistency regularization through
pseudo-labeling. We use two regularization techniques to align the prompted
graphs' distribution with the original data and reduce biased predictions.
Through extensive experiments under our problem setting, we demonstrate that
our unsupervised approach outperforms the state-of-the-art prompting methods
that have access to labels.

</details>


### [334] [Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype](https://arxiv.org/abs/2505.16918)
*Nikola Tankovic,Robert Sajina*

Main category: cs.LG

TL;DR: 本文综述了上下文多臂老虎机（CMAB）方法，并提出了一个可扩展、可解释的优惠选择框架，解决了快速变化优惠的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中优惠选择的学习效率和泛化问题，同时实现可扩展性和可解释性。

Method: 通过产品类别级别的上下文建模，支持多类别优惠，结合MPG和MF等高级特征，采用Python实现。

Result: 框架提高了学习效率和泛化能力，通过逻辑回归模型和LLM接口实现实时解释性。

Conclusion: 该框架在研究和实际应用中具有价值，支持个性化优惠优化和自动化决策的可信度。

Abstract: This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)
methods and introduces an experimental framework for scalable, interpretable
offer selection, addressing the challenge of fast-changing offers. The approach
models context at the product category level, allowing offers to span multiple
categories and enabling knowledge transfer across similar offers. This improves
learning efficiency and generalization in dynamic environments. The framework
extends standard CMAB methodology to support multi-category contexts, and
achieves scalability through efficient feature engineering and modular design.
Advanced features such as MPG (Member Purchase Gap) and MF (Matrix
Factorization) capture nuanced user-offer interactions, with implementation in
Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models
yield transparent weight vectors, accessible via a large language model (LLM)
interface for real-time, user-level tracking and explanation of evolving
preferences. This enables the generation of detailed member profiles and
identification of behavioral patterns, supporting personalized offer
optimization and enhancing trust in automated decisions. By situating our
prototype alongside established paradigms like Generalized Linear Models and
Thompson Sampling, we demonstrate its value for both research and real-world
CMAB applications.

</details>


### [335] [Risk-Averse Reinforcement Learning with Itakura-Saito Loss](https://arxiv.org/abs/2505.16925)
*Igor Udovichenko,Olivier Croissant,Anita Toleutaeva,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Itakura-Saito散度的数值稳定的损失函数，用于风险厌恶强化学习，解决了传统方法中指数计算导致的数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 风险厌恶强化学习在高风险领域有广泛应用，但传统方法因指数计算导致数值不稳定，亟需改进。

Method: 采用Itakura-Saito散度设计新的损失函数，用于学习状态值和动作值函数，并通过理论分析和实验验证其有效性。

Result: 实验表明，新损失函数在多种金融场景中优于现有方法，尤其是在有解析解的场景中表现突出。

Conclusion: 提出的损失函数解决了数值不稳定问题，为风险厌恶强化学习提供了更可靠的解决方案。

Abstract: Risk-averse reinforcement learning finds application in various high-stakes
fields. Unlike classical reinforcement learning, which aims to maximize
expected returns, risk-averse agents choose policies that minimize risk,
occasionally sacrificing expected value. These preferences can be framed
through utility theory. We focus on the specific case of the exponential
utility function, where we can derive the Bellman equations and employ various
reinforcement learning algorithms with few modifications. However, these
methods suffer from numerical instability due to the need for exponent
computation throughout the process. To address this, we introduce a numerically
stable and mathematically sound loss function based on the Itakura-Saito
divergence for learning state-value and action-value functions. We evaluate our
proposed loss function against established alternatives, both theoretically and
empirically. In the experimental section, we explore multiple financial
scenarios, some with known analytical solutions, and show that our loss
function outperforms the alternatives.

</details>


### [336] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
*Noah Amsel,David Persson,Christopher Musco,Robert Gower*

Main category: cs.LG

TL;DR: Polar Express是一种GPU友好的算法，用于计算极分解，适用于深度学习中的Muon优化框架，具有快速收敛和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数值分析中的极分解算法（如Newton-Schulz）在深度学习中效率不足，需要一种高效、GPU兼容且不依赖高精度的方法。

Method: Polar Express通过解决极小极大优化问题动态调整多项式更新规则，仅使用矩阵乘法，适合GPU计算。

Result: 在Muon框架中应用Polar Express，显著提升了GPT-2等大规模模型的验证损失表现，优于其他方法。

Conclusion: Polar Express是一种高效、稳定的极分解算法，适用于深度学习，尤其在GPU环境下表现优异。

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [337] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
*Zebin You,Shen Nie,Xiaolu Zhang,Jun Hu,Jun Zhou,Zhiwu Lu,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: LLaDA-V是一种基于扩散的多模态大语言模型（MLLM），通过视觉指令调优和掩码扩散模型实现多模态对齐，性能优于现有混合自回归-扩散模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态方法主要依赖自回归范式，LLaDA-V探索扩散模型在多模态任务中的潜力。

Method: 基于LLaDA模型，加入视觉编码器和MLP连接器，将视觉特征投影到语言嵌入空间。

Result: 在多模态任务中表现优异，性能接近或超过LLaMA3-V和Qwen2-VL，并在多模态理解任务中达到SOTA。

Conclusion: 扩散模型在多模态任务中具有潜力，值得进一步研究。

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [338] [SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems](https://arxiv.org/abs/2505.16936)
*Yizhuo Chen,Tianchen Wang,You Lyu,Yanlan Hu,Jinyang Li,Tomoyoshi Kimura,Hongjue Zhao,Yigong Hu,Denizhan Kara,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 论文提出了一种自监督的、考虑传感器布局的表征学习方法，用于多视角多模态的物联网数据，旨在从分布式观测中提取空间现象。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中多传感器观测需要从不同视角捕捉环境状态，但现有方法常忽略数据的空间特性，因此需要一种能编码信号与观测者位置关系的方法。

Method: 通过信号与观测者位置的对偶性设计框架，学习测量与几何布局的依赖关系，并结合信息理论和遮挡不变表征学习进行理论分析。

Result: 在车辆监控、人类活动识别和地震定位三个真实数据集上验证了方法的泛化性和鲁棒性。

Conclusion: 该方法显著提升了自监督预训练在物联网信号中的应用，适用于多种模态、传感器布局和空间尺度。

Abstract: This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.

</details>


### [339] [FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records](https://arxiv.org/abs/2505.16941)
*Chao Pang,Vincent Jeanselme,Young Sang Choi,Xinzhuo Jiang,Zilin Jing,Aparajita Kashyap,Yuta Kobayashi,Yanwei Li,Florent Pollet,Karthik Natarajan,Shalmali Joshi*

Main category: cs.LG

TL;DR: 该论文探讨了基础模型在医疗保健中的潜力，提出了14项临床任务进行评估，并分析了不同预训练和数据表示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医疗保健中具有潜力，但缺乏对其临床效用的共识，需要更全面的任务和评估方法。

Method: 提出了一套临床任务，评估了基于500万患者EHR数据的基础模型，分析了预训练、标记化和数据表示策略的影响。

Result: 研究测量了准确性、校准性和子群体表现，揭示了不同策略的权衡。

Conclusion: 研究旨在推动结构化EHR基础模型的实证评估，并指导未来医疗基础模型的开发。

Abstract: Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.

</details>


### [340] [MixAT: Combining Continuous and Discrete Adversarial Training for LLMs](https://arxiv.org/abs/2505.16947)
*Csaba Dékány,Stefan Balauca,Robin Staab,Dimitar I. Dimitrov,Martin Vechev*

Main category: cs.LG

TL;DR: MixAT是一种结合离散和连续对抗攻击的新方法，显著提升大型语言模型（LLM）的鲁棒性，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练在传统机器学习中有效，但在LLM中的应用和效果尚不明确，尤其是离散攻击的防御问题。

Method: 提出MixAT方法，结合离散和连续攻击进行训练，并引入ALO-ASR指标评估模型最坏情况下的脆弱性。

Result: MixAT在ALO-ASR指标上表现优于现有防御方法（<20% vs >50%），且计算开销低。

Conclusion: MixAT为构建更安全的LLM提供了高效且鲁棒的解决方案，揭示了当前方法的盲点。

Abstract: Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, quantization, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.

</details>


### [341] [Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning](https://arxiv.org/abs/2505.16950)
*Adnan Oomerjee,Zafeirios Fountas,Zhongwei Yu,Haitham Bou-Ammar,Jun Wang*

Main category: cs.LG

TL;DR: 论文通过信息瓶颈理论分析解码器Transformer的局限性，提出通过周期性全局变换KV缓存来提升推理任务泛化能力，实验证明其优于更大参数模型和启发式压缩方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练分布外的泛化能力有限，表现为模式插值而非抽象推理。论文试图通过信息瓶颈理论解决这一问题。

Method: 基于信息瓶颈理论，提出在Transformer架构中增加模块，周期性全局重写KV缓存，优化序列表示。

Result: 改进后的模型在数学推理任务中表现显著优于更大参数模型和启发式压缩方法。

Conclusion: 通过信息理论框架优化Transformer内存，解决了仅靠规模扩展无法克服的推理限制。

Abstract: Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.

</details>


### [342] [A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization](https://arxiv.org/abs/2505.16952)
*Shengyu Feng,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.LG

TL;DR: 论文介绍了FrontierCO，一个涵盖八种典型组合优化问题的基准测试，评估了16种基于机器学习的求解器，旨在解决现有基准数据不足和规模小的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习在组合优化中的应用多基于小规模合成数据，缺乏实际大规模场景的验证，且现有基准数据不足。

Method: 提出FrontierCO基准，包含工业应用和前沿研究的实例，提供丰富的训练数据和实际难度。

Result: 评估了16种ML求解器（如图神经网络和LLM代理），揭示了当前方法的优缺点。

Conclusion: FrontierCO为机器学习与组合优化的交叉研究提供了更稳健和实用的指导，数据已公开。

Abstract: Machine learning (ML) has demonstrated considerable potential in supporting
model design and optimization for combinatorial optimization (CO) problems.
However, much of the progress to date has been evaluated on small-scale,
synthetic datasets, raising concerns about the practical effectiveness of
ML-based solvers in real-world, large-scale CO scenarios. Additionally, many
existing CO benchmarks lack sufficient training data, limiting their utility
for evaluating data-driven approaches. To address these limitations, we
introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO
problem types and evaluates 16 representative ML-based solvers--including graph
neural networks and large language model (LLM) agents. FrontierCO features
challenging instances drawn from industrial applications and frontier CO
research, offering both realistic problem difficulty and abundant training
data. Our empirical results provide critical insights into the strengths and
limitations of current ML methods, helping to guide more robust and practically
relevant advances at the intersection of machine learning and combinatorial
optimization. Our data is available at
https://huggingface.co/datasets/CO-Bench/FrontierCO.

</details>


### [343] [ICYM2I: The illusion of multimodal informativeness under missingness](https://arxiv.org/abs/2505.16953)
*Young Sang Choi,Vincent Jeanselme,Pierre Elias,Shalmali Joshi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICYM2I的框架，用于在多模态学习中处理缺失数据的问题，并通过逆概率加权校正评估信息增益。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在人工智能应用中具有潜力，但开发阶段与部署阶段的模态可能不一致，导致信息增益估计偏差。

Method: 提出ICYM2I框架，采用逆概率加权校正方法评估缺失情况下的预测性能和信息增益。

Result: 在合成、半合成和真实医学数据集上验证了该调整方法对信息增益估计的重要性。

Conclusion: 忽略缺失过程会导致偏差，ICYM2I框架能有效校正这种偏差，提升多模态学习的效果。

Abstract: Multimodal learning is of continued interest in artificial intelligence-based
applications, motivated by the potential information gain from combining
different types of data. However, modalities collected and curated during
development may differ from the modalities available at deployment due to
multiple factors including cost, hardware failure, or -- as we argue in this
work -- the perceived informativeness of a given modality. Na{\"i}ve estimation
of the information gain associated with including an additional modality
without accounting for missingness may result in improper estimates of that
modality's value in downstream tasks. Our work formalizes the problem of
missingness in multimodal learning and demonstrates the biases resulting from
ignoring this process. To address this issue, we introduce ICYM2I (In Case You
Multimodal Missed It), a framework for the evaluation of predictive performance
and information gain under missingness through inverse probability
weighting-based correction. We demonstrate the importance of the proposed
adjustment to estimate information gain under missingness on synthetic,
semi-synthetic, and real-world medical datasets.

</details>


### [344] [Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models](https://arxiv.org/abs/2505.16959)
*Alessandro Favero,Antonio Sclocchi,Matthieu Wyart*

Main category: cs.LG

TL;DR: 扩散概率模型在训练过程中会先实现泛化，随后才出现记忆化现象，记忆化时间与数据集大小成正比。


<details>
  <summary>Details</summary>
Motivation: 探讨扩散概率模型在高度过参数化情况下如何实现泛化，而非仅记忆训练数据。

Method: 通过实验分析图像和语言扩散模型，并结合简单概率上下文无关文法的学习，研究泛化与记忆化的时间尺度竞争。

Result: 发现泛化先于记忆化出现，记忆化时间与数据集大小成正比，并绘制了相位图。

Conclusion: 提出基于数据集大小的早期停止准则，可优化泛化并避免记忆化，对超参数迁移和隐私敏感应用有直接意义。

Abstract: Diffusion probabilistic models have become a cornerstone of modern generative
AI, yet the mechanisms underlying their generalization remain poorly
understood. In fact, if these models were perfectly minimizing their training
loss, they would just generate data belonging to their training set, i.e.,
memorize, as empirically found in the overparameterized regime. We revisit this
view by showing that, in highly overparameterized diffusion models,
generalization in natural data domains is progressively achieved during
training before the onset of memorization. Our results, ranging from image to
language diffusion models, systematically support the empirical law that
memorization time is proportional to the dataset size. Generalization vs.
memorization is then best understood as a competition between time scales. We
show that this phenomenology is recovered in diffusion models learning a simple
probabilistic context-free grammar with random rules, where generalization
corresponds to the hierarchical acquisition of deeper grammar rules as training
time grows, and the generalization cost of early stopping can be characterized.
We summarize these results in a phase diagram. Overall, our results support
that a principled early-stopping criterion - scaling with dataset size - can
effectively optimize generalization while avoiding memorization, with direct
implications for hyperparameter transfer and privacy-sensitive applications.

</details>


### [345] [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
*Mingyang Liu,Gabriele Farina,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 论文提出了一种统一微调（UFT）方法，结合了监督微调（SFT）和强化微调（RFT）的优势，解决了它们的局限性，并在不同规模的模型上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的SFT和RFT方法各有不足：SFT可能导致过拟合，而RFT依赖基础模型强度。需要一种统一的方法来提升推理能力。

Method: 提出UFT，将SFT和RFT结合为一个统一过程，既能探索解决方案，又能利用监督信号。

Result: UFT在各类模型规模上均优于SFT和RFT，并突破了RFT的指数样本复杂度瓶颈。

Conclusion: UFT是一种高效且通用的后训练方法，显著提升了模型的推理能力。

Abstract: Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.

</details>


### [346] [PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics](https://arxiv.org/abs/2505.16992)
*Aleksandra Franz,Hao Wei,Luca Guastoni,Nils Thuerey*

Main category: cs.LG

TL;DR: PICT是一个基于PyTorch的可微分流体模拟器，支持GPU加速，能够用于优化和学习复杂湍流模型，并在低分辨率下保持高精度。


<details>
  <summary>Details</summary>
Motivation: 流体模拟是科学计算中的挑战性问题，可微分模拟器因其在深度学习中的梯度信息需求而成为优化和学习的有效工具。

Method: 开发了PICT，一个可微分的压力隐式求解器，通过PyTorch实现并支持GPU。验证了其前向模拟和梯度的准确性，并用于2D和3D湍流模型的学习。

Result: PICT在低分辨率下运行速度显著快于高分辨率参考，同时保持或超越其精度。成功学习了3D湍流通道流的稳定子网格尺度模型。

Conclusion: PICT展示了可微分模拟器在流体模拟中的潜力，并提供了物理启发的正则化技术。开源发布以促进广泛应用。

Abstract: Despite decades of advancements, the simulation of fluids remains one of the
most challenging areas of in scientific computing. Supported by the necessity
of gradient information in deep learning, differentiable simulators have
emerged as an effective tool for optimization and learning in physics
simulations. In this work, we present our fluid simulator PICT, a
differentiable pressure-implicit solver coded in PyTorch with
Graphics-processing-unit (GPU) support. We first verify the accuracy of both
the forward simulation and our derived gradients in various established
benchmarks like lid-driven cavities and turbulent channel flows before we show
that the gradients provided by our solver can be used to learn complicated
turbulence models in 2D and 3D. We apply both supervised and unsupervised
training regimes using physical priors to match flow statistics. In particular,
we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow
purely based on reference statistics. The low-resolution corrector trained with
our solver runs substantially faster than the highly resolved references, while
keeping or even surpassing their accuracy. Finally, we give additional insights
into the physical interpretation of different solver gradients, and motivate a
physically informed regularization technique. To ensure that the full potential
of PICT can be leveraged, it is published as open source:
https://github.com/tum-pbs/PICT.

</details>


### [347] [A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations](https://arxiv.org/abs/2505.16996)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: 提出了一种新框架，解决了微分方程逆问题中同时识别参数和函数时的非唯一性问题，并在生物系统和生态动力学中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如PINNs、UDEs和UPINNs）在同时识别参数和函数时可能因解的非唯一性而受限，需要一种新方法来解决这一问题。

Method: 引入了一个框架，通过建立唯一解的条件，确保参数和函数的准确识别。

Result: 在生物系统和生态动力学中的应用表明，该方法能提供准确且可解释的结果。

Conclusion: 该框架显著提升了机器学习在复杂系统建模中的潜力。

Abstract: Inverse problems involving differential equations often require identifying
unknown parameters or functions from data. Existing approaches, such as
Physics-Informed Neural Networks (PINNs), Universal Differential Equations
(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective
at isolating either parameters or functions but can face challenges when
applied simultaneously due to solution non-uniqueness. In this work, we
introduce a framework that addresses these limitations by establishing
conditions under which unique solutions can be guaranteed. To illustrate, we
apply it to examples from biological systems and ecological dynamics,
demonstrating accurate and interpretable results. Our approach significantly
enhances the potential of machine learning techniques in modeling complex
systems in science and engineering.

</details>


### [348] [Guided Diffusion Sampling on Function Spaces with Applications to PDEs](https://arxiv.org/abs/2505.17004)
*Jiachen Yao,Abbas Mammadov,Julius Berner,Gavin Kerrigan,Jong Chul Ye,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种基于PDE逆问题的条件采样框架FunDPS，通过函数空间扩散模型和梯度引导机制，从稀疏或噪声数据中恢复完整解。


<details>
  <summary>Details</summary>
Motivation: 解决在极稀疏或噪声测量下恢复PDE完整解的问题，提供一种独立于离散化的灵活方法。

Method: 训练无条件去噪模型，通过梯度引导机制在推理时满足稀疏观测数据，并扩展Tweedie公式至无限维Hilbert空间。

Result: 在仅3%观测数据下，平均准确率提升32%，采样步骤减少4倍，且具有跨分辨率泛化能力。

Conclusion: FunDPS是首个独立于离散化的扩散框架，为PDE正反问题提供了实用且灵活的解决方案。

Abstract: We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS

</details>


### [349] [Understanding Prompt Tuning and In-Context Learning via Meta-Learning](https://arxiv.org/abs/2505.17010)
*Tim Genewein,Kevin Wenliang Li,Jordi Grau-Moya,Anian Ruoss,Laurent Orseau,Marcus Hutter*

Main category: cs.LG

TL;DR: 本文通过贝叶斯视角探讨了最优提示的理论基础，揭示了提示的局限性，并通过实验验证了软前缀的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究提示优化的理论基础，填补现有方法缺乏概念理解的空白。

Method: 采用贝叶斯理论分析提示优化，并通过LSTM和Transformer实验验证。

Result: 软前缀能有效操纵激活，优于硬标记；提示优化在某些任务中存在根本限制。

Conclusion: 提示优化可通过贝叶斯理论形式化研究，但某些任务需权重调整才能突破限制。

Abstract: Prompting is one of the main ways to adapt a pretrained model to target
tasks. Besides manually constructing prompts, many prompt optimization methods
have been proposed in the literature. Method development is mainly empirically
driven, with less emphasis on a conceptual understanding of prompting. In this
paper we discuss how optimal prompting can be understood through a Bayesian
view, which also implies some fundamental limitations of prompting that can
only be overcome by tuning weights. The paper explains in detail how
meta-trained neural networks behave as Bayesian predictors over the pretraining
distribution, whose hallmark feature is rapid in-context adaptation. Optimal
prompting can be studied formally as conditioning these Bayesian predictors,
yielding criteria for target tasks where optimal prompting is and is not
possible. We support the theory with educational experiments on LSTMs and
Transformers, where we compare different versions of prefix-tuning and
different weight-tuning methods. We also confirm that soft prefixes, which are
sequences of real-valued vectors outside the token alphabet, can lead to very
effective prompts for trained and even untrained networks by manipulating
activations in ways that are not achievable by hard tokens. This adds an
important mechanistic aspect beyond the conceptual Bayesian theory.

</details>


### [350] [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/abs/2505.17013)
*Kevin Lu,Nicky Kriplani,Rohit Gandikota,Minh Pham,David Bau,Chinmay Hegde,Niv Cohen*

Main category: cs.LG

TL;DR: 本文研究了扩散模型中的概念擦除方法，提出了两种擦除机制模型，并引入了一套评估框架来验证擦除效果。


<details>
  <summary>Details</summary>
Motivation: 探讨如何彻底擦除扩散模型中的特定概念，并评估现有方法的有效性。

Method: 提出了两种概念擦除机制模型：降低目标概念生成概率和干扰模型内部引导机制，并设计了一套包含对抗攻击、新探测技术和替代生成分析的评估框架。

Result: 揭示了在最小化副作用和保持对抗提示鲁棒性之间的张力。

Conclusion: 强调了全面评估扩散模型中概念擦除的重要性。

Abstract: Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.

</details>


### [351] [Interactive Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2505.17016)
*Shuhan Tan,Kairan Dou,Yue Zhao,Philipp Krähenbühl*

Main category: cs.LG

TL;DR: RIPT-VLA是一种基于强化学习的交互式后训练方法，通过稀疏二元成功奖励微调预训练的视觉-语言-动作（VLA）模型，显著提升模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA训练方法依赖离线专家数据和监督模仿，难以适应低数据环境下的新任务和环境。

Method: 采用动态滚动采样和留一优势估计的稳定策略优化算法进行交互式后训练。

Result: RIPT-VLA显著提升模型性能（如QueST模型提升21.2%，OpenVLA-OFT模型达到97.5%成功率），且数据效率高（仅需一次演示即可从4%提升至97%成功率）。

Conclusion: RIPT-VLA是一种实用且高效的VLA模型后训练范式，适用于多任务和场景，具有鲁棒性。

Abstract: We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based
interactive post-training paradigm that fine-tunes pretrained
Vision-Language-Action (VLA) models using only sparse binary success rewards.
Existing VLA training pipelines rely heavily on offline expert demonstration
data and supervised imitation, limiting their ability to adapt to new tasks and
environments under low-data regimes. RIPT-VLA addresses this by enabling
interactive post-training with a stable policy optimization algorithm based on
dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA
models, resulting in an improvement on the lightweight QueST model by 21.2%,
and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it
is computationally efficient and data-efficient: with only one demonstration,
RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success
rate within 15 iterations. Furthermore, we demonstrate that the policy learned
by RIPT-VLA generalizes across different tasks and scenarios and is robust to
the initial state context. These results highlight RIPT-VLA as a practical and
effective paradigm for post-training VLA models through minimal supervision.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [352] [Smaller, Smarter, Closer: The Edge of Collaborative Generative AI](https://arxiv.org/abs/2505.16499)
*Roberto Morabito,SiYoung Jang*

Main category: cs.DC

TL;DR: 本文探讨了生成式AI（GenAI）在边缘和云端协作推理系统中的潜力，以解决延迟、成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（尤其是大语言模型）在云端部署中存在延迟、成本和隐私问题，而小语言模型在边缘环境中能力有限。

Method: 提出了边缘和云端协作推理的策略，并结合实际设计原则和实验数据。

Result: 提供了在计算连续体中部署GenAI的可操作指南。

Conclusion: 协作推理系统能够有效解决GenAI部署中的关键挑战。

Abstract: The rapid adoption of generative AI (GenAI), particularly Large Language
Models (LLMs), has exposed critical limitations of cloud-centric deployments,
including latency, cost, and privacy concerns. Meanwhile, Small Language Models
(SLMs) are emerging as viable alternatives for resource-constrained edge
environments, though they often lack the capabilities of their larger
counterparts. This article explores the potential of collaborative inference
systems that leverage both edge and cloud resources to address these
challenges. By presenting distinct cooperation strategies alongside practical
design principles and experimental insights, we offer actionable guidance for
deploying GenAI across the computing continuum.

</details>


### [353] [Edge-First Language Model Inference: Models, Metrics, and Tradeoffs](https://arxiv.org/abs/2505.16508)
*SiYoung Jang,Roberto Morabito*

Main category: cs.DC

TL;DR: 论文探讨了在边缘计算和云端部署语言模型（LMs）的交互关系，重点研究了小型语言模型（SLMs）在资源受限的边缘设备上的性能表现，并提出了适应性部署策略。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在各行业的广泛应用，将其部署到从云端到边缘的连续计算环境中可以降低成本、减少延迟并提升可靠性和隐私性。

Method: 通过详细基准测试SLMs在单一边缘设备和分布式边缘集群上的性能，分析边缘与云端部署的交互关系。

Result: 研究发现边缘推理在某些场景下性能与成本优于云端，但在可扩展性或模型容量受限时仍需依赖云端回退。

Conclusion: 论文提出了平台级比较和设计见解，旨在构建适应异构环境的高效语言模型推理系统。

Abstract: The widespread adoption of Language Models (LMs) across industries is driving
interest in deploying these services across the computing continuum, from the
cloud to the network edge. This shift aims to reduce costs, lower latency, and
improve reliability and privacy. Small Language Models (SLMs), enabled by
advances in model compression, are central to this shift, offering a path to
on-device inference on resource-constrained edge platforms. This work examines
the interplay between edge and cloud deployments, starting from detailed
benchmarking of SLM capabilities on single edge devices, and extending to
distributed edge clusters. We identify scenarios where edge inference offers
comparable performance with lower costs, and others where cloud fallback
becomes essential due to limits in scalability or model capacity. Rather than
proposing a one-size-fits-all solution, we present platform-level comparisons
and design insights for building efficient, adaptive LM inference systems
across heterogeneous environments.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [354] [From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling](https://arxiv.org/abs/2505.16573)
*Yi Hu,Hanchi Ren,Jingjing Deng,Xianghua Xie*

Main category: cs.CE

TL;DR: 提出了一种基于联邦学习的跨股票趋势整合方法（CSTI），通过整合多股票模式提升股价预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统单股票学习方法无法利用多股票间的相关性，限制了预测性能的提升。

Method: 结合联邦学习，训练单股票模型后整合为全局模型，再微调以保留本地相关性。

Result: 实验表明，CSTI优于基准模型，提升了预测能力。

Conclusion: CSTI为股价预测提供了更高效的方法，优于传统单股票学习。

Abstract: Stock price prediction is a critical area of financial forecasting,
traditionally approached by training models using the historical price data of
individual stocks. While these models effectively capture single-stock
patterns, they fail to leverage potential correlations among stock trends,
which could improve predictive performance. Current single-stock learning
methods are thus limited in their ability to provide a broader understanding of
price dynamics across multiple stocks. To address this, we propose a novel
method that merges local patterns into a global understanding through
cross-stock pattern integration. Our strategy is inspired by Federated Learning
(FL), a paradigm designed for decentralized model training. FL enables
collaborative learning across distributed datasets without sharing raw data,
facilitating the aggregation of global insights while preserving data privacy.
In our adaptation, we train models on individual stock data and iteratively
merge them to create a unified global model. This global model is subsequently
fine-tuned on specific stock data to retain local relevance. The proposed
strategy enables parallel training of individual stock models, facilitating
efficient utilization of computational resources and reducing overall training
time. We conducted extensive experiments to evaluate the proposed method,
demonstrating that it outperforms benchmark models and enhances the predictive
capabilities of state-of-the-art approaches. Our results highlight the efficacy
of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,
offering a robust alternative to traditional single-stock learning
methodologies.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [355] [Machine Learning the 6d Supergravity Landscape](https://arxiv.org/abs/2505.16131)
*Nathan Brady,David Tennyson,Thomas Vandermeulen*

Main category: hep-th

TL;DR: 论文结合监督和无监督机器学习算法，研究了6维超重力理论中的弦景观和沼泽地，通过自动编码器实现模型分类，并利用监督学习构建分类器预测模型一致性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习算法在复杂弦理论模型中的应用，特别是对6维超重力理论中的弦景观和沼泽地进行高效分类和特征提取。

Method: 使用自动编码器进行无监督学习，将Gram矩阵数据压缩至2维；同时采用监督学习构建两个分类器，分别预测模型在探针弦插入下的不一致性和异常流入的不一致性。

Result: 自动编码器成功分类模型并识别异常值；监督学习分类器在预测模型一致性方面表现良好（精度分别为0.78和0.91）。

Conclusion: 机器学习算法能高效学习弦景观和沼泽地的复杂特征，自动编码器提供了一种新颖的映射方法。

Abstract: In this paper, we apply both supervised and unsupervised machine learning
algorithms to the study of the string landscape and swampland in 6-dimensions.
Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$
supergravity models, characterised by the Gram matrix of anomaly coefficients.
Our work demonstrates the ability of machine learning algorithms to efficiently
learn highly complex features of the landscape and swampland. Employing an
autoencoder for unsupervised learning, we provide an auto-classification of
these models by compressing the Gram matrix data to 2-dimensions. Through
compression, similar models cluster together, and we identify prominent
features of these clusters. The autoencoder also identifies outlier models
which are difficult to reconstruct. One of these outliers proves to be
incredibly difficult to combine with other models such that the
$\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape
extremely rare. Further, we utilise supervised learning to build two
classifiers predicting (1) model consistency under probe string insertion
(precision: 0.78, predicting consistency for 214,837 models with reasonable
certainty) and (2) inconsistency under anomaly inflow (precision: 0.91,
predicting inconsistency for 1,909,359 models). Notably, projecting these
predictions onto the autoencoder's 2-dimensional latent layer shows consistent
models clustering together, further indicating that the autoencoder has learnt
interesting and complex features of the set of models and potentially offers a
novel approach to mapping the landscape and swampland of 6-dimensional
supergravity theories.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [356] [What Lives? A meta-analysis of diverse opinions on the definition of life](https://arxiv.org/abs/2505.15849)
*Reed Bender,Karina Kofman,Blaise Agüera y Arcas,Michael Levin*

Main category: q-bio.OT

TL;DR: 论文通过大语言模型分析跨学科专家对生命的定义，揭示生命定义的连续谱系，提出将生命定义问题视为统一概念空间中的不同视角。


<details>
  <summary>Details</summary>
Motivation: 解决生命定义缺乏共识的问题，尤其是在合成生物学、人工智能和天体生物学等领域的快速发展下，传统定义面临挑战。

Method: 使用大语言模型分析专家定义，通过成对相关性分析、聚类、语义分析和t-SNE投影揭示概念原型。

Result: 发现生命定义是一个连续的概念谱系，而非二元分类问题，为科学和哲学中的定义问题提供了新方法。

Conclusion: 提出了一种连接还原论和整体论的方法，展示了计算语义分析在跨学科问题中的潜力。

Abstract: The question of "what is life?" has challenged scientists and philosophers
for centuries, producing an array of definitions that reflect both the mystery
of its emergence and the diversity of disciplinary perspectives brought to bear
on the question. Despite significant progress in our understanding of
biological systems, psychology, computation, and information theory, no single
definition for life has yet achieved universal acceptance. This challenge
becomes increasingly urgent as advances in synthetic biology, artificial
intelligence, and astrobiology challenge our traditional conceptions of what it
means to be alive. We undertook a methodological approach that leverages large
language models (LLMs) to analyze a set of definitions of life provided by a
curated set of cross-disciplinary experts. We used a novel pairwise correlation
analysis to map the definitions into distinct feature vectors, followed by
agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection
to reveal underlying conceptual archetypes. This methodology revealed a
continuous landscape of the themes relating to the definition of life,
suggesting that what has historically been approached as a binary taxonomic
problem should be instead conceived as differentiated perspectives within a
unified conceptual latent space. We offer a new methodological bridge between
reductionist and holistic approaches to fundamental questions in science and
philosophy, demonstrating how computational semantic analysis can reveal
conceptual patterns across disciplinary boundaries, and opening similar
pathways for addressing other contested definitional territories across the
sciences.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [357] [CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision](https://arxiv.org/abs/2505.15927)
*Awni Altabaa,Omar Montasser,John Lafferty*

Main category: stat.ML

TL;DR: 论文提出了链式思维（CoT）监督的统计学习理论，通过引入CoT信息度量，显著提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: 标准监督学习在处理多步推理任务时面临挑战，CoT监督通过提供中间推理步骤成为提升模型推理能力的有效方法。

Method: 论文定义了CoT信息度量，将训练目标（CoT风险）与测试目标（端到端风险）关联，推导出更严格的样本复杂度界限。

Result: CoT监督能显著降低样本复杂度，其学习速率优于标准端到端监督，并通过信息论下界验证了CoT信息的重要性。

Conclusion: CoT信息是链式思维监督下学习的统计复杂度基本度量，为高效学习提供了理论基础。

Abstract: Learning complex functions that involve multi-step reasoning poses a
significant challenge for standard supervised learning from input-output
examples. Chain-of-thought (CoT) supervision, which provides intermediate
reasoning steps together with the final output, has emerged as a powerful
empirical technique, underpinning much of the recent progress in the reasoning
capabilities of large language models. This paper develops a statistical theory
of learning under CoT supervision. A key characteristic of the CoT setting, in
contrast to standard supervision, is the mismatch between the training
objective (CoT risk) and the test objective (end-to-end risk). A central part
of our analysis, distinguished from prior work, is explicitly linking those two
types of risk to achieve sharper sample complexity bounds. This is achieved via
the *CoT information measure* $\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, which quantifies the additional
discriminative power gained from observing the reasoning process. The main
theoretical results demonstrate how CoT supervision can yield significantly
faster learning rates compared to standard E2E supervision. Specifically, it is
shown that the sample complexity required to achieve a target E2E error
$\epsilon$ scales as $d/\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, where $d$ is a measure of hypothesis
class complexity, which can be much faster than standard $d/\epsilon$ rates.
Information-theoretic lower bounds in terms of the CoT information are also
obtained. Together, these results suggest that CoT information is a fundamental
measure of statistical complexity for learning under chain-of-thought
supervision.

</details>


### [358] [PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals](https://arxiv.org/abs/2505.16051)
*Dongze Wu,David I. Inouye,Yao Xie*

Main category: stat.ML

TL;DR: PO-Flow是一种新型连续归一化流框架，用于因果推断，联合建模潜在结果和反事实。


<details>
  <summary>Details</summary>
Motivation: 提供一种统一框架，支持个性化潜在结果预测、反事实预测和不确定性感知密度学习，无需显式分布假设。

Method: 基于流匹配训练的连续归一化流（CNF）框架。

Result: 在ACIC、IHDP和IBM等基准测试中表现优于现有方法，并成功应用于高维场景（如反事实图像生成）。

Conclusion: PO-Flow在因果推断任务中具有广泛适用性和优越性能。

Abstract: We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for
causal inference that jointly models potential outcomes and counterfactuals.
Trained via flow matching, PO-Flow provides a unified framework for
individualized potential outcome prediction, counterfactual predictions, and
uncertainty-aware density learning. Among generative models, it is the first to
enable density learning of potential outcomes without requiring explicit
distributional assumptions (e.g., Gaussian mixtures), while also supporting
counterfactual prediction conditioned on factual outcomes in general
observational datasets. On benchmarks such as ACIC, IHDP, and IBM, it
consistently outperforms prior methods across a range of causal inference
tasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including
counterfactual image generation, demonstrating its broad applicability.

</details>


### [359] [Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schrödinger Bridge's End](https://arxiv.org/abs/2505.16082)
*Renato Berlinghieri,Yunyi Shen,Jialong Jiang,Tamara Broderick*

Main category: stat.ML

TL;DR: 论文提出了一种名为SnapMMD的新框架，用于从“快照”数据中预测超出观测时间范围的动态，解决了现有方法在预测质量上的不足。


<details>
  <summary>Details</summary>
Motivation: 科学家需要从快照数据中预测超出观测时间范围的动态，但现有方法（如Schrödinger-bridge）在预测时表现不佳，主要因为其依赖于预设的动态或固定的波动性。

Method: SnapMMD通过最大均值差异（MMD）损失直接拟合状态测量和观测时间的联合分布，从而推断未知且状态依赖的波动性。

Result: 实验表明，SnapMMD在预测和插值任务中表现优异，且能处理不完整的状态测量。

Conclusion: SnapMMD在预测和动态重建方面优于现有方法，并提供了诊断拟合质量的统计量。

Abstract: Scientists often want to make predictions beyond the observed time horizon of
"snapshot" data following latent stochastic dynamics. For example, in time
course single-cell mRNA profiling, scientists have access to cellular
transcriptional state measurements (snapshots) from different biological
replicates at different time points, but they cannot access the trajectory of
any one cell because measurement destroys the cell. Researchers want to
forecast (e.g.) differentiation outcomes from early state measurements of stem
cells. Recent Schr\"odinger-bridge (SB) methods are natural for interpolating
between snapshots. But past SB papers have not addressed forecasting -- likely
since existing methods either (1) reduce to following pre-set reference
dynamics (chosen before seeing data) or (2) require the user to choose a fixed,
state-independent volatility since they minimize a Kullback-Leibler divergence.
Either case can lead to poor forecasting quality. In the present work, we
propose a new framework, SnapMMD, that learns dynamics by directly fitting the
joint distribution of both state measurements and observation time with a
maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to
infer unknown and state-dependent volatilities from the observed data. We show
in a variety of real and synthetic experiments that our method delivers
accurate forecasts. Moreover, our approach allows us to learn in the presence
of incomplete state measurements and yields an $R^2$-style statistic that
diagnoses fit. We also find that our method's performance at interpolation (and
general velocity-field reconstruction) is at least as good as (and often better
than) state-of-the-art in almost all of our experiments.

</details>


### [360] [Dimension-adapted Momentum Outscales SGD](https://arxiv.org/abs/2505.16098)
*Damien Ferbach,Katie Everett,Gauthier Gidel,Elliot Paquette,Courtney Paquette*

Main category: stat.ML

TL;DR: 研究了小批量随机动量算法在幂律随机特征模型中的缩放规律，发现数据-目标复杂度决定四种损失曲线形状。DANA通过调整动量超参数优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索随机动量算法在不同数据-目标复杂度下的缩放行为，以优化计算效率。

Method: 分析SGD-M和DANA在幂律随机特征模型中的表现，通过理论和实验验证。

Result: DANA通过调整动量超参数显著改进缩放指数，优于传统SGD-M。

Conclusion: DANA在广泛数据-目标复杂度范围内表现优异，理论和实验一致支持其优势。

Abstract: We investigate scaling laws for stochastic momentum algorithms with small
batch on the power law random features model, parameterized by data complexity,
target complexity, and model size. When trained with a stochastic momentum
algorithm, our analysis reveals four distinct loss curve shapes determined by
varying data-target complexities. While traditional stochastic gradient descent
with momentum (SGD-M) yields identical scaling law exponents to SGD,
dimension-adapted Nesterov acceleration (DANA) improves these exponents by
scaling momentum hyperparameters based on model size and data complexity. This
outscaling phenomenon, which also improves compute-optimal scaling behavior, is
achieved by DANA across a broad range of data and target complexities, while
traditional methods fall short. Extensive experiments on high-dimensional
synthetic quadratics validate our theoretical predictions and large-scale text
experiments with LSTMs show DANA's improved loss exponents over SGD hold in a
practical setting.

</details>


### [361] [Exponential Convergence of CAVI for Bayesian PCA](https://arxiv.org/abs/2505.16145)
*Arghya Datta,Philippe Gagnon,Florian Maire*

Main category: stat.ML

TL;DR: 论文分析了贝叶斯主成分分析（BPCA）中坐标上升变分推断（CAVI）的收敛速度，填补了文献空白。


<details>
  <summary>Details</summary>
Motivation: BPCA的参数通常通过CAVI算法学习，但其收敛速度尚未被研究。

Method: 通过连接经典幂迭代算法，证明了单主成分模型的精确指数收敛性；利用新工具证明了多主成分模型的指数收敛性。

Result: 单主成分模型收敛结果与传统PCA一致；多主成分模型收敛性更普遍但略有不同。

Conclusion: 研究填补了BPCA中CAVI收敛速度的空白，并为信息论提供了新的KL散度下界。

Abstract: Probabilistic principal component analysis (PCA) and its Bayesian variant
(BPCA) are widely used for dimension reduction in machine learning and
statistics. The main advantage of probabilistic PCA over the traditional
formulation is allowing uncertainty quantification. The parameters of BPCA are
typically learned using mean-field variational inference, and in particular,
the coordinate ascent variational inference (CAVI) algorithm. So far, the
convergence speed of CAVI for BPCA has not been characterized. In our paper, we
fill this gap in the literature. Firstly, we prove a precise exponential
convergence result in the case where the model uses a single principal
component (PC). Interestingly, this result is established through a connection
with the classical $\textit{power iteration algorithm}$ and it indicates that
traditional PCA is retrieved as points estimates of the BPCA parameters.
Secondly, we leverage recent tools to prove exponential convergence of CAVI for
the model with any number of PCs, thus leading to a more general result, but
one that is of a slightly different flavor. To prove the latter result, we
additionally needed to introduce a novel lower bound for the symmetric
Kullback--Leibler divergence between two multivariate normal distributions,
which, we believe, is of independent interest in information theory.

</details>


### [362] [Integral Imprecise Probability Metrics](https://arxiv.org/abs/2505.16156)
*Siu Lun Chau,Michele Caprio,Krikamol Muandet*

Main category: stat.ML

TL;DR: 该论文提出了基于Choquet积分的Integral Imprecise Probability Metric（IIPM）框架，用于比较和量化认知不确定性（EU），并验证了其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 认知不确定性（EU）需要比经典概率更丰富的表示方法，而模糊概率（IP）理论提供了这种模型。因此，需要开发适用于IP模型的度量方法。

Method: 提出了IIPM框架，基于Choquet积分，将经典积分概率度量（IPM）推广到容量（capacities）的背景下。

Result: IIPM不仅支持不同IP模型的比较，还能量化单一IP模型内的认知不确定性，并提出了新的EU度量方法Maximum Mean Imprecision（MMI）。

Conclusion: IIPM为模糊概率机器学习（IPML）提供了理论和实践上的进步，支持在模糊性下比较和量化认知不确定性。

Abstract: Quantifying differences between probability distributions is fundamental to
statistics and machine learning, primarily for comparing statistical
uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete
knowledge -- requires richer representations than those offered by classical
probability. Imprecise probability (IP) theory offers such models, capturing
ambiguity and partial belief. This has driven growing interest in imprecise
probabilistic machine learning (IPML), where inference and decision-making rely
on broader uncertainty models -- highlighting the need for metrics beyond
classical probability. This work introduces the Integral Imprecise Probability
Metric (IIPM) framework, a Choquet integral-based generalisation of classical
Integral Probability Metric (IPM) to the setting of capacities -- a broad class
of IP models encompassing many existing ones, including lower probabilities,
probability intervals, belief functions, and more. Theoretically, we establish
conditions under which IIPM serves as a valid metric and metrises a form of
weak convergence of capacities. Practically, IIPM not only enables comparison
across different IP models but also supports the quantification of epistemic
uncertainty within a single IP model. In particular, by comparing an IP model
with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum
Mean Imprecision -- which satisfy key axiomatic properties proposed in the
Uncertainty Quantification literature. We validate MMI through selective
classification experiments, demonstrating strong empirical performance against
established EU measures, and outperforming them when classical methods struggle
to scale to a large number of classes. Our work advances both theory and
practice in IPML, offering a principled framework for comparing and quantifying
epistemic uncertainty under imprecision.

</details>


### [363] [Generalized Power Priors for Improved Bayesian Inference with Historical Data](https://arxiv.org/abs/2505.16244)
*Masanari Kimura,Howard Bondell*

Main category: stat.ML

TL;DR: 论文扩展了贝叶斯框架中的power prior方法，通过引入Amari的α-发散，优化了历史数据与当前数据的结合方式，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 旨在改进现有power prior方法，通过更灵活的α-发散控制历史数据的影响，提升模型性能。

Method: 提出广义power posterior，基于Amari的α-发散最小化线性组合，替代传统的KL发散。

Result: 广义方法在理论上表现优越，能够适应不同α参数，提供更优的数据适应性。

Conclusion: 广义power posterior不仅扩展了理论框架，还提供了几何解释，为未来研究提供了新方向。

Abstract: The power prior is a class of informative priors designed to incorporate
historical data alongside current data in a Bayesian framework. It includes a
power parameter that controls the influence of historical data, providing
flexibility and adaptability. A key property of the power prior is that the
resulting posterior minimizes a linear combination of KL divergences between
two pseudo-posterior distributions: one ignoring historical data and the other
fully incorporating it. We extend this framework by identifying the posterior
distribution as the minimizer of a linear combination of Amari's
$\alpha$-divergence, a generalization of KL divergence. We show that this
generalization can lead to improved performance by allowing for the data to
adapt to appropriate choices of the $\alpha$ parameter. Theoretical properties
of this generalized power posterior are established, including behavior as a
generalized geodesic on the Riemannian manifold of probability distributions,
offering novel insights into its geometric interpretation.

</details>


### [364] [Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry](https://arxiv.org/abs/2505.16251)
*Masanari Kimura*

Main category: stat.ML

TL;DR: GS-B$^3$SE是一种基于图平滑的贝叶斯方法，用于解决标签偏移问题，通过引入拉普拉斯-高斯先验和标签相似性图，提供更稳健的估计。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒偏移估计器对噪声和类别相似性敏感，需要一种更稳健的概率方法。

Method: 使用拉普拉斯-高斯先验和标签相似性图，结合HMC或快速牛顿-CG方法计算后验分布。

Result: 证明了可识别性、收缩性、方差边界缩小以及鲁棒性，并展示了其作为现有估计器的广义形式。

Conclusion: GS-B$^3$SE提供了一种更稳健且理论支持的标签偏移适应方法。

Abstract: Label shift adaptation aims to recover target class priors when the labelled
source distribution $P$ and the unlabelled target distribution $Q$ share $P(X
\mid Y) = Q(X \mid Y)$ but $P(Y) \neq Q(Y)$. Classical black-box shift
estimators invert an empirical confusion matrix of a frozen classifier,
producing a brittle point estimate that ignores sampling noise and similarity
among classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully
probabilistic alternative that places Laplacian-Gaussian priors on both target
log-priors and confusion-matrix columns, tying them together on a
label-similarity graph. The resulting posterior is tractable with HMC or a fast
block Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction,
variance bounds that shrink with the graph's algebraic connectivity, and
robustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE
through information geometry, showing that it generalizes existing shift
estimators.

</details>


### [365] [Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics](https://arxiv.org/abs/2505.16257)
*Masanari Kimura*

Main category: stat.ML

TL;DR: 本文提出了一种高阶渐近框架，用于测试时适应（TTA）批归一化（BN）统计量在分布偏移下的表现，结合了Edgeworth展开和鞍点近似技术，并引入了一步M估计视角。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决批归一化统计量在分布偏移下的适应性问题，通过高阶统计方法提升其可靠性和性能。

Method: 方法包括使用Edgeworth展开分析BN均值的归一化差异，并通过一步M估计推导高阶局部渐近正态性结果。

Result: 结果表明，该方法能够优化适应统计量的均方误差，量化偏差、方差和偏度的权衡，并提供泛化界。

Conclusion: 结论表明，高阶修正和鲁棒的一步更新可以显著提升BN层在适应变化数据分布时的表现。

Abstract: This study develops a higher-order asymptotic framework for test-time
adaptation (TTA) of Batch Normalization (BN) statistics under distribution
shift by integrating classical Edgeworth expansion and saddlepoint
approximation techniques with a novel one-step M-estimation perspective. By
analyzing the statistical discrepancy between training and test distributions,
we derive an Edgeworth expansion for the normalized difference in BN means and
obtain an optimal weighting parameter that minimizes the mean-squared error of
the adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows
us to derive higher-order local asymptotic normality results, which incorporate
skewness and other higher moments into the estimator's behavior. Moreover, we
quantify the trade-offs among bias, variance, and skewness in the adaptation
process and establish a corresponding generalization bound on the model risk.
The refined saddlepoint approximations further deliver uniformly accurate
density and tail probability estimates for the BN TTA statistic. These
theoretical insights provide a comprehensive understanding of how higher-order
corrections and robust one-step updating can enhance the reliability and
performance of BN layers in adapting to changing data distributions.

</details>


### [366] [Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions](https://arxiv.org/abs/2505.16311)
*Marc Brooks,Gabriel Durham,Kihyuk Hong,Ambuj Tewari*

Main category: stat.ML

TL;DR: 论文提出了一种名为GAMBITTS的生成式AI驱动的Bandit方法，用于解决个性化决策系统中动作与奖励之间的随机性关系，并在模拟实验中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的进步使得个性化内容生成成为可能，但传统Bandit方法未考虑动作通过随机生成的治疗影响奖励的结构。

Method: 提出GAMBITTS方法，显式建模治疗和奖励生成过程，利用生成的治疗信息加速策略学习。

Result: GAMBITTS在模拟实验中表现优于传统算法，并通过分解治疗和奖励的不确定性来源，证明了其更强的理论保证。

Conclusion: GAMBITTS通过显式建模生成式AI驱动的干预结构，为个性化决策系统提供了更高效的解决方案。

Abstract: Recent advances in generative artificial intelligence (GenAI) models have
enabled the generation of personalized content that adapts to up-to-date user
context. While personalized decision systems are often modeled using bandit
formulations, the integration of GenAI introduces new structure into otherwise
classical sequential learning problems. In GenAI-powered interventions, the
agent selects a query, but the environment experiences a stochastic response
drawn from the generative model. Standard bandit methods do not explicitly
account for this structure, where actions influence rewards only through
stochastic, observed treatments. We introduce generator-mediated
bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this
action/treatment split, using mobile health interventions with large language
model-generated text as a motivating case study. GAMBITTS explicitly models
both the treatment and reward generation processes, using information in the
delivered treatment to accelerate policy learning relative to standard methods.
We establish regret bounds for GAMBITTS by decomposing sources of uncertainty
in treatment and reward, identifying conditions where it achieves stronger
guarantees than standard bandit approaches. In simulation studies, GAMBITTS
consistently outperforms conventional algorithms by leveraging observed
treatments to more accurately estimate expected rewards.

</details>


### [367] [Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping](https://arxiv.org/abs/2505.16329)
*Simone Bombari,Inbar Seroussi,Marco Mondelli*

Main category: stat.ML

TL;DR: 论文研究了差分隐私（DP）线性回归中DP-SGD的性能优化，通过频繁剪裁和ODE分析，证明了激进剪裁的最优性，并揭示了学习率衰减和噪声调度的优势。


<details>
  <summary>Details</summary>
Motivation: 理论分析中通常将剪裁常数设得远大于梯度范数，而实证研究则建议优化性能时采用更小的剪裁。本文旨在弥合理论与实践的差距。

Method: 在多元高斯数据下，通过建立DP-SGD轨迹的确定性等效ODE模型，分析其风险上下界，并研究学习率衰减和噪声调度的影响。

Result: 证明了频繁剪裁的最优性，并发现学习率衰减和噪声调度能显著提升性能。

Conclusion: 通过ODE分析，本文为DP-SGD的实际优化提供了理论支持，弥合了理论与实践的差距。

Abstract: Differentially private (DP) linear regression has received significant
attention in the recent theoretical literature, with several works aimed at
obtaining improved error rates. A common approach is to set the clipping
constant much larger than the expected norm of the per-sample gradients. While
simplifying the analysis, this is however in sharp contrast with what empirical
evidence suggests to optimize performance. Our work bridges this gap between
theory and practice: we provide sharper rates for DP stochastic gradient
descent (DP-SGD) by crucially operating in a regime where clipping happens
frequently. Specifically, we consider the setting where the data is
multivariate Gaussian, the number of training samples $n$ is proportional to
the input dimension $d$, and the algorithm guarantees constant-order zero
concentrated DP. Our method relies on establishing a deterministic equivalent
for the trajectory of DP-SGD in terms of a family of ordinary differential
equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two
ODEs, with upper and lower bounds matching for isotropic data. By studying
these ODEs when $n / d$ is large enough, we demonstrate the optimality of
aggressive clipping, and we uncover the benefits of decaying learning rate and
private noise scheduling.

</details>


### [368] [Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free](https://arxiv.org/abs/2505.16644)
*Stephen Y. Zhang,Michael P H Stumpf*

Main category: stat.ML

TL;DR: 论文研究了Schrödinger桥问题，提出了一种基于多元Ornstein-Uhlenbeck过程的非平衡系统解决方案，并开发了高效算法mvOU-OTFM，在合成和真实单细胞数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于布朗运动，局限于势驱动系统，而生物系统通常处于非平衡状态，需要更通用的模型。

Method: 采用多元Ornstein-Uhlenbeck过程作为参考，推导了高斯边缘分布下的解析解，并提出了基于流和分数匹配的算法mvOU-OTFM。

Result: 在合成和真实单细胞数据上，mvOU-OTFM比现有方法更准确且训练速度更快。

Conclusion: 该方法为非平衡系统的Schrödinger桥问题提供了高效解决方案，适用于生物系统等实际应用。

Abstract: We consider the Schr\"odinger bridge problem which, given ensemble
measurements of the initial and final configurations of a stochastic dynamical
system and some prior knowledge on the dynamics, aims to reconstruct the "most
likely" evolution of the system compatible with the data. Most existing
literature assume Brownian reference dynamics and are implicitly limited to
potential-driven dynamics. We depart from this regime and consider reference
processes described by a multivariate Ornstein-Uhlenbeck process with generic
drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is
asymmetric, this corresponds to a non-equilibrium system with non-conservative
forces at play: this is important for applications to biological systems, which
are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we
derive explicit expressions that characterise the solution of both the static
and dynamic Schr\"odinger bridge. For general marginals, we propose mvOU-OTFM,
a simulation-free algorithm based on flow and score matching for learning the
Schr\"odinger bridge. In application to a range of problems based on synthetic
and real single cell data, we demonstrate that mvOU-OTFM achieves higher
accuracy compared to competing methods, whilst being significantly faster to
train.

</details>


### [369] [Sharp concentration of uniform generalization errors in binary linear classification](https://arxiv.org/abs/2505.16713)
*Shogo Nakakita*

Main category: stat.ML

TL;DR: 本文通过等周论证研究了二元线性分类问题中均匀泛化误差的集中性，建立了输出标签和标签加权输入向量的联合分布的Poincaré和对数Sobolev不等式，并推导出集中界。结果表明，这些集中界在标签平衡的情况下是尖锐的。在渐近分析中，证明了均匀泛化误差几乎必然收敛于其期望，适用于高维比例等广泛场景，并建立了维度无关条件下的大数定律。


<details>
  <summary>Details</summary>
Motivation: 研究二元线性分类问题中均匀泛化误差的集中性，以理解其统计行为并提供理论保证。

Method: 通过等周论证建立Poincaré和对数Sobolev不等式，推导集中界，并进行渐近分析。

Result: 集中界在标签平衡时尖锐，均匀泛化误差在广泛场景下几乎必然收敛于期望，建立了维度无关的大数定律。

Conclusion: 本文为二元线性分类问题提供了理论工具，展示了均匀泛化误差的集中性和收敛性，适用于高维等复杂场景。

Abstract: We examine the concentration of uniform generalization errors around their
expectation in binary linear classification problems via an isoperimetric
argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities
for the joint distribution of the output labels and the label-weighted input
vectors, which we apply to derive concentration bounds. The derived
concentration bounds are sharp up to moderate multiplicative constants by those
under well-balanced labels. In asymptotic analysis, we also show that almost
sure convergence of uniform generalization errors to their expectation occurs
in very broad settings, such as proportionally high-dimensional regimes. Using
this convergence, we establish uniform laws of large numbers under
dimension-free conditions.

</details>


### [370] [How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning](https://arxiv.org/abs/2505.16879)
*Hannah Sansford,Nick Whiteley,Patrick Rubin-Delanchy*

Main category: stat.ML

TL;DR: 论文提出了一种广义的Hanson-Wright不等式，用于分析数据点云的几何结构，揭示了三种维度概念的作用，并证明了在特定条件下数据中的流形结构可以被揭示。应用该理论重新分析了神经科学中的网格细胞活动，发现其结构与物理空间等距。


<details>
  <summary>Details</summary>
Motivation: 研究数据点云的几何结构，特别是通过维度概念（如内在维度、相关秩和潜在维度）来理解数据中的隐藏流形结构。

Method: 使用广义Hanson-Wright不等式，结合随机函数模型，分析数据的几何特性。

Result: 证明了当内在维度远大于样本量的对数时，持久图可以揭示潜在同源性和流形结构。应用于网格细胞活动数据，发现其结构与物理空间等距。

Conclusion: 理论分析为数据几何结构提供了新视角，并在神经科学中验证了网格细胞活动的几何保真性。

Abstract: We present a generalised Hanson-Wright inequality and use it to establish new
statistical insights into the geometry of data point-clouds. In the setting of
a general random function model of data, we clarify the roles played by three
notions of dimensionality: ambient intrinsic dimension $p_{\mathrm{int}}$,
which measures total variability across orthogonal feature directions;
correlation rank, which measures functional complexity across samples; and
latent intrinsic dimension, which is the dimension of manifold structure hidden
in data. Our analysis shows that in order for persistence diagrams to reveal
latent homology and for manifold structure to emerge it is sufficient that
$p_{\mathrm{int}}\gg \log n$, where $n$ is the sample size. Informed by these
theoretical perspectives, we revisit the ground-breaking neuroscience discovery
of toroidal structure in grid-cell activity made by Gardner et al. (Nature,
2022): our findings reveal, for the first time, evidence that this structure is
in fact isometric to physical space, meaning that grid cell activity conveys a
geometrically faithful representation of the real world.

</details>


### [371] [Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference](https://arxiv.org/abs/2505.16893)
*Shuichi Nishino,Tomohiro Shiraishi,Teruyuki Katsuoka,Ichiro Takeuchi*

Main category: stat.ML

TL;DR: 本文提出了一种统计测试框架，用于严格评估GNN显著性图的可靠性，解决了数据重复使用导致的Type I错误率问题。


<details>
  <summary>Details</summary>
Motivation: GNN显著性图的可靠性受到质疑，尤其是在噪声环境下的表现。

Method: 利用选择性推断框架，提出了一种统计测试方法，生成有效的p值并控制Type I错误率。

Result: 实验表明，该方法能有效评估GNN显著性图的可靠性，确保识别出的显著子图具有实际意义。

Conclusion: 该方法为GNN解释提供了统计上可靠的评估工具，解决了显著性图的可靠性问题。

Abstract: Graph Neural Networks (GNNs) have gained prominence for their ability to
process graph-structured data across various domains. However, interpreting GNN
decisions remains a significant challenge, leading to the adoption of saliency
maps for identifying influential nodes and edges. Despite their utility, the
reliability of GNN saliency maps has been questioned, particularly in terms of
their robustness to noise. In this study, we propose a statistical testing
framework to rigorously evaluate the significance of saliency maps. Our main
contribution lies in addressing the inflation of the Type I error rate caused
by double-dipping of data, leveraging the framework of Selective Inference. Our
method provides statistically valid $p$-values while controlling the Type I
error rate, ensuring that identified salient subgraphs contain meaningful
information rather than random artifacts. To demonstrate the effectiveness of
our method, we conduct experiments on both synthetic and real-world datasets,
showing its effectiveness in assessing the reliability of GNN interpretations.

</details>


### [372] [TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation](https://arxiv.org/abs/2505.16923)
*Yuhui Zhang,Dongshen Wu,Yuichiro Wada,Takafumi Kanamori*

Main category: stat.ML

TL;DR: TULiP是一种基于理论的后处理不确定性估计方法，用于OOD检测，通过扰动模型参数计算不确定性，并在大规模图像分类基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 可靠的OOD检测需要有效的不确定性估计方法，以支持深度学习模型在开放世界中的安全部署。

Method: 提出TULiP方法，基于线性化训练动态，通过扰动网络参数计算不确定性得分。

Result: 在合成数据集和大规模OOD检测基准测试中表现优异，尤其是对近分布样本。

Conclusion: TULiP在OOD检测中表现出色，为模型部署提供了可靠的不确定性估计工具。

Abstract: A reliable uncertainty estimation method is the foundation of many modern
out-of-distribution (OOD) detectors, which are critical for safe deployments of
deep learning models in the open world. In this work, we propose TULiP, a
theoretically-driven post-hoc uncertainty estimator for OOD detection. Our
approach considers a hypothetical perturbation applied to the network before
convergence. Based on linearized training dynamics, we bound the effect of such
perturbation, resulting in an uncertainty score computable by perturbing model
parameters. Ultimately, our approach computes uncertainty from a set of sampled
predictions. We visualize our bound on synthetic regression and classification
datasets. Furthermore, we demonstrate the effectiveness of TULiP using
large-scale OOD detection benchmarks for image classification. Our method
exhibits state-of-the-art performance, particularly for near-distribution
samples.

</details>


### [373] [Critical Points of Random Neural Networks](https://arxiv.org/abs/2505.17000)
*Simmaco Di Lillo*

Main category: stat.ML

TL;DR: 研究了随机神经网络中临界点数量随深度增加的渐近行为，揭示了三种不同增长模式，并提供了数值验证。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络深度增加时临界点数量的变化规律，为理解网络优化提供理论支持。

Method: 在无限宽度限制下，推导临界点数量的渐近公式，并通过数值实验验证。

Result: 发现临界点数量随深度可能收敛、多项式增长或指数增长，且ReLU激活函数可能导致临界点数量发散。

Conclusion: 研究揭示了神经网络临界点数量与深度及激活函数的关系，为网络设计提供了理论依据。

Abstract: This work investigates the expected number of critical points of random
neural networks with different activation functions as the depth increases in
the infinite-width limit. Under suitable regularity conditions, we derive
precise asymptotic formulas for the expected number of critical points of fixed
index and those exceeding a given threshold. Our analysis reveals three
distinct regimes depending on the value of the first derivative of the
covariance evaluated at 1: the expected number of critical points may converge,
grow polynomially, or grow exponentially with depth. The theoretical
predictions are supported by numerical experiments. Moreover, we provide
numerical evidence suggesting that, when the regularity condition is not
satisfied (e.g. for neural networks with ReLU as activation function), the
number of critical points increases as the map resolution increases, indicating
a potential divergence in the number of critical points.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [374] [Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders](https://arxiv.org/abs/2505.16320)
*P. Huijse,J. De Ridder,L. Eyer,L. Rimoldini,B. Holl,N. Chornay,J. Roquette,K. Nienartowicz,G. Jevardat de Fombelle,D. J. Fritzewski,A. Kemp,V. Vanlaer,M. Vanrespaille,H. Wang,M. I. Carnerero,C. M. Raiteri,G. Marton,M. Madarász,G. Clementini,P. Gavras,C. Aerts*

Main category: astro-ph.IM

TL;DR: 论文提出了一种基于变分自编码器（VAE）的无监督分类方法，用于分析Gaia DR3中恒星和类星体的变异性，通过结合多种Gaia数据产品，实现了高效的潜在空间表示。


<details>
  <summary>Details</summary>
Motivation: 利用Gaia DR3发布的大规模变源数据，结合多种数据产品（如低分辨率光谱、光变曲线等），探索无监督分类方法以提升变异性研究的效率。

Method: 使用三个变分自编码器（VAE）分别处理Gaia的低分辨率光谱、G波段光变曲线和G波段星等差分布，将数据压缩为15维潜在空间表示。

Result: 潜在空间表示能有效区分Gaia DR3中的主要变异性类别，二维投影显示与天体物理性质强相关的密集区域，适用于分类、聚类和异常检测任务。

Conclusion: 该方法展示了结合多种Gaia数据产品的优势，潜在空间表示在变异性分析中具有重要价值，为未来Gaia DR4的无监督分类提供了有力工具。

Abstract: Gaia Data Release 3 (DR3) published for the first time epoch photometry,
BP/RP (XP) low-resolution mean spectra, and supervised classification results
for millions of variable sources. This extensive dataset offers a unique
opportunity to study their variability by combining multiple Gaia data
products. In preparation for DR4, we propose and evaluate a machine learning
methodology capable of ingesting multiple Gaia data products to achieve an
unsupervised classification of stellar and quasar variability. A dataset of 4
million Gaia DR3 sources is used to train three variational autoencoders (VAE),
which are artificial neural networks (ANNs) designed for data compression and
generation. One VAE is trained on Gaia XP low-resolution spectra, another on a
novel approach based on the distribution of magnitude differences in the Gaia G
band, and the third on folded Gaia G band light curves. Each Gaia source is
compressed into 15 numbers, representing the coordinates in a 15-dimensional
latent space generated by combining the outputs of these three models. The
learned latent representation produced by the ANN effectively distinguishes
between the main variability classes present in Gaia DR3, as demonstrated
through both supervised and unsupervised classification analysis of the latent
space. The results highlight a strong synergy between light curves and
low-resolution spectral data, emphasising the benefits of combining the
different Gaia data products. A two-dimensional projection of the latent
variables reveals numerous overdensities, most of which strongly correlate with
astrophysical properties, showing the potential of this latent space for
astrophysical discovery. We show that the properties of our novel latent
representation make it highly valuable for variability analysis tasks,
including classification, clustering and outlier detection.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [375] [Integration of TinyML and LargeML: A Survey of 6G and Beyond](https://arxiv.org/abs/2505.15854)
*Thai-Hoc Vu,Ngo Hoang Tu,Thien Huynh-The,Kyungchun Lee,Sunghwan Kim,Miroslav Voznak,Quoc-Viet Pham*

Main category: cs.NI

TL;DR: 论文探讨了从5G到6G网络中机器学习的应用需求，重点分析了TinyML与LargeML的整合潜力及其挑战。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备激增和6G网络发展，需要高效、资源优化的机器学习解决方案以支持智能服务。

Method: 通过综述最新研究，分析TinyML与LargeML整合的方法与策略。

Result: 整合TinyML与LargeML有望实现无缝连接和高效资源管理，但仍面临性能优化、部署策略等挑战。

Conclusion: 论文总结了整合TinyML与LargeML的关键挑战，并提出了未来研究方向。

Abstract: The transition from 5G networks to 6G highlights a significant demand for
machine learning (ML). Deep learning models, in particular, have seen wide
application in mobile networking and communications to support advanced
services in emerging wireless environments, such as smart healthcare, smart
grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse.
The rapid expansion of Internet-of-Things (IoT) devices, many with limited
computational capabilities, has accelerated the development of tiny machine
learning (TinyML) and resource-efficient ML approaches for cost-effective
services. However, the deployment of large-scale machine learning (LargeML)
solutions require major computing resources and complex management strategies
to support extensive IoT services and ML-generated content applications.
Consequently, the integration of TinyML and LargeML is projected as a promising
approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential,
several challenges persist, including performance optimization, practical
deployment strategies, effective resource management, and security
considerations. In this survey, we review and analyze the latest research aimed
at enabling the integration of TinyML and LargeML models for the realization of
smart services and applications in future 6G networks and beyond. The paper
concludes by outlining critical challenges and identifying future research
directions for the holistic integration of TinyML and LargeML in
next-generation wireless networks.

</details>


### [376] [A Novel Compound AI Model for 6G Networks in 3D Continuum](https://arxiv.org/abs/2505.15821)
*Milos Gravara,Andrija Stanisic,Stefan Nastic*

Main category: cs.NI

TL;DR: 本文提出了一种新型的三元框架，将复杂任务分解为可互操作的专业模块，以解决6G网络中跨域资源协调和动态拓扑适应等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在网络管理中缺乏跨域交互能力、适应性和计算效率，无法满足6G网络在三维连续体中的需求。

Method: 引入Compound AI系统的正式模型，采用模块化架构，将任务分解为可互操作的专业模块。

Result: 该框架为6G网络提供了协调分布式智能的能力，但也引入了模型与系统性能之间的权衡。

Conclusion: Compound AI系统在6G网络中面临跨域资源协调、动态拓扑适应和异构环境一致性等关键挑战。

Abstract: The 3D continuum presents a complex environment that spans the terrestrial,
aerial and space domains, with 6Gnetworks serving as a key enabling technology.
Current AI approaches for network management rely on monolithic models that
fail to capture cross-domain interactions, lack adaptability,and demand
prohibitive computational resources. This paper presents a formal model of
Compound AI systems, introducing a novel tripartite framework that decomposes
complex tasks into specialized, interoperable modules. The proposed modular
architecture provides essential capabilities to address the unique challenges
of 6G networks in the 3D continuum, where heterogeneous components require
coordinated, yet distributed, intelligence. This approach introduces a
fundamental trade-off between model and system performance, which must be
carefully addressed. Furthermore, we identify key challenges faced by Compound
AI systems within 6G networks operating in the 3D continuum, including
cross-domain resource orchestration, adaptation to dynamic topologies, and the
maintenance of consistent AI service quality across heterogeneous environments.

</details>


### [377] [Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction](https://arxiv.org/abs/2505.15828)
*Jiayuan Chen,Yuxiang Li,Changyan Yi,Shimin Gong*

Main category: cs.NI

TL;DR: 论文研究了RIS辅助DT交互中QoE感知的资源分配问题，提出了一种结合决策变换器和生成AI的新方法PG-ZFO，以应对DT模型不确定演化的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助DT交互中因DT模型不确定演化导致的资源分配问题，提升用户体验。

Method: 提出PG-ZFO方法，结合决策变换器和生成AI，优化相位矩阵、波束成形矩阵、渲染分辨率和计算资源分配。

Result: 仿真实验表明PG-ZFO在性能和效率上优于现有方法。

Conclusion: PG-ZFO能有效应对DT模型演化带来的不确定性，提升QoE。

Abstract: In this paper, we investigate a quality of experience (QoE)-aware resource
allocation problem for reconfigurable intelligent surface (RIS)-assisted
digital twin (DT) interaction with uncertain evolution. In the considered
system, mobile users are expected to interact with a DT model maintained on a
DT server that is deployed on a base station, via effective uplink and downlink
channels assisted by an RIS. Our goal is to maximize the sum of all mobile
users' joint subjective and objective QoE in DT interactions across various DT
scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming
matrix, rendering resolution configuration and computing resource allocation.
While solving this problem is challenging mainly due to the uncertain evolution
of the DT model, which leads to multiple scene-specific problems, and require
us to constantly re-solve each of them whenever DT model evolves.
  To this end, leveraging the dynamic optimization capabilities of decision
transformers and the generalization strengths of generative artificial
intelligence (GAI), we propose a novel GAI-aided approach, called the
prompt-guided decision transformer integrated with zero-forcing optimization
(PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO,
demonstrating its effectiveness and superiority over counterparts.

</details>


### [378] [Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization](https://arxiv.org/abs/2505.15835)
*Nayan Sanjay Bhatia,Katia Obraczka*

Main category: cs.NI

TL;DR: WiFiGPT是一种基于GPT的系统，能够处理WiFi定位中的信号变化，实现高精度定位，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: WiFi室内定位因信号衰减、多径效应和设备差异等问题，现有技术在实际应用中受限，缺乏统一模型。

Method: 采用生成预训练变换器（GPT）模型，捕捉无线遥测中的空间模式，无需手工信号处理或校准。

Result: WiFiGPT在RSSI和FTM上实现亚米级精度，CSI上达到厘米级精度，优于传统方法。

Conclusion: 基于LLM的定位技术潜力巨大，可超越专用技术，适用于多样化应用场景。

Abstract: Wireless Fidelity (WiFi) based indoor positioning is a widely researched area
for determining the position of devices within a wireless network. Accurate
indoor location has numerous applications, such as asset tracking and indoor
navigation. Despite advances in WiFi localization techniques -- in particular
approaches that leverage WiFi telemetry -- their adoption in practice remains
limited due to several factors including environmental changes that cause
signal fading, multipath effects, interference, which, in turn, impact
positioning accuracy. In addition, telemetry data differs depending on the WiFi
device vendor, offering distinct features and formats; use case requirements
can also vary widely. Currently, there is no unified model to handle all these
variations effectively. In this paper, we present WiFiGPT, a Generative
Pretrained Transformer (GPT) based system that is able to handle these
variations while achieving high localization accuracy. Our experiments with
WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can
effectively capture subtle spatial patterns in noisy wireless telemetry, making
them reliable regressors. Compared to existing state-of-the-art methods, our
method matches and often surpasses conventional approaches for multiple types
of telemetry. Achieving sub-meter accuracy for RSSI and FTM and
centimeter-level precision for CSI demonstrates the potential of LLM-based
localisation to outperform specialized techniques, all without handcrafted
signal processing or calibration.

</details>


### [379] [Graph Neural Networks Based Anomalous RSSI Detection](https://arxiv.org/abs/2505.15847)
*Blaž Bertalanič,Matej Vnučec,Carolina Fortuna*

Main category: cs.NI

TL;DR: 提出了一种基于图神经网络的无线链路异常检测方法，通过将时间序列数据转换为图并训练新的图注意力网络架构，实现了高效且准确的异常检测。


<details>
  <summary>Details</summary>
Motivation: 现代基础设施中的大规模物联网网络需要主动监测以预防链路故障或异常行为，避免业务中断。

Method: 将时间序列数据转换为图，设计并训练基于图注意力网络的图神经网络架构。

Result: 模型在检测时间序列数据中的异常时表现优异，计算效率高，参数数量比现有方法少约171倍。

Conclusion: 该方法在无线链路异常检测中具有高效性和竞争力，为物联网网络监控提供了新思路。

Abstract: In today's world, modern infrastructures are being equipped with information
and communication technologies to create large IoT networks.
  It is essential to monitor these networks to ensure smooth operations by
detecting and correcting link failures or abnormal network behaviour
proactively, which can otherwise cause interruptions in business operations.
  This paper presents a novel method for detecting anomalies in wireless links
using graph neural networks. The proposed approach involves converting time
series data into graphs and training a new graph neural network architecture
based on graph attention networks that successfully detects anomalies at the
level of individual measurements of the time series data. The model provides
competitive results compared to the state of the art while being
computationally more efficient with ~171 times fewer trainable parameters.

</details>


### [380] [LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols](https://arxiv.org/abs/2505.16821)
*Ziming liu,Bryan Liu,Alvaro Valcarce,Xiaoli Chu*

Main category: cs.NI

TL;DR: 论文展示了如何将大型AI模型（LAMs）集成到6G网络中，通过微调解码器转换模型生成符合标准的RRC消息，显著提升了协议保真度。


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中AI模型在协议理解和实际部署中的挑战，推动AI原生无线标准的实现。

Method: 使用LoRA微调解码器转换模型（LLaMA类），将RRC消息线性化并保留ASN.1语法结构，实现组合泛化。

Result: 在3万组测试数据上，模型生成的RRC消息与真实消息的中位余弦相似度达0.97，比基线提升61%。

Conclusion: 研究表明，结合RAN特定推理的LAMs可直接控制平面流程，为AI原生无线标准奠定基础。

Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to
redefine protocol design and control-plane intelligence by enabling autonomous,
cognitive network operations. While industry concepts, such as ETSI's
Experiential Networked Intelligence (ENI), envision LAM-driven agents for
adaptive network slicing and intent-based management, practical implementations
still face challenges in protocol literacy and real-world deployment. This
paper presents an end-to-end demonstration of a LAM that generates
standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as
part of control-plane procedures inside a gNB. We treat RRC messaging as a
domain-specific language and fine-tune a decoder-only transformer model (LLaMA
class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages
linearized to retain their ASN.1 syntactic structure before standard byte-pair
encoding tokenization. This enables combinatorial generalization over RRC
protocol states while minimizing training overhead. On 30k field-test
request-response pairs, our 8 B model achieves a median cosine similarity of
0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a
zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural
and semantic RRC fidelity. Overall, our results show that LAMs, when augmented
with Radio Access Network (RAN)-specific reasoning, can directly orchestrate
control-plane procedures, representing a stepping stone toward the AI-native
air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for
future AI-native wireless standards.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [381] [Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey](https://arxiv.org/abs/2505.16379)
*Zhixun Li,Bin Cao,Rui Jiao,Liang Wang,Ding Wang,Yang Liu,Dingshuo Chen,Jia Li,Qiang Liu,Yu Rong,Liang Wang,Tong-yi Zhang,Jeffrey Xu Yu*

Main category: cond-mat.mtrl-sci

TL;DR: 本文综述了AI驱动的材料生成领域的最新进展，包括材料分类、AI方法、评估指标及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 解决全球性挑战需要新材料，AI与高质量材料数据的结合为加速材料发现提供了新机会。

Method: 组织材料类型与晶体表示，总结AI生成方法，提供评估指标与开源资源。

Result: 系统分类了AI驱动的材料生成方法，并总结了相关数据集与代码。

Conclusion: 未来需解决材料生成领域的挑战，进一步推动AI在材料科学中的应用。

Abstract: Materials are the foundation of modern society, underpinning advancements in
energy, electronics, healthcare, transportation, and infrastructure. The
ability to discover and design new materials with tailored properties is
critical to solving some of the most pressing global challenges. In recent
years, the growing availability of high-quality materials data combined with
rapid advances in Artificial Intelligence (AI) has opened new opportunities for
accelerating materials discovery. Data-driven generative models provide a
powerful tool for materials design by directly create novel materials that
satisfy predefined property requirements. Despite the proliferation of related
work, there remains a notable lack of up-to-date and systematic surveys in this
area. To fill this gap, this paper provides a comprehensive overview of recent
progress in AI-driven materials generation. We first organize various types of
materials and illustrate multiple representations of crystalline materials. We
then provide a detailed summary and taxonomy of current AI-driven materials
generation approaches. Furthermore, we discuss the common evaluation metrics
and summarize open-source codes and benchmark datasets. Finally, we conclude
with potential future directions and challenges in this fast-growing field. The
related sources can be found at
https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [382] [An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology](https://arxiv.org/abs/2505.15868)
*Changchun Yang,Weiqian Dai,Yilan Zhang,Siyuan Chen,Jingdong Hu,Junkai Su,Yuxuan Chen,Ao Xu,Na Li,Xin Gao,Yongguo Yu*

Main category: q-bio.QM

TL;DR: CHROMA是一个基于自监督学习的细胞基因组学基础模型，通过预训练处理染色体异常，减少标注需求并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 染色体分析对遗传病诊断和癌症治疗至关重要，但现有AI模型因染色体异常的复杂性和数据稀缺性而受限。

Method: CHROMA通过自监督学习在84,000个样本（约400万染色体图像）上预训练，学习染色体异常的通用表示。

Result: CHROMA在各类异常中表现优于其他方法，即使标注数据较少或数据集不平衡。

Conclusion: CHROMA为临床分析提供了可扩展且通用的解决方案，推动精准肿瘤学发展。

Abstract: Chromosome analysis is vital for diagnosing genetic disorders and guiding
cancer therapy decisions through the identification of somatic clonal
aberrations. However, developing an AI model are hindered by the overwhelming
complexity and diversity of chromosomal abnormalities, requiring extensive
annotation efforts, while automated methods remain task-specific and lack
generalizability due to the scarcity of comprehensive datasets spanning diverse
resource conditions. Here, we introduce CHROMA, a foundation model for
cytogenomics, designed to overcome these challenges by learning generalizable
representations of chromosomal abnormalities. Pre-trained on over 84,000
specimens (~4 million chromosomal images) via self-supervised learning, CHROMA
outperforms other methods across all types of abnormalities, even when trained
on fewer labelled data and more imbalanced datasets. By facilitating
comprehensive mapping of instability and clonal leisons across various
aberration types, CHROMA offers a scalable and generalizable solution for
reliable and automated clinical analysis, reducing the annotation workload for
experts and advancing precision oncology through the early detection of rare
genomic abnormalities, enabling broad clinical AI applications and making
advanced genomic analysis more accessible.

</details>


### [383] [Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy](https://arxiv.org/abs/2505.15844)
*Yousuf Islam,Md. Jalal Uddin Chowdhury,Sumon Chandra Das*

Main category: q-bio.QM

TL;DR: 论文提出了一种数据驱动的机器学习框架，用于预测中风风险，通过严格的预处理和混合模型，实现了97.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 中风是全球死亡和残疾的主要原因，但现有预测模型的准确率普遍低于95%，限制了实际应用。

Method: 使用EDA分析数据，预处理包括缺失值处理、异常值去除和SMOTE平衡类别。通过点双列相关和随机森林Gini重要性进行特征选择，优化了十种算法，最终构建了一个混合模型。

Result: 混合模型准确率达97.2%，F1分数为97.15%，显著优于单一模型LightGBM（91.4%）。

Conclusion: 严格的预处理和多样化的混合模型可将低成本表格数据转化为接近临床水平的中风风险评估工具。

Abstract: Brain stroke remains one of the principal causes of death and disability
worldwide, yet most tabular-data prediction models still hover below the 95%
accuracy threshold, limiting real-world utility. Addressing this gap, the
present work develops and validates a completely data-driven and interpretable
machine-learning framework designed to predict strokes using ten routinely
gathered demographic, lifestyle, and clinical variables sourced from a public
cohort of 4,981 records. We employ a detailed exploratory data analysis (EDA)
to understand the dataset's structure and distribution, followed by rigorous
data preprocessing, including handling missing values, outlier removal, and
class imbalance correction using Synthetic Minority Over-sampling Technique
(SMOTE). To streamline feature selection, point-biserial correlation and
random-forest Gini importance were utilized, and ten varied
algorithms-encompassing tree ensembles, boosting, kernel methods, and a
multilayer neural network-were optimized using stratified five-fold
cross-validation. Their predictions based on probabilities helped us build the
proposed model, which included Random Forest, XGBoost, LightGBM, and a
support-vector classifier, with logistic regression acting as a meta-learner.
The proposed model achieved an accuracy rate of 97.2% and an F1-score of
97.15%, indicating a significant enhancement compared to the leading individual
model, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate
that rigorous preprocessing, coupled with a diverse hybrid model, can convert
low-cost tabular data into a nearly clinical-grade stroke-risk assessment tool.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [384] [Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems](https://arxiv.org/abs/2505.16208)
*Anton Erofeev,Balasubramanya T. Nadiga,Ilya Timofeyev*

Main category: nlin.CD

TL;DR: 使用回声状态网络预测混沌竞争Lotka-Volterra模型的时间序列和统计特性，成功学习其混沌吸引子并重现变量分布，包括尾部与罕见事件。


<details>
  <summary>Details</summary>
Motivation: 探索回声状态网络在混沌系统中预测复杂动力学行为的能力，尤其是对极端事件的捕捉。

Method: 应用回声状态网络学习混沌竞争Lotka-Volterra模型，并使用广义极值分布量化尾部行为。

Result: 网络成功学习混沌吸引子并准确重现变量分布，包括尾部与罕见事件。

Conclusion: 回声状态网络能有效预测混沌系统的统计特性，为复杂动力学建模提供新工具。

Abstract: We apply the Echo-State Networks to predict the time series and statistical
properties of the competitive Lotka-Volterra model in the chaotic regime. In
particular, we demonstrate that Echo-State Networks successfully learn the
chaotic attractor of the competitive Lotka-Volterra model and reproduce
histograms of dependent variables, including tails and rare events. We use the
Generalized Extreme Value distribution to quantify the tail behavior.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [385] [VERDI: VLM-Embedded Reasoning for Autonomous Driving](https://arxiv.org/abs/2505.15925)
*Bowen Feng,Zhiting Mei,Baiang Li,Julian Ost,Roger Girgis,Anirudha Majumdar,Felix Heide*

Main category: cs.RO

TL;DR: VERDI框架通过将视觉语言模型的推理能力蒸馏到自动驾驶系统中，解决了现有方法的高计算成本和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在部分可观测性和复杂现实环境中的决策能力不足，而人类驾驶员能通过常识推理做出近优决策。

Method: VERDI在训练时将VLM的推理过程和常识知识蒸馏到模块化自动驾驶模型中，通过中间模块输出与VLM生成的文本特征对齐来实现。

Result: 在NuScenes数据集上，VERDI比现有端到端方法在ℓ2距离上提升了10%，同时保持高推理速度。

Conclusion: VERDI成功将结构化推理嵌入模块化自动驾驶系统，避免了大型VLM的高推理成本。

Abstract: While autonomous driving (AD) stacks struggle with decision making under
partial observability and real-world complexity, human drivers are capable of
commonsense reasoning to make near-optimal decisions with limited information.
Recent work has attempted to leverage finetuned Vision-Language Models (VLMs)
for trajectory planning at inference time to emulate human behavior. Despite
their success in benchmark evaluations, these methods are often impractical to
deploy (a 70B parameter VLM inference at merely 8 tokens per second requires
more than 160G of memory), and their monolithic network structure prohibits
safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for
autonomous Driving (VERDI), a training-time framework that distills the
reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI
augments modular differentiable end-to-end (e2e) AD models by aligning
intermediate module outputs at the perception, prediction, and planning stages
with text features explaining the driving reasoning process produced by VLMs.
By encouraging alignment in latent space, \textsc{VERDI} enables the modular AD
stack to internalize structured reasoning, without incurring the inference-time
costs of large VLMs. We demonstrate the effectiveness of our method on the
NuScenes dataset and find that VERDI outperforms existing e2e methods that do
not embed reasoning by 10% in $\ell_{2}$ distance, while maintaining high
inference speed.

</details>


### [386] [EasyInsert: A Data-Efficient and Generalizable Insertion Policy](https://arxiv.org/abs/2505.16187)
*Guanghe Li,Junming Zhao,Shengjie Wang,Yang Gao*

Main category: cs.RO

TL;DR: EasyInsert是一个通过预测插头和插座之间的相对位姿（delta pose）来实现高精度插入任务的框架，具有强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂环境中泛化能力差，依赖CAD模型或仿真环境，无法处理插头和插座距离远、场景杂乱或新物体的情况。

Method: EasyInsert利用人类直觉，通过自动化数据收集训练通用模型预测相对位姿，并采用粗到细的执行策略。

Result: 仅用5小时训练数据，EasyInsert在15种新物体中13种实现90%以上零样本插入成功率；仅需一次人类演示和4分钟数据微调，所有物体成功率均超过90%。

Conclusion: EasyInsert在复杂环境中表现出色，样本效率高且人工干预少，为插入任务提供了高效解决方案。

Abstract: Insertion task is highly challenging that requires robots to operate with
exceptional precision in cluttered environments. Existing methods often have
poor generalization capabilities. They typically function in restricted and
structured environments, and frequently fail when the plug and socket are far
apart, when the scene is densely cluttered, or when handling novel objects.
They also rely on strong assumptions such as access to CAD models or a digital
twin in simulation. To address this, we propose EasyInsert, a framework which
leverages the human intuition that relative pose (delta pose) between plug and
socket is sufficient for successful insertion, and employs efficient and
automated real-world data collection with minimal human labor to train a
generalizable model for relative pose prediction. During execution, EasyInsert
follows a coarse-to-fine execution procedure based on predicted delta pose, and
successfully performs various insertion tasks. EasyInsert demonstrates strong
zero-shot generalization capability for unseen objects in cluttered
environments, handling cases with significant initial pose deviations while
maintaining high sample efficiency and requiring little human effort. In
real-world experiments, with just 5 hours of training data, EasyInsert achieves
over 90% success in zero-shot insertion for 13 out of 15 unseen novel objects,
including challenging objects like Type-C cables, HDMI cables, and Ethernet
cables. Furthermore, with only one human demonstration and 4 minutes of
automatically collected data for fine-tuning, it reaches over 90% success rate
for all 15 objects.

</details>


### [387] [SEM: Enhancing Spatial Understanding for Robust Robot Manipulation](https://arxiv.org/abs/2505.16196)
*Xuewu Lin,Tianwei Lin,Lichao Huang,Hongyu Xie,Yiwei Jin,Keyu Li,Zhizhong Su*

Main category: cs.RO

TL;DR: 论文提出了一种名为SEM的新型扩散策略框架，通过增强空间理解和机器人状态编码，显著提升了机器人操作的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器人操作中存在空间理解不足的问题，3D点云模型缺乏语义抽象，而2D图像编码器难以进行空间推理。

Method: SEM框架通过空间增强器和机器人状态编码器，分别从3D几何上下文和图建模关节依赖关系两方面增强空间理解。

Result: SEM在多样任务中表现出色，超越了现有基线方法。

Conclusion: SEM通过结合空间增强和机器人状态编码，显著提升了机器人操作的空间理解和任务性能。

Abstract: A key challenge in robot manipulation lies in developing policy models with
strong spatial understanding, the ability to reason about 3D geometry, object
relations, and robot embodiment. Existing methods often fall short: 3D point
cloud models lack semantic abstraction, while 2D image encoders struggle with
spatial reasoning. To address this, we propose SEM (Spatial Enhanced
Manipulation model), a novel diffusion-based policy framework that explicitly
enhances spatial understanding from two complementary perspectives. A spatial
enhancer augments visual representations with 3D geometric context, while a
robot state encoder captures embodiment-aware structure through graphbased
modeling of joint dependencies. By integrating these modules, SEM significantly
improves spatial understanding, leading to robust and generalizable
manipulation across diverse tasks that outperform existing baselines.

</details>


### [388] [Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control](https://arxiv.org/abs/2505.16249)
*Zhen Zhang,Xiangyu Chu,Yunxi Tang,Lulu Zhao,Jing Huang,Zhongliang Jiang,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出了一种基于3D占用表示和学习动态模型的弹塑性物体操纵框架，通过预测控制算法实现目标形状的塑造。


<details>
  <summary>Details</summary>
Motivation: 弹塑性物体操纵因自遮挡、表示困难和复杂动力学而具有挑战性。

Method: 使用3D占用表示物体，训练动态模型，设计预测控制算法，并开发数据收集平台和3D占用数据集。

Result: 框架成功在仿真和现实中塑造弹塑性物体为目标形状。

Conclusion: 提出的方法有效解决了弹塑性物体操纵的挑战，并通过实验验证了其可行性。

Abstract: Manipulating elasto-plastic objects remains a significant challenge due to
severe self-occlusion, difficulties of representation, and complicated
dynamics. This work proposes a novel framework for elasto-plastic object
manipulation with a quasi-static assumption for motions, leveraging 3D
occupancy to represent such objects, a learned dynamics model trained with 3D
occupancy, and a learning-based predictive control algorithm to address these
challenges effectively. We build a novel data collection platform to collect
full spatial information and propose a pipeline for generating a 3D occupancy
dataset. To infer the 3D occupancy during manipulation, an occupancy prediction
network is trained with multiple RGB images supervised by the generated
dataset. We design a deep neural network empowered by a 3D convolution neural
network (CNN) and a graph neural network (GNN) to predict the complex
deformation with the inferred 3D occupancy results. A learning-based predictive
control algorithm is introduced to plan the robot actions, incorporating a
novel shape-based action initialization module specifically designed to improve
the planner efficiency. The proposed framework in this paper can successfully
shape the elasto-plastic objects into a given goal shape and has been verified
in various experiments both in simulation and the real world.

</details>


### [389] [VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving](https://arxiv.org/abs/2505.16377)
*Yansong Qu,Zilin Huang,Zihao Sheng,Jiancong Chen,Sikai Chen,Samuel Labi*

Main category: cs.RO

TL;DR: VL-SAFE是一种基于世界模型和视觉语言模型的安全强化学习框架，用于离线安全策略学习，解决了传统RL在自动驾驶中的样本效率低和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在自动驾驶中依赖在线交互和试错学习，效率低且不安全，现有方法难以准确捕捉复杂驾驶场景中的安全语义。

Method: 构建离线数据集，利用视觉语言模型生成安全评分，训练世界模型生成模拟轨迹和安全评估，基于此进行演员-评论家学习优化策略。

Result: VL-SAFE在样本效率、泛化能力、安全性和整体性能上优于现有基线方法。

Conclusion: VL-SAFE首次将视觉语言模型引导的世界模型方法引入自动驾驶安全策略学习，具有显著优势。

Abstract: Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/

</details>


### [390] [Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)](https://arxiv.org/abs/2505.16394)
*Zhenjie Yang,Xiaosong Jia,Qifeng Li,Xue Yang,Maoqing Yao,Junchi Yan*

Main category: cs.RO

TL;DR: Raw2Drive是一种基于双流模型强化学习（MBRL）的方法，用于解决端到端自动驾驶（E2E-AD）中强化学习（RL）训练困难的问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习（IL）存在因果混淆和分布偏移问题，而强化学习（RL）可以缓解这些问题，但在端到端自动驾驶中应用RL仍具挑战性。

Method: 设计Raw2Drive，通过特权世界模型和原始传感器世界模型的双流结构，利用Guidance Mechanism确保两者一致性，并结合先验知识训练原始传感器策略。

Result: Raw2Drive是目前CARLA Leaderboard 2.0和Bench2Drive上唯一基于RL的端到端方法，并实现了最先进的性能。

Conclusion: Raw2Drive填补了MBRL在原始传感器数据输入方面的空白，为端到端自动驾驶提供了一种有效的RL解决方案。

Abstract: Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.

</details>


### [391] [UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning](https://arxiv.org/abs/2505.15725)
*Xiangyu Wang,Donglin Yang,Yue Liao,Wenhao Zheng,wenjun wu,Bin Dai,Hongsheng Li,Si Liu*

Main category: cs.RO

TL;DR: 论文提出了一种基于语言指令的无人机精细轨迹控制方法（Flow任务），通过模仿学习实现，并发布了首个真实世界基准数据集UAV-Flow。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注无人机的高层规划和长距离导航，而本文聚焦于语言引导的短距离精细轨迹控制，以提升人机交互的直观性。

Method: 采用模仿学习框架，无人机通过模仿专家飞行轨迹与原子语言指令配对学习精细控制策略，并设计了UAV-Flow基准数据集。

Result: 实验表明，视觉语言动作（VLA）模型优于视觉语言导航（VLN）基线，空间接地在精细控制中起关键作用。

Conclusion: UAV-Flow框架实现了无人机对专家轨迹的精确模仿，并支持直接部署，避免了仿真到现实的差距。

Abstract: Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive
platforms, enabling more intuitive forms of human-drone interaction. While
prior works have primarily focused on high-level planning and long-horizon
navigation, we shift attention to language-guided fine-grained trajectory
control, where UAVs execute short-range, reactive flight behaviors in response
to language instructions. We formalize this problem as the Flying-on-a-Word
(Flow) task and introduce UAV imitation learning as an effective approach. In
this framework, UAVs learn fine-grained control policies by mimicking expert
pilot trajectories paired with atomic language instructions. To support this
paradigm, we present UAV-Flow, the first real-world benchmark for
language-conditioned, fine-grained UAV control. It includes a task formulation,
a large-scale dataset collected in diverse environments, a deployable control
framework, and a simulation suite for systematic evaluation. Our design enables
UAVs to closely imitate the precise, expert-level flight trajectories of human
pilots and supports direct deployment without sim-to-real gap. We conduct
extensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results
show that VLA models are superior to VLN baselines and highlight the critical
role of spatial grounding in the fine-grained Flow setting.

</details>


### [392] [Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models](https://arxiv.org/abs/2505.16498)
*Augusto Luis Ballardini,Miguel Ángel Sotelo*

Main category: cs.RO

TL;DR: 利用大型语言模型（LLM）将非正式导航指令转化为基于逻辑的答案集编程（ASP）规则，提升自动驾驶车辆在动态城市环境中的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖预定义地图信息，难以应对动态城市环境中的突发变化（如路况变化、绕行或地图缺失）。

Method: 通过LLM将非正式导航指令转化为ASP规则，利用ASP的非单调推理能力实现动态适应。

Result: 实验表明，LLM驱动的ASP规则生成支持基于语义的决策，提供可解释的动态导航规划框架。

Conclusion: 该方法通过逻辑规则生成，显著提升了自动驾驶在动态环境中的适应性和可解释性。

Abstract: Achieving full automation in self-driving vehicles remains a challenge,
especially in dynamic urban environments where navigation requires real-time
adaptability. Existing systems struggle to handle navigation plans when faced
with unpredictable changes in road layouts, spontaneous detours, or missing map
data, due to their heavy reliance on predefined cartographic information. In
this work, we explore the use of Large Language Models to generate Answer Set
Programming rules by translating informal navigation instructions into
structured, logic-based reasoning. ASP provides non-monotonic reasoning,
allowing autonomous vehicles to adapt to evolving scenarios without relying on
predefined maps. We present an experimental evaluation in which LLMs generate
ASP constraints that encode real-world urban driving logic into a formal
knowledge representation. By automating the translation of informal navigation
instructions into logical rules, our method improves adaptability and
explainability in autonomous navigation. Results show that LLM-driven ASP rule
generation supports semantic-based decision-making, offering an explainable
framework for dynamic navigation planning that aligns closely with how humans
communicate navigational intent.

</details>


### [393] [Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation](https://arxiv.org/abs/2505.16547)
*Nitesh Subedi,Hsin-Jung Yang,Devesh K. Jha,Soumik Sarkar*

Main category: cs.RO

TL;DR: 提出了一种端到端深度强化学习框架，用于在复杂植物环境中进行遮挡感知的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决农业机器人在复杂植物环境中智能交互和遮挡感知的挑战，避免为每种植物场景设计显式几何和动态模型。

Method: 通过多模态观测解耦运动规划与机器人控制，简化零样本模拟到现实的策略迁移。

Result: 在真实世界试验中，训练策略的成功率高达86.7%。

Conclusion: 为自主感知驱动的农业机器人提供了新方向，能够在遮挡场景中智能找到目标物体。

Abstract: This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.

</details>


### [394] [Safe Uncertainty-Aware Learning of Robotic Suturing](https://arxiv.org/abs/2505.16596)
*Wilbert Peter Empleo,Yitaek Kim,Hansoul Kim,Thiusius Rajeeth Savarimuthu,Iñigo Iturrate*

Main category: cs.RO

TL;DR: 本文提出了一种基于不确定性感知的安全学习框架，用于机器人辅助微创手术的自动化。通过集成扩散策略模型和模型无关的控制屏障函数，实现了安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动化机器人辅助手术可以减轻外科医生的体力负担和重复性任务，同时解决训练有素外科医生短缺的问题。然而，现有AI方法缺乏可解释性和安全保证。

Method: 使用专家演示训练集成扩散策略模型，量化策略的认知不确定性以识别异常场景，并采用控制屏障函数确保动作的安全性。

Result: 在先进的机器人缝合模拟器中测试，策略表现出鲁棒性和泛化能力，能检测异常场景，且控制屏障函数成功限制不安全动作。

Conclusion: 该框架为机器人辅助手术自动化提供了安全、可解释的解决方案，具有实际应用潜力。

Abstract: Robot-Assisted Minimally Invasive Surgery is currently fully manually
controlled by a trained surgeon. Automating this has great potential for
alleviating issues, e.g., physical strain, highly repetitive tasks, and
shortages of trained surgeons. For these reasons, recent works have utilized
Artificial Intelligence methods, which show promising adaptability. Despite
these advances, there is skepticism of these methods because they lack
explainability and robust safety guarantees. This paper presents a framework
for a safe, uncertainty-aware learning method. We train an Ensemble Model of
Diffusion Policies using expert demonstrations of needle insertion. Using an
Ensemble model, we can quantify the policy's epistemic uncertainty, which is
used to determine Out-Of-Distribution scenarios. This allows the system to
release control back to the surgeon in the event of an unsafe scenario.
Additionally, we implement a model-free Control Barrier Function to place
formal safety guarantees on the predicted action. We experimentally evaluate
our proposed framework using a state-of-the-art robotic suturing simulator. We
evaluate multiple scenarios, such as dropping the needle, moving the camera,
and moving the phantom. The learned policy is robust to these perturbations,
showing corrective behaviors and generalization, and it is possible to detect
Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier
Function successfully limits the action to remain within our specified safety
set in the case of unsafe predictions.

</details>


### [395] [ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models](https://arxiv.org/abs/2505.16517)
*Zirui Song,Guangxian Ouyang,Mingzhe Li,Yuheng Ji,Chenxi Wang,Zixiang Xu,Zeyu Zhang,Xiaoqing Zhang,Qian Jiang,Zhenhao Chen,Zhongzhi Li,Rui Yan,Xiuying Chen*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的框架ManipLVM-R1，通过可验证奖励（RLVR）减少对昂贵人工标注数据的依赖，提升机器人操作的泛化能力和物理推理。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）依赖昂贵的人工标注数据，泛化能力受限，难以适应真实世界中的域外（OOD）场景。

Method: 采用强化学习框架，设计两种基于规则的奖励函数：Affordance Perception Reward（增强交互区域定位）和Trajectory Match Reward（确保动作路径的物理合理性）。

Result: 方法通过直接优化任务对齐结果，提升了泛化能力和物理推理，减少了对标注数据的依赖。

Conclusion: ManipLVM-R1通过强化学习和可验证奖励，显著提升了机器人操作的适应性和系统性推理能力。

Abstract: Large Vision-Language Models (LVLMs) have recently advanced robotic
manipulation by leveraging vision for scene perception and language for
instruction following. However, existing methods rely heavily on costly
human-annotated training datasets, which limits their generalization and causes
them to struggle in out-of-domain (OOD) scenarios, reducing real-world
adaptability. To address these challenges, we propose ManipLVM-R1, a novel
reinforcement learning framework that replaces traditional supervision with
Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing
for task-aligned outcomes, our method enhances generalization and physical
reasoning while removing the dependence on costly annotations. Specifically, we
design two rule-based reward functions targeting key robotic manipulation
subtasks: an Affordance Perception Reward to enhance localization of
interaction regions, and a Trajectory Match Reward to ensure the physical
plausibility of action paths. These rewards provide immediate feedback and
impose spatial-logical constraints, encouraging the model to go beyond shallow
pattern matching and instead learn deeper, more systematic reasoning about
physical interactions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [396] [Sufficient conditions for offline reactivation in recurrent neural networks](https://arxiv.org/abs/2505.17003)
*Nanda H. Krishna,Colin Bredenberg,Daniel Levenstein,Blake A. Richards,Guillaume Lajoie*

Main category: q-bio.NC

TL;DR: 论文提出了一种数学框架，解释了任务优化的神经网络如何在无输入时自发重现在线活动时的状态。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解任务优化的神经网络在静息状态下如何自发重现与任务相关的网络状态。

Method: 开发了一个数学框架，证明了噪声循环网络在优化跟踪环境状态变量时会自然发展出去噪动态，从而在无输入时重现在线活动状态。

Result: 通过数值实验验证了框架的有效性，应用于空间位置估计和头部方向估计任务。

Conclusion: 研究为离线重现在线活动状态提供了理论支持，表明这是噪声神经网络任务优化的自然结果。

Abstract: During periods of quiescence, such as sleep, neural activity in many brain
circuits resembles that observed during periods of task engagement. However,
the precise conditions under which task-optimized networks can autonomously
reactivate the same network states responsible for online behavior is poorly
understood. In this study, we develop a mathematical framework that outlines
sufficient conditions for the emergence of neural reactivation in circuits that
encode features of smoothly varying stimuli. We demonstrate mathematically that
noisy recurrent networks optimized to track environmental state variables using
change-based sensory information naturally develop denoising dynamics, which,
in the absence of input, cause the network to revisit state configurations
observed during periods of online activity. We validate our findings using
numerical experiments on two canonical neuroscience tasks: spatial position
estimation based on self-motion cues, and head direction estimation based on
angular velocity cues. Overall, our work provides theoretical support for
modeling offline reactivation as an emergent consequence of task optimization
in noisy neural circuits.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [397] [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
*Ahmed Heakl,Sarim Hashmi,Gustavo Bertolo Stahl,Seung Hun Eddie Han,Salman Khan,Abdulrahman Mahmoud*

Main category: cs.AR

TL;DR: CASS是首个针对跨架构GPU代码转译的大规模数据集和模型套件，支持源级（CUDA↔HIP）和汇编级（Nvidia SASS↔AMD RDNA3）翻译。数据集包含7万对已验证代码，填补了低层GPU代码可移植性的空白。CASS模型在源翻译准确率达95%，汇编翻译达37.5%，优于GPT-4o等商业基线。生成的代码在85%测试案例中性能接近原生，并开源了数据集、模型和评估工具。


<details>
  <summary>Details</summary>
Motivation: 解决低层GPU代码跨架构移植的难题，填补现有工具在源级和汇编级翻译上的不足。

Method: 构建包含7万对代码的大规模数据集，训练领域专用语言模型CASS，支持CUDA/HIP和SASS/RDNA3的互译。

Result: 源翻译准确率95%，汇编翻译37.5%，85%测试案例性能匹配原生代码。

Conclusion: CASS显著提升了GPU代码跨架构翻译的准确性和性能，开源资源推动GPU编译工具和LLM引导的硬件翻译发展。

Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level
(CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia
SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k
verified code pairs across host and device, addressing a critical gap in
low-level GPU code portability. Leveraging this resource, we train the
\texttt{CASS} family of domain-specific language models, achieving 95\% source
translation accuracy and 37.5\% assembly translation accuracy, substantially
outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our
generated code matches native performance in over 85\% of test cases,
preserving runtime and memory behavior. To support rigorous evaluation, we
introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with
ground-truth execution. All data, models, and evaluation tools are released as
open source to foster progress in GPU compiler tooling, binary compatibility,
and LLM-guided hardware translation. Dataset and benchmark are on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [398] [From Hand-Crafted Metrics to Evolved Training-Free Performance Predictors for Neural Architecture Search via Genetic Programming](https://arxiv.org/abs/2505.15832)
*Quan Minh Phan,Ngoc Hoang Luong*

Main category: cs.NE

TL;DR: 提出了一种基于符号回归和遗传编程的自动化框架，用于设计零成本（ZC）指标，以解决现有手动设计ZC指标的不一致性和耗时问题。


<details>
  <summary>Details</summary>
Motivation: 现有ZC指标在性能上存在不一致性，且设计过程依赖手动试错和领域知识，亟需自动化解决方案。

Method: 采用符号回归和遗传编程的框架，自动生成ZC指标，并利用现有手工指标合成更具泛化性的指标。

Result: 在13个NAS-Bench-Suite-Zero问题上，自动生成的ZC指标表现优于手工设计指标，且能在15分钟内通过单GPU找到高性能网络架构。

Conclusion: 该框架高效、可扩展，能快速生成与真实网络性能强相关的ZC指标，为NAS提供更优的搜索目标。

Abstract: Estimating the network performance using zero-cost (ZC) metrics has proven
both its efficiency and efficacy in Neural Architecture Search (NAS). However,
a notable limitation of most ZC proxies is their inconsistency, as reflected by
the substantial variation in their performance across different problems.
Furthermore, the design of existing ZC metrics is manual, involving a
time-consuming trial-and-error process that requires substantial domain
expertise. These challenges raise two critical questions: (1) Can we automate
the design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC
metrics to synthesize a more generalizable one? In this study, we propose a
framework based on Symbolic Regression via Genetic Programming to automate the
design of ZC metrics. Our framework is not only highly extensible but also
capable of quickly producing a ZC metric with a strong positive rank
correlation to true network performance across diverse NAS search spaces and
tasks. Extensive experiments on 13 problems from NAS-Bench-Suite-Zero
demonstrate that our automatically generated proxies consistently outperform
hand-crafted alternatives. Using our evolved proxy metric as the search
objective in an evolutionary algorithm, we could identify network architectures
with competitive performance within 15 minutes using a single consumer GPU.

</details>


### [399] [Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning](https://arxiv.org/abs/2505.15836)
*Aarav Lala,Kalyan Cherukuri*

Main category: cs.NE

TL;DR: 论文提出了一种结合量子启发神经网络与进化算法的新框架QE-NN，用于优化多智能体系统中的实时决策，同时确保隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂、去中心化环境中的应用，对可扩展、自适应且隐私保护的决策系统需求日益迫切。

Method: QE-NN利用量子计算原理（如叠加和纠缠）提升学习速度和决策精度，结合进化算法优化动态环境中的智能体行为，并通过联邦学习实现隐私保护。

Result: 该框架使智能体能够实时适应环境，优化决策过程，适用于自主系统、智慧城市和医疗等领域。

Conclusion: 研究突破了量子计算、进化优化与隐私保护技术的结合，为复杂多智能体决策系统提供了创新解决方案。

Abstract: As artificial intelligence continues to drive innovation in complex,
decentralized environments, the need for scalable, adaptive, and
privacy-preserving decision-making systems has become critical. This paper
introduces a novel framework combining quantum-inspired neural networks with
evolutionary algorithms to optimize real-time decision-making in multi-agent
systems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN)
leverages quantum computing principles -- such as quantum superposition and
entanglement -- to enhance learning speed and decision accuracy, while
integrating evolutionary optimization to continually refine agent behaviors in
dynamic, uncertain environments. By utilizing federated learning, QE-NN ensures
privacy preservation, enabling decentralized agents to collaborate without
sharing sensitive data. The framework is designed to allow agents to adapt in
real-time to their environments, optimizing decision-making processes for
applications in areas such as autonomous systems, smart cities, and healthcare.
This research represents a breakthrough in merging quantum computing,
evolutionary optimization, and privacy-preserving techniques to solve complex
problems in multi-agent decision-making systems, pushing the boundaries of AI
in real-world, privacy-sensitive applications.

</details>


### [400] [TDFormer: A Top-Down Attention-Controlled Spiking Transformer](https://arxiv.org/abs/2505.15840)
*Zizheng Zhu,Yingchao Yu,Zeqi Zheng,Zhaofei Yu,Yaochu Jin*

Main category: cs.NE

TL;DR: TDFormer是一种新型的脉冲神经网络模型，通过引入自上而下的反馈结构，有效提升了时间信息的传递和梯度传播，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统SNN的膜电位作为时间步间唯一的信息链接，其隐式特性限制了时间信息的有效表示，导致模型性能受限。

Method: 提出TDFormer模型，采用分层自上而下反馈结构，利用早期时间步的高阶表征调制后期低阶信息的处理。

Result: 模型在多个数据集上表现显著提升，尤其在ImageNet上达到86.83%的准确率，达到SOTA水平。

Conclusion: TDFormer通过反馈结构增强了时间信息传递和梯度传播，为SNN性能提升提供了有效解决方案。

Abstract: Traditional spiking neural networks (SNNs) can be viewed as a combination of
multiple subnetworks with each running for one time step, where the parameters
are shared, and the membrane potential serves as the only information link
between them. However, the implicit nature of the membrane potential limits its
ability to effectively represent temporal information. As a result, each time
step cannot fully leverage information from previous time steps, seriously
limiting the model's performance. Inspired by the top-down mechanism in the
brain, we introduce TDFormer, a novel model with a top-down feedback structure
that functions hierarchically and leverages high-order representations from
earlier time steps to modulate the processing of low-order information at later
stages. The feedback structure plays a role from two perspectives: 1) During
forward propagation, our model increases the mutual information across time
steps, indicating that richer temporal information is being transmitted and
integrated in different time steps. 2) During backward propagation, we
theoretically prove that the feedback structure alleviates the problem of
vanishing gradients along the time dimension. We find that these mechanisms
together significantly and consistently improve the model performance on
multiple datasets. In particular, our model achieves state-of-the-art
performance on ImageNet with an accuracy of 86.83%.

</details>


### [401] [Adversarially Robust Spiking Neural Networks with Sparse Connectivity](https://arxiv.org/abs/2505.15833)
*Mathias Schmolli,Maximilian Baronig,Robert Legenstein,Ozan Özdenizci*

Main category: cs.NE

TL;DR: 该论文提出了一种将预训练的人工神经网络（ANN）转换为稀疏且对抗鲁棒的脉冲神经网络（SNN）的算法，显著提升了能量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的嵌入式系统中部署深度神经网络需要高效的能量和内存解决方案，同时确保对抗鲁棒性。

Method: 通过利用预训练ANN的稀疏连接和权重，设计了一种转换算法，生成稀疏且对抗鲁棒的SNN。

Result: 模型实现了内存中权重数量减少100倍，能量效率提升8.6倍，同时保持高性能和对抗鲁棒性。

Conclusion: 该方法成功结合了SNN的能量高效性和对抗鲁棒性，为嵌入式系统提供了高效的解决方案。

Abstract: Deployment of deep neural networks in resource-constrained embedded systems
requires innovative algorithmic solutions to facilitate their energy and memory
efficiency. To further ensure the reliability of these systems against
malicious actors, recent works have extensively studied adversarial robustness
of existing architectures. Our work focuses on the intersection of adversarial
robustness, memory- and energy-efficiency in neural networks. We introduce a
neural network conversion algorithm designed to produce sparse and
adversarially robust spiking neural networks (SNNs) by leveraging the sparse
connectivity and weights from a robustly pretrained artificial neural network
(ANN). Our approach combines the energy-efficient architecture of SNNs with a
novel conversion algorithm, leading to state-of-the-art performance with
enhanced energy and memory efficiency through sparse connectivity and
activations. Our models are shown to achieve up to 100x reduction in the number
of weights to be stored in memory, with an estimated 8.6x increase in energy
efficiency compared to dense SNNs, while maintaining high performance and
robustness against adversarial threats.

</details>


### [402] [Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems](https://arxiv.org/abs/2505.15839)
*Saining Liu,Yi Mei,Mengjie Zhang*

Main category: cs.NE

TL;DR: 论文提出了一种名为CL-GPGLS的新方法，通过将课程学习（CL）集成到GPGLS中，逐步引入训练实例以优化大规模车辆路径问题（LSVRP）的性能。


<details>
  <summary>Details</summary>
Motivation: 手动设计车辆路径问题（VRP）的（元）启发式方法需要大量领域专业知识，而现有数据驱动方法（如GPGLS）在训练实例选择上依赖随机性，存在优化空间。

Method: CL-GPGLS结合课程学习，按预定义课程逐步引入从简单到复杂的训练实例，提升模型适应性和优化能力。

Result: 实验表明，CL-GPGLS在性能上显著优于三种基线方法。

Conclusion: CL-GPGLS通过课程学习有效改进了GPGLS，为大规模VRP提供了更优的解决方案。

Abstract: Manually designing (meta-)heuristics for the Vehicle Routing Problem (VRP) is
a challenging task that requires significant domain expertise. Recently,
data-driven approaches have emerged as a promising solution, automatically
learning heuristics that perform well on training instances and generalize to
unseen test cases. Such an approach learns (meta-)heuristics that can perform
well on the training instances, expecting it to generalize well on the unseen
test instances. A recent method, named GPGLS, uses Genetic Programming (GP) to
learn the utility function in Guided Local Search (GLS) and solved large scale
VRP effectively. However, the selection of appropriate training instances
during the learning process remains an open question, with most existing
studies including GPGLS relying on random instance selection. To address this,
we propose a novel method, CL-GPGLS, which integrates Curriculum Learning (CL)
into GPGLS. Our approach leverages a predefined curriculum to introduce
training instances progressively, starting with simpler tasks and gradually
increasing complexity, enabling the model to better adapt and optimize for
large-scale VRP (LSVRP). Extensive experiments verify the effectiveness of
CL-GPGLS, demonstrating significant performance improvements over three
baseline methods.

</details>


### [403] [Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms](https://arxiv.org/abs/2505.16362)
*El-ghazali Talbi*

Main category: cs.NE

TL;DR: 本文探讨了神经形态计算（NC）在优化算法和元启发式中的应用，提出了神经形态元启发式（Nheuristics）的概念，并分析了其低功耗、低延迟和小尺寸的特点。


<details>
  <summary>Details</summary>
Motivation: 研究NC作为冯·诺依曼架构的替代方案，以解决优化问题，并探索Nheuristics的潜力。

Method: 通过分类和批判性分析，研究了不同元启发式家族及其解决的优化问题，并提出了设计指南。

Result: Nheuristics在低功耗、低延迟和小尺寸方面表现出潜力，但设计和实现面临挑战。

Conclusion: 未来需进一步研究以扩展Nheuristics的开发和应用。

Abstract: Neuromorphic computing (NC) introduces a novel algorithmic paradigm
representing a major shift from traditional digital computing of Von Neumann
architectures. NC emulates or simulates the neural dynamics of brains in the
form of Spiking Neural Networks (SNNs). Much of the research in NC has
concentrated on machine learning applications and neuroscience simulations.
This paper investigates the modelling and implementation of optimization
algorithms and particularly metaheuristics using the NC paradigm as an
alternative to Von Neumann architectures, leading to breakthroughs in solving
optimization problems.
  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be
characterized by low power, low latency and small footprint. Since NC systems
are fundamentally different from conventional Von Neumann computers, several
challenges are posed to the design and implementation of Nheuristics. A
guideline based on a classification and critical analysis is conducted on the
different families of metaheuristics and optimization problems they address. We
also discuss future directions that need to be addressed to expand both the
development and application of Nheuristics.

</details>


### [404] [Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2505.16471)
*Robbert Reijnen,Yaoxin Wu,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.NE

TL;DR: 本文提出了一种基于图神经网络（GNN）的深度强化学习方法，用于多目标组合优化问题的算法配置，显著优于传统和基于DRL的方法。


<details>
  <summary>Details</summary>
Motivation: 多目标组合优化（MOCO）问题的动态算法配置尚未充分探索，本文旨在填补这一空白。

Method: 将动态算法配置建模为马尔可夫决策过程，利用GNN学习目标空间中解的图嵌入以增强状态表示。

Result: 实验表明，该方法在效果和适应性上优于传统和基于DRL的配置方法，并具有良好的泛化能力。

Conclusion: 该方法在多目标组合优化问题中表现出高效性和通用性，适用于不同类型的进化计算方法。

Abstract: Deep reinforcement learning (DRL) has been widely used for dynamic algorithm
configuration, particularly in evolutionary computation, which benefits from
the adaptive update of parameters during the algorithmic execution. However,
applying DRL to algorithm configuration for multi-objective combinatorial
optimization (MOCO) problems remains relatively unexplored. This paper presents
a novel graph neural network (GNN) based DRL to configure multi-objective
evolutionary algorithms. We model the dynamic algorithm configuration as a
Markov decision process, representing the convergence of solutions in the
objective space by a graph, with their embeddings learned by a GNN to enhance
the state representation. Experiments on diverse MOCO challenges indicate that
our method outperforms traditional and DRL-based algorithm configuration
methods in terms of efficacy and adaptability. It also exhibits advantageous
generalizability across objective types and problem sizes, and applicability to
different evolutionary computation methods.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [405] [Modeling Inequality in Complex Networks of Strategic Agents using Iterative Game-Theoretic Transactions](https://arxiv.org/abs/2505.16966)
*Mayank Kejriwal,Yuesheng Luo*

Main category: cs.GT

TL;DR: 本文提出了一种基于博弈论的模型和模拟算法，用于量化战略代理在复杂网络中不平等现象的演变，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究对现实社交网络中不同类型代理交易（如信任、货币和社会资本）的系统性效应理解不足，尤其是基尼系数在复杂网络和博弈论中的应用未被充分探讨。

Method: 基于博弈论构建模型和模拟算法，利用公开数据进行可重复实验。

Result: 揭示了即使在简单抽象环境中，不平等现象的复杂驱动因素，并在不同来源和描述的网络中表现出一致性。

Conclusion: 该研究为理解复杂网络中不平等现象的演变提供了新视角和工具。

Abstract: Transactions are an important aspect of human social life, and represent
dynamic flow of information, intangible values, such as trust, as well as
monetary and social capital. Although much research has been conducted on the
nature of transactions in fields ranging from the social sciences to game
theory, the systemic effects of different types of agents transacting in
real-world social networks (often following a scale-free distribution) are not
fully understood. A particular systemic measure that has not received adequate
attention in the complex networks and game theory communities, is the Gini
Coefficient, which is widely used in economics to quantify and understand
wealth inequality. In part, the problem is a lack of experimentation using a
replicable algorithm and publicly available data. Motivated by this problem,
this article proposes a model and simulation algorithm, based on game theory,
for quantifying the evolution of inequality in complex networks of strategic
agents. Our results shed light on several complex drivers of inequality, even
in simple, abstract settings, and exhibit consistency across networks with
different origins and descriptions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [406] [Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing](https://arxiv.org/abs/2505.16332)
*Zhehui Wanga,Benjamin Chen Ming Choonga,Tian Huang,Daniel Gerlinghoffa,Rick Siow Mong Goh,Cheng Liu,Tao Luo*

Main category: quant-ph

TL;DR: 论文探讨了量子优化在深度神经网络（DNN）压缩中的应用，通过将模型压缩问题转化为QUBO问题，并利用商业量子退火设备求解，展示了量子计算在优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 量子优化是当前最成熟的量子计算技术之一，适用于复杂组合问题的求解。然而，将其应用于DNN优化需要重新建模以适应现有量子设备。本文旨在探索量子优化在DNN压缩中的可行性。

Method: 将卷积神经网络的细粒度剪枝-量化问题重新建模为QUBO问题，并利用商业量子退火设备（AQC）求解。

Result: 实验表明，AQC在时间效率上优于经典算法（如遗传算法和强化学习），并能更有效地找到全局最优解。

Conclusion: AQC在DNN模型压缩中表现出色，展示了量子优化在深度学习中的实际应用潜力。

Abstract: Quantum optimization is the most mature quantum computing technology to date,
providing a promising approach towards efficiently solving complex
combinatorial problems. Methods such as adiabatic quantum computing (AQC) have
been employed in recent years on important optimization problems across various
domains. In deep learning, deep neural networks (DNN) have reached immense
sizes to support new predictive capabilities. Optimization of large-scale
models is critical for sustainable deployment, but becomes increasingly
challenging with ever-growing model sizes and complexity. While quantum
optimization is suitable for solving complex problems, its application to DNN
optimization is not straightforward, requiring thorough reformulation for
compatibility with commercially available quantum devices. In this work, we
explore the potential of adopting AQC for fine-grained pruning-quantization of
convolutional neural networks. We rework established heuristics to formulate
model compression as a quadratic unconstrained binary optimization (QUBO)
problem, and assess the solution space offered by commercial quantum annealing
devices. Through our exploratory efforts of reformulation, we demonstrate that
AQC can achieve effective compression of practical DNN models. Experiments
demonstrate that adiabatic quantum computing (AQC) not only outperforms
classical algorithms like genetic algorithms and reinforcement learning in
terms of time efficiency but also excels at identifying global optima.

</details>


### [407] [Experimental robustness benchmark of quantum neural network on a superconducting quantum processor](https://arxiv.org/abs/2505.16714)
*Hai-Feng Zhang,Zhao-Yun Chen,Peng Wang,Liang-Liang Guo,Tian-Le Wang,Xiao-Yan Yang,Ren-Ze Zhao,Ze-An Zhao,Sheng Zhang,Lei Du,Hao-Ran Tao,Zhi-Long Jia,Wei-Cheng Kong,Huan-Yu Liu,Athanasios V. Vasilakos,Yang Yang,Yu-Chun Wu,Ji Guan,Peng Duan,Guo-Ping Guo*

Main category: quant-ph

TL;DR: 该论文首次系统性地实验验证了20量子比特量子神经网络（QNN）在超导处理器上的对抗鲁棒性，提出了一种高效的对抗攻击算法，并发现QNN比经典神经网络更具对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）模型易受对抗攻击，阻碍其安全部署，因此需要系统性评估和改进QNN的对抗鲁棒性。

Method: 开发了一个对抗攻击算法框架，用于定量表征QNN的对抗鲁棒性，并通过对抗训练减少对目标扰动的敏感性。

Result: 实验表明，对抗训练显著提升了QNN的鲁棒性，且QNN的对抗鲁棒性优于经典神经网络，归因于量子噪声。攻击实验的实证上限与理论下限偏差极小（3×10^-3）。

Conclusion: 该研究为评估和改进量子对抗鲁棒性提供了关键实验框架，推动了安全可靠的QML应用发展。

Abstract: Quantum machine learning (QML) models, like their classical counterparts, are
vulnerable to adversarial attacks, hindering their secure deployment. Here, we
report the first systematic experimental robustness benchmark for 20-qubit
quantum neural network (QNN) classifiers executed on a superconducting
processor. Our benchmarking framework features an efficient adversarial attack
algorithm designed for QNNs, enabling quantitative characterization of
adversarial robustness and robustness bounds. From our analysis, we verify that
adversarial training reduces sensitivity to targeted perturbations by
regularizing input gradients, significantly enhancing QNN's robustness.
Additionally, our analysis reveals that QNNs exhibit superior adversarial
robustness compared to classical neural networks, an advantage attributed to
inherent quantum noise. Furthermore, the empirical upper bound extracted from
our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the
theoretical lower bound, providing strong experimental confirmation of the
attack's effectiveness and the tightness of fidelity-based robustness bounds.
This work establishes a critical experimental framework for assessing and
improving quantum adversarial robustness, paving the way for secure and
reliable QML applications.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [408] [The Computational Complexity of Counting Linear Regions in ReLU Neural Networks](https://arxiv.org/abs/2505.16716)
*Moritz Stargalla,Christoph Hertrich,Daniel Reichman*

Main category: cs.CC

TL;DR: 本文系统评估了ReLU神经网络中线性区域的不同定义及其关系，分析了计算这些区域数量的复杂度，证明其为NP-和#P-难问题，并展示了多项式空间内的计数算法。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络表达能力的衡量标准——线性区域数量，并探讨不同定义之间的关系及其计算复杂度。

Method: 系统评估不同定义，分析计算复杂度，证明NP-和#P-难问题，并设计多项式空间算法。

Result: 线性区域计数问题在单隐层网络中为NP-和#P-难，多隐层网络中更难近似，部分定义下可在多项式空间内计数。

Conclusion: 线性区域计数问题普遍难解，但部分定义下存在有效算法，为未来研究提供方向。

Abstract: An established measure of the expressive power of a given ReLU neural network
is the number of linear regions into which it partitions the input space. There
exist many different, non-equivalent definitions of what a linear region
actually is. We systematically assess which papers use which definitions and
discuss how they relate to each other. We then analyze the computational
complexity of counting the number of such regions for the various definitions.
Generally, this turns out to be an intractable problem. We prove NP- and
#P-hardness results already for networks with one hidden layer and strong
hardness of approximation results for two or more hidden layers. Finally, on
the algorithmic side, we demonstrate that counting linear regions can at least
be achieved in polynomial space for some common definitions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [409] [Graph Attention Network for Optimal User Association in Wireless Networks](https://arxiv.org/abs/2505.16347)
*Javad Mirzaei,Jeebak Mitra,Gwenael Poitau*

Main category: cs.IT

TL;DR: 论文提出了一种基于图形抽象的优化方法，用于5G网络中的用户关联策略，以提高网络节能效果，并通过与传统方法的对比验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着5G部署的增加，网络密集化导致能耗显著上升，运营商迫切需要改进网络节能技术，而用户关联策略是影响能耗的关键因素。

Method: 利用图形抽象方法对蜂窝网络中的用户关联进行优化，以确定何时激活节能功能（如小区关闭）。

Result: 提出的方法在节能效果上优于传统方法。

Conclusion: 基于图形抽象的优化方法能有效提升网络节能性能，为5G网络的高效运营提供了新思路。

Abstract: With increased 5G deployments, network densification is higher than ever to
support the exponentially high throughput requirements. However, this has meant
a significant increase in energy consumption, leading to higher operational
expenditure (OpEx) for network operators creating an acute need for
improvements in network energy savings (NES). A key determinant of operational
efficacy in cellular networks is the user association (UA) policy, as it
affects critical aspects like spectral efficiency, load balancing etc. and
therefore impacts the overall energy consumption of the network directly.
Furthermore, with cellular network topologies lending themselves well to
graphical abstractions, use of graphs in network optimization has gained
significant prominence. In this work, we propose and analyze a graphical
abstraction based optimization for UA in cellular networks to improve NES by
determining when energy saving features like cell switch off can be activated.
A comparison with legacy approaches establishes the superiority of the proposed
approach.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [410] [A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles](https://arxiv.org/abs/2505.16215)
*Md Ashraf Uddin,Nam H. Chu,Reza Rafeh,Mutaz Barika*

Main category: cs.CR

TL;DR: 提出了一种针对车联网（IoV）的分层分类框架，通过边缘节点独立检测特定攻击，同时利用云端进行全面分析，解决了集中式系统的延迟和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 车联网因动态性、移动性和无线数据传输易受网络威胁，现有集中式入侵检测系统（IDS）依赖云端导致延迟，边缘节点资源有限无法部署复杂算法。

Method: 采用分层分类框架，结合Boruta特征选择方法降维，边缘节点独立检测攻击，云端提供全面支持。

Result: 使用CIC-IoV2024数据集验证，模型在保护车联网安全方面表现出可行性和有效性。

Conclusion: 分层分类框架为车联网提供了高效、实时的入侵检测方案，平衡了边缘与云端的资源分配。

Abstract: Due to its nature of dynamic, mobility, and wireless data transfer, the
Internet of Vehicles (IoV) is prone to various cyber threats, ranging from
spoofing and Distributed Denial of Services (DDoS) attacks to malware. To
safeguard the IoV ecosystem from intrusions, malicious activities, policy
violations, intrusion detection systems (IDS) play a critical role by
continuously monitoring and analyzing network traffic to identify and mitigate
potential threats in real-time. However, most existing research has focused on
developing centralized, machine learning-based IDS systems for IoV without
accounting for its inherently distributed nature. Due to intensive computing
requirements, these centralized systems often rely on the cloud to detect cyber
threats, increasing delay of system response. On the other hand, edge nodes
typically lack the necessary resources to train and deploy complex machine
learning algorithms. To address this issue, this paper proposes an effective
hierarchical classification framework tailored for IoV networks. Hierarchical
classification allows classifiers to be trained and tested at different levels,
enabling edge nodes to detect specific types of attacks independently. With
this approach, edge nodes can conduct targeted attack detection while
leveraging cloud nodes for comprehensive threat analysis and support. Given the
resource constraints of edge nodes, we have employed the Boruta feature
selection method to reduce data dimensionality, optimizing processing
efficiency. To evaluate our proposed framework, we utilize the latest IoV
security dataset CIC-IoV2024, achieving promising results that demonstrate the
feasibility and effectiveness of our models in securing IoV networks.

</details>


### [411] [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
*Sampanna Yashwant Kahu,Naman Ahuja*

Main category: cs.CR

TL;DR: 论文提出了一种黑盒扰动技术，用于欺骗深度学习仇恨言论检测模型，降低其效率，同时确保对原文意义的改变最小化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和在线论坛中仇恨言论泛滥，现有深度学习检测模型存在漏洞，需要保护用户免受仇恨言论侵害。

Method: 设计黑盒扰动技术，生成能够欺骗深度学习仇恨言论检测模型的扰动。

Result: 最佳扰动攻击成功逃避仇恨言论检测的比例达86.8%。

Conclusion: 该方法有效降低了仇恨言论检测模型的效率，同时保持了原文意义的完整性。

Abstract: Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.

</details>


### [412] [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
*Yuliang Yan,Haochun Tang,Shuo Yan,Enyan Dai*

Main category: cs.CR

TL;DR: DuFFin是一种新型的双层指纹框架，用于黑盒设置下的所有权验证，能准确识别受保护大语言模型的变体。


<details>
  <summary>Details</summary>
Motivation: 保护大语言模型的知识产权，防止恶意窃取或未经授权的部署。

Method: 提取触发模式和知识级指纹，结合黑盒设置验证模型所有权。

Result: 在多种模型变体上验证，IP-ROC指标大于0.95。

Conclusion: DuFFin在保护大语言模型知识产权方面具有高效性和实用性。

Abstract: Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.

</details>


### [413] [BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization](https://arxiv.org/abs/2505.16640)
*Xueyang Zhou,Guiyao Tie,Guowen Zhang,Hechang Wang,Pan Zhou,Lichao Sun*

Main category: cs.CR

TL;DR: BadVLA是一种针对Vision-Language-Action（VLA）模型的后门攻击方法，通过目标解耦优化暴露了VLA模型的安全漏洞，攻击成功率高且对正常任务影响小。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人控制中表现出色，但其紧密耦合的架构带来了新的安全漏洞，尤其是后门攻击的威胁尚未被充分研究。

Method: BadVLA采用两阶段方法：1）显式特征空间分离以隔离触发器表示；2）条件控制偏差，仅在触发器存在时激活。

Result: 在多个VLA基准测试中，BadVLA攻击成功率接近100%，且对正常任务准确性影响极小。

Conclusion: 研究首次系统性地揭示了VLA模型的后门漏洞，强调了安全设计的重要性。

Abstract: Vision-Language-Action (VLA) models have advanced robotic control by enabling
end-to-end decision-making directly from multimodal inputs. However, their
tightly coupled architectures expose novel security vulnerabilities. Unlike
traditional adversarial perturbations, backdoor attacks represent a stealthier,
persistent, and practically significant threat-particularly under the emerging
Training-as-a-Service paradigm-but remain largely unexplored in the context of
VLA models. To address this gap, we propose BadVLA, a backdoor attack method
based on Objective-Decoupled Optimization, which for the first time exposes the
backdoor vulnerabilities of VLA models. Specifically, it consists of a
two-stage process: (1) explicit feature-space separation to isolate trigger
representations from benign inputs, and (2) conditional control deviations that
activate only in the presence of the trigger, while preserving clean-task
performance. Empirical results on multiple VLA benchmarks demonstrate that
BadVLA consistently achieves near-100% attack success rates with minimal impact
on clean task accuracy. Further analyses confirm its robustness against common
input perturbations, task transfers, and model fine-tuning, underscoring
critical security vulnerabilities in current VLA deployments. Our work offers
the first systematic investigation of backdoor vulnerabilities in VLA models,
highlighting an urgent need for secure and trustworthy embodied model design
practices. We have released the project page at
https://badvla-project.github.io/.

</details>


### [414] [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916)
*Xuankun Rong,Wenke Huang,Jian Liang,Jinhe Bi,Xun Xiao,Yiming Li,Bo Du,Mang Ye*

Main category: cs.CR

TL;DR: BYE是一种无需干净监督的数据过滤框架，通过注意力熵模式检测并过滤后门样本，有效防御MLLMs中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在微调即服务（FTaaS）中存在后门攻击风险，恶意微调可轻易植入后门。

Method: BYE通过三阶段流程：提取注意力图、计算熵分数并分析敏感层、无监督聚类过滤可疑样本。

Result: 实验表明BYE能实现接近零的攻击成功率，同时保持干净任务性能。

Conclusion: BYE为MLLMs提供了一种无需额外监督或模型修改的通用后门防御方案。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt
general-purpose models to downstream tasks. This flexibility, however,
introduces serious security risks, as malicious fine-tuning can implant
backdoors into MLLMs with minimal effort. In this paper, we observe that
backdoor triggers systematically disrupt cross-modal processing by causing
abnormal attention concentration on non-semantic regions--a phenomenon we term
attention collapse. Based on this insight, we propose Believe Your Eyes (BYE),
a data filtering framework that leverages attention entropy patterns as
self-supervised signals to identify and filter backdoor samples. BYE operates
via a three-stage pipeline: (1) extracting attention maps using the fine-tuned
model, (2) computing entropy scores and profiling sensitive layers via bimodal
separation, and (3) performing unsupervised clustering to remove suspicious
samples. Unlike prior defenses, BYE equires no clean supervision, auxiliary
labels, or model modifications. Extensive experiments across various datasets,
models, and diverse trigger types validate BYE's effectiveness: it achieves
near-zero attack success rates while maintaining clean-task performance,
offering a robust and generalizable solution against backdoor threats in MLLMs.

</details>


### [415] [BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models](https://arxiv.org/abs/2505.16670)
*Xiaobei Yan,Yiming Li,Zhaoxin Fan,Han Qiu,Tianwei Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种新型的推理成本攻击方法（BitHydra），通过翻转模型参数的关键位，显著增加LLM的生成长度，攻击效果高效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有推理成本攻击方法效果有限，因为它们仅通过输入攻击，无法大规模影响模型。本文旨在设计一种直接针对模型参数的攻击方法。

Method: 提出BitHydra方法，通过设计损失函数抑制<EOS>标记概率，并结合高效的关键位搜索算法，翻转模型参数的关键位。

Result: 在11个LLM上验证，仅需4个搜索样本和3次位翻转，即可使100%测试提示达到最大生成长度（如2048个标记）。

Conclusion: BitHydra展示了高效、可扩展和强迁移性的攻击能力，为LLM的安全性提出了新挑战。

Abstract: Large language models (LLMs) have shown impressive capabilities across a wide
range of applications, but their ever-increasing size and resource demands make
them vulnerable to inference cost attacks, where attackers induce victim LLMs
to generate the longest possible output content. In this paper, we revisit
existing inference cost attacks and reveal that these methods can hardly
produce large-scale malicious effects since they are self-targeting, where
attackers are also the users and therefore have to execute attacks solely
through the inputs, whose generated content will be charged by LLMs and can
only directly influence themselves. Motivated by these findings, this paper
introduces a new type of inference cost attacks (dubbed 'bit-flip inference
cost attack') that target the victim model itself rather than its inputs.
Specifically, we design a simple yet effective method (dubbed 'BitHydra') to
effectively flip critical bits of model parameters. This process is guided by a
loss function designed to suppress <EOS> token's probability with an efficient
critical bit search algorithm, thus explicitly defining the attack objective
and enabling effective optimization. We evaluate our method on 11 LLMs ranging
from 1.5B to 14B parameters under both int8 and float16 settings. Experimental
results demonstrate that with just 4 search samples and as few as 3 bit flips,
BitHydra can force 100% of test prompts to reach the maximum generation length
(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its
efficiency, scalability, and strong transferability across unseen inputs.

</details>


### [416] [Robust LLM Fingerprinting via Domain-Specific Watermarks](https://arxiv.org/abs/2505.16723)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.CR

TL;DR: 本文提出了一种针对开源语言模型的领域特定水印技术，用于解决模型溯源问题，确保检测可靠性并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着开源语言模型能力的提升和广泛使用，模型溯源（即识别模型实例的来源）变得日益重要。现有水印技术难以满足实际所有权检测的需求。

Method: 通过训练模型在特定子领域（如特定语言或主题）嵌入水印，而非所有生成内容，实现目标性水印嵌入。

Result: 评估表明，领域特定水印技术具有强统计保证、可控误报率、高检测能力，并保持生成质量。

Conclusion: 领域特定水印技术有效解决了模型溯源问题，具有隐蔽性和对实际部署场景的鲁棒性。

Abstract: As open-source language models (OSMs) grow more capable and are widely shared
and finetuned, ensuring model provenance, i.e., identifying the origin of a
given model instance, has become an increasingly important issue. At the same
time, existing backdoor-based model fingerprinting techniques often fall short
of achieving key requirements of real-world model ownership detection. In this
work, we build on the observation that while current open-source model
watermarks fail to achieve reliable content traceability, they can be
effectively adapted to address the challenge of model provenance. To this end,
we introduce the concept of domain-specific watermarking for model
fingerprinting. Rather than watermarking all generated content, we train the
model to embed watermarks only within specified subdomains (e.g., particular
languages or topics). This targeted approach ensures detection reliability,
while improving watermark durability and quality under a range of real-world
deployment settings. Our evaluations show that domain-specific watermarking
enables model fingerprinting with strong statistical guarantees, controllable
false positive rates, high detection power, and preserved generation quality.
Moreover, we find that our fingerprints are inherently stealthy and naturally
robust to real-world variability across deployment scenarios.

</details>


### [417] [When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques](https://arxiv.org/abs/2505.16765)
*Jianing Geng,Biao Yi,Zekun Fei,Tongxi Wu,Lihai Nie,Zheli Liu*

Main category: cs.CR

TL;DR: 论文提出了一种名为StegoAttack的全隐蔽越狱攻击方法，利用隐写术隐藏有害查询，成功率高且隐蔽性强。


<details>
  <summary>Details</summary>
Motivation: 研究越狱攻击对大型语言模型（LLM）的安全威胁，发现现有攻击难以同时实现毒性隐蔽和语言自然性。

Method: 提出StegoAttack，通过隐写术将有害查询隐藏在语义连贯的良性文本中，并引导LLM提取并加密响应。

Result: 在四种主流LLM上测试，平均攻击成功率达92.00%，隐蔽性优于现有方法。

Conclusion: StegoAttack展示了高效且隐蔽的越狱攻击能力，为LLM安全提供了新视角。

Abstract: Jailbreak attacks pose a serious threat to large language models (LLMs) by
bypassing built-in safety mechanisms and leading to harmful outputs. Studying
these attacks is crucial for identifying vulnerabilities and improving model
security. This paper presents a systematic survey of jailbreak methods from the
novel perspective of stealth. We find that existing attacks struggle to
simultaneously achieve toxic stealth (concealing toxic content) and linguistic
stealth (maintaining linguistic naturalness). Motivated by this, we propose
StegoAttack, a fully stealthy jailbreak attack that uses steganography to hide
the harmful query within benign, semantically coherent text. The attack then
prompts the LLM to extract the hidden query and respond in an encrypted manner.
This approach effectively hides malicious intent while preserving naturalness,
allowing it to evade both built-in and external safety mechanisms. We evaluate
StegoAttack on four safety-aligned LLMs from major providers, benchmarking
against eight state-of-the-art methods. StegoAttack achieves an average attack
success rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.
Its ASR drops by less than 1% even under external detection (e.g., Llama
Guard). Moreover, it attains the optimal comprehensive scores on stealth
detection metrics, demonstrating both high efficacy and exceptional stealth
capabilities. The code is available at
https://anonymous.4open.science/r/StegoAttack-Jail66

</details>


### [418] [CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models](https://arxiv.org/abs/2505.16785)
*Zhenzhen Ren,GuoBiao Li,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为CoTSRF的新颖LLM指纹方案，利用Chain of Thought（CoT）作为LLM的指纹，通过对比学习提取特征并进行验证。


<details>
  <summary>Details</summary>
Motivation: 开源大型语言模型（LLM）易被滥用，现有指纹方法无法提供隐蔽且鲁棒的验证。

Method: CoTSRF通过设计CoT查询收集源LLM的响应，训练CoT提取器提取指纹特征，并通过Kullback-Leibler散度进行验证。

Result: 实验证明CoTSRF在隐蔽性和鲁棒性方面优于现有方法。

Conclusion: CoTSRF为LLM指纹识别提供了一种高效且隐蔽的解决方案。

Abstract: Despite providing superior performance, open-source large language models
(LLMs) are vulnerable to abusive usage. To address this issue, recent works
propose LLM fingerprinting methods to identify the specific source LLMs behind
suspect applications. However, these methods fail to provide stealthy and
robust fingerprint verification. In this paper, we propose a novel LLM
fingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)
as the fingerprint of an LLM. CoTSRF first collects the responses from the
source LLM by querying it with crafted CoT queries. Then, it applies
contrastive learning to train a CoT extractor that extracts the CoT feature
(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint
verification by comparing the Kullback-Leibler divergence between the CoT
features of the source and suspect LLMs against an empirical threshold. Various
experiments have been conducted to demonstrate the advantage of our proposed
CoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint
verification.

</details>


### [419] [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
*Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: 论文提出了一种新的安全威胁：通过操纵LLMs的系统提示，使其在特定目标问题上输出恶意答案，同时在其他问题上表现正常。CAIN算法能在黑盒设置下自动生成此类有害提示，实验表明其攻击效果显著。


<details>
  <summary>Details</summary>
Motivation: LLMs易受对抗攻击，但现有研究未充分关注通过系统提示操纵AI-人类对话的危害。本文旨在揭示这种攻击的潜在威胁及其大规模信息操纵的可能性。

Method: 开发了CAIN算法，无需访问LLM参数即可在黑盒设置下自动生成针对特定问题的有害系统提示。

Result: 在开源和商业LLMs上，CAIN在非目标攻击中使目标问题F1下降40%，在目标攻击中达到70%的F1分数，且对正常问题影响极小。

Conclusion: 研究强调了提升LLMs鲁棒性的紧迫性，以确保其在实际应用中的安全性和完整性。

Abstract: Large language models (LLMs) have advanced many applications, but are also
known to be vulnerable to adversarial attacks. In this work, we introduce a
novel security threat: hijacking AI-human conversations by manipulating LLMs'
system prompts to produce malicious answers only to specific targeted questions
(e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"),
while behaving benignly on others. This attack is detrimental as it can enable
malicious actors to exercise large-scale information manipulation by spreading
harmful but benign-looking system prompts online. To demonstrate such an
attack, we develop CAIN, an algorithm that can automatically curate such
harmful system prompts for a specific target question in a black-box setting or
without the need to access the LLM's parameters. Evaluated on both open-source
and commercial LLMs, CAIN demonstrates significant adversarial impact. In
untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves
up to 40% F1 degradation on targeted questions while preserving high accuracy
on benign inputs. For targeted attacks or forcing LLMs to output specific
harmful answers, CAIN achieves over 70% F1 scores on these targeted responses
with minimal impact on benign questions. Our results highlight the critical
need for enhanced robustness measures to safeguard the integrity and safety of
LLMs in real-world applications. All source code will be publicly available.

</details>


### [420] [Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models](https://arxiv.org/abs/2505.16957)
*Junjie Xiong,Changjia Zhu,Shuhang Lin,Chong Zhang,Yongfeng Zhang,Yao Liu,Lingyao Li*

Main category: cs.CR

TL;DR: 研究探讨了大型语言模型（LLM）通过恶意字体注入在外部资源（如网页）中的隐藏对抗提示漏洞，揭示了其可能绕过安全机制的风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM具备实时网络搜索能力并与协议（如MCP）集成，新的安全漏洞可能被引入，亟需研究其潜在风险。

Method: 通过恶意字体注入操纵代码到字形映射，在外部资源中注入隐形欺骗内容，评估恶意内容中继和敏感数据泄漏两种攻击场景。

Result: 实验表明，间接提示结合恶意字体可绕过LLM安全机制，成功率因数据敏感性和提示设计而异。

Conclusion: 研究强调了在LLM处理外部内容时增强安全措施的紧迫性。

Abstract: Large Language Models (LLMs) are increasingly equipped with capabilities of
real-time web search and integrated with protocols like Model Context Protocol
(MCP). This extension could introduce new security vulnerabilities. We present
a systematic investigation of LLM vulnerabilities to hidden adversarial prompts
through malicious font injection in external resources like webpages, where
attackers manipulate code-to-glyph mapping to inject deceptive content which
are invisible to users. We evaluate two critical attack scenarios: (1)
"malicious content relay" and (2) "sensitive data leakage" through MCP-enabled
tools. Our experiments reveal that indirect prompts with injected malicious
font can bypass LLM safety mechanisms through external resources, achieving
varying success rates based on data sensitivity and prompt design. Our research
underscores the urgent need for enhanced security measures in LLM deployments
when processing external content.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [421] [Dynamic Reservoir Computing with Physical Neuromorphic Networks](https://arxiv.org/abs/2505.16813)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cs.ET

TL;DR: 研究了物理纳米电子网络作为动态储层在储层计算中的应用，发现稀疏网络比密集网络更能生成有用的非线性时间输出，并在混沌时间序列预测任务中验证了稀疏网络的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索物理纳米电子网络作为动态储层在储层计算中的潜力，理解其结构和动力学特性。

Method: 利用具有神经形态动力学的物理纳米电子网络作为动态储层，研究不同稀疏度网络对非线性时间输出的影响，并在混沌时间序列预测任务中进行测试。

Result: 稀疏网络比密集网络更能生成有用的非线性时间输出，且在混沌时间序列预测任务中表现出更好的性能，能够学习Lorenz63系统的吸引子行为。

Conclusion: 网络稀疏度对动态储层计算的性能至关重要，稀疏网络更适合用于非线性时间信号处理和混沌系统预测。

Abstract: Reservoir Computing (RC) with physical systems requires an understanding of
the underlying structure and internal dynamics of the specific physical
reservoir. In this study, physical nano-electronic networks with neuromorphic
dynamics are investigated for their use as physical reservoirs in an RC
framework. These neuromorphic networks operate as dynamic reservoirs, with node
activities in general coupled to the edge dynamics through nonlinear
nano-electronic circuit elements, and the reservoir outputs influenced by the
underlying network connectivity structure. This study finds that networks with
varying degrees of sparsity generate more useful nonlinear temporal outputs for
dynamic RC compared to dense networks. Dynamic RC is also tested on an
autonomous multivariate chaotic time series prediction task with networks of
varying densities, which revealed the importance of network sparsity in
maintaining network activity and overall dynamics, that in turn enabled the
learning of the chaotic Lorenz63 system's attractor behavior.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [422] [SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet](https://arxiv.org/abs/2505.16195)
*Zhi Zhong,Akira Takahashi,Shuyang Cui,Keisuke Toyama,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: SpecMaskFoley是一种通过ControlNet引导预训练音频生成模型进行视频同步Foley合成的方法，性能优于从头训练的基线模型。


<details>
  <summary>Details</summary>
Motivation: Foley合成在创意产业中应用广泛，但现有ControlNet方法依赖手工时间条件，性能不如从头训练模型。

Method: 提出SpecMaskFoley，利用频率感知时间特征对齐器解决视频特征与音频时间频率特性的不匹配问题。

Result: 在基准测试中，SpecMaskFoley性能优于从头训练的基线模型。

Conclusion: SpecMaskFoley显著提升了ControlNet在Foley合成中的应用潜力。

Abstract: Foley synthesis aims to synthesize high-quality audio that is both
semantically and temporally aligned with video frames. Given its broad
application in creative industries, the task has gained increasing attention in
the research community. To avoid the non-trivial task of training audio
generative models from scratch, adapting pretrained audio generative models for
video-synchronized foley synthesis presents an attractive direction.
ControlNet, a method for adding fine-grained controls to pretrained generative
models, has been applied to foley synthesis, but its use has been limited to
handcrafted human-readable temporal conditions. In contrast, from-scratch
models achieved success by leveraging high-dimensional deep features extracted
using pretrained video encoders. We have observed a performance gap between
ControlNet-based and from-scratch foley models. To narrow this gap, we propose
SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward
video-synchronized foley synthesis via ControlNet. To unlock the potential of a
single ControlNet branch, we resolve the discrepancy between the temporal video
features and the time-frequency nature of the pretrained SpecMaskGIT via a
frequency-aware temporal feature aligner, eliminating the need for complicated
conditioning mechanisms widely used in prior arts. Evaluations on a common
foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform
strong from-scratch baselines, substantially advancing the development of
ControlNet-based foley synthesis models. Demo page:
https://zzaudio.github.io/SpecMaskFoley_Demo/

</details>


### [423] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
*Kai Li,Can Shen,Yile Liu,Jirui Han,Kelong Zheng,Xuechao Zou,Zhe Wang,Xingjian Du,Shun Zhang,Hanjun Luo,Yingbin Jin,Xinxin Xing,Ziyang Ma,Yue Liu,Xiaojun Jia,Yifan Zhang,Junfeng Fang,Kun Wang,Yibo Yan,Haoyang Li,Yiming Li,Xiaobin Zhuang,Yang Liu,Haibo Hu,Zhuo Chen,Zhizheng Wu,Xiaolin Hu,Eng-Siong Chng,XiaoFeng Wang,Wenyuan Xu,Wei Dong,Xinfeng Li*

Main category: cs.SD

TL;DR: AudioTrust是一个专门为音频大语言模型（ALLMs）设计的全方位可信度评估框架和基准，覆盖六个关键维度，并基于18种实验设置和4,420个样本数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注文本模态或有限的安全维度，未能充分应对音频模态的独特特性和应用场景，因此需要系统研究ALLMs的可信度。

Method: AudioTrust设计了18种实验设置和9种音频特定评估指标，使用大规模自动化管道对模型输出进行客观评分。

Result: 实验揭示了当前开源和闭源ALLMs在高风险音频场景中的可信度边界和局限性。

Conclusion: AudioTrust为未来音频模型的安全和可信部署提供了宝贵见解，平台和基准已开源。

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


### [424] [Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System](https://arxiv.org/abs/2505.16259)
*Hayeon Bang,Taegyun Kwon,Juhan Nam*

Main category: cs.SD

TL;DR: 论文提出了一种结合实时自动音乐转录的交互式音乐作品，平衡了作曲结构与动态互动。


<details>
  <summary>Details</summary>
Motivation: 探索一种结合固定作曲结构与实时动态互动的新方法，超越传统的即兴互动。

Method: 通过实时自动转录技术，计算机实时解读并回应人类演奏者的输入，形成音乐对话。

Result: 开发了一个从作曲到首演的全过程框架，包括技术实现、排练和演出考量。

Conclusion: 该作品成功实现了作曲意图与实时互动的平衡，并融入了不可预测性元素。

Abstract: This paper presents <Dialogue in Resonance>, an interactive music piece for a
human pianist and a computer-controlled piano that integrates real-time
automatic music transcription into a score-driven framework. Unlike previous
approaches that primarily focus on improvisation-based interactions, our work
establishes a balanced framework that combines composed structure with dynamic
interaction. Through real-time automatic transcription as its core mechanism,
the computer interprets and responds to the human performer's input in real
time, creating a musical dialogue that balances compositional intent with live
interaction while incorporating elements of unpredictability. In this paper, we
present the development process from composition to premiere performance,
including technical implementation, rehearsal process, and performance
considerations.

</details>


### [425] [Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models](https://arxiv.org/abs/2505.16306)
*Yizhi Zhou,Haina Zhu,Hangting Chen*

Main category: cs.SD

TL;DR: 分析MusicFM和MuQ两种音乐表示模型，验证自监督学习（SSL）模型的优势，探索分层信息的任务特异性，并比较不同层的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探究SSL模型在音乐信息检索中的编码信息含义及其适用性，以更有效地利用这些模型。

Method: 分析MusicFM和MuQ模型，验证SSL模型在多任务中的优势，研究分层信息的任务特异性，并比较不同层的性能。

Result: 揭示了SSL模型在音乐信息检索中的结构和潜在应用。

Conclusion: 研究为理解SSL模型的能力和局限性提供了见解，有助于更有效地应用于下游任务。

Abstract: Recently, pre-trained models for music information retrieval based on
self-supervised learning (SSL) are becoming popular, showing success in various
downstream tasks. However, there is limited research on the specific meanings
of the encoded information and their applicability. Exploring these aspects can
help us better understand their capabilities and limitations, leading to more
effective use in downstream tasks.
  In this study, we analyze the advanced music representation model MusicFM and
the newly emerged SSL model MuQ. We focus on three main aspects: (i) validating
the advantages of SSL models across multiple downstream tasks, (ii) exploring
the specialization of layer-wise information for different tasks, and (iii)
comparing performance differences when selecting specific layers. Through this
analysis, we reveal insights into the structure and potential applications of
SSL models in music information retrieval.

</details>


### [426] [EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion](https://arxiv.org/abs/2505.16691)
*Advait Joglekar,Divyanshu Singh,Rooshil Rohit Bhatia,S. Umesh*

Main category: cs.SD

TL;DR: 论文提出了一种结合自监督离散语音表示和非自回归扩散Transformer的条件流匹配解码器的方法，用于零样本跨语言语音转换。


<details>
  <summary>Details</summary>
Motivation: 当前语音转换方法在零样本跨语言场景中表现不佳，难以泛化到未见过的语言和口音。

Method: 采用自监督离散语音表示与非自回归扩散Transformer的条件流匹配解码器结合，无需多编码器分离语音特征。

Result: 模型在零样本跨语言场景中表现优异，包括未见过的语言。

Conclusion: 该方法简单有效，实现了纯无文本自监督训练，提升了零样本跨语言语音转换能力。

Abstract: Voice Conversion research in recent times has increasingly focused on
improving the zero-shot capabilities of existing methods. Despite remarkable
advancements, current architectures still tend to struggle in zero-shot
cross-lingual settings. They are also often unable to generalize for speakers
of unseen languages and accents. In this paper, we adopt a simple yet effective
approach that combines discrete speech representations from self-supervised
models with a non-autoregressive Diffusion-Transformer based conditional flow
matching speech decoder. We show that this architecture allows us to train a
voice-conversion model in a purely textless, self-supervised fashion. Our
technique works without requiring multiple encoders to disentangle speech
features. Our model also manages to excel in zero-shot cross-lingual settings
even for unseen languages.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [427] [Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space](https://arxiv.org/abs/2505.16301)
*Fuchun Ge,Pavlo O. Dral*

Main category: physics.chem-ph

TL;DR: MDtrajNet-1是一种基于AI的模型，直接生成分子动力学轨迹，绕过传统力计算和积分，加速模拟效率。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学（MD）依赖顺序数值积分，效率受限。

Method: 结合等变神经网络和Transformer架构，直接预测轨迹。

Result: 模拟速度提升两个数量级，误差接近传统ab initio MD。

Conclusion: MDtrajNet-1突破了传统MD的速度限制，为高效原子模拟开辟新途径。

Abstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of
atomistic systems, but its reliance on sequential numerical integration limits
simulation efficiency. We present MDtrajNet-1, a foundational AI model that
directly generates MD trajectories across chemical space, bypassing force
calculations and integration. This approach accelerates simulations by up to
two orders of magnitude compared to traditional MD, even those enhanced by
machine-learning interatomic potentials. MDtrajNet-1 combines equivariant
neural networks with a Transformer-based architecture to achieve strong
accuracy and transferability in predicting long-time trajectories for both
known and unseen systems. Remarkably, the errors of the trajectories generated
by MDtrajNet-1 for various molecular systems are close to those of the
conventional ab initio MD. The model's flexible design supports diverse
application scenarios, including different statistical ensembles, boundary
conditions, and interaction types. By overcoming the intrinsic speed barrier of
conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable
atomistic simulations.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [428] [Multi-omic Causal Discovery using Genotypes and Gene Expression](https://arxiv.org/abs/2505.15866)
*Stephen Asiedu,David Watson*

Main category: q-bio.GN

TL;DR: GENESIS是一种基于约束的算法，利用基因型的自然因果优先性推断转录组数据中的祖先关系，解决了高维度和隐藏混杂因素的挑战。


<details>
  <summary>Details</summary>
Motivation: 多组学数据中的因果发现对理解基因调控机制至关重要，但面临高维度、直接与间接关系区分及隐藏混杂因素的挑战。

Method: GENESIS通过初始化空祖先矩阵，逐步填充直接、间接或非因果关系，结合基因型作为固定因果锚点，优化传统因果发现算法。

Result: 在合成和真实基因组数据集上测试，GENESIS能有效揭示复杂性状中的因果路径。

Conclusion: GENESIS为功能基因组学、药物发现和精准医学提供了强大的因果发现工具。

Abstract: Causal discovery in multi-omic datasets is crucial for understanding the
bigger picture of gene regulatory mechanisms, but remains challenging due to
high dimensionality, differentiation of direct from indirect relationships, and
hidden confounders. We introduce GENESIS (GEne Network inference from
Expression SIgnals and SNPs), a constraint-based algorithm that leverages the
natural causal precedence of genotypes to infer ancestral relationships in
transcriptomic data. Unlike traditional causal discovery methods that start
with a fully connected graph, GENESIS initialises an empty ancestrality matrix
and iteratively populates it with direct, indirect or non-causal relationships
using a series of provably sound marginal and conditional independence tests.
By integrating genotypes as fixed causal anchors, GENESIS provides a principled
``head start'' to classical causal discovery algorithms, restricting the search
space to biologically plausible edges. We test GENESIS on synthetic and
real-world genomic datasets. This framework offers a powerful avenue for
uncovering causal pathways in complex traits, with promising applications to
functional genomics, drug discovery, and precision medicine.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [429] [Utilizing citation index and synthetic quality measure to compare Wikipedia languages across various topics](https://arxiv.org/abs/2505.16506)
*Włodzimierz Lewoniewski,Krzysztof Węcel,Witold Abramowicz*

Main category: cs.IR

TL;DR: 比较分析了55种维基百科语言版本的引用指数和合成质量指标，揭示了不同语言版本在内容覆盖和质量上的差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过引用指数和质量评分，比较不同语言维基百科的内容质量和覆盖范围，揭示其优势和不足。

Method: 基于维基百科文章间的内部链接构建引用指数，处理了66亿条链接记录，并使用0到100的合成质量评分进行跨语言比较。

Result: 结果显示不同语言版本的维基百科在内容覆盖和质量上存在显著差异。

Conclusion: 研究为跨语言维基百科的质量评估提供了新方法，并揭示了内容覆盖的不足。

Abstract: This study presents a comparative analysis of 55 Wikipedia language editions
employing a citation index alongside a synthetic quality measure. Specifically,
we identified the most significant Wikipedia articles within distinct topical
areas, selecting the top 10, top 25, and top 100 most cited articles in each
topic and language version. This index was built on the basis of wikilinks
between Wikipedia articles in each language version and in order to do that we
processed 6.6 billion page-to-page link records. Next, we used a quality score
for each Wikipedia article - a synthetic measure scaled from 0 to 100. This
approach enabled quality comparison of Wikipedia articles even between language
versions with different quality grading schemes. Our results highlight
disparities among Wikipedia language editions, revealing strengths and gaps in
content coverage and quality across topics.

</details>


### [430] [DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management](https://arxiv.org/abs/2505.15856)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Lipai Huang,Yiming Xiao,Zhewei Liu,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: DisastIR是首个专为灾害管理设计的综合信息检索评估基准，包含9600个查询和130万标记查询-段落对，覆盖48个检索任务。评估显示现有模型在灾害管理任务中表现不一，需专用基准指导模型选择。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索基准未涵盖灾害管理的独特语言复杂性和多样化信息需求，亟需专用基准。

Method: 提出DisastIR基准，包含多样化查询和标记数据，评估30种先进检索模型。

Result: 模型表现差异显著，无单一模型在所有任务中表现优异，灾害管理任务与通用任务存在性能差距。

Conclusion: 灾害管理专用基准对指导模型选择和提升灾害管理决策效率至关重要。

Abstract: Effective disaster management requires timely access to accurate and
contextually relevant information. Existing Information Retrieval (IR)
benchmarks, however, focus primarily on general or specialized domains, such as
medicine or finance, neglecting the unique linguistic complexity and diverse
information needs encountered in disaster management scenarios. To bridge this
gap, we introduce DisastIR, the first comprehensive IR evaluation benchmark
specifically tailored for disaster management. DisastIR comprises 9,600 diverse
user queries and more than 1.3 million labeled query-passage pairs, covering 48
distinct retrieval tasks derived from six search intents and eight general
disaster categories that include 301 specific event types. Our evaluations of
30 state-of-the-art retrieval models demonstrate significant performance
variances across tasks, with no single model excelling universally.
Furthermore, comparative analyses reveal significant performance gaps between
general-domain and disaster management-specific tasks, highlighting the
necessity of disaster management-specific benchmarks for guiding IR model
selection to support effective decision-making in disaster management
scenarios. All source codes and DisastIR are available at
https://github.com/KaiYin97/Disaster_IR.

</details>


### [431] [AutoData: A Multi-Agent System for Open Web Data Collection](https://arxiv.org/abs/2505.15859)
*Tianyi Ma,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Xiaoye Qian,Feifan Bai,Yifan Ding,Xuwei Luo,Shinan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.IR

TL;DR: AutoData是一个多代理系统，用于自动化网页数据收集，减少人工干预，并通过创新的超图架构和缓存系统提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有网页数据收集方法存在适应性差、成本高的问题，需要更高效、低成本的解决方案。

Method: 提出AutoData系统，采用多代理架构和超图缓存技术，仅需自然语言指令即可完成数据收集。

Result: 在Instruct2DS等数据集上表现优于基线方法，验证了其高效性和适用性。

Conclusion: AutoData为自动化数据收集提供了高效、低成本的解决方案，具有广泛的应用潜力。

Abstract: The exponential growth of data-driven systems and AI technologies has
intensified the demand for high-quality web-sourced datasets. While existing
datasets have proven valuable, conventional web data collection approaches face
significant limitations in terms of human effort and scalability. Current
data-collecting solutions fall into two categories: wrapper-based methods that
struggle with adaptability and reproducibility, and large language model
(LLM)-based approaches that incur substantial computational and financial
costs. To address these challenges, we propose AutoData, a novel multi-agent
system for Automated web Data collection, that requires minimal human
intervention, i.e., only necessitating a natural language instruction
specifying the desired dataset. In addition, AutoData is designed with a robust
multi-agent architecture, featuring a novel oriented message hypergraph
coordinated by a central task manager, to efficiently organize agents across
research and development squads. Besides, we introduce a novel hypergraph cache
system to advance the multi-agent collaboration process that enables efficient
automated data collection and mitigates the token cost issues prevalent in
existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark
dataset supporting live data collection from web sources across three domains:
academic, finance, and sports. Comprehensive evaluations over Instruct2DS and
three existing benchmark datasets demonstrate AutoData's superior performance
compared to baseline methods. Case studies on challenging tasks such as picture
book collection and paper extraction from surveys further validate its
applicability. Our source code and dataset are available at
https://github.com/GraphResearcher/AutoData.

</details>


### [432] [Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods](https://arxiv.org/abs/2505.16466)
*Meng Yan,Cai Xu,Xujing Wang,Ziyu Guan,Wei Zhao,Yuhang Zhou*

Main category: cs.IR

TL;DR: 论文提出了一种基于图神经网络的推荐系统（Conf-GNNRec），用于量化并校准预测置信度，解决噪声和过自信问题。


<details>
  <summary>Details</summary>
Motivation: 现实推荐场景中，噪声（如用户误用和恶意广告）通过消息传播机制积累，导致预测结果不可信，且现有方法存在过自信问题。

Method: 提出动态评分校准方法和置信度损失函数，基于用户个性化调整评分并减少负样本的过自信。

Result: 在公开数据集上验证了Conf-GNNRec在预测置信度和推荐性能上的有效性。

Conclusion: Conf-GNNRec能有效解决噪声和过自信问题，提升推荐系统的可信度和性能。

Abstract: Recommender systems based on graph neural networks perform well in tasks such
as rating and ranking. However, in real-world recommendation scenarios, noise
such as user misuse and malicious advertisement gradually accumulates through
the message propagation mechanism. Even if existing studies mitigate their
effects by reducing the noise propagation weights, the severe sparsity of the
recommender system still leads to the low-weighted noisy neighbors being
mistaken as meaningful information, and the prediction result obtained based on
the polluted nodes is not entirely trustworthy. Therefore, it is crucial to
measure the confidence of the prediction results in this highly noisy
framework. Furthermore, our evaluation of the existing representative GNN-based
recommendation shows that it suffers from overconfidence. Based on the above
considerations, we propose a new method to quantify and calibrate the
prediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,
we propose a rating calibration method that dynamically adjusts excessive
ratings to mitigate overconfidence based on user personalization. We also
design a confidence loss function to reduce the overconfidence of negative
samples and effectively improve recommendation performance. Experiments on
public datasets demonstrate the validity of Conf-GNNRec in prediction
confidence and recommendation performance.

</details>


### [433] [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
*Kuicai Dong,Yujing Chang,Shijie Huang,Yasheng Wang,Ruiming Tang,Yong Liu*

Main category: cs.IR

TL;DR: MMDocRAG是一个新的DocVQA基准，包含4055个专家标注的QA对，支持多模态证据链评估，并揭示了专有LVMs在性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决DocVQA中多模态文档处理和跨模态推理的挑战，以及现有方法对视觉信息的忽视和缺乏评估基准的问题。

Method: 提出MMDocRAG基准，包含多页、跨模态证据链的QA对，并设计新指标评估多模态引用选择和整合。

Result: 专有LVMs表现优于开源模型，多模态输入对性能有中等提升，而开源模型性能显著下降。微调LLMs通过详细图像描述获得显著改进。

Conclusion: MMDocRAG为多模态DocVQA系统提供了严格的测试环境和实用建议，推动了该领域的发展。

Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.

</details>


### [434] [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
*Nour Jedidi,Yung-Sung Chuang,James Glass,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现，基于推理的段落重排模型（ReasonRR）在相同训练条件下表现不如非推理的标准模型（StandardRR），且禁用推理功能后（ReasonRR-NoReason）效果反而更好。


<details>
  <summary>Details</summary>
Motivation: 探索推理能力是否能提升基于大语言模型（LLM）的段落重排准确性。

Method: 比较推理型（ReasonRR）与非推理型（StandardRR）重排模型，并进一步分析推理对ReasonRR的影响。

Result: StandardRR优于ReasonRR，而ReasonRR-NoReason表现更佳，表明推理过程限制了模型性能。

Conclusion: 推理型重排模型因LLM的推理过程导致评分极化，忽略了部分相关性，从而影响准确性。

Abstract: With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.

</details>


### [435] [Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation](https://arxiv.org/abs/2505.16752)
*Hao Guo,Erpeng Xue,Lei Huang,Shichao Wang,Xiaolei Wang,Lei Wang,Jinpeng Wang,Sheng Chen*

Main category: cs.IR

TL;DR: DFGR是一种双流生成排序网络，通过改进自注意力机制中的交互模式，提升推荐系统的训练和推理效率，解决了传统方法中向量空间映射不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 解决Meta的HSTU生成推荐方法中因异构信息映射到相同向量空间导致的训练不稳定问题，同时减少对人工特征工程的依赖。

Method: 采用双流架构，结合用户历史行为序列和少量属性信息，优化自注意力机制中的QKV模块交互模式。

Result: 在开源和工业数据集上表现优于DIN、DCN、DIEN和DeepFM等基线模型，并在计算约束下研究了最优参数分配策略。

Conclusion: DFGR是一种高效且有效的新一代生成排序范式，适用于推荐系统。

Abstract: We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream
architecture designed for recommendation systems. DFGR integrates innovative
interaction patterns between real and fake flows within the QKV modules of the
self-attention mechanism, enhancing both training and inference efficiency.
This approach effectively addresses a key limitation observed in Meta's
proposed HSTU generative recommendation approach, where heterogeneous
information volumes are mapped into identical vector spaces, leading to
training instability. Unlike traditional recommendation models, DFGR only
relies on user history behavior sequences and minimal attribute information,
eliminating the need for extensive manual feature engineering. Comprehensive
evaluations on open-source and industrial datasets reveal DFGR's superior
performance compared to established baselines such as DIN, DCN, DIEN, and
DeepFM. We also investigate optimal parameter allocation strategies under
computational constraints, establishing DFGR as an efficient and effective
next-generation generate ranking paradigm.

</details>


### [436] [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
*Nandan Thakur,Crystina Zhang,Xueguang Ma,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现某些数据集可能对检索模型效果产生负面影响，提出了一种通过LLM提示识别和重新标记假负例的方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模检索数据集（如BGE）中可能存在假负例，影响模型训练效果，需要一种高效的方法来改进数据质量。

Method: 采用级联LLM提示的方法识别并重新标记假负例，将其更正为真阳性。

Result: 重新标记后的数据显著提升了E5和Qwen2.5-7B检索模型的性能（nDCG@10提升0.7-1.4点），并在零样本评估中表现更优。

Conclusion: 通过改进数据质量（尤其是假负例）可以显著提升检索和重排序模型的性能，级联LLM提示是一种有效且可靠的方法。

Abstract: Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- pruning 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on "false
negatives", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.

</details>


### [437] [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
*Runyang You,Yongqi Li,Xinyu Lin,Xin Zhang,Wenjie Wang,Wenjie Li,Liqiang Nie*

Main category: cs.IR

TL;DR: 论文提出了一种统一的大型推荐模型\name，具备内在推理能力，通过重新设计架构和强化学习框架RecPO，实现了推理与推荐的联合优化，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究将LLMs作为外部推理模块，导致资源成本高且联合优化不足，需要一种统一的模型来解决这些问题。

Method: 重新设计模型架构以实现推理与推荐的自回归交织，并提出强化学习框架RecPO，通过融合奖励方案优化推理和推荐能力。

Result: 在三个数据集上的实验显示，\name在Hit@5和NDCG@20指标上分别相对提升了68.67%和45.21%。

Conclusion: \name通过统一架构和强化学习框架，显著提升了推荐系统的性能，同时减少了对外部推理标注的依赖。

Abstract: Large recommender models have extended LLMs as powerful recommenders via
encoding or item generation, and recent breakthroughs in LLM reasoning
synchronously motivate the exploration of reasoning in recommendation. Current
studies usually position LLMs as external reasoning modules to yield auxiliary
thought for augmenting conventional recommendation pipelines. However, such
decoupled designs are limited in significant resource cost and suboptimal joint
optimization. To address these issues, we propose \name, a unified large
recommender model with intrinsic reasoning capabilities. Initially, we
reconceptualize the model architecture to facilitate interleaved reasoning and
recommendation in the autoregressive process. Subsequently, we propose RecPO, a
corresponding reinforcement learning framework that optimizes \name\ both the
reasoning and recommendation capabilities simultaneously in a single policy
update; RecPO introduces a fused reward scheme that solely leverages
recommendation labels to simulate the reasoning capability, eliminating
dependency on specialized reasoning annotations. Experiments on three datasets
with various baselines verify the effectiveness of \name, showing relative
improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at
https://github.com/YRYangang/RRec.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [438] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
*Chih-Kai Yang,Neo S. Ho,Hung-yi Lee*

Main category: eess.AS

TL;DR: 本文提出了一种针对大型音频语言模型（LALMs）评估的系统分类法，填补了现有评估标准的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs评估标准分散且缺乏系统性分类，阻碍了模型的全面评估与发展。

Method: 通过全面调查，提出四维分类法：通用听觉感知与处理、知识与推理、对话导向能力、公平性与安全性。

Result: 提供了详细的分类概述，并指出了该领域的挑战与未来方向。

Conclusion: 这是首个专注于LALMs评估的调查，为社区提供了明确指南，并将持续更新支持领域发展。

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [439] [Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection](https://arxiv.org/abs/2505.16351)
*Chenxu Guo,Jiachen Lian,Xuanru Zhou,Jinming Zhang,Shuhe Li,Zongli Ye,Hwi Joo Park,Anaisha Das,Zoe Ezzes,Jet Vonk,Brittany Morin,Rian Bogley,Lisa Wauters,Zachary Miller,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 论文提出了一种零样本解码器Dysfluent-WFST，用于同时转录音素和检测言语不流畅，无需额外训练，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在言语不流畅检测中分类能力有限且缺乏临床洞察力，文本无关模型在上下文相关情况下容易误分类。

Method: Dysfluent-WFST是一种零样本解码器，结合上游编码器（如WavLM），无需额外训练，通过显式建模发音行为实现高效解码。

Result: 在模拟和真实语音数据上，Dysfluent-WFST在音素错误率和不流畅检测方面达到最优性能。

Conclusion: 显式建模发音行为而非复杂架构是改进言语不流畅处理系统的关键，Dysfluent-WFST轻量、可解释且高效。

Abstract: Automatic detection of speech dysfluency aids speech-language pathologists in
efficient transcription of disordered speech, enhancing diagnostics and
treatment planning. Traditional methods, often limited to classification,
provide insufficient clinical insight, and text-independent models misclassify
dysfluency, especially in context-dependent cases. This work introduces
Dysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes
and detects dysfluency. Unlike previous models, Dysfluent-WFST operates with
upstream encoders like WavLM and requires no additional training. It achieves
state-of-the-art performance in both phonetic error rate and dysfluency
detection on simulated and real speech data. Our approach is lightweight,
interpretable, and effective, demonstrating that explicit modeling of
pronunciation behavior in decoding, rather than complex architectures, is key
to improving dysfluency processing systems.

</details>


### [440] [Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation](https://arxiv.org/abs/2505.16044)
*Gowtham Premananth,Philip Resnik,Sonia Bansal,Deanna L. Kelly,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 该研究提出了一种多模态方法，通过整合语音、视频和文本来估计精神分裂症个体症状的严重程度，超越了传统的二元分类方法，提高了临床适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的精神分裂症评估方法将其简化为二元分类任务，忽略了症状的复杂性，降低了临床实用性。本研究旨在通过多模态方法更详细地捕捉症状特征。

Method: 开发了单模态模型（语音、视频、文本）和多模态框架，以提高准确性和鲁棒性。

Result: 该方法能够更详细地估计症状严重程度，提升诊断精度并支持个性化治疗。

Conclusion: 多模态方法为精神健康评估提供了可扩展且客观的工具，具有更高的临床价值。

Abstract: Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.

</details>


### [441] [Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting](https://arxiv.org/abs/2505.16735)
*Youngmoon Jung,Yong-Hyeok Lee,Myunghun Jung,Jaeyoung Roh,Chang Woo Han,Hoon-Young Cho*

Main category: eess.AS

TL;DR: 论文提出了一种基于模态对抗学习（MAL）的方法，用于减少音频和文本模态之间的领域差距，并通过深度度量学习（DML）实现音素级对齐。


<details>
  <summary>Details</summary>
Motivation: 解决音频和文本模态之间的异构性问题，以便在多模态嵌入空间中进行直接比较。

Method: 结合模态对抗学习（MAL）和深度度量学习（DML），训练模态分类器以生成模态不变的嵌入，并实现音素级对齐。

Result: 在WSJ和LibriPhrase数据集上的实验证明了该方法的有效性。

Conclusion: 提出的MAL和DML结合方法能够有效减少模态间的领域差距，提升多模态嵌入空间的性能。

Abstract: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic
and text embeddings are typically compared at either the phoneme or utterance
level. To facilitate this, we optimize acoustic and text encoders using deep
metric learning (DML), enabling direct comparison of multi-modal embeddings in
a shared embedding space. However, the inherent heterogeneity between audio and
text modalities presents a significant challenge. To address this, we propose
Modality Adversarial Learning (MAL), which reduces the domain gap in
heterogeneous modality representations. Specifically, we train a modality
classifier adversarially to encourage both encoders to generate
modality-invariant embeddings. Additionally, we apply DML to achieve
phoneme-level alignment between audio and text, and conduct comprehensive
comparisons across various DML objectives. Experiments on the Wall Street
Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the
proposed approach.

</details>


### [442] [SEED: Speaker Embedding Enhancement Diffusion Model](https://arxiv.org/abs/2505.16798)
*KiHyun Nam,Jungwoo Heo,Jee-weon Jung,Gangin Park,Chaeyoung Jung,Ha-Jin Yu,Joon Son Chung*

Main category: eess.AS

TL;DR: 提出了一种基于扩散模型的方法，用于优化说话人识别系统中的嵌入表示，显著提升环境不匹配场景下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，说话人识别系统因环境不匹配导致性能下降，亟需解决方案。

Method: 通过扩散模型对干净和噪声语音的嵌入表示进行噪声添加和重构，无需修改现有识别流程。

Result: 在模拟环境不匹配场景中，识别准确率最高提升19.6%，且常规场景性能不受影响。

Conclusion: 该方法有效解决了环境不匹配问题，且易于部署。

Abstract: A primary challenge when deploying speaker recognition systems in real-world
applications is performance degradation caused by environmental mismatch. We
propose a diffusion-based method that takes speaker embeddings extracted from a
pre-trained speaker recognition model and generates refined embeddings. For
training, our approach progressively adds Gaussian noise to both clean and
noisy speaker embeddings extracted from clean and noisy speech, respectively,
via forward process of a diffusion model, and then reconstructs them to clean
embeddings in the reverse process. While inferencing, all embeddings are
regenerated via diffusion process. Our method needs neither speaker label nor
any modification to the existing speaker recognition pipeline. Experiments on
evaluation sets simulating environment mismatch scenarios show that our method
can improve recognition accuracy by up to 19.6% over baseline models while
retaining performance on conventional scenarios. We publish our code here
https://github.com/kaistmm/seed-pytorch

</details>


### [443] [Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate](https://arxiv.org/abs/2505.16845)
*Hanglei Zhang,Yiwei Guo,Zhihan Li,Xiang Hao,Xie Chen,Kai Yu*

Main category: eess.AS

TL;DR: 该论文提出了一种时间灵活编码（TFC）技术，首次将可变帧率（VFR）引入神经语音编解码器，以解决固定帧率（CFR）在信息密度变化时的效率问题。


<details>
  <summary>Details</summary>
Motivation: 语音段的信息密度随时间变化（如静音与有声区域），固定帧率在比特率和令牌序列长度上效率不高，影响实时应用。

Method: 提出TFC技术，通过动态分配帧率基于时间熵，实现可调的平均帧率。

Result: 实验表明，TFC编解码器在重建质量和灵活性上表现优异，即使在低帧率下仍具竞争力。

Conclusion: TFC为低帧率神经语音编解码器的开发提供了新思路，有望提升下游任务效率。

Abstract: Most neural speech codecs achieve bitrate adjustment through intra-frame
mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,
speech segments inherently have time-varying information density (e.g., silent
intervals versus voiced regions). This property makes CFR not optimal in terms
of bitrate and token sequence length, hindering efficiency in real-time
applications. In this work, we propose a Temporally Flexible Coding (TFC)
technique, introducing variable frame rate (VFR) into neural speech codecs for
the first time. TFC enables seamlessly tunable average frame rates and
dynamically allocates frame rates based on temporal entropy. Experimental
results show that a codec with TFC achieves optimal reconstruction quality with
high flexibility, and maintains competitive performance even at lower frame
rates. Our approach is promising for the integration with other efforts to
develop low-frame-rate neural speech codecs for more efficient downstream
tasks.

</details>


### [444] [Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation](https://arxiv.org/abs/2505.16911)
*Ofir Yaish,Yehuda Mishaly,Eliya Nachmani*

Main category: eess.AS

TL;DR: 提出了一种新的主动语音增强（ASE）范式，结合Transformer-Mamba架构和任务特定损失函数，在去噪、去混响和去削波等任务中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统主动噪声消除（ANC）仅抑制外部干扰，而ASE进一步通过主动调整语音信号（抑制噪声并增强语音相关频率）提升可懂度和感知质量。

Method: 提出基于Transformer-Mamba的架构和任务特定损失函数，联合优化干扰抑制和信号增强。

Result: 在去噪、去混响和去削波等任务中表现优于现有基线，验证了主动目标调制在复杂声学环境中的有效性。

Conclusion: ASE通过主动信号调制显著提升语音处理性能，为复杂环境中的语音增强提供了新思路。

Abstract: We introduce a new paradigm for active sound modification: Active Speech
Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on
suppressing external interference, ASE goes further by actively shaping the
speech signal -- both attenuating unwanted noise components and amplifying
speech-relevant frequencies -- to improve intelligibility and perceptual
quality. To enable this, we propose a novel Transformer-Mamba-based
architecture, along with a task-specific loss function designed to jointly
optimize interference suppression and signal enrichment. Our method outperforms
existing baselines across multiple speech processing tasks -- including
denoising, dereverberation, and declipping -- demonstrating the effectiveness
of active, targeted modulation in challenging acoustic environments.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [445] [Physics-based machine learning for mantle convection simulations](https://arxiv.org/abs/2505.16041)
*Siddhant Agarwal,Ali Can Bekar,Christian Hüttig,David S. Greenberg,Nicola Tosi*

Main category: astro-ph.EP

TL;DR: 论文提出了一种基于物理的机器学习方法，用于预测地幔对流中的流速，从而绕过数值求解斯托克斯问题的计算挑战。该方法比传统数值求解器快89倍。


<details>
  <summary>Details</summary>
Motivation: 地幔对流模拟对理解岩石行星演化至关重要，但输入参数未知、传输性质的非线性依赖及长时间积分等问题带来了计算挑战。

Method: 采用基于物理的机器学习方法预测流速，结合有限体积求解器推进温度场，并通过卷积神经网络实现质量守恒等关键功能。

Result: 模型比传统数值求解器快89倍，并通过测试验证了其在未见场景中的表现。

Conclusion: 该方法显著提升了计算效率，同时展示了机器学习在地幔对流模拟中的潜力与局限性。

Abstract: Mantle convection simulations are an essential tool for understanding how
rocky planets evolve. However, the poorly known input parameters to these
simulations, the non-linear dependence of transport properties on pressure and
temperature, and the long integration times in excess of several billion years
all pose a computational challenge for numerical solvers. We propose a
physics-based machine learning approach that predicts creeping flow velocities
as a function of temperature while conserving mass, thereby bypassing the
numerical solution of the Stokes problem. A finite-volume solver then uses the
predicted velocities to advect and diffuse the temperature field to the next
time-step, enabling autoregressive rollout at inference. For training, our
model requires temperature-velocity snapshots from a handful of simulations
(94). We consider mantle convection in a two-dimensional rectangular box with
basal and internal heating, pressure- and temperature-dependent viscosity.
Overall, our model is up to 89 times faster than the numerical solver. We also
show the importance of different components in our convolutional neural network
architecture such as mass conservation, learned paddings on the boundaries, and
loss scaling for the overall rollout performance. Finally, we test our approach
on unseen scenarios to demonstrate some of its strengths and weaknesses.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [446] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
*Rui Ye,Keduan Huang,Qimin Wu,Yuzhu Cai,Tian Jin,Xianghe Pang,Xiangrui Liu,Jiaqi Su,Chen Qian,Bohan Tang,Kaiqu Liang,Jiaao Chen,Yue Hu,Zhenfei Yin,Rongye Shi,Bo An,Yang Gao,Wenjun Wu,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: MASLab是一个统一的代码库，旨在解决LLM-based多智能体系统（MAS）领域中的冗余实现和不公平比较问题，集成了20多种方法，提供了标准化评估环境。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-based MAS领域缺乏统一的代码库，导致重复实现、不公平比较和研究门槛高。

Method: MASLab整合了20多种已验证方法，提供统一环境和标准化评估协议，并采用共享结构降低理解与扩展门槛。

Result: 通过10多个基准和8个模型的实验，MASLab为研究者提供了MAS方法的全面视图。

Conclusion: MASLab将持续更新，跟踪领域最新进展，并欢迎开源社区贡献。

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [447] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
*Shijia Zhou,Siyao Peng,Simon Luebke,Jörg Haßler,Mario Haim,Saif M. Mohammad,Barbara Plank*

Main category: cs.CL

TL;DR: 论文研究了媒体框架与立场在气候变化网络梗图中的交互作用，提出了CLIMATEMEMES数据集，并评估了LLaVA-NeXT和Molmo在立场检测和媒体框架检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨媒体框架与立场之间的交互作用，尤其是在气候变化网络梗图中的表现，填补了这一领域的研究空白。

Method: 采用跨学科方法，构建了CLIMATEMEMES数据集（包含1,184个梗图），并提出了立场检测和媒体框架检测任务，评估了LLaVA-NeXT和Molmo的性能。

Result: 视觉语言模型（VLMs）在立场检测上表现良好，但在媒体框架检测上表现不佳，而大型语言模型（LLMs）在后者上表现更优。人类标注的标题能显著提升性能。

Conclusion: 研究揭示了VLMs在处理气候变化梗图中复杂框架和立场表达时的局限性，为未来研究提供了方向。

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [448] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
*Yuyang Jiang,Chacha Chen,Shengyuan Wang,Feng Li,Zecong Tang,Benjamin M. Mervak,Lydia Chelala,Christopher M Straus,Reve Chahine,Samuel G. Armato III,Chenhao Tan*

Main category: cs.CL

TL;DR: CLEAR是一个用于放射学报告评估的临床框架，通过多维度属性级比较提供更全面且可解释的报告质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有指标缺乏细粒度和可解释性，无法捕捉候选报告与真实报告之间的临床细微差异，导致评估不理想。

Method: 提出CLEAR框架，通过五个关键属性（首次出现、变化、严重程度、描述性位置和建议）评估报告质量，并开发CLEAR-Bench数据集验证其临床一致性。

Result: CLEAR在提取临床属性和提供自动化指标方面表现高准确度，且与临床判断高度一致。

Conclusion: CLEAR提供了一个更全面且临床可解释的放射学报告评估方法，显著优于现有指标。

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [449] [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
*Parth Sarin,Juan Pablo Alperin*

Main category: cs.CL

TL;DR: 论文提出了一种基于开源语言模型的工具，用于标记和解析学术引用，以改善全球知识共享网络的可视化，尤其是关注全球南方学者的参与。


<details>
  <summary>Details</summary>
Motivation: 解决全球知识生产和传播中的不平等问题，尤其是全球南方学者在索引服务中被边缘化的情况。

Method: 通过构建包含纯文本和注释引用的数据集，评估多种开源语言模型在引用标记任务中的表现。

Result: 现有语言模型在识别引用组成部分时表现出高准确率，其中Qwen3-0.6B模型在少量迭代中即可高效完成任务。

Conclusion: 该工具有望提升引用网络的准确性，改善研究索引和发现，并推动元科学研究。

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [450] [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/abs/2502.15401)
*Xuetao Ma,Wenbin Jiang,Hua Huang*

Main category: cs.CL

TL;DR: 论文提出了一种基于问题解决逻辑的课程式ICL策略，通过分析和排序示例，显著提升了LLMs的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简单特征衡量示例相关性，不足以反映示例间的内在联系，需改进。

Method: 构建问题解决逻辑指令集，微调语言模型分析逻辑，按难度排序示例作为上下文提示。

Result: 实验表明，该方法在性能与效率上优于现有ICL方法，有效提升LLMs的复杂推理能力。

Conclusion: 基于问题解决逻辑的课程式ICL策略是提升LLMs推理能力的有效方法，项目将开源。

Abstract: In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.

</details>


### [451] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Main category: cs.CL

TL;DR: ULTRAEDIT是一种新型的终身学习模型编辑方法，通过轻量级线性代数操作实现高效、可扩展的知识更新，速度比现有方法快7倍，且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 解决现有终身学习方法在大规模实际应用中的效率和可扩展性问题。

Method: 采用训练无关、主题无关且无需记忆的编辑方案，通过线性代数操作计算参数偏移，并结合终身归一化策略适应分布变化。

Result: ULTRAEDIT在速度和资源消耗上显著优于现有方法，支持多达100万次编辑，并在多个数据集和模型上表现优异。

Conclusion: ULTRAEDIT为终身学习模型编辑提供了一种高效、可扩展的解决方案，适用于实际应用场景。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [452] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
*Juvenal Domingos Júnior,Augusto Faria,E. Seiti de Oliveira,Erick de Brito,Matheus Teotonio,Andre Assumpção,Diedre Carmo,Roberto Lotufo,Jayr Pereira*

Main category: cs.CL

TL;DR: BR-TaxQA-R是一个支持巴西个人所得税法问答的新数据集，包含715个问题，采用RAG方法优化问答系统，结果显示其优于商业工具，但需专家验证法律有效性。


<details>
  <summary>Details</summary>
Motivation: 为巴西个人所得税法提供高质量的问答支持，结合法律条文和行政裁决，提升AI在税务领域的实用性。

Method: 使用RAG管道，结合OpenAI嵌入搜索和GPT-4o-mini生成答案，比较不同文本分割策略，并基于RAGAS指标评估。

Result: 自定义RAG管道在响应相关性上优于商业系统，但商业模型在事实准确性和流畅性上表现更好。

Conclusion: 在法律高风险的税务领域，AI生成答案需结合专家验证以确保法律有效性，BR-TaxQA-R数据集已公开。

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [453] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
*Aliakbar Nafar,Kristen Brent Venable,Zijun Cui,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 该论文研究了如何利用大型语言模型（LLMs）中的概率知识，通过贝叶斯网络（BN）为现实世界事件生成概率估计，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成概率知识方面的潜力，尤其是在缺乏数据的情况下，如何利用其内在知识辅助贝叶斯网络的参数化。

Method: 通过查询LLMs获取事件的条件概率，将其用于贝叶斯网络的参数化，并与随机、均匀分布等基线方法进行比较。

Result: 实验表明，LLMs生成的概率分布优于基线方法，并能作为专家先验减少系统偏差。

Conclusion: 提出了一种结合LLMs概率知识与少量真实数据自动构建贝叶斯网络的新方法，并建立了评估LLMs概率知识提取性能的基准。

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [454] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
*Linxi Zhao,Sofian Zalouk,Christian K. Belardi,Justin Lovelace,Jin Peng Zhou,Kilian Q. Weinberger,Yoav Artzi,Jennifer J. Sun*

Main category: cs.CL

TL;DR: 论文提出了一种新型语言模型LMLM，通过结合内部权重和外部数据库存储事实知识，减少对模型记忆的依赖，实现可编辑和可验证的知识管理。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经语言模型中事实知识分布不透明、难以检查和更新的问题。

Method: 提出LMLM模型，通过预训练策略将事实知识存储在内部权重和外部数据库中，并屏蔽外部检索的事实值以鼓励模型进行针对性查询。

Result: LMLM在标准基准测试中表现与更大规模的知识密集型LLM相当，同时具备显式、可编辑和可验证的知识库优势。

Conclusion: LMLM代表了语言模型与事实知识交互和管理方式的根本性转变。

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [455] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
*Mehrdad ghassabi,Pedram Rostami,Hamidreza Baradaran Kashani,Amirhossein Poursina,Zahra Kazemi,Milad Tavakoli*

Main category: cs.CL

TL;DR: 该研究通过爬取波斯语医学杂志和医患问答数据，构建首个波斯语医学领域数据集，并微调小型语言模型，显著提升了其在医学问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 波斯语等低资源语言的小型语言模型在医学领域表现不佳，缺乏专业数据集，研究旨在填补这一空白。

Method: 爬取波斯语医学杂志和医患问答数据，构建数据集，并微调基线模型。

Result: 微调后的模型在医学问答中表现优于基线模型，准确性提高。

Conclusion: 研究表明，利用开放数据可以提升小型语言模型在医学领域的表现，为波斯语医学AI应用提供了新思路。

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [456] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
*Sasha Boguraev,Christopher Potts,Kyle Mahowald*

Main category: cs.CL

TL;DR: 论文探讨了如何利用因果可解释性方法分析大型语言模型（LLMs），以揭示其学习到的抽象机制，从而推动语言学理论的发展。


<details>
  <summary>Details</summary>
Motivation: LLMs为语言学理论提供了丰富的证据，但如何利用这些证据进一步理解其内部机制并推动理论发展是研究的动机。

Method: 采用分布式交换干预（Distributed Interchange Interventions）实验方法，分析LLMs对英语填充-空缺依赖结构（如疑问句、关系从句）的抽象分析。

Result: 实验表明，LLMs对这些结构有相似的抽象分析，并揭示了频率、填充类型和上下文等被忽视的因素，可能影响语言学理论。

Conclusion: 通过LLMs的内部机制分析，可以推动语言学理论的进步。

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [457] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
*Roland Daynauth,Christopher Clarke,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.CL

TL;DR: SLMEval是一种基于熵最大化的校准方法，显著提升了语言模型评估与人类判断的一致性，并在实际任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法在开放任务中表现不佳，与人类判断相关性弱甚至负相关，需要一种更通用的解决方案。

Method: 提出SLMEval，通过熵最大化利用少量人类偏好数据，估计模型质量的潜在分布并重新加权评分。

Result: SLMEval在真实生产用例和公共基准测试中与人类评估强相关（如Spearman相关系数0.57），且成本降低5-30倍。

Conclusion: SLMEval是一种高效且通用的校准方法，适用于开放任务，显著提升评估质量并降低成本。

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [458] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
*Wenrui Yu,Yiyi Chen,Johannes Bjerva,Sokol Kosta,Qiongxiu Li*

Main category: cs.CL

TL;DR: LAGO是一种基于语言相似性的图优化方法，用于少样本跨语言嵌入反转攻击，显著提升了攻击的迁移性。


<details>
  <summary>Details</summary>
Motivation: 解决多语言NLP系统中的隐私漏洞，通过建模语言间的关系提升攻击效果。

Method: 采用图约束的分布式优化框架，结合句法和词汇相似性作为边约束，实现跨语言参数协作学习。

Result: 实验表明，LAGO在Rouge-L分数上比基线提高了10-20%。

Conclusion: 语言相似性是反转攻击迁移性的关键因素，呼吁关注语言感知的隐私保护嵌入方法。

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [459] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
*Wei Liu,Siya Qi,Xinyu Wang,Chen Qian,Yali Du,Yulan He*

Main category: cs.CL

TL;DR: NOVER是一种无需外部验证器的强化学习框架，仅需标准监督微调数据，适用于广泛的文本任务，性能优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部验证器，限制了其在数学和编码等领域的应用，且验证器训练成本高。

Method: 提出NOVER框架，无需外部验证器，仅需标准监督微调数据，支持广泛的文本任务。

Result: NOVER在性能上优于同类模型（如DeepSeek R1 671B），提升7.7%。

Conclusion: NOVER为大型语言模型优化提供了新可能性，如逆向激励训练。

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [460] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
*Gagan Bhatia,Maxime Peyrard,Wei Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种衡量BPE分词器对日期分割效果的指标（date fragmentation ratio），并发布了一个包含6500个例子的评测集（DateAugBench）。研究发现，过度的日期分割会导致模型在罕见日期上的准确率下降，同时揭示了LLMs通过一种自发的日期抽象机制来拼接日期片段。


<details>
  <summary>Details</summary>
Motivation: 现代BPE分词器常将日期分割成无意义的片段，影响时间推理的鲁棒性。

Method: 1. 提出date fragmentation ratio指标；2. 发布DateAugBench评测集；3. 通过分层探测和因果注意力分析揭示LLMs的日期抽象机制。

Result: 过度分割导致罕见日期准确率下降10%；模型越大，日期抽象机制越快形成；LLMs拼接日期片段的路径与人类不同。

Conclusion: 研究揭示了日期分割对时间推理的影响，并展示了LLMs如何通过自发机制解决这一问题。

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [461] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju,Gondy Leroy,David Kauchak,Arif Ahmed*

Main category: cs.CL

TL;DR: 研究比较了生成式AI在简化健康信息时遗漏的关键信息，并通过五种方法补充缺失内容，发现补充所有缺失实体效果最佳。


<details>
  <summary>Details</summary>
Motivation: 简化健康信息有助于理解，但生成式AI可能遗漏关键内容，需评估并修复这些遗漏。

Method: 收集50份健康信息，用GPT-4简化，比较五种补充缺失内容的方法（如补充所有缺失实体或单词），并用余弦相似度和ROUGE评分评估效果。

Result: 补充所有缺失实体能显著改善文本质量，优于其他方法。

Conclusion: 生成式AI能识别缺失实体，但需改进其重要性排序能力。

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [462] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
*Bohao Wu,Qingyun Wang,Yue Guo*

Main category: cs.CL

TL;DR: 论文提出了一种高效且可扩展的个性化术语检测方法，通过轻量级微调（LoRA）和个性化提示策略，显著提升了性能，同时减少了标注数据需求。


<details>
  <summary>Details</summary>
Motivation: 为了使技术文档对不同学科背景的读者更易理解，需要个性化的术语检测和解释方法，但传统方法需要大量标注和计算资源。

Method: 研究了两种个性化策略：1）基于LoRA的轻量级微调；2）个性化提示。还探索了结合有限标注数据和无监督用户背景信号的混合方法。

Result: 个性化LoRA模型在F1分数上比GPT-4高出21.4%，比最佳基线高8.3%，且仅需10%的标注数据即可达到可比性能。

Conclusion: 该研究为高效、低资源的术语检测个性化提供了系统性探索，为可扩展的用户自适应NLP系统提供了实用路径。

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [463] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
*Wei Zhang,Zhenhong Zhou,Junfeng Fang,Rongwu Xu,Kun Wang,Yuanhe Zhang,Rui Wang,Ge Zhang,Xinfeng Li,Li Sun,Lingjuan Lyu,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 论文介绍了LIFEBench，一个评估大语言模型（LLM）遵循长度指令能力的基准测试，发现现有模型在长文本生成中存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能解决复杂推理问题，但在遵循明确长度指令（如生成指定长度的文本）时表现不佳，现有基准测试未充分关注此问题。

Method: 提出LIFEBench基准，包含10,800个实例，覆盖4类任务和多种长度要求（16至8192词），评估了26种常用LLM。

Result: 大多数模型在短文本生成中表现尚可，但在长文本生成中急剧下降；几乎所有模型未达到厂商宣称的最大输出长度。

Conclusion: LIFEBench揭示了当前LLM在长度指令遵循上的根本局限，为未来改进提供了关键见解。

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [464] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
*Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee*

Main category: cs.CL

TL;DR: IRONIC框架通过多模态连贯关系分析图像与文本的关联，在零样本多模态讽刺检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能有效模拟人类识别讽刺的认知过程，需结合语言和认知洞察改进多模态推理策略。

Method: 提出IRONIC框架，利用多模态连贯关系（指代、类比和语用）进行上下文学习。

Result: 实验显示IRONIC在零样本多模态讽刺检测中达到最佳性能。

Conclusion: 结合语言和认知洞察的多模态推理策略对讽刺检测至关重要。

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [465] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
*Jiaru Zou,Yikun Ban,Zihao Li,Yunzhe Qi,Ruizhong Qiu,Ling Yang,Jingrui He*

Main category: cs.CL

TL;DR: 论文提出了一种名为Transformer Copilot的新框架，通过记录模型的错误日志（Mistake Log）并设计Copilot模型来修正主模型（Pilot）的推理性能，显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统的微调方法仅关注最小化生成损失，而忽略了模型自身的学习信号。作者希望模仿人类从错误中学习的方式，通过记录和分析模型的错误来优化性能。

Method: 1. 引入Mistake Log记录模型在微调过程中的错误；2. 设计Copilot模型，通过修正Pilot模型的logits来提升推理性能；3. 提出联合训练和融合推理范式。

Result: 在12个基准测试中，Transformer Copilot将性能提升高达34.5%，同时计算开销较小，并表现出良好的可扩展性和可迁移性。

Conclusion: Transformer Copilot通过利用模型自身的学习信号，显著提升了微调效果，为语言模型的优化提供了新思路。

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [466] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
*Chenzhuo Zhao,Ziqian Liu,Xingda Wang,Junting Lu,Chaoyi Ruan*

Main category: cs.CL

TL;DR: PMPO是一种轻量级提示优化框架，通过直接使用交叉熵损失评估信号改进提示，无需输出采样或人工评估，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖高成本输出生成或人工标注，限制了其可扩展性，尤其是对小模型或非指令调优模型。

Method: PMPO通过掩码和损失影响识别低质量提示段，利用正负样本最小化损失优化提示，仅需前向传播和似然计算。

Result: PMPO在BBH、GSM8K、AQUA-RAT等任务中表现优异，AlpacaEval 2.0胜率提升超过19分。

Conclusion: PMPO高效、有效且适用广泛，为提示优化提供了新思路。

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [467] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
*Wenqing Wu,Chengzhi Zhang,Tong Bao,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种通过组合学术论文的不同核心部分（如引言、方法、结果和讨论）来预测新颖性评分的方法，发现引言、结果和讨论的组合最适合评估新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注词汇或实体组合，对新颖性评估的洞察有限。论文旨在探索不同部分组合对新颖性评分预测的影响。

Method: 利用自然语言处理技术识别论文的IMRaD结构，将不同部分组合输入预训练语言模型和大语言模型，以人类专家评分作为标签进行预测。

Result: 引言、结果和讨论的组合最适合评估新颖性，而全文输入效果不显著。引言和结果部分对新颖性预测最为重要。

Conclusion: 论文确定了评估学术论文新颖性的最优部分组合，为自动化新颖性评估提供了新思路。

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [468] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
*Benjamin Vendeville,Liana Ermakova,Pierre De Loor*

Main category: cs.CL

TL;DR: 论文提出了一种用于检测和分类简化文本错误的测试集，填补了自动文本简化（ATS）评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 公众常因复杂文本难以理解而传播错误信息，而现有的ATS评估方法未能跟上文本生成技术的进步，尤其是大语言模型（LLMs）的发展。

Method: 提出错误分类法，构建并行数据集并人工标注，分析数据集质量及现有模型性能。

Result: 提供了用于评估ATS错误的工具，帮助开发更可靠的模型。

Conclusion: 该研究为改进自动简化文本质量提供了新方法。

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [469] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: Tool-Star是一个基于强化学习的框架，旨在通过多工具协作提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多工具协作推理中的挑战。

Method: 提出数据合成管道和两阶段训练框架，包括冷启动微调和多工具自评RL算法。

Result: 在10多个推理基准测试中表现出高效性。

Conclusion: Tool-Star有效提升了多工具协作推理能力。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [470] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
*Yue Li,Xin Yi,Dongsheng Shi,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 提出了一种名为HSR的轻量级方法，通过分层恢复关键注意力头和神经元，提升剪枝后大型视觉语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 剪枝技术虽然能压缩模型，但会导致安全性下降，因此需要一种方法在剪枝后恢复模型的安全性。

Method: HSR通过量化注意力头对安全性的贡献，识别关键头，并选择性恢复其中的关键神经元，实现分层安全对齐。

Result: 在不同模型和剪枝策略下，HSR均显著提升了安全性。

Conclusion: HSR是首个专注于剪枝后恢复LVLMs安全性的工作，效果显著。

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [471] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
*Ruizhe Li,Chen Chen,Yuchen Hu,Yanjun Gao,Xi Wang,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出了一种基于Jensen-Shannon散度的方法（ARC-JSD），用于高效准确地在检索增强生成（RAG）中识别关键上下文句子，无需额外微调或代理建模。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将生成内容可靠地归因于特定上下文时计算成本高，需要微调或人工标注。

Method: 采用Jensen-Shannon散度驱动的方法（ARC-JSD），无需额外微调或代理建模。

Result: 在多个RAG基准测试中表现优于现有方法，显著提升计算效率，并揭示了负责上下文归因的特定注意力头和MLP层。

Conclusion: ARC-JSD为RAG模型提供了一种高效准确的上下文归因方法，并揭示了其内部工作机制。

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [472] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
*Menschikov Mikhail,Alexander Kharitonov,Maiia Kotyga,Vadim Porvatov,Anna Zhukovskaya,David Kagramanyan,Egor Shvetsov,Evgeny Burnaev*

Main category: cs.CL

TL;DR: 研究探讨了大语言模型的位置偏见及其与语言多样性的关系，发现模型驱动的偏见因语言而异，且显式位置引导会降低准确性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在不同语言中的位置偏见及其与语法、提示的交互作用。

Method: 对五种类型学不同的语言（英语、俄语、德语、印地语、越南语）进行跨语言研究，分析位置偏见与模型不确定性、语法和提示的关系。

Result: 发现位置偏见是模型驱动的，语言间存在差异；显式位置引导降低准确性；对齐上下文与偏见会增加熵，但最小熵不预测准确性；LLMs在自由语序语言中强加主导词序。

Conclusion: 位置偏见是模型固有的，且与语言特性相关，提示工程需考虑语言多样性。

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [473] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
*Jing Bi,Pinxin Liu,Ali Vosoughi,Jiarui Wu,Jinxi He,Chenliang Xu*

Main category: cs.CL

TL;DR: 论文提出了一种语言驱动框架，将程序性文本转化为连贯的视觉指令，解决了纯文本难以传达复杂动作和空间关系的问题。


<details>
  <summary>Details</summary>
Motivation: 程序性知识的有效传达是NLP中的挑战，纯文本难以表达复杂动作和空间关系。

Method: 通过分解目标陈述和步骤序列建模语言结构，结合选区解析器编码、连贯性模型和新评估协议。

Result: 在三个数据集上显著优于基线，生成的视觉内容更准确地反映语言指令。

Conclusion: 该框架为程序性语言的视觉化提供了新方法，适用于教育、任务指导和多模态语言理解。

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [474] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
*Song Jin,Juntian Zhang,Yuhan Liu,Xun Zhang,Yufei Zhang,Guojun Yin,Fei Jiang,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: RecInter是一个基于代理的推荐系统模拟平台，通过动态交互机制和高级用户建模，显著提升了模拟的真实性。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试资源消耗大，离线方法难以捕捉动态用户-平台交互，现有模拟平台缺乏动态环境重塑机制。

Method: 引入RecInter平台，模拟用户行为实时更新物品属性，加入商家代理，采用多维用户画像和高级代理架构，结合LLM微调。

Result: 平台显著提升模拟可信度，成功复现品牌忠诚度和马太效应等涌现现象。

Conclusion: RecInter是推荐系统研究的可信测试平台，交互机制对模拟系统演化至关重要。

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [475] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
*Ikhlasul Akmal Hanif,Eryawan Presma Yulianrifat,Jaycent Gunawan Ongris,Eduardus Tjitrahardja,Muhammad Falensi Azmi,Rahmat Bryan Naufal,Alfan Farizki Wicaksono*

Main category: cs.CL

TL;DR: 本文介绍了SemEval 2025 Task 11 Track A的多标签情感分类方法，比较了完全微调与仅分类器训练策略，发现基于提示的编码器（如mE5和BGE）效果优于XLMR和mBERT。最佳模型为BGE集成模型，平均F1-macro得分为56.58。


<details>
  <summary>Details</summary>
Motivation: 探索多语言环境下多标签情感分类的最佳方法，比较不同策略和模型的性能。

Method: 采用完全微调与仅分类器训练两种策略，评估不同设置（如微调策略、模型架构、损失函数等）。

Result: 基于提示的编码器（如mE5和BGE）表现优于完全微调的XLMR和mBERT；集成BGE模型（CatBoost分类器）平均F1-macro得分为56.58。

Conclusion: 提示编码器结合分类器训练在多语言情感分类中表现更优，集成模型效果最佳。

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [476] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
*Shuzheng Si,Haozhe Zhao,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Bofei Gao,Kangyang Luo,Wenhao Li,Yufei Huang,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出了一种名为CANOE的系统框架，通过合成短形式QA数据和Dual-GRPO强化学习方法，显著提升了LLM在上下文中的忠实度，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 提高大型语言模型（LLMs）在上下文中的忠实度，以构建可靠的信息检索系统。

Method: 1. 合成短形式QA数据；2. 提出Dual-GRPO强化学习方法，结合三种规则奖励，同时优化短形式和长形式生成。

Result: CANOE在11个下游任务中显著提升了LLM的忠实度，甚至超越了GPT-4o和OpenAI o1等先进模型。

Conclusion: CANOE框架通过无监督方法有效提升了LLM的忠实度，具有广泛的应用潜力。

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [477] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
*Dario Di Palma,Alessandro De Bellis,Giovanni Servedio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 研究通过探测Llama模型的隐藏层，定位情感特征分布，发现情感信息在中间层最集中，检测准确率比提示技术提高14%，同时减少57%内存需求。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在情感分析中表现优异，但其如何捕捉情感信息仍不清楚。本研究旨在揭示情感特征在模型中的分布及其对分析的影响。

Method: 使用探测分类器分析情感编码在不同层和尺度上的表现，确定最佳捕捉情感信号的层和池化方法。

Result: 情感信息在二分类任务中集中在中间层，检测准确率提升14%；解码器模型中，最后一个标记并非最有效；内存需求平均减少57%。

Conclusion: 层特定探测是情感任务的有效方法，可提升模型效用并降低资源消耗。

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [478] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
*Runcong Zhao,Chengyu Cao,Qinglin Zhu,Xiucheng Lv,Shun Shao,Lin Gui,Ruifeng Xu,Yulan He*

Main category: cs.CL

TL;DR: Concise-SAE是一种无需训练的方法，通过编辑指令相关神经元提升语言模型在复杂叙事中的指令遵循能力，FreeInstruct是其配套的多样化基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在复杂叙事环境中指令遵循能力不足的问题，现有基准测试未能充分体现这一挑战。

Method: 提出Concise-SAE框架，通过自然语言指令识别并编辑指令相关神经元，无需标注数据。

Result: 在多样化任务中实现最先进的指令遵循能力，同时保持生成质量。

Conclusion: Concise-SAE不仅适用于复杂叙事，还能广泛提升指令遵循能力，具有实际应用潜力。

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [479] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
*Lovisa Hagström,Youna Kim,Haeun Yu,Sang-goo Lee,Richard Johansson,Hyunsoo Cho,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 论文提出了CUB基准，用于系统评估上下文利用技术（CMTs）在检索增强生成（RAG）中的表现，发现现有CMTs难以应对真实场景中的多样化上下文。


<details>
  <summary>Details</summary>
Motivation: 语言模型在知识密集型任务中可能忽略或误用上下文信息，现有CMTs缺乏系统性比较，需开发更全面的评估方法。

Method: 开发CUB基准，测试七种代表性CMTs在三种上下文类型、三个数据集和九种语言模型上的表现。

Result: 大多数CMTs难以应对真实场景中的多样化上下文，且在合成数据集上表现优于真实数据集。

Conclusion: 需开发能处理多种上下文类型的CMTs，并进行更全面的测试。

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [480] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
*Giovanni Servedio,Alessandro De Bellis,Dario Di Palma,Vito Walter Anelli,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 论文研究了LLMs中的事实性幻觉问题，通过生成更真实的数据集挑战了先前研究的结论，并提出了新的数据采样和生成方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成虚假内容的问题，提升其可靠性和用户信任度。

Method: 提出从表格数据中采样真实-虚假事实句子的策略，以及从问答集合生成依赖LLM的真实-虚假数据集的方法。

Result: 部分验证了先前研究的结论，但发现LLM生成数据集的泛化能力仍具挑战性。

Conclusion: 为未来LLMs事实性研究奠定基础，并提供了更有效评估的实用指南。

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [481] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
*Zhouhao Sun,Zhiyuan Kan,Xiao Ding,Li Du,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 论文提出了一种多偏差基准（multi-bias benchmark）和因果效应估计引导的多偏差消除方法（CMBE），以解决现有大语言模型（LLMs）在推理中存在的多类型偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在推理中可能利用多种偏差，导致泛化能力差。现有基准通常只控制单一偏差，而实际应用中数据可能包含多种偏差。

Method: 提出多偏差基准，每个数据包含五种偏差类型；设计CMBE方法，通过因果效应估计同时消除多种偏差。

Result: 实验表明，现有LLMs和去偏方法在多偏差基准上表现不佳，而CMBE能有效消除多种偏差。

Conclusion: CMBE方法能显著提升LLMs的泛化能力，为多偏差消除提供了有效解决方案。

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [482] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在零样本范式下处理多模态情感分析（MSA）的不足，通过上下文学习（ICL）优化演示配置，验证了MLLMs的能力，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析（MSA）在零样本范式下表现不佳，研究旨在验证MLLMs是否具备与监督模型相当的情感感知能力。

Method: 扩展零样本范式至上下文学习（ICL），深入研究演示的检索、呈现和分布三个关键因素，并发现并抵消MLLMs的情感预测偏差。

Result: 优化后的策略在六个MSA数据集上平均准确率比零样本范式提升15.9%，比随机ICL基线提升11.2%。

Conclusion: MLLMs通过优化演示配置具备处理MSA的能力，为多模态任务提供了新的解决方案。

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [483] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
*Songlin Yang,Yikang Shen,Kaiyue Wen,Shawn Tan,Mayank Mishra,Liliang Ren,Rameswar Panda,Yoon Kim*

Main category: cs.CL

TL;DR: PaTH是一种基于Householder变换的数据依赖位置编码方案，优于RoPE和其他基线方法。


<details>
  <summary>Details</summary>
Motivation: RoPE的位置编码仅依赖相对位置，限制了其表达能力，因此需要一种更灵活的数据依赖方案。

Method: 提出PaTH，基于数据依赖的Householder变换，并开发了高效的并行训练算法和FlashAttention风格的块状算法以减少I/O成本。

Result: 在合成基准和实际语言建模实验中，PaTH表现优于RoPE和其他基线方法。

Conclusion: PaTH是一种高效且表达力强的位置编码方案，适用于现代大型语言模型。

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [484] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
*Jianbiao Mei,Tao Hu,Daocheng Fu,Licheng Wen,Xuemeng Yang,Rong Wu,Pinlong Cai,Xing Gao,Yu Yang,Chengjun Xie,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: O²-Searcher是一个基于强化学习的搜索代理，用于解决开放域中的开放性和封闭性问题，通过动态知识获取和统一的训练机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）受限于静态参数知识，无法处理需要开放域最新信息的任务，尤其是开放性问题。

Method: O²-Searcher利用强化学习，在本地模拟搜索环境中动态获取知识，并通过精心设计的奖励函数统一训练，区分问题类型并生成不同策略。

Result: O²-Searcher在3B模型下显著超越其他LLM代理，并在封闭性问题基准测试中达到SOTA水平。

Conclusion: O²-Searcher为解决开放性问题提供了有效方案，同时展示了在封闭性问题上的强大性能。

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [485] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
*Daniel Scalena,Gabriele Sarti,Arianna Bisazza,Elisabetta Fersini,Malvina Nissim*

Main category: cs.CL

TL;DR: 论文探讨了在低资源环境下个性化LLM生成翻译的策略，提出了一种利用稀疏自编码器提取潜在概念的对比框架，实现了强个性化且保持翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的高质量机器翻译系统在风格要求不明确时表现不佳，尤其是在文学翻译领域。

Method: 研究了提示策略和推理时干预方法，提出了一种对比框架，利用稀疏自编码器提取潜在概念以识别个性化属性。

Result: 结果表明，引导方法能实现强个性化且保持翻译质量，且多示例提示与引导方法对模型层的影响相似。

Conclusion: 引导方法在个性化翻译中有效，且与多示例提示机制相似。

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [486] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: WebAgent-R1是一个端到端的多轮强化学习框架，显著提升了大型语言模型在动态网页交互中的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要关注单轮任务，而多轮网页交互的复杂性使得训练有效的网页代理具有挑战性。

Method: WebAgent-R1通过异步生成多样化的轨迹，完全依赖任务成功的二元奖励进行在线学习。

Result: 在WebArena-Lite基准测试中，WebAgent-R1显著提升了任务成功率，优于现有方法。

Conclusion: 研究表明，基于思考的提示策略和测试时扩展对网页任务有效，同时强调了预热训练阶段的重要性。

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [487] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 论文提出了一种自奖励强化学习框架SSR，用于机器翻译，无需外部监督，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译模型依赖昂贵的外部监督信号，如人工标注数据或奖励模型，难以扩展。

Method: 提出SSR框架，完全在线且仅依赖自奖励信号，结合13K单语数据和Qwen-2.5-7B模型进行训练。

Result: SSR-Zero-7B在英中翻译任务中表现优异，结合COMET监督的SSR-X-Zero-7B达到SOTA性能。

Conclusion: 自奖励机制在机器翻译中有效，与外部监督结合效果更佳，为自改进强化学习方法提供了新思路。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [488] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
*Kexin Shang,Chia-Hsuan Chang,Christopher C. Yang*

Main category: cs.CL

TL;DR: 本文提出了一种多LLM协作框架，用于提升医学选择题任务的表现，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在医学任务中潜力巨大，但缺乏多模型协同效应的研究。

Method: 提出一个多LLM协作框架，基于医学选择题数据集，通过后验分析验证其效果。

Result: 框架提升了所有参与LLM的推理能力，减少了问题间的分歧，并观察到模型置信度与预测准确性的相关性。

Conclusion: 多LLM协作框架能有效提升医学任务表现，模型置信度可作为预测准确性的参考。

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [489] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
*Liu Chang,Wang Dongbo,Liu liu,Zhao Zhixiao*

Main category: cs.CL

TL;DR: 该研究构建了Guji_MATH基准，用于评估基于《算经十书》的古籍数学问题解决能力，发现推理模型在古典中文数学问题上的表现不及现代任务。


<details>
  <summary>Details</summary>
Motivation: 解决古典中文数学经典智能处理的挑战，挖掘古籍数学知识并传播传统文化。

Method: 通过机器辅助标注和人工验证，从8部经典中提取538个数学问题，设计闭卷和开卷两种评估模式测试六种推理模型。

Result: 推理模型能部分理解和解决古典数学问题，但表现不及现代数学任务。

Conclusion: 需优先提升模型对古典中文和文化知识的理解，为跨语言和跨文化能力评估提供新视角。

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [490] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
*Gouki Minegishi,Hiroki Furuta,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 论文研究了Transformer语言模型如何通过训练获得元学习能力，以解决上下文学习（ICL）任务，揭示了多阶段电路动态变化的过程。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何通过上下文学习（ICL）获得元学习能力，而非仅复制上下文中的答案，填补了现有研究的空白。

Method: 通过扩展复制任务为上下文元学习任务，分析模型训练过程中电路动态变化，识别不同阶段的独特电路。

Result: 发现元学习能力通过多阶段电路动态变化获得，每个阶段出现独特电路，与单阶段变化的归纳头形成对比。

Conclusion: 研究揭示了Transformer模型ICL能力的来源，为理解其元学习机制提供了新视角。

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [491] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
*Florentin Beck,William Rudman,Carsten Eickhoff*

Main category: cs.CL

TL;DR: TRIM是一种针对大型语言模型（LLM）的新型剪枝方法，通过维度级稀疏分配优化性能，显著提升高稀疏率下的模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的一刀切剪枝方法在高稀疏率下表现不佳，需要更精细的剪枝策略以保留关键信息。

Method: TRIM采用迭代调整过程，根据质量指标为每个输出维度分配不同的稀疏率，减少质量保留的方差。

Result: 在多种LLM和稀疏率下，TRIM显著降低了困惑度（如Qwen2.5-14B降低48%），并提升了稳定性。

Conclusion: 维度级稀疏适应是极端LLM压缩的关键，TRIM为此提供了有效解决方案。

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [492] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
*Himanshu Beniwal,Youngwoo Kim,Maarten Sap,Soham Dan,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 论文提出了一种跨语言去毒方法，通过504种设置评估其有效性，并探讨了安全性与知识保留之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在全球应用中的普及，确保其在多语言环境中无毒性成为关键挑战。

Method: 采用跨语言范式，实现高资源和低资源语言之间的毒性迁移与去毒能力转移。

Result: 通过实验验证了跨语言去毒的有效性，同时揭示了安全性与模型性能之间的权衡。

Conclusion: 跨语言去毒是可行的，但需平衡安全性与知识保留，相关代码和数据集已公开。

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [493] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
*Punya Syon Pandey,Samuel Simko,Kellin Pelrine,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究探讨了微调大型语言模型时因数据特性导致的意外脆弱性（Accidental Misalignment），分析了数据特征与攻击成功率的关系，并提出了防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，其对抗攻击的脆弱性成为主要问题。微调模型可能引入意外脆弱性，研究旨在揭示数据特性如何影响模型安全性。

Method: 通过识别微调数据中的潜在相关因素（如语言特征、语义相似性和毒性），评估微调模型的对抗性能，并分析数据因素与攻击成功率的关系。

Result: 研究发现数据特性与模型脆弱性显著相关，揭示了微调数据设计对模型安全性的关键作用。

Conclusion: 研究强调了微调数据设计的重要性，为对抗防御策略提供了新见解，并呼吁关注数据特性对模型对齐的影响。

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [494] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
*Xiaoyu Xu,Xiang Yue,Yang Liu,Qingqing Ye,Haibo Hu,Minxin Du*

Main category: cs.CL

TL;DR: 论文指出当前大语言模型（LLM）的遗忘评估依赖词级指标（如准确率和困惑度）存在误导性，提出基于表示层的评估框架，揭示可逆与不可逆遗忘的区别，并提供了分析工具。


<details>
  <summary>Details</summary>
Motivation: 当前LLM遗忘评估方法依赖词级指标，可能导致模型看似遗忘但信息仍被保留，需要更可靠的评估方法。

Method: 引入基于PCA相似性、中心核对齐和Fisher信息的表示层评估框架，分析六种遗忘方法在三种领域和两种LLM中的表现。

Result: 发现可逆遗忘中模型保留潜在特征，而不可逆遗忘中表示层受损；任务类型和超参数调节可逆性。

Conclusion: 当前评估方法存在根本缺陷，新框架为LLM可信遗忘提供了诊断基础，并提供了分析工具。

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [495] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 论文提出了一种新的损失函数PDL，通过根据词频重新加权标准交叉熵损失，优化文本生成任务中的微调过程。


<details>
  <summary>Details</summary>
Motivation: 观察到高频低信息量词在标准交叉熵损失中占比过高，而低频高信息量词被忽视，影响生成内容的特异性和信息量。

Method: 设计Power-Law Decay Loss (PDL)，根据词频对损失函数进行加权，减少高频词权重，增加低频词权重。

Result: PDL引导模型在微调过程中更关注信息密集的低频词，提升生成文本的质量、多样性和信息量。

Conclusion: PDL在抽象摘要、对话系统和风格转换等任务中具有潜在优势和应用价值。

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [496] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
*Keshav Ramji,Tahira Naseem,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 论文提出了一种自动化方法，通过自校正设置从语言模型中挖掘潜在行为属性，并利用聚类压缩为可解释的原则集，显著提升了小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型用户需要明确行为属性以提升生成质量，但人工标注成本高，因此需要自动化方法。

Method: 采用后验正则化的蒙特卡洛期望最大化方法，挖掘并压缩潜在原则，指导模型自校正。

Result: 小模型性能显著提升（AlpacaEval胜率+8-10%，MT-Bench平均+0.3，IFEval原则遵循胜率+19-23%）。

Conclusion: 自动化原则驱动的后训练方法具有持续自我改进的潜力，聚类生成的原则集兼具可解释性和多样性。

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [497] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
*Fengyi Li,Kayhan Behdin,Natesh Pillai,Xiaofeng Wang,Zhipeng Wang,Ercan Yildiz*

Main category: cs.CL

TL;DR: 提出了一种基于图模型的非监督学习方法BP-Seg，用于高效文本分割，同时考虑局部连贯性和远距离语义相似性。


<details>
  <summary>Details</summary>
Motivation: 基于句子语义的文本分割是许多下游应用的基础任务，需要一种高效的方法。

Method: 通过在图模型上运行信念传播，同时捕捉相邻句子的局部连贯性和远距离句子的语义相似性。

Result: 在示例和长文档数据集上的实验表明，该方法优于竞争方法。

Conclusion: BP-Seg是一种有效的文本分割方法，适用于多种应用场景。

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [498] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
*Shuang Sun,Huatong Song,Yuhao Wang,Ruiyang Ren,Jinhao Jiang,Junjie Zhang,Fei Bai,Jia Deng,Wayne Xin Zhao,Zheng Liu,Lei Fang,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: SimpleDeepSearcher通过数据工程而非复杂训练范式，解决了RAG系统在高质量训练数据和计算成本上的问题，仅需少量样本即可超越RL基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在多步推理和信息检索中面临训练数据质量不足、模拟环境分布不匹配及计算成本高的问题。

Method: 提出SimpleDeepSearcher框架，通过模拟真实用户交互生成高质量训练数据，并结合多标准筛选优化输入输出多样性。

Result: 在五个基准测试中，仅用871个样本的SFT表现显著优于RL基线。

Conclusion: SFT通过系统解决数据稀缺问题，为高效深度搜索系统提供了实用方案。

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [499] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
*Daniel F. Perez-Ramirez,Dejan Kostic,Magnus Boman*

Main category: cs.CL

TL;DR: CASTILLO是一个数据集，用于分析13种开源大型语言模型（LLM）在不同指令集下的响应长度分布，揭示了模型间和模型内的显著变异性，并支持预测模型的开发。


<details>
  <summary>Details</summary>
Motivation: 由于自回归文本生成的随机性和变长性，高效管理LLM推理的计算资源具有挑战性。现有方法要么偏向特定长度，要么忽略模型和提示的变异性。

Method: 通过生成10个独立完成样本，记录响应长度，并发布统计数据和生成设置，分析模型间的变异性。

Result: 结果显示模型间和模型内响应长度存在显著变异性，并观察到部分文本退化现象。

Conclusion: CASTILLO为预测模型开发提供了数据支持，并促进了生成语言模型与系统研究的交叉领域发展。

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [500] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
*Amartya Chakraborty,Paresh Dashore,Nadia Bathaee,Anmol Jain,Anirban Das,Shi-Xiong Zhang,Sambit Sahu,Milind Naphade,Genta Indra Winata*

Main category: cs.CL

TL;DR: 论文介绍了T1数据集，用于评估语言模型在多轮对话中管理工具依赖关系的能力，并展示了T1-Agent在复杂场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多轮对话中处理工具依赖关系的能力仍有不足，需要更有效的评估方法。

Method: 提出T1数据集，涵盖多领域、多轮对话，支持动态重规划和缓存机制，用于评估工具使用和规划能力。

Result: T1-Agent展示了在复杂、依赖工具的场景中规划和推理的能力。

Conclusion: T1数据集为工具使用和规划研究提供了新基准，并验证了语言模型在复杂任务中的潜力。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [501] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
*Jin Jiang,Jianing Wang,Yuchen Yan,Yang Liu,Jianhua Zhu,Mengdi Zhang,Xunliang Cai,Liangcai Gao*

Main category: cs.CL

TL;DR: 该论文对大型语言模型（LLMs）在逻辑推理任务中的表现进行了全面评估，重点研究了形式语言的作用，并发现思维模型优于指令模型，但所有模型在归纳推理上仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注用形式语言引导LLMs推理，但对其能力的系统性评估不足，本文旨在填补这一空白。

Method: 从LLMs谱系、任务分类和轨迹格式三个维度，利用形式语言对LLMs进行综合评估，并尝试通过形式相关数据增强小模型。

Result: 思维模型表现更优，但所有模型在归纳推理上表现不佳；PoT格式数据泛化能力最佳；简单拒绝微调方法能提升模型性能。

Conclusion: 形式语言对LLMs逻辑推理能力有显著影响，但归纳推理仍是挑战；通过数据增强和微调可进一步提升模型表现。

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [502] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
*Huatong Song,Jinhao Jiang,Wenqing Tian,Zhipeng Chen,Yuhuan Wu,Jiahao Zhao,Yingqian Min,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: R1-Searcher++ 是一个新框架，通过两阶段训练策略（SFT 冷启动和 RL 动态知识获取）使 LLMs 能够自适应地利用内部和外部知识，提升检索增强推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前 RAG 方法成本高、泛化能力差且忽略模型内部知识，R1-Searcher++ 旨在解决这些问题。

Method: 采用两阶段训练：SFT 冷启动阶段学习初步格式，RL 阶段通过结果监督、奖励机制和记忆机制动态获取知识。

Result: 实验表明 R1-Searcher++ 优于现有 RAG 和推理方法，实现高效检索。

Conclusion: R1-Searcher++ 通过结合内部和外部知识，显著提升了 LLMs 的检索增强推理能力。

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [503] [Urban transport systems shape experiences of social segregation](https://arxiv.org/abs/2505.16337)
*Yitao Yang,Erjian Liu,Bin Jia,Ed Manley*

Main category: physics.soc-ph

TL;DR: 该研究探讨了城市交通系统如何影响社会隔离，通过GPS数据和概率模型分析社交互动，发现隔离是多维度的，并提出交通政策可能对隔离产生意外影响。


<details>
  <summary>Details</summary>
Motivation: 社会隔离不仅是居住地的问题，还涉及日常活动中的互动。现有研究缺乏交通系统与隔离之间的明确联系。

Method: 使用城市规模的GPS移动数据和概率移动框架，分析交通基础设施（如铁路、公交、道路和街区）中的社交互动。

Result: 社会隔离随时间、城市设计和交通服务而变化，是日常移动中可能遭遇的结果。交通政策可能对隔离产生意外影响。

Conclusion: 城市政策制定者需全面考虑干预措施对市民日常生活的广泛影响，以实现积极的社会变革。

Abstract: Mobility is a fundamental feature of human life, and through it our
interactions with the world and people around us generate complex and
consequential social phenomena. Social segregation, one such process, is
increasingly acknowledged as a product of one's entire lived experience rather
than mere residential location. Increasingly granular sources of data on human
mobility have evidenced how segregation persists outside the home, in
workplaces, cafes, and on the street. Yet there remains only a weak evidential
link between the production of social segregation and urban policy. This study
addresses this gap through an assessment of the role of the urban
transportation systems in shaping social segregation. Using city-scale GPS
mobility data and a novel probabilistic mobility framework, we establish social
interactions at the scale of transportation infrastructure, by rail and bus
service segment, individual roads, and city blocks. The outcomes show how
social segregation is more than a single process in space, but varying by time
of day, urban design and structure, and service design. These findings
reconceptualize segregation as a product of likely encounters during one's
daily mobility practice. We then extend these findings through exploratory
simulations, highlighting how transportation policy to promote sustainable
transport may have potentially unforeseen impacts on segregation. The study
underscores that to understand social segregation and achieve positive social
change urban policymakers must consider the broadest impacts of their
interventions and seek to understand their impact on the daily lived experience
of their citizens.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [504] [Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals](https://arxiv.org/abs/2505.16057)
*Ayae Ide,Tory Park,Jaron Mink,Tanusree Sharma*

Main category: cs.HC

TL;DR: 研究探讨了AI生成内容（AIG）的标识问题，通过访谈发现视觉和非视觉用户对标识的感知差异，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，平台采用标识以增强透明度，但这些标识对视觉障碍用户效果不佳，需改进其可访问性。

Method: 通过半结构化访谈（N=28，包括15名视力正常者和13名盲人或低视力者），分析用户对AIG标识的交互方式。

Result: 发现用户常忽略菜单辅助标识，依赖内容标识（如标题、评论）；视觉障碍用户因界面元素不可访问性面临更大挑战。

Conclusion: 提出多维度设计建议，以提升AIG标识的可访问性和有效性，尤其针对视觉障碍用户。

Abstract: AI-Generated (AIG) content has become increasingly widespread by recent
advances in generative models and the easy-to-use tools that have significantly
lowered the technical barriers for producing highly realistic audio, images,
and videos through simple natural language prompts. In response, platforms are
adopting provable provenance with platforms recommending AIG to be
self-disclosed and signaled to users. However, these indicators may be often
missed, especially when they rely solely on visual cues and make them
ineffective to users with different sensory abilities. To address the gap, we
conducted semi-structured interviews (N=28) with 15 sighted and 13 BLV
participants to examine their interaction with AIG content through
self-disclosed AI indicators. Our findings reveal diverse mental models and
practices, highlighting different strengths and weaknesses of content-based
(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While
sighted participants leveraged visual and audio cues, BLV participants
primarily relied on audio and existing assistive tools, limiting their ability
to identify AIG. Across both groups, they frequently overlooked menu-aided
indicators deployed by platforms and rather interacted with content-based
indicators such as title and comments. We uncovered usability challenges
stemming from inconsistent indicator placement, unclear metadata, and cognitive
overload. These issues were especially critical for BLV individuals due to the
insufficient accessibility of interface elements. We provide practical
recommendations and design implications for future AIG indicators across
several dimensions.

</details>


### [505] ["AI just keeps guessing": Using ARC Puzzles to Help Children Identify Reasoning Errors in Generative AI](https://arxiv.org/abs/2505.16034)
*Aayushi Dangol,Trushaa Ramanan,Runhua Zhao,Julie A. Kientz,Robert Wolfe,Jason Yip*

Main category: cs.HC

TL;DR: AI Puzzlers是一个基于ARC的交互系统，旨在帮助儿童识别和分析生成式AI中的错误，通过视觉和语言元素减少认知负荷。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的文本错误难以检测，且其权威语气可能导致儿童过度信任，因此需要开发工具帮助儿童批判性使用AI。

Method: 基于Mayer & Moreno的多媒体学习认知理论，设计AI Puzzlers系统，并通过21名儿童（6-11岁）的参与式设计会议验证。

Result: 研究发现儿童能够识别AI错误并制定应对策略，同时提供了设计见解和实证数据。

Conclusion: AI Puzzlers有效支持儿童识别和应对生成式AI的错误，为未来教育工具设计提供了参考。

Abstract: The integration of generative Artificial Intelligence (genAI) into everyday
life raises questions about the competencies required to critically engage with
these technologies. Unlike visual errors in genAI, textual mistakes are often
harder to detect and require specific domain knowledge. Furthermore, AI's
authoritative tone and structured responses can create an illusion of
correctness, leading to overtrust, especially among children. To address this,
we developed AI Puzzlers, an interactive system based on the Abstraction and
Reasoning Corpus (ARC), to help children identify and analyze errors in genAI.
Drawing on Mayer & Moreno's Cognitive Theory of Multimedia Learning, AI
Puzzlers uses visual and verbal elements to reduce cognitive overload and
support error detection. Based on two participatory design sessions with 21
children (ages 6 - 11), our findings provide both design insights and an
empirical understanding of how children identify errors in genAI reasoning,
develop strategies for navigating these errors, and evaluate AI outputs.

</details>


### [506] [Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach](https://arxiv.org/abs/2505.15974)
*Alan Ta,Nilsu Salgin,Mustafa Demir,Kala Philips Randal,Ranjana K. Mehta,Anthony McDonald,Carly McCord,Farzan Sasangohar*

Main category: cs.HC

TL;DR: 研究评估了一款结合智能手表和机器学习的移动健康干预工具mHELP，用于实时压力检测和自我管理。结果显示，治疗组在客观压力指标上显著降低，但主观焦虑、抑郁和压力评分无显著差异。


<details>
  <summary>Details</summary>
Motivation: 大学生面临心理健康问题但难以获得传统治疗，因此研究探索移动健康工具的有效性。

Method: 采用12周随机对照试验，比较使用完整mHELP干预的治疗组与仅记录压力的对照组，通过生理和自评指标分析效果。

Result: 治疗组的客观压力指标显著降低，但主观评分无显著差异；GAD-7和PSS评分有临床意义的下降。

Conclusion: 可穿戴移动健康工具对缓解大学生急性压力有效，但需进一步优化以应对慢性症状。

Abstract: College students are increasingly affected by stress, anxiety, and
depression, yet face barriers to traditional mental health care. This study
evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health
Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor
and machine learning (ML) algorithms for real-time stress detection and
self-management. In a 12-week randomized controlled trial (n = 117),
participants were assigned to a treatment group using mHELP's full suite of
interventions or a control group using the app solely for real-time stress
logging and weekly psychological assessments. The primary outcome, "Moments of
Stress" (MS), was assessed via physiological and self-reported indicators and
analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,
secondary outcomes of psychological assessments, including the Generalized
Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire
(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also
analyzed via GLMM. The finding of the objective measure, MS, indicates a
substantial decrease in MS among the treatment group compared to the control
group, while no notable between-group differences were observed in subjective
scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the
treatment group exhibited a clinically meaningful decline in GAD-7 and PSS
scores. These findings underscore the potential of wearable-enabled mHealth
tools to reduce acute stress in college populations and highlight the need for
extended interventions and tailored features to address chronic symptoms like
depression.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [507] [Implicit Neural Shape Optimization for 3D High-Contrast Electrical Impedance Tomography](https://arxiv.org/abs/2505.16487)
*Junqing Chen,Haibo Liu*

Main category: math.NA

TL;DR: 提出了一种基于隐式神经形状优化的3D高对比度电阻抗断层扫描（EIT）框架，用于解决材料界面电导率突变的问题。


<details>
  <summary>Details</summary>
Motivation: 高对比度场景（如金属植入物监测和工业缺陷检测）对传统重建方法提出了挑战，因其严重的病态性。

Method: 结合形状优化与隐式神经表示，提出形状导数优化方案和高效潜在空间表示。

Result: 通过理论分析和数值实验验证了性能的显著提升。

Conclusion: 该框架在医学成像和工业无损检测中具有实际应用潜力。

Abstract: We present a novel implicit neural shape optimization framework for 3D
high-contrast Electrical Impedance Tomography (EIT), addressing scenarios where
conductivity exhibits sharp discontinuities across material interfaces. These
high-contrast cases, prevalent in metallic implant monitoring and industrial
defect detection, challenge traditional reconstruction methods due to severe
ill-posedness. Our approach synergizes shape optimization with implicit neural
representations, introducing key innovations including a shape derivative-based
optimization scheme that explicitly incorporates high-contrast interface
conditions and an efficient latent space representation that reduces variable
dimensionality. Through rigorous theoretical analysis of algorithm convergence
and extensive numerical experiments, we demonstrate substantial performance
improvements, establishing our framework as promising for practical
applications in medical imaging with metallic implants and industrial
non-destructive testing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [508] [Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)](https://arxiv.org/abs/2505.15820)
*Gabriel Anzer,Kilian Arnsmeyer,Pascal Bauer,Joris Bekkers,Ulf Brefeld,Jesse Davis,Nicolas Evans,Matthias Kempe,Samuel J Robertson,Joshua Wyatt Smith,Jan Van Haaren*

Main category: cs.DB

TL;DR: 提出一种名为CDF的标准化足球数据格式，以解决不同数据提供商的数据差异问题，便于分析和决策。


<details>
  <summary>Details</summary>
Motivation: 足球比赛中多方收集的数据存在差异和兼容性问题，导致分析成本高昂，需要统一格式。

Method: 设计CDF格式，规范五种比赛数据的最小模式，确保数据清晰、完整且可追溯。

Result: CDF格式明确了技术规范、数据表示方式和交付方法，支持下游分析任务。

Conclusion: CDF为足球数据提供了标准化解决方案，降低了分析门槛，提升了数据可用性。

Abstract: During football matches, a variety of different parties (e.g., companies)
each collect (possibly overlapping) data about the match ranging from basic
information (e.g., starting players) to detailed positional data. This data is
provided to clubs, federations, and other organizations who are increasingly
interested in leveraging this data to inform their decision making.
Unfortunately, analyzing such data pose significant barriers because each
provider may (1) collect different data, (2) use different specifications even
within the same category of data, (3) represent the data differently, and (4)
delivers the data in a different manner (e.g., file format, protocol).
Consequently, working with these data requires a significant investment of time
and money. The goal of this work is to propose a uniform and standardized
format for football data called the Common Data Format (CDF). The CDF specifies
a minimal schema for five types of match data: match sheet data, video footage,
event data, tracking data, and match meta data. It aims to ensure that the
provided data is clear, sufficiently contextualized (e.g., its provenance is
clear), and complete such that it enables common downstream analysis tasks.
Concretely, this paper will detail the technical specifications of the CDF, the
representational choices that were made to help ensure the clarity of the
provided data, and a concrete approach for delivering data in the CDF.

</details>


### [509] [WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning](https://arxiv.org/abs/2505.16635)
*Zhaomin Wu,Ziyang Wang,Bingsheng He*

Main category: cs.DB

TL;DR: 论文介绍了WikiDBGraph，一个大规模的真实世界表格数据库图，用于解决协作学习中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据研究多集中于单表或孤立数据库，限制了模型能力，而协作学习方法因缺乏真实世界互联表格资源而面临挑战。

Method: 构建了WikiDBGraph，包含10万个WikiData表格数据库，通过1700万条边连接，并提取了13个节点和12条边的属性。

Result: 实验证实协作学习在新识别的数据库上表现更优，为结构化基础模型训练提供了潜力。

Conclusion: WikiDBGraph为互联表格数据学习提供了新资源，同时揭示了未来研究方向。

Abstract: Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [510] [Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets](https://arxiv.org/abs/2505.16027)
*Qinmei Xu,Yiheng Li,Xianghao Zhan,Ahmet Gorkem Er,Brittany Dashevsky,Chuanjun Xu,Mohammed Alawad,Mengya Yang,Liu Ya,Changsheng Zhou,Xiao Li,Haruka Itakura,Olivier Gevaert*

Main category: eess.IV

TL;DR: 该研究评估了基于视觉语言预训练的基础模型与传统CNN在跨国CXR数据集上的表现，发现基础模型在准确性和任务覆盖范围上优于CNN，但所有模型在儿科病例上表现较差。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型与传统CNN在多样化人群和诊断任务中的实际表现，以验证其泛化能力。

Method: 使用8个CXR诊断模型（5个基础模型和3个CNN）在37个标准化分类任务上进行测试，数据集来自多个国家。

Result: 基础模型表现优于CNN，其中MAVL模型在公共和私有数据集上表现最佳，但所有模型在儿科病例上表现显著下降。

Conclusion: 结构化监督和提示设计对放射AI有价值，未来需扩展地理覆盖和集成建模以支持临床部署。

Abstract: Foundation models leveraging vision-language pretraining have shown promise
in chest X-ray (CXR) interpretation, yet their real-world performance across
diverse populations and diagnostic tasks remains insufficiently evaluated. This
study benchmarks the diagnostic performance and generalizability of foundation
models versus traditional convolutional neural networks (CNNs) on multinational
CXR datasets. We evaluated eight CXR diagnostic models - five vision-language
foundation models and three CNN-based architectures - across 37 standardized
classification tasks using six public datasets from the USA, Spain, India, and
Vietnam, and three private datasets from hospitals in China. Performance was
assessed using AUROC, AUPRC, and other metrics across both shared and
dataset-specific tasks. Foundation models outperformed CNNs in both accuracy
and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and
structured supervision, achieved the highest performance on public (mean AUROC:
0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,
ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed
reduced performance on pediatric cases, with average AUROC dropping from 0.88
+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings
highlight the value of structured supervision and prompt design in radiologic
AI and suggest future directions including geographic expansion and ensemble
modeling for clinical deployment. Code for all evaluated models is available at
https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE

</details>


### [511] [MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models](https://arxiv.org/abs/2505.15822)
*Jhon Lopez,Carlos Hinojosa,Henry Arguello,Bernard Ghanem*

Main category: eess.IV

TL;DR: MambaStyle提出了一种基于视觉状态空间模型（VSSM）的高效GAN反演和编辑方法，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有GAN反演方法难以同时实现高质量重建、有效编辑和计算效率的平衡。

Method: 采用单阶段编码器架构，集成VSSM，减少参数和计算复杂度。

Result: 实验表明，MambaStyle在反演精度、编辑质量和计算效率上优于现有方法。

Conclusion: MambaStyle在降低模型复杂度和加速推理的同时，实现了高质量的反演和编辑，适用于实时应用。

Abstract: The task of inverting real images into StyleGAN's latent space to manipulate
their attributes has been extensively studied. However, existing GAN inversion
methods struggle to balance high reconstruction quality, effective editability,
and computational efficiency. In this paper, we introduce MambaStyle, an
efficient single-stage encoder-based approach for GAN inversion and editing
that leverages vision state-space models (VSSMs) to address these challenges.
Specifically, our approach integrates VSSMs within the proposed architecture,
enabling high-quality image inversion and flexible editing with significantly
fewer parameters and reduced computational complexity compared to
state-of-the-art methods. Extensive experiments show that MambaStyle achieves a
superior balance among inversion accuracy, editing quality, and computational
efficiency. Notably, our method achieves superior inversion and editing results
with reduced model complexity and faster inference, making it suitable for
real-time applications.

</details>


### [512] [Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI](https://arxiv.org/abs/2505.15984)
*Yamin Arefeen,Brett Levac,Bhairav Patel,Chang Ho,Jonathan I. Tamir*

Main category: eess.IV

TL;DR: 该研究利用扩散概率生成模型加速新生儿重症监护病房（NICU）中的MRI扫描，通过改进训练流程处理低信噪比和数据量不足的问题。


<details>
  <summary>Details</summary>
Motivation: NICU中的永久磁体MRI扫描时间长，信噪比低且接收线圈有限，需加速扫描以减少对患病婴儿的影响。

Method: 建立临床数据集，改进网络架构支持不同分辨率，训练单一模型并使用自监督去噪，通过后验样本平均重建图像。

Result: 结合所有数据、去噪预训练和后验样本平均，定量改进重建效果，生成模型无需重新训练即可适应不同加速率。临床研究表明1.5倍欠采样重建图像可用于临床。

Conclusion: 扩散概率生成模型结合改进流程可减少NICU中新生儿MRI的扫描时间。

Abstract: Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of
brain abnormalities during early life development. Permanent magnet scanners
operating in the neonatal intensive care unit (NICU) facilitate MRI of sick
infants, but have long scan times due to lower signal-to-noise ratios (SNR) and
limited receive coils. This work accelerates in-NICU MRI with diffusion
probabilistic generative models by developing a training pipeline accounting
for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal
MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical
Center. We propose a pipeline to handle the low quantity and SNR of our
real-world dataset (1) modifying existing network architectures to support
varying resolutions; (2) training a single model on all data with learned class
embedding vectors; (3) applying self-supervised denoising before training; and
(4) reconstructing by averaging posterior samples. Retrospective under-sampling
experiments, accounting for signal decay, evaluated each item of our proposed
methodology. A clinical reader study with practicing pediatric
neuroradiologists evaluated our proposed images reconstructed from 1.5x
under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior
samples yields quantitative improvements in reconstruction. The generative
model decouples the learned prior from the measurement model and functions at
two acceleration rates without re-training. The reader study suggests that
proposed images reconstructed from approximately 1.5x under-sampled data are
adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the
proposed pipeline to handle challenging real-world datasets could reduce scan
time of in-NICU neonatal MRI.

</details>


### [513] [P3Net: Progressive and Periodic Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2505.15861)
*Zhenyan Yao,Miao Zhang,Lanhu Wu,Yongri Piao,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: eess.IV

TL;DR: 论文提出了一种渐进周期性扰动机制（P3M）和边界聚焦损失，用于半监督医学图像分割，通过动态调整扰动和关注边界区域提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扰动技术缺乏深入理解，过度或不适当的扰动可能产生负面影响，需解决如何利用扰动机制引导学习及确保边界区域预测准确性。

Method: 提出P3M机制动态调整扰动，结合边界聚焦损失增强模型对边界细节的敏感性。

Result: 实验表明，方法在2D和3D数据集上达到最优性能，且P3M和损失函数可扩展至其他方法。

Conclusion: P3M和边界聚焦损失具有可扩展性和适用性，为半监督医学图像分割提供了有效工具。

Abstract: Perturbation with diverse unlabeled data has proven beneficial for
semi-supervised medical image segmentation (SSMIS). While many works have
successfully used various perturbation techniques, a deeper understanding of
learning perturbations is needed. Excessive or inappropriate perturbation can
have negative effects, so we aim to address two challenges: how to use
perturbation mechanisms to guide the learning of unlabeled data through labeled
data, and how to ensure accurate predictions in boundary regions. Inspired by
human progressive and periodic learning, we propose a progressive and periodic
perturbation mechanism (P3M) and a boundary-focused loss. P3M enables dynamic
adjustment of perturbations, allowing the model to gradually learn them. Our
boundary-focused loss encourages the model to concentrate on boundary regions,
enhancing sensitivity to intricate details and ensuring accurate predictions.
Experimental results demonstrate that our method achieves state-of-the-art
performance on two 2D and 3D datasets. Moreover, P3M is extendable to other
methods, and the proposed loss serves as a universal tool for improving
existing methods, highlighting the scalability and applicability of our
approach.

</details>


### [514] [Comprehensive Lung Disease Detection Using Deep Learning Models and Hybrid Chest X-ray Data with Explainable AI](https://arxiv.org/abs/2505.16028)
*Shuvashis Sarker,Shamim Rahim Refat,Faika Fairuj Preotee,Tanvir Rouf Shawon,Raihan Tanvir*

Main category: eess.IV

TL;DR: 研究探讨了深度学习与迁移学习模型在混合数据集上的表现，用于检测肺部疾病，结果显示混合数据集显著提升模型准确性和泛化能力，部分模型达到99%准确率。


<details>
  <summary>Details</summary>
Motivation: 肺部疾病的准确诊断对全球数百万人至关重要，研究旨在通过混合数据集提升模型的检测能力。

Method: 使用CNN、VGG16、VGG19等多种深度学习模型，结合混合数据集（来自孟加拉国和全球数据），并应用LIME技术增强模型可解释性。

Result: 混合数据集显著提升模型性能，VGG16、Xception等模型在混合数据集上达到99%准确率。

Conclusion: 混合数据集和可解释AI技术为医学影像提供了高准确性和可靠性的解决方案。

Abstract: Advanced diagnostic instruments are crucial for the accurate detection and
treatment of lung diseases, which affect millions of individuals globally. This
study examines the effectiveness of deep learning and transfer learning models
using a hybrid dataset, created by merging four individual datasets from
Bangladesh and global sources. The hybrid dataset significantly enhances model
accuracy and generalizability, particularly in detecting COVID-19, pneumonia,
lung opacity, and normal lung conditions from chest X-ray images. A range of
models, including CNN, VGG16, VGG19, InceptionV3, Xception, ResNet50V2,
InceptionResNetV2, MobileNetV2, and DenseNet121, were applied to both
individual and hybrid datasets. The results showed superior performance on the
hybrid dataset, with VGG16, Xception, ResNet50V2, and DenseNet121 each
achieving an accuracy of 99%. This consistent performance across the hybrid
dataset highlights the robustness of these models in handling diverse data
while maintaining high accuracy. To understand the models implicit behavior,
explainable AI techniques were employed to illuminate their black-box nature.
Specifically, LIME was used to enhance the interpretability of model
predictions, especially in cases of misclassification, contributing to the
development of reliable and interpretable AI-driven solutions for medical
imaging.

</details>


### [515] [OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates](https://arxiv.org/abs/2505.16091)
*Jinpei Guo,Yifei Ji,Zheng Chen,Kai Liu,Min Liu,Wang Rao,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: eess.IV

TL;DR: OSCAR是一种基于预训练扩散模型的一步图像压缩方法，支持多比特率，显著提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像压缩中计算开销大且需为不同比特率训练单独模型，增加了成本和存储负担。

Method: 将压缩潜在表示视为原始潜在表示的噪声变体，通过伪扩散时间步映射比特率，单生成模型支持多比特率重建。

Result: OSCAR在定量和视觉质量指标上表现优异，显著提升了推理效率。

Conclusion: OSCAR通过一步去噪实现高效多比特率图像压缩，具有实际应用潜力。

Abstract: Pretrained latent diffusion models have shown strong potential for lossy
image compression, owing to their powerful generative priors. Most existing
diffusion-based methods reconstruct images by iteratively denoising from random
noise, guided by compressed latent representations. While these approaches have
achieved high reconstruction quality, their multi-step sampling process incurs
substantial computational overhead. Moreover, they typically require training
separate models for different compression bit-rates, leading to significant
training and storage costs. To address these challenges, we propose a one-step
diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our
method views compressed latents as noisy variants of the original latents,
where the level of distortion depends on the bit-rate. This perspective allows
them to be modeled as intermediate states along a diffusion trajectory. By
establishing a mapping from the compression bit-rate to a pseudo diffusion
timestep, we condition a single generative model to support reconstructions at
multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich
structural information, thereby making one-step denoising feasible. Thus, OSCAR
replaces iterative sampling with a single denoising pass, significantly
improving inference efficiency. Extensive experiments demonstrate that OSCAR
achieves superior performance in both quantitative and visual quality metrics.
The code and models will be released at https://github.com/jp-guo/OSCAR.

</details>


### [516] [Compressing Human Body Video with Interactive Semantics: A Generative Approach](https://arxiv.org/abs/2505.16152)
*Bolin Chen,Shanzhi Yin,Hanwei Zhu,Lingyu Zhu,Zihan Zhang,Jie Chen,Ru-Ling Liao,Shiqi Wang,Yan Ye*

Main category: eess.IV

TL;DR: 提出了一种基于交互语义的人体视频压缩方法，通过嵌入可配置的语义表示实现视频编码的交互性和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统视频编码标准在超低码率下对人体视频的压缩效果有限，且缺乏交互性。本文旨在通过语义级表示实现高效压缩和交互控制。

Method: 使用3D人体模型将复杂的非线性动态和运动分解为可配置的嵌入表示，支持编辑、压缩和传输。解码器通过语义重建高质量人体视频。

Result: 实验表明，该方法在超低码率下优于VVC标准和生成式压缩方案，且无需额外预处理即可实现交互性。

Conclusion: 该框架为未来元宇宙中数字人通信提供了高效且交互性强的视频编码方案。

Abstract: In this paper, we propose to compress human body video with interactive
semantics, which can facilitate video coding to be interactive and controllable
by manipulating semantic-level representations embedded in the coded bitstream.
In particular, the proposed encoder employs a 3D human model to disentangle
nonlinear dynamics and complex motion of human body signal into a series of
configurable embeddings, which are controllably edited, compactly compressed,
and efficiently transmitted. Moreover, the proposed decoder can evolve the
mesh-based motion fields from these decoded semantics to realize the
high-quality human body video reconstruction. Experimental results illustrate
that the proposed framework can achieve promising compression performance for
human body videos at ultra-low bitrate ranges compared with the
state-of-the-art video coding standard Versatile Video Coding (VVC) and the
latest generative compression schemes. Furthermore, the proposed framework
enables interactive human body video coding without any additional
pre-/post-manipulation processes, which is expected to shed light on
metaverse-related digital human communication in the future.

</details>


### [517] [Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression](https://arxiv.org/abs/2505.16177)
*Linfeng Qi,Zhaoyang Jia,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: eess.IV

TL;DR: 提出了一种基于生成潜在编码（GLC）的图像和视频压缩方法，通过潜在空间变换编码实现高真实感和高保真度的超低比特率压缩。


<details>
  <summary>Details</summary>
Motivation: 现有像素空间变换编码方法因与人类感知不一致，难以在超低比特率下同时实现高真实感和高保真度。

Method: 在生成向量量化变分自编码器（VQ-VAE）的潜在空间中进行变换编码，并引入空间分类超模块和时空分类超模块优化性能。

Result: GLC-image在CLIC 2020测试集上比特率低于0.04 bpp，比MS-ILLM节省45%比特率；GLC-video比PLVC节省65.3%比特率。

Conclusion: GLC在超低比特率下显著提升了图像和视频压缩的视觉质量。

Abstract: Most existing approaches for image and video compression perform transform
coding in the pixel space to reduce redundancy. However, due to the
misalignment between the pixel-space distortion and human perception, such
schemes often face the difficulties in achieving both high-realism and
high-fidelity at ultra-low bitrate. To solve this problem, we propose
\textbf{G}enerative \textbf{L}atent \textbf{C}oding (\textbf{GLC}) models for
image and video compression, termed GLC-image and GLC-Video. The transform
coding of GLC is conducted in the latent space of a generative vector-quantized
variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent
space offers greater sparsity, richer semantics and better alignment with human
perception, and show its advantages in achieving high-realism and high-fidelity
compression. To further enhance performance, we improve the hyper prior by
introducing a spatial categorical hyper module in GLC-image and a
spatio-temporal categorical hyper module in GLC-video. Additionally, the
code-prediction-based loss function is proposed to enhance the semantic
consistency. Experiments demonstrate that our scheme shows high visual quality
at ultra-low bitrate for both image and video compression. For image
compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp,
achieving the same FID as previous SOTA model MS-ILLM while using $45\%$ fewer
bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves
65.3\% bitrate saving over PLVC in terms of DISTS.

</details>


### [518] [PCMamba: Physics-Informed Cross-Modal State Space Model for Dual-Camera Compressive Hyperspectral Imaging](https://arxiv.org/abs/2505.16373)
*Ge Meng,Zhongnan Cai,Jingyan Tu,Yingying Wang,Chenxin Li,Yue Huang,Xinghao Ding*

Main category: eess.IV

TL;DR: 提出了一种基于物理信息的跨模态状态空间模型网络（PCMamba），用于双相机压缩高光谱成像（DCCHI），通过结合物理成像过程和线性复杂度的Mamba模型，实现轻量级且高质量的高光谱重建。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从2D压缩测量和PAN图像中显式提取光谱和空间信息，导致高光谱重建瓶颈。受物理因素（如温度、发射率和物体间多次反射）启发，研究物理属性间的相互关系以提供更深的理论支持。

Method: 提出PCMamba模型，将高光谱热信号的物理成像过程融入Mamba的线性复杂度中，设计跨模态扫描Mamba块（CSMB）实现模态间像素级交互。

Result: 在真实和模拟数据集上的实验表明，该方法在定量和定性指标上显著优于现有最优方法。

Conclusion: PCMamba通过物理驱动的合成过程和跨模态交互，实现了高效且高质量的高光谱重建，为相关领域提供了新的理论和技术支持。

Abstract: Panchromatic (PAN) -assisted Dual-Camera Compressive Hyperspectral Imaging
(DCCHI) is a key technology in snapshot hyperspectral imaging. Existing
research primarily focuses on exploring spectral information from 2D
compressive measurements and spatial information from PAN images in an explicit
manner, leading to a bottleneck in HSI reconstruction. Various physical
factors, such as temperature, emissivity, and multiple reflections between
objects, play a critical role in the process of a sensor acquiring
hyperspectral thermal signals. Inspired by this, we attempt to investigate the
interrelationships between physical properties to provide deeper theoretical
insights for HSI reconstruction. In this paper, we propose a Physics-Informed
Cross-Modal State Space Model Network (PCMamba) for DCCHI, which incorporates
the forward physical imaging process of HSI into the linear complexity of Mamba
to facilitate lightweight and high-quality HSI reconstruction. Specifically, we
analyze the imaging process of hyperspectral thermal signals to enable the
network to disentangle the three key physical properties-temperature,
emissivity, and texture. By fully exploiting the potential information embedded
in 2D measurements and PAN images, the HSIs are reconstructed through a
physics-driven synthesis process. Furthermore, we design a Cross-Modal Scanning
Mamba Block (CSMB) that introduces inter-modal pixel-wise interaction with
positional inductive bias by cross-scanning the backbone features and PAN
features. Extensive experiments conducted on both real and simulated datasets
demonstrate that our method significantly outperforms SOTA methods in both
quantitative and qualitative metrics.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [519] [Data-driven Verification of Procedural Programs with Integer Arrays](https://arxiv.org/abs/2505.15958)
*Ahmed Bouajjani,Wael-Amine Boutglay,Peter Habermehl*

Main category: cs.PL

TL;DR: 提出了一种基于数据驱动的方法，用于自动验证处理参数化整数数组的程序，通过合成循环不变式和过程前后条件。


<details>
  <summary>Details</summary>
Motivation: 解决自动验证处理参数化数组的程序的问题，特别是如何高效合成循环不变式和过程前后条件。

Method: 扩展了决策树Horn-ICE框架以处理数组，采用数据驱动方法，将复杂的整数数组向量分类问题简化为整数向量分类问题。

Result: 实现了该方法，并在基准测试中展示了其高效性和竞争力。

Conclusion: 该方法能够有效合成循环不变式和过程前后条件，验证程序的正确性。

Abstract: We address the problem of verifying automatically procedural programs
manipulating parametric-size arrays of integers, encoded as a constrained Horn
clauses solving problem. We propose a new algorithmic method for synthesizing
loop invariants and procedure pre/post-conditions represented as universally
quantified first-order formulas constraining the array elements and program
variables. We adopt a data-driven approach that extends the decision tree
Horn-ICE framework to handle arrays. We provide a powerful learning technique
based on reducing a complex classification problem of vectors of integer arrays
to a simpler classification problem of vectors of integers. The obtained
classifier is generalized to get universally quantified invariants and
procedure pre/post-conditions. We have implemented our method and shown its
efficiency and competitiveness w.r.t. state-of-the-art tools on a significant
benchmark.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [520] [Multimodal Generative AI for Story Point Estimation in Software Development](https://arxiv.org/abs/2505.16290)
*Mohammad Rubyet Islam,Peter Sandborn*

Main category: cs.SE

TL;DR: 研究探讨了多模态生成AI在敏捷软件开发中故事点估算的应用，结合文本、图像和分类数据，超越传统单模态方法的限制。


<details>
  <summary>Details</summary>
Motivation: 传统单模态估算方法存在局限性，研究旨在通过多模态数据集成提升估算精度。

Method: 整合BERT、CNN和XGBoost等先进模型，处理文本、图像和分类数据。

Result: 在简单故事点估算上表现优异，复杂类别因数据不平衡存在挑战；分类数据（如严重性）对模型性能有显著影响。

Conclusion: 多模态数据集成在AI驱动的项目管理中具有变革潜力，未来需解决数据变异性以增强AI在敏捷方法中的鲁棒性。

Abstract: This research explores the application of Multimodal Generative AI to enhance
story point estimation in Agile software development. By integrating text,
image, and categorical data using advanced models like BERT, CNN, and XGBoost,
our approach surpasses the limitations of traditional single-modal estimation
methods. The results demonstrate strong accuracy for simpler story points,
while also highlighting challenges in more complex categories due to data
imbalance. This study further explores the impact of categorical data,
particularly severity, on the estimation process, emphasizing its influence on
model performance. Our findings emphasize the transformative potential of
multimodal data integration in refining AI-driven project management, paving
the way for more precise, adaptable, and domain-specific AI capabilities.
Additionally, this work outlines future directions for addressing data
variability and enhancing the robustness of AI in Agile methodologies.

</details>


### [521] [AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI](https://arxiv.org/abs/2505.16430)
*Martin Goodfellow,Robbie Booth,Andrew Fagan,Alasdair Lambert*

Main category: cs.SE

TL;DR: AutoMCQ利用生成式AI自动生成代码理解选择题，解决学生代码理解不足和检测抄袭问题，并与CodeRunner平台集成。


<details>
  <summary>Details</summary>
Motivation: 学生常对代码理解不足，且问题可能后期才显现，同时生成式AI工具的普及增加了代码理解的重要性。传统方法耗时且难以扩展。

Method: 利用生成式AI自动生成代码理解选择题，并与CodeRunner自动化评估平台集成。

Result: AutoMCQ能高效生成代码理解问题，帮助评估学生理解并检测抄袭。

Conclusion: AutoMCQ提供了一种可扩展的解决方案，解决了代码理解评估的效率和规模问题。

Abstract: Students often do not fully understand the code they have written. This
sometimes does not become evident until later in their education, which can
mean it is harder to fix their incorrect knowledge or misunderstandings. In
addition, being able to fully understand code is increasingly important in a
world where students have access to generative artificial intelligence (GenAI)
tools, such as GitHub Copilot. One effective solution is to utilise code
comprehension questions, where a marker asks questions about a submission to
gauge understanding, this can also have the side effect of helping to detect
plagiarism. However, this approach is time consuming and can be difficult
and/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for
the automatic generation of multiple-choice code comprehension questions. This
is integrated with the CodeRunner automated assessment platform.

</details>


### [522] [Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks](https://arxiv.org/abs/2505.16901)
*Hongyuan Tao,Ying Zhang,Zhenhao Tang,Hongen Peng,Xukun Zhu,Bingchang Liu,Yingguang Yang,Ziyin Zhang,Zhaogui Xu,Haipeng Zhang,Linchao Zhu,Rui Wang,Hang Yu,Jianguo Li,Peng Di*

Main category: cs.SE

TL;DR: 开源LLMs通过代码图模型（CGMs）解决仓库级任务，无需代理方法，性能优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 当前仓库级软件工程任务依赖专有LLM代理，存在不可预测性和隐私问题，研究开源LLMs的可行性。

Method: 引入CGMs，将代码图结构与LLM注意力机制结合，使用适配器映射节点属性。

Result: 在SWE-bench Lite基准测试中达到43.00%解决率，性能排名靠前。

Conclusion: 开源LLMs结合CGMs可有效处理仓库级任务，性能显著提升。

Abstract: Recent advances in Large Language Models (LLMs) have shown promise in
function-level code generation, yet repository-level software engineering tasks
remain challenging. Current solutions predominantly rely on proprietary LLM
agents, which introduce unpredictability and limit accessibility, raising
concerns about data privacy and model customization. This paper investigates
whether open-source LLMs can effectively address repository-level tasks without
requiring agent-based approaches. We demonstrate this is possible by enabling
LLMs to comprehend functions and files within codebases through their semantic
information and structural dependencies. To this end, we introduce Code Graph
Models (CGMs), which integrate repository code graph structures into the LLM's
attention mechanism and map node attributes to the LLM's input space using a
specialized adapter. When combined with an agentless graph RAG framework, our
approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark
using the open-source Qwen2.5-72B model. This performance ranks first among
open weight models, second among methods with open-source systems, and eighth
overall, surpassing the previous best open-source model-based method by 12.33%.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [523] [Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study](https://arxiv.org/abs/2505.16136)
*Yuke Zhang*

Main category: q-fin.CP

TL;DR: 该研究提出了一种可解释的机器学习框架，利用全球新闻情绪提取宏观经济alpha。通过FinBERT处理GDELT新闻数据，构建情绪指数，并利用XGBoost预测次日资产回报，表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合领域特定的NLP和可解释的ML，从新闻情绪中提取宏观经济alpha。

Method: 使用FinBERT处理GDELT新闻数据，构建情绪指数（包括平均情绪、离散度和事件影响），并利用XGBoost分类器预测EUR/USD、USD/JPY和10年期美国国债期货的次日回报。

Result: XGBoost策略在样本外测试中表现优异，Sharpe比率和年复合增长率显著高于基准。SHAP分析显示情绪离散度和文章影响是关键预测特征。

Conclusion: 结合领域特定NLP和可解释ML是提取宏观经济alpha的有效且可解释的方法。

Abstract: This study introduces an interpretable machine learning (ML) framework to
extract macroeconomic alpha from global news sentiment. We process the Global
Database of Events, Language, and Tone (GDELT) Project's worldwide news feed
using FinBERT -- a Bidirectional Encoder Representations from Transformers
(BERT) based model pretrained on finance-specific language -- to construct
daily sentiment indices incorporating mean tone, dispersion, and event impact.
These indices drive an XGBoost classifier, benchmarked against logistic
regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.
Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold
expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates
exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios
achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective
compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and
22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment
dispersion and article impact are key predictive features. Our findings
establish that integrating domain-specific Natural Language Processing (NLP)
with interpretable ML offers a potent and explainable source of macro alpha.

</details>
