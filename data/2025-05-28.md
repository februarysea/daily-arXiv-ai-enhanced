<div id=toc></div>

# Table of Contents

- [cs.CY](#cs.CY) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.SI](#cs.SI) [Total: 5]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.CV](#cs.CV) [Total: 140]
- [cs.LG](#cs.LG) [Total: 139]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.RO](#cs.RO) [Total: 11]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 13]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [math.CT](#math.CT) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1] [Opacity as a Feature, Not a Flaw: The LoBOX Governance Ethic for Role-Sensitive Explainability and Institutional Trust in AI](https://arxiv.org/abs/2505.20304)
*Francisco Herrera,Reyes Calderón*

Main category: cs.CY

TL;DR: LoBOX框架提出了一种管理AI不透明性的伦理治理方法，强调通过角色校准解释和制度问责来治理不透明性，而非追求完全透明。


<details>
  <summary>Details</summary>
Motivation: AI的不透明性常被视为设计缺陷，但LoBOX将其视为需主动治理的条件，以建立可信赖的AI系统。

Method: 框架包含三阶段路径：减少意外不透明性、界定不可减少的不透明性、通过结构化监督委托信任，并结合RED/BLUE XAI模型与法律工具。

Result: LoBOX提供了一种可扩展且情境感知的替代方案，将信任重新定义为制度可信度、结构化解释和利益相关者问责的结果。

Conclusion: 不透明性应被视为需主动治理的特征，而非缺陷，强调AI可信赖性需制度化和情境化。

Abstract: This paper introduces LoBOX (Lack of Belief: Opacity \& eXplainability)
governance ethic structured framework for managing artificial intelligence (AI)
opacity when full transparency is infeasible. Rather than treating opacity as a
design flaw, LoBOX defines it as a condition that can be ethically governed
through role-calibrated explanation and institutional accountability. The
framework comprises a three-stage pathway: reduce accidental opacity, bound
irreducible opacity, and delegate trust through structured oversight.
Integrating the RED/BLUE XAI model for stakeholder-sensitive explanation and
aligned with emerging legal instruments such as the EU AI Act, LoBOX offers a
scalable and context-aware alternative to transparency-centric approaches.
Reframe trust not as a function of complete system explainability, but as an
outcome of institutional credibility, structured justification, and
stakeholder-responsive accountability. A governance loop cycles back to ensure
that LoBOX remains responsive to evolving technological contexts and
stakeholder expectations, to ensure the complete opacity governance. We move
from transparency ideals to ethical governance, emphasizing that
trustworthiness in AI must be institutionally grounded and contextually
justified. We also discuss how cultural or institutional trust varies in
different contexts. This theoretical framework positions opacity not as a flaw
but as a feature that must be actively governed to ensure responsible AI
systems.

</details>


### [2] [Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI](https://arxiv.org/abs/2505.20305)
*Francisco Herrera*

Main category: cs.CY

TL;DR: 本文探讨了可解释AI（XAI）在大型语言模型（LLMs）中的重要性，围绕三个核心问题展开：为什么需要可解释性？其技术和伦理维度是什么？如何在实际部署中发挥作用？


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在敏感领域（如医疗、法律、教育）的广泛应用，透明、可解释且负责任的AI系统需求日益迫切。XAI作为LLMs与利益相关者之间的桥梁，对高风险决策至关重要。

Method: 论文围绕四个核心维度（忠实性、真实性、合理性、对比性）展开，探讨XAI的技术和伦理设计，并分析其在现实部署中的作用。

Result: XAI能够支持认知清晰、法规合规和针对不同受众的可理解性。论文还讨论了XAI的评估方法及新兴技术（如受众敏感XAI、机制可解释性等）。

Conclusion: 可解释性需发展为一种公民基础设施，以促进信任、支持争议解决，并确保AI系统与机构问责和以人为本的决策保持一致。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
domains such as healthcare, law, and education, the demand for transparent,
interpretable, and accountable AI systems becomes more urgent. Explainable AI
(XAI) acts as a crucial interface between the opaque reasoning of LLMs and the
diverse stakeholders who rely on their outputs in high-risk decisions. This
paper presents a comprehensive reflection and survey of XAI for LLMs, framed
around three guiding questions: Why is explainability essential? What technical
and ethical dimensions does it entail? And how can it fulfill its role in
real-world deployment?
  We highlight four core dimensions central to explainability in LLMs,
faithfulness, truthfulness, plausibility, and contrastivity, which together
expose key design tensions and guide the development of explanation strategies
that are both technically sound and contextually appropriate. The paper
discusses how XAI can support epistemic clarity, regulatory compliance, and
audience-specific intelligibility across stakeholder roles and decision
settings.
  We further examine how explainability is evaluated, alongside emerging
developments in audience-sensitive XAI, mechanistic interpretability, causal
reasoning, and adaptive explanation systems. Emphasizing the shift from
surface-level transparency to governance-ready design, we identify critical
challenges and future research directions for ensuring the responsible use of
LLMs in complex societal contexts. We argue that explainability must evolve
into a civic infrastructure fostering trust, enabling contestability, and
aligning AI systems with institutional accountability and human-centered
decision-making.

</details>


### [3] [The EU AI Act, Stakeholder Needs, and Explainable AI: Aligning Regulatory Compliance in a Clinical Decision Support System](https://arxiv.org/abs/2505.20311)
*Anton Hummel,Håkan Burden,Susanne Stenberg,Jan-Philipp Steghöfer,Niklas Kühl*

Main category: cs.CY

TL;DR: 论文探讨了可解释AI（XAI）与欧盟AI法案的关系，旨在弥合两者在关注点上的分歧，并通过案例分析展示XAI如何满足法案要求与用户需求。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案与XAI的关注点存在脱节，前者侧重提供者和部署者的义务，后者则关注终端用户需求。研究旨在弥合这一分歧。

Method: 通过跨学科团队讨论，分析一个AI临床决策支持系统，评估XAI技术如何满足法案要求与用户需求。

Result: XAI技术可以填补法案要求与用户需求之间的空白，并展示了两者的相似与差异。

Conclusion: 研究鼓励XAI社区反思其在AI法案中的角色，促进跨学科对XAI与法案影响的理解。

Abstract: Explainable AI (XAI) is a promising solution to ensure compliance with the EU
AI Act, the first multi-national regulation for AI. XAI aims to enhance
transparency and human oversight of AI systems, particularly ``black-box
models'', which are criticized as incomprehensible. However, the discourse
around the main stakeholders in the AI Act and XAI appears disconnected. While
XAI prioritizes the end user's needs as the primary goal, the AI Act focuses on
the obligations of the provider and deployer of the AI system. We aim to bridge
this divide and provide guidance on how these two worlds are related. By
fostering an interdisciplinary discussion in a cross-functional team with XAI,
AI Act, legal, and requirements engineering experts, we walk through the steps
necessary to analyze an AI-based clinical decision support system to clarify
the end-user needs and assess AI Act applicability. By analyzing our justified
understanding using an AI system under development as a case, we show that XAI
techniques can fill a gap between stakeholder needs and the requirements of the
AI Act. We look at the similarities and contrasts between the legal
requirements and the needs of stakeholders. In doing so, we encourage
researchers and practitioners from the XAI community to reflect on their role
towards the AI Act by achieving a mutual understanding of the implications of
XAI and the AI Act within different disciplines.

</details>


### [4] [Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions](https://arxiv.org/abs/2505.20312)
*Aditya Bhattacharya,Katrien Verbert*

Main category: cs.CY

TL;DR: 论文提出了一种基于多代理AI和大型语言模型的招聘系统，旨在提高招聘过程的透明度和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统招聘方法缺乏透明度，申请者难以获得决策的合理解释。

Method: 采用用户中心设计方法，分两阶段研究并开发多代理AI系统，随后通过用户访谈评估原型。

Result: 参与者认为该系统比传统方法更具可操作性、可信度和公平性。

Conclusion: 研究为构建用户对齐的多代理可解释AI系统提供了设计启示。

Abstract: During job recruitment, traditional applicant selection methods often lack
transparency. Candidates are rarely given sufficient justifications for
recruiting decisions, whether they are made manually by human recruiters or
through the use of black-box Applicant Tracking Systems (ATS). To address this
problem, our work introduces a multi-agent AI system that uses Large Language
Models (LLMs) to guide job seekers during the recruitment process. Using an
iterative user-centric design approach, we first conducted a two-phased
exploratory study with four active job seekers to inform the design and
development of the system. Subsequently, we conducted an in-depth, qualitative
user study with 20 active job seekers through individual one-to-one interviews
to evaluate the developed prototype. The results of our evaluation demonstrate
that participants perceived our multi-agent recruitment system as significantly
more actionable, trustworthy, and fair compared to traditional methods. Our
study further helped us uncover in-depth insights into factors contributing to
these perceived user experiences. Drawing from these insights, we offer broader
design implications for building user-aligned, multi-agent explainable AI
systems across diverse domains.

</details>


### [5] [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
*Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.CY

TL;DR: 本文提出了一种新框架，用于系统评估视觉语言模型（VLMs）在种族、性别和身体特征方面的文化偏见，揭示了模型如何无意中强化社会刻板印象。


<details>
  <summary>Details</summary>
Motivation: VLMs在不同文化背景中的内部偏见尚未被充分理解，因此需要一种系统方法来评估这些偏见。

Method: 设计了三种基于检索的任务：种族与国家关联、个人特质与国家关联、以及身体特征与国家关联。

Result: 研究发现VLMs存在持续的偏见，表明视觉表征可能无意中强化社会刻板印象。

Conclusion: 研究强调了评估和解决VLMs中文化偏见的重要性，以避免模型对社会刻板印象的进一步强化。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in diverse cultural
contexts, yet their internal biases remain poorly understood. In this work, we
propose a novel framework to systematically evaluate how VLMs encode cultural
differences and biases related to race, gender, and physical traits across
countries. We introduce three retrieval-based tasks: (1) Race to Country
retrieval, which examines the association between individuals from specific
racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and
Black) and different countries; (2) Personal Traits to Country retrieval, where
images are paired with trait-based prompts (e.g., Smart, Honest, Criminal,
Violent) to investigate potential stereotypical associations; and (3) Physical
Characteristics to Country retrieval, focusing on visual attributes like
skinny, young, obese, and old to explore how physical appearances are
culturally linked to nations. Our findings reveal persistent biases in VLMs,
highlighting how visual representations may inadvertently reinforce societal
stereotypes.

</details>


### [6] [Generative AI in Computer Science Education: Accelerating Python Learning with ChatGPT](https://arxiv.org/abs/2505.20329)
*Ian McCulloh,Pedro Rodriguez,Srivaths Kumar,Manu Gupta,Viplove Raj Sharma,Benjamin Johnson,Anthony N. Johnson*

Main category: cs.CY

TL;DR: 研究评估了将生成式AI（如ChatGPT）融入Python编程模块的效果，发现其能有效缩小初学者与有经验者的差距，支持快速学习。


<details>
  <summary>Details</summary>
Motivation: 数字素养和AI能力需求增长，需要可扩展的高效编程教学方法。

Method: 在16周职业培训课程中，86名学员使用ChatGPT完成异步Python学习，通过30次代码评估测试效果。

Result: 无编程经验的学员初期表现较差，但完成模块后差距消失，表明ChatGPT支持快速学习。

Conclusion: 生成式AI可降低学习门槛，但需平衡AI辅助与独立解决问题能力，适合数字经济的再培训。

Abstract: The increasing demand for digital literacy and artificial intelligence (AI)
fluency in the workforce has highlighted the need for scalable, efficient
programming instruction. This study evaluates the effectiveness of integrating
generative AI, specifically OpenAIs ChatGPT, into a self-paced Python
programming module embedded within a sixteen-week professional training course
on applied generative AI. A total of 86 adult learners with varying levels of
programming experience completed asynchronous Python instruction in Weeks three
and four, using ChatGPT to generate, interpret, and debug code. Python
proficiency and general coding knowledge was assessed across 30 different
assessments during the first 13 weeks of the course through timed, code-based
evaluations. A mixed-design ANOVA revealed that learners without prior
programming experience scored significantly lower than their peers on early
assessments. However, following the completion of the accelerated Python
instruction module, these group differences were no longer statistically
significant,, indicating that the intervention effectively closed initial
performance gaps and supported proficiency gains across all learner groups.
These findings suggest that generative AI can support accelerated learning
outcomes and reduce entry barriers for learners with no prior coding
background. While ChatGPT effectively facilitated foundational skill
acquisition, the study also highlights the importance of balancing AI
assistance with opportunities for independent problem-solving. The results
support the potential of AI-augmented instruction as a scalable model for
reskilling in the digital economy.

</details>


### [7] [Racism, Resistance, and Reddit: How Popular Culture Sparks Online Reckonings](https://arxiv.org/abs/2505.21016)
*Sherry Mason,Tawfiq Ammari*

Main category: cs.CY

TL;DR: 研究分析了Reddit用户对《Lovecraft Country》和《Watchmen》两部剧集中种族叙事的互动，揭示了三种动态社交角色及其在种族讨论中的转变。


<details>
  <summary>Details</summary>
Motivation: 探讨用户如何在匿名平台中通过种族叙事互动，以及这种互动如何影响集体对种族和历史记忆的理解。

Method: 结合叙事说服和多步流动理论，对3,879条Reddit评论进行主题建模和批判性话语分析。

Result: 发现三种动态角色（支持者、反对者和适应者），并揭示匿名性如何促进角色流动、意见领导和道德参与。

Conclusion: 研究表明流行文化和参与式平台在塑造种族和历史记忆的集体意义建构中具有重要作用。

Abstract: This study examines how Reddit users engaged with the racial narratives of
Lovecraft Country and Watchmen, two television series that reimagine historical
racial trauma. Drawing on narrative persuasion and multistep flow theory, we
analyze 3,879 Reddit comments using topic modeling and critical discourse
analysis. We identify three dynamic social roles advocates, adversaries, and
adaptives and explore how users move between them in response to racial
discourse. Findings reveal how Reddits pseudonymous affordances shape role
fluidity, opinion leadership, and moral engagement. While adversaries minimized
or rejected racism as exaggerated, advocates shared standpoint experiences and
historical resources to challenge these claims. Adaptive users shifted
perspectives over time, demonstrating how online publics can foster critical
racial learning. This research highlights how popular culture and participatory
platforms intersect in shaping collective meaning making around race and
historical memory.

</details>


### [8] [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
*Anna Neumann,Elisabeth Kirsten,Muhammad Bilal Zafar,Jatinder Singh*

Main category: cs.CY

TL;DR: 研究探讨了大型语言模型（LLM）中系统提示对模型行为的影响，揭示了因系统提示不透明导致的潜在偏见和危害。


<details>
  <summary>Details</summary>
Motivation: 系统提示在LLM中优先于用户输入，但其多层叠加和不透明性可能引发未察觉的偏见和危害，需要深入研究。

Method: 比较了六种商用LLM中系统提示与用户提示对50个人口统计群体信息处理的影响。

Result: 发现系统提示显著影响模型行为，导致用户表征和决策场景中的偏见，这些偏见难以被用户察觉或纠正。

Conclusion: 建议将系统提示分析纳入AI审计流程，以应对商业AI部署中日益普遍的系统提示定制化问题。

Abstract: System prompts in Large Language Models (LLMs) are predefined directives that
guide model behaviour, taking precedence over user inputs in text processing
and generation. LLM deployers increasingly use them to ensure consistent
responses across contexts. While model providers set a foundation of system
prompts, deployers and third-party developers can append additional prompts
without visibility into others' additions, while this layered implementation
remains entirely hidden from end-users. As system prompts become more complex,
they can directly or indirectly introduce unaccounted for side effects. This
lack of transparency raises fundamental questions about how the position of
information in different directives shapes model outputs. As such, this work
examines how the placement of information affects model behaviour. To this end,
we compare how models process demographic information in system versus user
prompts across six commercially available LLMs and 50 demographic groups. Our
analysis reveals significant biases, manifesting in differences in user
representation and decision-making scenarios. Since these variations stem from
inaccessible and opaque system-level configurations, they risk
representational, allocative and potential other biases and downstream harms
beyond the user's ability to detect or correct. Our findings draw attention to
these critical issues, which have the potential to perpetuate harms if left
unexamined. Further, we argue that system prompt analysis must be incorporated
into AI auditing processes, particularly as customisable system prompts become
increasingly prevalent in commercial AI deployments.

</details>


### [9] [Simulating Ethics: Using LLM Debate Panels to Model Deliberation on Medical Dilemmas](https://arxiv.org/abs/2505.21112)
*Hazem Zohny*

Main category: cs.CY

TL;DR: ADEPT系统利用LLM角色模拟多视角伦理辩论，展示了不同伦理框架对决策的影响。


<details>
  <summary>Details</summary>
Motivation: 研究多视角伦理辩论的模拟方法，以解决复杂道德问题，如稀缺医疗资源分配。

Method: 通过LLM角色（如义务论者、结果论者等）组成辩论小组，分析不同伦理框架对决策的影响。

Result: 不同伦理框架的辩论小组最终支持相同的政策（加权抽签系统），但论证路径和投票联盟因角色变化而改变。

Conclusion: ADEPT为生物伦理学的多智能体辩论提供了透明、可复现的工作流程，并展示了伦理视角对决策的重要性。

Abstract: This paper introduces ADEPT, a system using Large Language Model (LLM)
personas to simulate multi-perspective ethical debates. ADEPT assembles panels
of 'AI personas', each embodying a distinct ethical framework or stakeholder
perspective (like a deontologist, consequentialist, or disability rights
advocate), to deliberate on complex moral issues. Its application is
demonstrated through a scenario about prioritizing patients for a limited
number of ventilators inspired by real-world challenges in allocating scarce
medical resources. Two debates, each with six LLM personas, were conducted;
they only differed in the moral viewpoints represented: one included a Catholic
bioethicist and a care theorist, the other substituted a rule-based Kantian
philosopher and a legal adviser. Both panels ultimately favoured the same
policy -- a lottery system weighted for clinical need and fairness, crucially
avoiding the withdrawal of ventilators for reallocation. However, each panel
reached that conclusion through different lines of argument, and their voting
coalitions shifted once duty- and rights-based voices were present. Examination
of the debate transcripts shows that the altered membership redirected
attention toward moral injury, legal risk and public trust, which in turn
changed four continuing personas' final positions. The work offers three
contributions: (i) a transparent, replicable workflow for running and analysing
multi-agent AI debates in bioethics; (ii) evidence that the moral perspectives
included in such panels can materially change the outcome even when the factual
inputs remain constant; and (iii) an analysis of the implications and future
directions for such AI-mediated approaches to ethical deliberation and policy.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [10] [THE WASTIVE: An Interactive Ebb and Flow of Digital Fabrication Waste](https://arxiv.org/abs/2505.21153)
*Yifan Shan,Bo Liu,Sebastian Bidegain,Thijs Roumen*

Main category: cs.MM

TL;DR: "THE WASTIVE" 是一个互动艺术装置，将数字制造废料拟人化为有感知的观察者，通过诗意互动引发观众对可持续实践的反思。


<details>
  <summary>Details</summary>
Motivation: 探索数字制造废料与人类及环境的互动关系，呼吁更可持续的创作与消费习惯。

Method: 通过互动艺术装置，让废料“观察”并回应观众，模拟海洋的节奏。

Result: 将技术废料转化为感官体验，激发观众对环境与消费的反思。

Conclusion: "THE WASTIVE" 通过艺术形式促进对可持续性和环境互联性的思考。

Abstract: What if digital fabrication waste could observe the world? What would they
see? What would they say? "THE WASTIVE" reimagines digital fabrication waste as
sentient observers, giving them a poetic voice through interactive art. As
viewers approach, the installation awakens, mimicking the rhythmic ebb and flow
of ocean waves - a silent dialogue where discarded materials "observe" and
respond to human presence. These interactions echo the gentle murmurs of the
sea, transforming technological residue into a reflective, sensory experience.
Through this artistic contemplation, "THE WASTIVE" invites audiences to
reconsider their creative processes and consumption habits. It serves as a
poetic call for more mindful, sustainable practices, provoking deeper
reflections on our interconnectedness with the environment.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [11] [xChemAgents: Agentic AI for Explainable Quantum Chemistry](https://arxiv.org/abs/2505.20574)
*Can Polat,Mehmet Tuncel,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Main category: cs.MA

TL;DR: xChemAgents提出了一种基于多模态图神经网络的合作代理框架，通过选择器和验证器优化化学描述符的使用，显著提升了预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在将大量异质化学描述符与原子几何信息结合时，可能降低对分子形状或对称性敏感任务的性能，并损害可解释性。xChemAgents旨在通过物理感知推理解决这一问题。

Method: xChemAgents包含两个基于语言模型的代理：Selector自适应选择与目标相关的稀疏加权描述符并提供自然语言解释；Validator通过对话强制执行物理约束（如单位一致性和缩放定律）。

Result: 在标准基准数据集上，xChemAgents将平均绝对误差降低了22%，同时生成可信且易于理解的解释。

Conclusion: 合作自验证代理在提升基础模型驱动的材料科学精度和透明度方面具有潜力。

Abstract: Recent progress in multimodal graph neural networks has demonstrated that
augmenting atomic XYZ geometries with textual chemical descriptors can enhance
predictive accuracy across a range of electronic and thermodynamic properties.
However, naively appending large sets of heterogeneous descriptors often
degrades performance on tasks sensitive to molecular shape or symmetry, and
undermines interpretability. xChemAgents proposes a cooperative agent framework
that injects physics-aware reasoning into multimodal property prediction.
xChemAgents comprises two language-model-based agents: a Selector, which
adaptively identifies a sparse, weighted subset of descriptors relevant to each
target, and provides a natural language rationale; and a Validator, which
enforces physical constraints such as unit consistency and scaling laws through
iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to
a 22\% reduction in mean absolute error over strong baselines, while producing
faithful, human-interpretable explanations. Experiment results highlight the
potential of cooperative, self-verifying agents to enhance both accuracy and
transparency in foundation-model-driven materials science. The implementation
and accompanying dataset are available anonymously at
https://github.com/KurbanIntelligenceLab/xChemAgents.

</details>


### [12] [MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](https://arxiv.org/abs/2505.20824)
*Kai Chen,Taihang Zhen,Hewei Wang,Kailai Liu,Xinfeng Li,Jing Huo,Tianpei Yang,Jinfeng Xu,Wei Dong,Yang Gao*

Main category: cs.MA

TL;DR: MedSentry是一个包含5000个对抗性医疗提示的基准测试，用于评估四种多代理拓扑结构在医疗领域的安全性，并提出了一种检测和修复恶意代理的机制。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在医疗领域的广泛应用，确保其在多代理协作配置中的安全性至关重要。

Method: 开发了MedSentry基准测试和端到端攻击-防御评估管道，分析了四种多代理拓扑结构（Layers、SharedPool、Centralized、Decentralized）的防御能力。

Result: 发现不同架构在信息污染和决策稳健性方面存在显著差异，SharedPool易受攻击，而Decentralized更具弹性。提出的检测和修复机制能恢复系统安全性。

Conclusion: MedSentry为医疗领域的LLM多代理系统提供了严格的评估框架和实用的防御策略。

Abstract: As large language models (LLMs) are increasingly deployed in healthcare,
ensuring their safety, particularly within collaborative multi-agent
configurations, is paramount. In this paper we introduce MedSentry, a benchmark
comprising 5 000 adversarial medical prompts spanning 25 threat categories with
100 subthemes. Coupled with this dataset, we develop an end-to-end
attack-defense evaluation pipeline to systematically analyze how four
representative multi-agent topologies (Layers, SharedPool, Centralized, and
Decentralized) withstand attacks from 'dark-personality' agents. Our findings
reveal critical differences in how these architectures handle information
contamination and maintain robust decision-making, exposing their underlying
vulnerability mechanisms. For instance, SharedPool's open information sharing
makes it highly susceptible, whereas Decentralized architectures exhibit
greater resilience thanks to inherent redundancy and isolation. To mitigate
these risks, we propose a personality-scale detection and correction mechanism
that identifies and rehabilitates malicious agents, restoring system safety to
near-baseline levels. MedSentry thus furnishes both a rigorous evaluation
framework and practical defense strategies that guide the design of safer
LLM-based multi-agent systems in medical domains.

</details>


### [13] [Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective](https://arxiv.org/abs/2505.20922)
*Yang Zhang,Xinran Li,Jianing Ye,Delin Qu,Shuang Qiu,Chongjie Zhang,Xiu Li,Chenjia Bai*

Main category: cs.MA

TL;DR: DIMA利用扩散模型构建多智能体世界模型，显著提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，联合动作空间大且动态高度不确定，传统建模方法复杂且不准确。

Method: 通过序列化智能体建模，减少建模复杂度，并借鉴扩散模型的反向过程，开发DIMA模型。

Result: DIMA在多个多智能体控制基准测试中表现最优，显著优于现有世界模型。

Conclusion: DIMA为多智能体世界模型构建提供了新范式，推动了MARL研究的前沿。

Abstract: World models have recently attracted growing interest in Multi-Agent
Reinforcement Learning (MARL) due to their ability to improve sample efficiency
for policy learning. However, accurately modeling environments in MARL is
challenging due to the exponentially large joint action space and highly
uncertain dynamics inherent in multi-agent systems. To address this, we reduce
modeling complexity by shifting from jointly modeling the entire state-action
transition dynamics to focusing on the state space alone at each timestep
through sequential agent modeling. Specifically, our approach enables the model
to progressively resolve uncertainty while capturing the structured
dependencies among agents, providing a more accurate representation of how
agents influence the state. Interestingly, this sequential revelation of
agents' actions in a multi-agent system aligns with the reverse process in
diffusion models--a class of powerful generative models known for their
expressiveness and training stability compared to autoregressive or latent
variable models. Leveraging this insight, we develop a flexible and robust
world model for MARL using diffusion models. Our method, Diffusion-Inspired
Multi-Agent world model (DIMA), achieves state-of-the-art performance across
multiple multi-agent control benchmarks, significantly outperforming prior
world models in terms of final return and sample efficiency, including MAMuJoCo
and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent
world models, advancing the frontier of MARL research.

</details>


### [14] [GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation](https://arxiv.org/abs/2505.21154)
*Hailin Zhong,Hanlin Wang,Yujun Ye,Meiyi Zhang,Shengxin Zhu*

Main category: cs.MA

TL;DR: 论文提出了一种高保真社交模拟平台，通过模拟用户认知代理和动态社交互动，解决现有推荐系统依赖静态数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要基于静态离线数据，难以捕捉用户偏好长期演变和社交影响动态变化。

Method: 平台包含Sim-User代理，采用五层认知架构和ICR2动机引擎，结合多层异质社交图（GGBond Graph）模拟用户行为和社交动态。

Result: 系统支持动态响应推荐算法，形成多轮反馈循环，为长期推荐效果评估提供可控环境。

Conclusion: 该设计突破了传统静态数据集的限制，为推荐系统研究提供了更真实的模拟环境。

Abstract: Current personalized recommender systems predominantly rely on static offline
data for algorithm design and evaluation, significantly limiting their ability
to capture long-term user preference evolution and social influence dynamics in
real-world scenarios. To address this fundamental challenge, we propose a
high-fidelity social simulation platform integrating human-like cognitive
agents and dynamic social interactions to realistically simulate user behavior
evolution under recommendation interventions. Specifically, the system
comprises a population of Sim-User Agents, each equipped with a five-layer
cognitive architecture that encapsulates key psychological mechanisms,
including episodic memory, affective state transitions, adaptive preference
learning, and dynamic trust-risk assessments. In particular, we innovatively
introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine
grounded in psychological and sociological theories, enabling more realistic
user decision-making processes. Furthermore, we construct a multilayer
heterogeneous social graph (GGBond Graph) supporting dynamic relational
evolution, effectively modeling users' evolving social ties and trust dynamics
based on interest similarity, personality alignment, and structural homophily.
During system operation, agents autonomously respond to recommendations
generated by typical recommender algorithms (e.g., Matrix Factorization,
MultVAE, LightGCN), deciding whether to consume, rate, and share content while
dynamically updating their internal states and social connections, thereby
forming a stable, multi-round feedback loop. This innovative design transcends
the limitations of traditional static datasets, providing a controlled,
observable environment for evaluating long-term recommender effects.

</details>


### [15] [Large Language Models Miss the Multi-Agent Mark](https://arxiv.org/abs/2505.21298)
*Emanuele La Malfa,Gabriele La Malfa,Samuele Marro,Jie M. Zhang,Elizabeth Black,Micheal Luck,Philip Torr,Michael Wooldridge*

Main category: cs.MA

TL;DR: 当前多智能体大语言模型（MAS LLMs）研究存在与多智能体系统（MAS）理论脱节的问题，主要集中在社会性、环境设计、协调协议和行为测量四个方面。


<details>
  <summary>Details</summary>
Motivation: 揭示当前MAS LLMs研究与MAS理论之间的差距，避免重复解决已有问题。

Method: 系统分析MAS LLMs与MAS理论的差异，重点关注社会性、环境设计、协调协议和行为测量。

Result: 指出当前MAS LLMs缺乏自主性、社会互动和结构化环境等特征，呼吁更精确地整合MAS概念。

Conclusion: 建议更好地整合MAS理论，避免术语误用和机会错失，以推动领域发展。

Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)
has led to an increase in frameworks leveraging multiple LLMs to tackle complex
tasks. However, much of this literature appropriates the terminology of MAS
without engaging with its foundational principles. In this position paper, we
highlight critical discrepancies between MAS theory and current MAS LLMs
implementations, focusing on four key areas: the social aspect of agency,
environment design, coordination and communication protocols, and measuring
emergent behaviours. Our position is that many MAS LLMs lack multi-agent
characteristics such as autonomy, social interaction, and structured
environments, and often rely on oversimplified, LLM-centric architectures. The
field may slow down and lose traction by revisiting problems the MAS literature
has already addressed. Therefore, we systematically analyse this issue and
outline associated research opportunities; we advocate for better integrating
established MAS concepts and more precise terminology to avoid
mischaracterisation and missed opportunities.

</details>


### [16] [Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery](https://arxiv.org/abs/2505.21418)
*Lina Zhao,Jiaxing Bai,Zihao Bian,Qingyue Chen,Yafang Li,Guangbo Li,Min He,Huaiyuan Yao,Zongjiu Zhang*

Main category: cs.MA

TL;DR: FUAS-Agents利用LLM的多模态理解和工具使用能力，通过整合患者数据和MRI，生成个性化治疗计划，显著提升了临床决策的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: FUAS临床实施中的复杂任务需要智能辅助以提高效率和可靠性。

Method: 集成患者数据和MRI，利用LLM驱动的FUAS-Agents协调专业医疗AI工具，生成个性化治疗计划。

Result: 在子宫肌瘤治疗中，专家评估显示生成计划在完整性、准确性、流畅性和临床合规性方面评分较高。

Conclusion: LLM驱动的代理系统在复杂临床工作流中具有潜力，展示了通用模型与专业系统结合的转化范式。

Abstract: Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising
non-invasive therapeutic modality, valued for its safety and precision.
Nevertheless, its clinical implementation entails intricate tasks such as
multimodal image interpretation, personalized dose planning, and real-time
intraoperative decision-making processes that demand intelligent assistance to
improve efficiency and reliability. We introduce FUAS-Agents, an autonomous
agent system that leverages the multimodal understanding and tool-using
capabilities of large language models (LLMs). By integrating patient profiles
and MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,
including segmentation, treatment dose prediction, and clinical guideline
retrieval, to generate personalized treatment plans comprising MRI image, dose
parameters, and therapeutic strategies. We evaluate the system in a uterine
fibroid treatment scenario. Human assessment by four senior FUAS experts
indicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated
4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,
and clinical compliance, respectively. These results demonstrate the potential
of LLM-driven agents in enhancing decision-making across complex clinical
workflows, and exemplify a translational paradigm that combines general-purpose
models with specialized expert systems to solve practical challenges in
vertical healthcare domains.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [17] [Hyperbolic embedding of multilayer networks](https://arxiv.org/abs/2505.20378)
*Martin Guillemaud,Vera Dinkelacker,Mario Chavez*

Main category: cs.SI

TL;DR: 本文提出了一种新颖的双曲嵌入框架，用于多层网络分析，支持异构节点集和层间连接，优于现有独立嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 多层网络能有效建模复杂系统中的多类型连接和相互依赖子系统，但现有方法通常独立嵌入各层，无法保留全局多层结构。

Method: 提出一种支持异构节点集和层间连接的双曲嵌入框架，生成层特定嵌入，同时保留全局多层结构。

Result: 在合成和真实脑网络实验中，该方法有效保留了社区结构，并成功聚类疾病相关脑区，优于独立嵌入方法。

Conclusion: 该框架为多层网络分析提供了强大工具，增强了可解释性，并为复杂系统的结构和功能提供了新见解。

Abstract: Multilayer networks offer a powerful framework for modeling complex systems
across diverse domains, effectively capturing multiple types of connections and
interdependent subsystems commonly found in real world scenarios. To analyze
these networks, embedding techniques that project nodes into a
lower-dimensional geometric space are essential. This paper introduces a novel
hyperbolic embedding framework that advances the state of the art in multilayer
network analysis. Our method, which supports heterogeneous node sets across
networks and inter-layer connections, generates layer-specific hyperbolic
embeddings, enabling detailed intra-layer analysis and inter-layer comparisons,
while simultaneously preserving the global multilayer structure within
hyperbolic space, a capability that sets it apart from existing approaches,
which typically rely on independent embedding of layers. Through experiments on
synthetic multilayer stochastic block models, we demonstrate that our approach
effectively preserves community structure, even when layers consist of
different node sets. When applied to real brain networks, the method
successfully clusters disease-related brain regions from different patients,
outperforming layer-independent approaches and highlighting its relevance for
comparative analysis. Overall, this work provides a robust tool for multilayer
network analysis, enhancing interpretability and offering new insights into the
structure and function of complex systems.

</details>


### [18] [A Dashboard Approach to Monitoring Mpox-Related Discourse and Misinformation on Social Media](https://arxiv.org/abs/2505.20584)
*Linfeng,Zhao,Rishul Bhuvanagiri,Blake Gonzales,Kellen Sharp,Dhiraj Murthy*

Main category: cs.SI

TL;DR: 该研究开发了一个针对Mpox（猴痘）疫情的社交媒体仪表盘，用于追踪和分析相关推文，以支持公共卫生沟通和监测信息传播。


<details>
  <summary>Details</summary>
Motivation: Mpox是一种全球公共卫生问题，社交媒体在传播信息的同时也可能传播错误信息，因此需要工具来实时监测和分析健康相关讨论。

Method: 研究团队开发了一个交互式仪表盘，供公共卫生利益相关者和公众搜索和可视化Mpox相关推文。

Result: 在2024年8月CDC将Mpox列为新兴病毒后，仪表盘记录的推文量显著增加，表明健康话题在数字平台上的快速传播。

Conclusion: 研究强调了实时社交媒体监测工具的必要性，以支持公共卫生沟通并追踪地方层面的情绪和错误信息趋势。

Abstract: Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus
closely related to variola and remains a significant global public health
concern. During outbreaks, social media platforms like X (formerly Twitter) can
both inform and misinform the public, complicating efforts to convey accurate
health information. To support local response efforts, we developed a
researcher-focused dashboard for use by public health stakeholders and the
public that enables searching and visualizing mpox-related tweets through an
interactive interface. Following the CDC's designation of mpox as an emerging
virus in August 2024, our dashboard recorded a marked increase in tweet volume
compared to 2023, illustrating the rapid spread of health discourse across
digital platforms. These findings underscore the continued need for real-time
social media monitoring tools to support public health communication and track
evolving sentiment and misinformation trends at the local level.

</details>


### [19] [Two-step dimensionality reduction of human mobility data: From potential landscapes to spatiotemporal insights](https://arxiv.org/abs/2505.20929)
*Yunhan Du,Takaaki Aoki,Naoya Fujiwara*

Main category: cs.SI

TL;DR: 该论文提出了一种两步降维框架，用于分析人类移动性的时空模式，解决了现有方法丢失位置细节的问题，并在疫情期间揭示了显著的移动性变化。


<details>
  <summary>Details</summary>
Motivation: 理解人类移动性的时空模式对社会挑战（如流行病控制和城市交通优化）至关重要，但现有方法难以捕捉移动的复杂性。

Method: 通过组合Hodge理论构建人类流动的潜在景观，并应用主成分分析（PCA）系统识别主要时空模式。

Result: 研究发现疫情期间移动性整体下降，工作日与节假日之间存在显著差异。

Conclusion: 该框架有效揭示了复杂的移动性模式，为城市规划和公共卫生干预提供了宝贵见解。

Abstract: Understanding the spatiotemporal patterns of human mobility is crucial for
addressing societal challenges, such as epidemic control and urban
transportation optimization. Despite advancements in data collection, the
complexity and scale of mobility data continue to pose significant analytical
challenges. Existing methods often result in losing location-specific details
and fail to fully capture the intricacies of human movement. This study
proposes a two-step dimensionality reduction framework to overcome existing
limitations. First, we construct a potential landscape of human flow from
origin-destination (OD) matrices using combinatorial Hodge theory, preserving
essential spatial and structural information while enabling an intuitive
visualization of flow patterns. Second, we apply principal component analysis
(PCA) to the potential landscape, systematically identifying major
spatiotemporal patterns. By implementing this two-step reduction method, we
reveal significant shifts during a pandemic, characterized by an overall
declines in mobility and stark contrasts between weekdays and holidays. These
findings underscore the effectiveness of our framework in uncovering complex
mobility patterns and provide valuable insights into urban planning and public
health interventions.

</details>


### [20] [Identifying Super Spreaders in Multilayer Networks](https://arxiv.org/abs/2505.20980)
*Michał Czuba,Mateusz Stolarski,Adam Piróg,Piotr Bielak,Piotr Bródka*

Main category: cs.SI

TL;DR: 论文提出了一种基于图神经网络的新方法（TopSpreadersNetwork），用于识别多层网络中的超级传播者，并通过模拟信息扩散构建数据集。该方法在性能上优于传统中心性启发式方法和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 多层网络能更准确地捕捉复杂交互关系，但现有方法在识别超级传播者方面存在局限性。因此，研究旨在开发一种更有效且可解释的方法。

Method: 通过模拟信息扩散构建数据集，将任务定义为基于四维向量的排名预测问题，并提出TopSpreadersNetwork模型，包含关系无关编码器和自定义聚合层。

Result: 模型在真实和合成多层网络上表现优异，优于传统方法和深度学习模型，同时提供结构化输出以提高可解释性。

Conclusion: TopSpreadersNetwork在识别高影响力节点方面具有显著优势，且适用于不同规模的网络，为多层网络中的超级传播者识别提供了新思路。

Abstract: Identifying super-spreaders can be framed as a subtask of the influence
maximisation problem. It seeks to pinpoint agents within a network that, if
selected as single diffusion seeds, disseminate information most effectively.
Multilayer networks, a specific class of heterogeneous graphs, can capture
diverse types of interactions (e.g., physical-virtual or professional-social),
and thus offer a more accurate representation of complex relational structures.
In this work, we introduce a novel approach to identifying super-spreaders in
such networks by leveraging graph neural networks. To this end, we construct a
dataset by simulating information diffusion across hundreds of networks - to
the best of our knowledge, the first of its kind tailored specifically to
multilayer networks. We further formulate the task as a variation of the
ranking prediction problem based on a four-dimensional vector that quantifies
each agent's spreading potential: (i) the number of activations; (ii) the
duration of the diffusion process; (iii) the peak number of activations; and
(iv) the simulation step at which this peak occurs. Our model,
TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom
aggregation layer. This design enables generalisation to previously unseen data
and adapts to varying graph sizes. In an extensive evaluation, we compare our
model against classic centrality-based heuristics and competitive deep learning
methods. The results, obtained across a broad spectrum of real-world and
synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves
superior performance in identifying high-impact nodes, while also offering
improved interpretability through its structured output.

</details>


### [21] [DeSocial: Blockchain-based Decentralized Social Networks](https://arxiv.org/abs/2505.21388)
*Jingyuan Huang,Xi Zhu,Minghao Guo,Yongfeng Zhang*

Main category: cs.SI

TL;DR: DeSocial是一个基于区块链的去中心化社交网络学习框架，通过用户驱动的模型选择和多数投票聚合结果，提升个性化社交预测效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统中心化社交平台中用户无法选择算法的问题，利用区块链技术实现个性化预测。

Method: 在以太坊本地开发链上部署DeSocial框架，结合分布式数据存储、节点级共识和用户驱动的模型选择，通过多数投票聚合结果。

Result: 实验表明，DeSocial在个性化社交预测上优于五种经典中心化模型，验证了多节点验证和个性化算法选择的重要性。

Conclusion: DeSocial展示了区块链在去中心化社交网络中提升用户赋权和预测效果的潜力。

Abstract: Web 2.0 social platforms are inherently centralized, with user data and
algorithmic decisions controlled by the platform. However, users can only
passively receive social predictions without being able to choose the
underlying algorithm, which limits personalization. Fortunately, with the
emergence of blockchain, users are allowed to choose algorithms that are
tailored to their local situation, improving prediction results in a
personalized way. In a blockchain environment, each user possesses its own
model to perform the social prediction, capturing different perspectives on
social interactions. In our work, we propose DeSocial, a decentralized social
network learning framework deployed on an Ethereum (ETH) local development
chain that integrates distributed data storage, node-level consensus, and
user-driven model selection through Ganache. In the first stage, each user
leverages DeSocial to evaluate multiple backbone models on their local
subgraph. DeSocial coordinates the execution and returns model-wise prediction
results, enabling the user to select the most suitable backbone for
personalized social prediction. Then, DeSocial uniformly selects several
validation nodes that possess the algorithm specified by each user, and
aggregates the prediction results by majority voting, to prevent errors caused
by any single model's misjudgment. Extensive experiments show that DeSocial has
an evident improvement compared to the five classical centralized social
network learning models, promoting user empowerment in blockchain-based
decentralized social networks, showing the importance of multi-node validation
and personalized algorithm selection based on blockchain. Our implementation is
available at: https://github.com/agiresearch/DeSocial.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review](https://arxiv.org/abs/2505.20306)
*Xueqiang Ouyang,Jia Wei*

Main category: cs.AI

TL;DR: 本文综述了多模态人工智能在胚胎分级和妊娠预测中的应用进展，并讨论了当前研究中的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 不孕症是全球性问题，传统辅助生殖技术存在胚胎分级主观性和多模态数据整合效率低等挑战，因此引入人工智能技术至关重要。

Method: 从静态图像、延时视频和结构化表格数据等多模态数据角度，分析了人工智能在胚胎分级和妊娠预测中的应用。

Result: 总结了多模态人工智能在解决不孕症问题中的进展，但指出了信息融合复杂性和数据稀缺等挑战。

Conclusion: 多模态人工智能在辅助生殖技术中具有潜力，但仍需克服信息融合和数据不足等难题。

Abstract: As a global disease, infertility has always affected human beings. The
development of assisted reproductive technology can effectively solve this
disease. However, the traditional in vitro fertilization-embryo transfer
technology still faces many challenges in improving the success rate of
pregnancy, such as the subjectivity of embryo grading and the inefficiency of
integrating multi-modal data. Therefore, the introduction of artificial
intelligence-based technologies is particularly crucial. This article reviews
the application progress of multi-modal artificial intelligence in embryo
grading and pregnancy prediction based on different data modalities (including
static images, time-lapse videos and structured table data) from a new
perspective, and discusses the main challenges in current research, such as the
complexity of multi-modal information fusion and data scarcity.

</details>


### [23] [Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System](https://arxiv.org/abs/2505.20310)
*Wanghan Xu,Wenlong Zhang,Fenghua Ling,Ben Fei,Yusong Hu,Fangxuan Ren,Jintai Lin,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: 提出了一种名为Manalyzer的多智能体系统，通过工具调用实现端到端自动化的元分析，显著减轻了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统元分析方法需要大量人力和时间，而基于LLM的方法仍存在幻觉问题。

Method: 采用混合审查、分层提取、自证明和反馈检查策略的多智能体系统。

Result: 在包含729篇论文的新基准上，Manalyzer在多任务元分析中显著优于LLM基线。

Conclusion: Manalyzer通过自动化策略有效解决了元分析中的幻觉问题，提升了性能。

Abstract: Meta-analysis is a systematic research methodology that synthesizes data from
multiple existing studies to derive comprehensive conclusions. This approach
not only mitigates limitations inherent in individual studies but also
facilitates novel discoveries through integrated data analysis. Traditional
meta-analysis involves a complex multi-stage pipeline including literature
retrieval, paper screening, and data extraction, which demands substantial
human effort and time. However, while LLM-based methods can accelerate certain
stages, they still face significant challenges, such as hallucinations in paper
screening and data extraction. In this paper, we propose a multi-agent system,
Manalyzer, which achieves end-to-end automated meta-analysis through tool
calls. The hybrid review, hierarchical extraction, self-proving, and feedback
checking strategies implemented in Manalyzer significantly alleviate these two
hallucinations. To comprehensively evaluate the performance of meta-analysis,
we construct a new benchmark comprising 729 papers across 3 domains,
encompassing text, image, and table modalities, with over 10,000 data points.
Extensive experiments demonstrate that Manalyzer achieves significant
performance improvements over the LLM baseline in multi meta-analysis tasks.
Project page: https://black-yt.github.io/meta-analysis-page/ .

</details>


### [24] [Reasoning in Neurosymbolic AI](https://arxiv.org/abs/2505.20313)
*Son Tran,Edjard Mota,Artur d'Avila Garcez*

Main category: cs.AI

TL;DR: 本文介绍了一种基于能量的神经符号AI系统，能够表示和推理命题逻辑公式，结合数据学习和逻辑推理，并探讨了其在解决LLMs数据效率、公平性和安全性问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在将神经网络的推理和学习能力与符号逻辑结合，解决当前AI领域（如LLMs）中的数据效率、公平性和安全性问题。

Method: 提出了一种基于能量的神经符号AI系统，使用受限玻尔兹曼机（RBM）实现逻辑推理与能量最小化的对应关系，并通过实验验证其性能。

Result: 实验表明，该系统能够有效结合数据学习和逻辑推理，性能优于纯符号、纯神经和神经符号系统。

Conclusion: 神经符号AI有望推动神经网络在逻辑推理中的大规模并行应用，并促进推理与学习的深度融合，但仍需解决深度学习可靠性等挑战。

Abstract: Knowledge representation and reasoning in neural networks have been a
long-standing endeavor which has attracted much attention recently. The
principled integration of reasoning and learning in neural networks is a main
objective of the area of neurosymbolic Artificial Intelligence (AI). In this
chapter, a simple energy-based neurosymbolic AI system is described that can
represent and reason formally about any propositional logic formula. This
creates a powerful combination of learning from data and knowledge and logical
reasoning. We start by positioning neurosymbolic AI in the context of the
current AI landscape that is unsurprisingly dominated by Large Language Models
(LLMs). We identify important challenges of data efficiency, fairness and
safety of LLMs that might be addressed by neurosymbolic reasoning systems with
formal reasoning capabilities. We then discuss the representation of logic by
the specific energy-based system, including illustrative examples and empirical
evaluation of the correspondence between logical reasoning and energy
minimization using Restricted Boltzmann Machines (RBM). Learning from data and
knowledge is also evaluated empirically and compared with a symbolic, neural
and a neurosymbolic system. Results reported in this chapter in an accessible
way are expected to reignite the research on the use of neural networks as
massively-parallel models for logical reasoning and promote the principled
integration of reasoning and learning in deep networks. We conclude the chapter
with a discussion of the importance of positioning neurosymbolic AI within a
broader framework of formal reasoning and accountability in AI, discussing the
challenges for neurosynbolic AI to tackle the various known problems of
reliability of deep learning.

</details>


### [25] [Reinforcement Speculative Decoding for Fast Ranking](https://arxiv.org/abs/2505.20316)
*Yingpeng Du,Tianjun Wei,Zhu Sun,Jie Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的推测解码方法，用于优化LLM在排序系统中的推理速度，解决了传统方法在尾部位置表现差和延迟不可控的问题。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法在排序系统中因左到右解码范式面临延迟不可控和丢弃未接受项知识的问题，影响了多令牌预测效果。

Method: 提出了一种自上而下的解码范式，通过强化学习优化多轮排序修改策略，并充分利用LLM验证的列表排序知识。

Result: 在信息检索和推荐系统任务中验证了方法的有效性。

Conclusion: 该方法在满足排序系统延迟要求的同时，显著提升了尾部位置的性能。

Abstract: Large Language Models (LLMs) have been widely adopted in ranking systems such
as information retrieval (IR) systems and recommender systems (RSs). To
alleviate the latency of auto-regressive decoding, some studies explore the
single (first) token decoding for ranking approximation, but they suffer from
severe degradation in tail positions. Although speculative decoding (SD)
methods can be a remedy with verification at different positions, they face
challenges in ranking systems due to their left-to-right decoding paradigm.
Firstly, ranking systems require strict latency constraints, but verification
rounds in SD methods remain agnostic; Secondly, SD methods usually discard
listwise ranking knowledge about unaccepted items in previous rounds, hindering
future multi-token prediction, especially when candidate tokens are the
unaccepted items. In this paper, we propose a Reinforcement Speculative
Decoding method for fast ranking inference of LLMs. To meet the ranking
systems' latency requirement, we propose an up-to-down decoding paradigm that
employs an agent to iteratively modify the ranking sequence under a constrained
budget. Specifically, we design a ranking-tailored policy optimization,
actively exploring optimal multi-round ranking modification policy verified by
LLMs via reinforcement learning (RL). To better approximate the target LLM
under the constrained budget, we trigger the agent fully utilizing the listwise
ranking knowledge about all items verified by LLMs across different rounds in
RL, enhancing the modification policy of the agent. More importantly, we
demonstrate the theoretical robustness and advantages of our paradigm and
implementation. Experiments on both IR and RS tasks show the effectiveness of
our proposed method.

</details>


### [26] [Challenges for artificial cognitive systems](https://arxiv.org/abs/2505.20339)
*Antoni Gomila,Vincent C. Müller*

Main category: cs.AI

TL;DR: 本文旨在填补认知系统研究中的空白，提出明确的挑战以定义进展。


<details>
  <summary>Details</summary>
Motivation: 认知系统研究缺乏明确的目标和进展标准，本文旨在通过定义挑战来解决这一问题。

Method: 基于EUCogII项目的框架，提出人工认知系统的挑战，并以系统的定义为基础（即从经验中学习并灵活运用知识以实现目标）。

Result: 明确了人工认知系统的挑战，并提供了其定义。

Conclusion: 通过定义挑战和认知系统的本质，为研究提供了明确的方向和目标。

Abstract: The declared goal of this paper is to fill this gap: "... cognitive systems
research needs questions or challenges that define progress. The challenges are
not (yet more) predictions of the future, but a guideline to what are the aims
and what would constitute progress." -- the quotation being from the project
description of EUCogII, the project for the European Network for Cognitive
Systems within which this formulation of the 'challenges' was originally
developed (http://www.eucognition.org). So, we stick out our neck and formulate
the challenges for artificial cognitive systems. These challenges are
articulated in terms of a definition of what a cognitive system is: a system
that learns from experience and uses its acquired knowledge (both declarative
and practical) in a flexible manner to achieve its own goals.

</details>


### [27] [Machine Theory of Mind and the Structure of Human Values](https://arxiv.org/abs/2505.20342)
*Paul de Font-Reaulx*

Main category: cs.AI

TL;DR: 论文探讨了AI如何从有限的行为样本中推断人类复杂价值观，提出生成性理性结构及贝叶斯心智理论模型解决价值泛化问题。


<details>
  <summary>Details</summary>
Motivation: 人类价值观复杂且难以通过行为完全体现，AI需从有限样本中推断未观察到的价值观，即价值泛化问题。

Method: 提出人类价值观具有生成性理性结构，利用贝叶斯心智理论模型从行为和其他价值观中推断人类价值观。

Result: 生成性价值间推理是构建可扩展机器心智理论的关键。

Conclusion: 开发生成性价值间推理方法对实现安全、伦理的AI至关重要。

Abstract: Value learning is a crucial aspect of safe and ethical AI. This is primarily
pursued by methods inferring human values from behaviour. However, humans care
about much more than we are able to demonstrate through our actions.
Consequently, an AI must predict the rest of our seemingly complex values from
a limited sample. I call this the value generalization problem. In this paper,
I argue that human values have a generative rational structure and that this
allows us to solve the value generalization problem. In particular, we can use
Bayesian Theory of Mind models to infer human values not only from behaviour,
but also from other values. This has been obscured by the widespread use of
simple utility functions to represent human values. I conclude that developing
generative value-to-value inference is a crucial component of achieving a
scalable machine theory of mind.

</details>


### [28] [SCAR: Shapley Credit Assignment for More Efficient RLHF](https://arxiv.org/abs/2505.20417)
*Meng Cao,Shuyuan Zhang,Xiao-Wen Chang,Doina Precup*

Main category: cs.AI

TL;DR: SCAR利用博弈论中的Shapley值，为RLHF中的稀疏奖励问题提供了一种密集奖励分配方法，显著提升了模型对齐效率和性能。


<details>
  <summary>Details</summary>
Motivation: RLHF中稀疏奖励信号导致信用分配困难，SCAR旨在通过Shapley值公平分配奖励，优化模型对齐过程。

Method: 提出SCAR方法，利用Shapley值将序列级奖励分配到各token或文本片段，无需额外训练或人工标注。

Result: SCAR在多项任务中表现优于标准RLHF和注意力基线，收敛更快且最终奖励更高。

Conclusion: SCAR为RLHF提供了一种更高效、理论可靠的信用分配方法，显著提升LLM对齐效果。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a widely used technique
for aligning Large Language Models (LLMs) with human preferences, yet it often
suffers from sparse reward signals, making effective credit assignment
challenging. In typical setups, the reward model provides a single scalar score
for an entire generated sequence, offering little insight into which token or
span-level decisions were responsible for the outcome. To address this, we
propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages
Shapley values in cooperative game theory. SCAR distributes the total
sequence-level reward among constituent tokens or text spans based on their
principled marginal contributions. This creates dense reward signals,
crucially, without necessitating the training of auxiliary critique models or
recourse to fine-grained human annotations at intermediate generation stages.
Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for
fair credit attribution. Theoretically, we demonstrate that SCAR preserves the
original optimal policy, and empirically, across diverse tasks including
sentiment control, text summarization, and instruction tuning, we show that
SCAR converges significantly faster and achieves higher final reward scores
compared to standard RLHF and attention-based dense reward baselines. Our
findings suggest that SCAR provides a more effective and theoretically sound
method for credit assignment in RLHF, leading to more efficient alignment of
LLMs.

</details>


### [29] [Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration](https://arxiv.org/abs/2505.20466)
*P. S. Kesavan,Pontus Nordenfelt*

Main category: cs.AI

TL;DR: 智能显微镜从被动工具转变为科学研究的主动合作伙伴，提出六项核心设计原则，以弥合观察与理解的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决细胞研究中可观察（经验领域）与需理解（认知领域）之间的鸿沟，推动显微镜从自动化向智能协作发展。

Method: 提出六项核心设计原则，包括认知-经验意识、层次上下文整合等，并设计多智能体架构。

Result: 框架为构建支持假设生成和理论发展的智能显微镜系统提供了路线图。

Conclusion: 智能显微镜重新定义科学仪器在知识创造中的角色，成为主动的科研伙伴。

Abstract: Smart microscopy represents a paradigm shift in biological imaging, moving
from passive observation tools to active collaborators in scientific inquiry.
Enabled by advances in automation, computational power, and artificial
intelligence, these systems are now capable of adaptive decision-making and
real-time experimental control. Here, we introduce a theoretical framework that
reconceptualizes smart microscopy as a partner in scientific investigation.
Central to our framework is the concept of the 'epistemic-empirical divide' in
cellular investigation-the gap between what is observable (empirical domain)
and what must be understood (epistemic domain). We propose six core design
principles: epistemic-empirical awareness, hierarchical context integration, an
evolution from detection to perception, adaptive measurement frameworks,
narrative synthesis capabilities, and cross-contextual reasoning. Together,
these principles guide a multi-agent architecture designed to align empirical
observation with the goals of scientific understanding. Our framework provides
a roadmap for building microscopy systems that go beyond automation to actively
support hypothesis generation, insight discovery, and theory development,
redefining the role of scientific instruments in the process of knowledge
creation.

</details>


### [30] [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luis Frazão,Nuno Costa,António Pereira*

Main category: cs.AI

TL;DR: Project Riley是一个多模态、多模型的对话AI架构，模拟受情绪状态影响的推理过程，灵感来自《头脑特工队》。它包含五种情绪代理，通过多轮对话生成、批评和优化回答，最终合成连贯输出。原型在本地离线环境中部署，评估显示其在情感对齐和沟通清晰度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受《头脑特工队》启发，旨在模拟情绪对推理的影响，提升对话AI的情感表达和实用性。

Method: 采用五种情绪代理（Joy、Sadness、Fear、Anger、Disgust）进行多轮对话，结合文本和视觉LLM，并通过RAG和上下文跟踪优化输出。

Result: 用户测试显示，在情感对齐、清晰度和实用性方面表现优异。

Conclusion: Project Riley在结构化场景中表现出色，为情感化对话AI提供了新思路。

Abstract: This paper presents Project Riley, a novel multimodal and multi-model
conversational AI architecture oriented towards the simulation of reasoning
influenced by emotional states. Drawing inspiration from Pixar's Inside Out,
the system comprises five distinct emotional agents - Joy, Sadness, Fear,
Anger, and Disgust - that engage in structured multi-round dialogues to
generate, criticise, and iteratively refine responses. A final reasoning
mechanism synthesises the contributions of these agents into a coherent output
that either reflects the dominant emotion or integrates multiple perspectives.
The architecture incorporates both textual and visual large language models
(LLMs), alongside advanced reasoning and self-refinement processes. A
functional prototype was deployed locally in an offline environment, optimised
for emotional expressiveness and computational efficiency. From this initial
prototype, another one emerged, called Armando, which was developed for use in
emergency contexts, delivering emotionally calibrated and factually accurate
information through the integration of Retrieval-Augmented Generation (RAG) and
cumulative context tracking. The Project Riley prototype was evaluated through
user testing, in which participants interacted with the chatbot and completed a
structured questionnaire assessing three dimensions: Emotional Appropriateness,
Clarity and Utility, and Naturalness and Human-likeness. The results indicate
strong performance in structured scenarios, particularly with respect to
emotional alignment and communicative clarity.

</details>


### [31] [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
*Jian Wang,Boyan Zhu,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.AI

TL;DR: 论文研究了大型推理模型（LRMs）在测试时计算扩展中的性能极限，提出了测试时扩展性能模型（TTSPM），并分析了并行扩展和顺序扩展的饱和点。


<details>
  <summary>Details</summary>
Motivation: 探索测试时计算扩展的极限，以实现更高效的资源分配和推理能力提升。

Method: 从概率建模角度理论分析了并行扩展和顺序扩展两种范式，并推导了它们的饱和点。

Result: 两种扩展范式在性能上限上具有统一的数学结构，并通过AIME、MATH-500和GPQA等基准验证了理论结果。

Conclusion: 研究为测试时扩展的成本效益权衡提供了见解，有助于开发更高效的推理策略。

Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing
reasoning performance via internal test-time scaling. Building upon this, a
promising direction is to further scale test-time compute to unlock even
greater reasoning capabilities. However, as we push these scaling boundaries,
systematically understanding the practical limits and achieving optimal
resource allocation becomes a critical challenge. In this paper, we investigate
the scaling Pareto of test-time scaling and introduce the Test-Time Scaling
Performance Model (TTSPM). We theoretically analyze two fundamental paradigms
for such extended scaling, parallel scaling and sequential scaling, from a
probabilistic modeling perspective. Our primary contribution is the derivation
of the saturation point on the scaling budget for both strategies, identifying
thresholds beyond which additional computation yields diminishing returns.
Remarkably, despite their distinct mechanisms, both paradigms converge to a
unified mathematical structure in their upper bounds. We empirically validate
our theoretical findings on challenging reasoning benchmarks, including AIME,
MATH-500, and GPQA, demonstrating the practical utility of these bounds for
test-time resource allocation. We hope that this work provides insights into
the cost-benefit trade-offs of test-time scaling, guiding the development of
more resource-efficient inference strategies for large reasoning models.

</details>


### [32] [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
*Hyungjun Park,Chang-Yun Woo,Seungjo Lim,Seunghwan Lim,Keunho Kwak,Ju Young Jeong,Chong Hyun Suh*

Main category: cs.AI

TL;DR: LLM-based AI interface outperformed physicians in diagnostic accuracy and efficiency for common internal medicine cases.


<details>
  <summary>Details</summary>
Motivation: To compare the diagnostic performance of an AI interface with physicians in real-time clinical settings.

Method: Nonrandomized trial with physicians and simulated patients using USMLE Step 2 CS-style cases.

Result: AI achieved higher accuracy (80% vs. 50-70%), faster diagnosis (44.6% shorter time), and lower cost (98.1% reduction).

Conclusion: AI shows potential to assist primary care with comparable accuracy and efficiency to physicians.

Abstract: Objective To develop an LLM based realtime compound diagnostic medical AI
interface and performed a clinical trial comparing this interface and
physicians for common internal medicine cases based on the United States
Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A
nonrandomized clinical trial was conducted on August 20, 2024. We recruited one
general physician, two internal medicine residents (2nd and 3rd year), and five
simulated patients. The clinical vignettes were adapted from the USMLE Step 2
CS style exams. We developed 10 representative internal medicine cases based on
actual patients and included information available on initial diagnostic
evaluation. Primary outcome was the accuracy of the first differential
diagnosis. Repeatability was evaluated based on the proportion of agreement.
Results The accuracy of the physicians' first differential diagnosis ranged
from 50% to 70%, whereas the realtime compound diagnostic medical AI interface
achieved an accuracy of 80%. The proportion of agreement for the first
differential diagnosis was 0.7. The accuracy of the first and second
differential diagnoses ranged from 70% to 90% for physicians, whereas the AI
interface achieved an accuracy rate of 100%. The average time for the AI
interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).
The AI interface ($0.08) also reduced costs by 98.1% compared to the
physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3
for care by physicians and were 3.9 for the AI interface Conclusion An LLM
based realtime compound diagnostic medical AI interface demonstrated diagnostic
accuracy and patient satisfaction comparable to those of a physician, while
requiring less time and lower costs. These findings suggest that AI interfaces
may have the potential to assist primary care consultations for common internal
medicine cases.

</details>


### [33] [CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models](https://arxiv.org/abs/2505.20642)
*Yi Zhan,Qi Liu,Weibo Gao,Zheng Zhang,Tianfu Wang,Shuanghong Shen,Junyu Lu,Zhenya Huang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的智能代理CoderAgent，用于模拟学生的编程学习过程，填补了现有方法在细粒度和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 个性化编程辅导（如练习推荐）能提升学习效率，但缺乏高质量数据和离线评估与真实学习的脱节阻碍了其实际应用。

Method: 设计了基于ACT-R认知架构的CoderAgent，引入编程思维树（PTOT）分解学习过程为四个步骤，实现细粒度模拟。

Result: 实验证明CoderAgent能提供可解释的学习轨迹分析，并实现准确的模拟。

Conclusion: CoderAgent为个性化编程教育提供了新思路，解决了数据不足和模拟不精确的问题。

Abstract: Personalized programming tutoring, such as exercise recommendation, can
enhance learners' efficiency, motivation, and outcomes, which is increasingly
important in modern digital education. However, the lack of sufficient and
high-quality programming data, combined with the mismatch between offline
evaluation and real-world learning, hinders the practical deployment of such
systems. To address this challenge, many approaches attempt to simulate learner
practice data, yet they often overlook the fine-grained, iterative nature of
programming learning, resulting in a lack of interpretability and granularity.
To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate
students' programming processes in a fine-grained manner without relying on
real data. Specifically, we equip each human learner with an intelligent agent,
the core of which lies in capturing the cognitive states of the human
programming practice process. Inspired by ACT-R, a cognitive architecture
framework, we design the structure of CoderAgent to align with human cognitive
architecture by focusing on the mastery of programming knowledge and the
application of coding ability. Recognizing the inherent patterns in
multi-layered cognitive reasoning, we introduce the Programming Tree of Thought
(PTOT), which breaks down the process into four steps: why, how, where, and
what. This approach enables a detailed analysis of iterative problem-solving
strategies. Finally, experimental evaluations on real-world datasets
demonstrate that CoderAgent provides interpretable insights into learning
trajectories and achieves accurate simulations, paving the way for personalized
programming education.

</details>


### [34] [AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage](https://arxiv.org/abs/2505.20662)
*Xuanle Zhao,Zilin Sang,Yuxuan Li,Qi Shi,Shuo Wang,Duzhen Zhang,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 论文提出AutoReproduce框架，通过多智能体系统和paper lineage算法自动复现实验，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 实验复现对AI研究至关重要，但复杂性和隐性知识阻碍了自动化。

Method: 提出paper lineage算法提取隐性知识，并设计AutoReproduce多智能体框架，生成单元测试以提高可执行性。

Result: 在ReproduceBench上，AutoReproduce峰值性能领先70%，平均性能差距22.1%。

Conclusion: AutoReproduce有效提升实验复现能力，代码将开源。

Abstract: Efficient experiment reproduction is critical to accelerating progress in
artificial intelligence. However, the inherent complexity of method design and
training procedures presents substantial challenges for automation. Notably,
reproducing experiments often requires implicit domain-specific knowledge not
explicitly documented in the original papers. To address this, we introduce the
paper lineage algorithm, which identifies and extracts implicit knowledge from
the relevant references cited by the target paper. Building on this idea, we
propose AutoReproduce, a multi-agent framework capable of automatically
reproducing experiments described in research papers in an end-to-end manner.
AutoReproduce enhances code executability by generating unit tests alongside
the reproduction process. To evaluate the reproduction capability, we construct
ReproduceBench, a benchmark annotated with verified implementations, and
introduce novel evaluation metrics to assess both the reproduction and
execution fidelity. Experimental results demonstrate that AutoReproduce
outperforms the existing strong agent baselines on all five evaluation metrics
by a peak margin of over $70\%$. In particular, compared to the official
implementations, AutoReproduce achieves an average performance gap of $22.1\%$
on $89.74\%$ of the executable experiment runs. The code will be available at
https://github.com/AI9Stars/AutoReproduce.

</details>


### [35] [MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning](https://arxiv.org/abs/2505.20670)
*Zikang Guo,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.AI

TL;DR: MIRROR框架通过引入行动前反思（intra-reflection）和行动后反思（inter-reflection），系统性提升LLM在复杂任务中的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在行动后利用反思能力，而人类在行动前也能反思。MIRROR旨在通过行动前反思预防错误传播，提升任务表现。

Method: 提出MIRROR框架，包含行动前反思（intra-reflection）和行动后反思（inter-reflection），系统性利用LLM的反思能力。

Result: 在StableToolBench和TravelPlanner基准测试中，MIRROR表现优于现有方法，达到最优结果。

Conclusion: MIRROR通过全面利用反思能力，显著提升了LLM在复杂任务中的表现，为多智能体工作流提供了新思路。

Abstract: Complex tasks involving tool integration pose significant challenges for
Large Language Models (LLMs), leading to the emergence of multi-agent workflows
as a promising solution. Reflection has emerged as an effective strategy for
correcting erroneous trajectories in agentic workflows. However, existing
approaches only exploit such capability in the post-action stage, where the
agent observes the execution outcomes. We argue that, like humans, LLMs can
also engage in reflection before action execution: the agent can anticipate
undesirable outcomes from its own decisions, which not only provides a
necessarily complementary perspective to evaluate the decision but also
prevents the propagation of errors throughout the trajectory. In this paper, we
propose MIRROR, a framework that consists of both intra-reflection, which
critically assesses intended actions before execution, and inter-reflection,
which further adjusts the trajectory based on observations. This design
systematically leverages LLM reflection capabilities to eliminate and rectify
erroneous actions on a more comprehensive scope. Evaluations on both the
StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior
performance, achieving state-of-the-art results compared to existing
approaches.

</details>


### [36] [Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks](https://arxiv.org/abs/2505.21426)
*Francesco Cozzi,Marco Pangallo,Alan Perotti,André Panisson,Corrado Monti*

Main category: cs.AI

TL;DR: 提出了一种新框架，通过观察生成的数据学习任何ABM的可微分替代模型，结合扩散模型和图神经网络，直接建模个体代理行为，保留ABM的分散性。


<details>
  <summary>Details</summary>
Motivation: ABM的规则通常不可微分，限制了基于梯度的方法的优化和与真实世界数据的整合。

Method: 结合扩散模型捕捉行为随机性和图神经网络建模代理交互，直接建模个体代理行为。

Result: 在Schelling隔离模型和捕食者-猎物生态系统中验证，能复制个体级模式并准确预测训练外的涌现动态。

Conclusion: 展示了扩散模型和图学习在数据驱动ABM模拟中的潜力。

Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties
in complex systems. In ABMs, agent behaviors are governed by local interactions
and stochastic rules. However, these rules are, in general, non-differentiable,
limiting the use of gradient-based methods for optimization, and thus
integration with real-world data. We propose a novel framework to learn a
differentiable surrogate of any ABM by observing its generated data. Our method
combines diffusion models to capture behavioral stochasticity and graph neural
networks to model agent interactions. Distinct from prior surrogate approaches,
our method introduces a fundamental shift: rather than approximating
system-level outputs, it models individual agent behavior directly, preserving
the decentralized, bottom-up dynamics that define ABMs. We validate our
approach on two ABMs (Schelling's segregation model and a Predator-Prey
ecosystem) showing that it replicates individual-level patterns and accurately
forecasts emergent dynamics beyond training. Our results demonstrate the
potential of combining diffusion models and graph learning for data-driven ABM
simulation.

</details>


### [37] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/abs/2505.20671)
*Heng Tan,Hua Yan,Yu Yang*

Main category: cs.AI

TL;DR: 论文提出了一种利用大型语言模型（LLM）指导强化学习（RL）训练的方法，通过LLM识别关键状态并提供动作建议，避免了额外模型训练或人工干预。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务中容易陷入局部最优，现有方法（如自动化策略优化或人工干预）存在成本高或扩展性差的问题。

Method: 设计了一个LLM指导的策略调制框架，LLM从次优代理的轨迹中识别关键状态，提供动作建议和隐式奖励以优化策略。

Result: 在标准RL基准测试中，该方法优于现有基线，证明了LLM在解决RL训练瓶颈中的有效性。

Conclusion: LLM指导的策略调制框架为RL训练提供了一种高效且可扩展的解决方案。

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


### [38] [GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning](https://arxiv.org/abs/2505.20672)
*Woochang Sim,Hyunseok Ryu,Kyungmin Choi,Sungwon Han,Sundong Kim*

Main category: cs.AI

TL;DR: 论文提出GIFARC数据集，通过类比方法改进AI在ARC任务中的表现，利用LLMs和VLMs生成新任务，显著提升解决效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI在ARC任务中表现远低于人类水平，需通过类比方法缩小差距。

Method: 利用GIF图像生成类比任务，结合LLMs和VLMs，明确视觉变换与概念的映射。

Result: GIFARC引导AI采用类比方法，显著减少问题复杂度，提升解决效率。

Conclusion: 类比方法有效提升AI在ARC任务中的表现，接近人类推理水平。

Abstract: The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general
AI capabilities, requiring solvers to infer abstract patterns from only a
handful of examples. Despite substantial progress in deep learning,
state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024
ARC Competition, indicative of a significant gap between their performance and
human-level reasoning. In this work, we seek to bridge that gap by introducing
an analogy-inspired ARC dataset, GIFARC. Leveraging large language models
(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks
from a variety of GIF images that include analogies. Each new task is paired
with ground-truth analogy, providing an explicit mapping between visual
transformations and everyday concepts. By embedding robust human-intuitive
analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task
analogically before engaging in brute-force pattern search, thus efficiently
reducing problem complexity and build a more concise and human-understandable
solution. We empirically validate that guiding LLM with analogic approach with
GIFARC affects task-solving approaches of LLMs to align with analogic approach
of human.

</details>


### [39] [Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.20728)
*Zesen Lyu,Dandan Zhang,Wei Ye,Fangdi Li,Zhihang Jiang,Yao Yang*

Main category: cs.AI

TL;DR: 论文提出Jigsaw-Puzzles基准，评估视觉语言模型的空间推理能力，发现现有模型表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 研究当前视觉语言模型是否具备类似人类的空间推理能力。

Method: 设计包含1,100张高空间复杂度图像的Jigsaw-Puzzles基准，并设计五项任务评估模型能力。

Result: 最强模型Gemini-2.5-Pro总体准确率仅77.14%，在Order Generation任务中表现尤其差（30.00%）。

Conclusion: 现有模型空间推理能力与人类差距显著，Jigsaw-Puzzles可作为推动研究的诊断性基准。

Abstract: Spatial reasoning is a core component of human cognition, enabling
individuals to perceive, comprehend, and interact with the physical world. It
relies on a nuanced understanding of spatial structures and inter-object
relationships, serving as the foundation for complex reasoning and
decision-making. To investigate whether current vision-language models (VLMs)
exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark
consisting of 1,100 carefully curated real-world images with high spatial
complexity. Based on this dataset, we design five tasks to rigorously evaluate
VLMs' spatial perception, structural understanding, and reasoning capabilities,
while deliberately minimizing reliance on domain-specific knowledge to better
isolate and assess the general spatial reasoning capability. We conduct a
comprehensive evaluation across 24 state-of-the-art VLMs. The results show that
even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy
and performs particularly poorly on the Order Generation task, with only 30.00%
accuracy, far below the performance exceeding 90% achieved by human
participants. This persistent gap underscores the need for continued progress,
positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for
advancing spatial reasoning research in VLMs.

</details>


### [40] [E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing](https://arxiv.org/abs/2505.20733)
*Cheonsu Jeong,Seongmin Sim,Hyoyoung Cho,Sungsu Kim,Byounggwan Shin*

Main category: cs.AI

TL;DR: 本文提出了一种结合生成式AI和智能文档处理技术的自动化代理方法，实现了企业财务费用处理任务的端到端自动化，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RPA在处理非结构化数据和复杂决策时存在局限，本研究旨在通过生成式AI和IDP技术克服这些限制。

Method: 设计了四阶段集成流程，包括OCR/IDP自动识别、基于策略的分类、生成式AI支持的异常处理，以及人机协作的最终决策。

Result: 在韩国某大型企业应用中，系统减少了80%以上的处理时间，降低了错误率，并提升了合规性和员工满意度。

Conclusion: 生成式AI、IDP和自动化代理的有机结合有效克服了传统自动化的局限，为复杂企业流程的端到端自动化提供了可行方案。

Abstract: This paper presents an intelligent work automation approach in the context of
contemporary digital transformation by integrating generative AI and
Intelligent Document Processing (IDP) technologies with an Automation Agent to
realize End-to-End (E2E) automation of corporate financial expense processing
tasks. While traditional Robotic Process Automation (RPA) has proven effective
for repetitive, rule-based simple task automation, it faces limitations in
handling unstructured data, exception management, and complex decision-making.
This study designs and implements a four-stage integrated process comprising
automatic recognition of supporting documents such as receipts via OCR/IDP,
item classification based on a policy-driven database, intelligent exception
handling supported by generative AI (large language models, LLMs), and
human-in-the-loop final decision-making with continuous system learning through
an Automation Agent. Applied to a major Korean enterprise (Company S), the
system demonstrated quantitative benefits including over 80% reduction in
processing time for paper receipt expense tasks, decreased error rates, and
improved compliance, as well as qualitative benefits such as enhanced accuracy
and consistency, increased employee satisfaction, and data-driven decision
support. Furthermore, the system embodies a virtuous cycle by learning from
human judgments to progressively improve automatic exception handling
capabilities. Empirically, this research confirms that the organic integration
of generative AI, IDP, and Automation Agents effectively overcomes the
limitations of conventional automation and enables E2E automation of complex
corporate processes. The study also discusses potential extensions to other
domains such as accounting, human resources, and procurement, and proposes
future directions for AI-driven hyper-automation development.

</details>


### [41] [RRO: LLM Agent Optimization Through Rising Reward Trajectories](https://arxiv.org/abs/2505.20737)
*Zilong Wang,Jingfeng Yang,Sreyashi Nag,Samarth Varshney,Xianfeng Tang,Haoming Jiang,Jingbo Shang,Sheikh Muhammad Sarwar*

Main category: cs.AI

TL;DR: 论文提出了一种名为Reward Rising Optimization (RRO)的方法，通过动态调整过程监督以减少探索成本，同时提高多步任务中语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂多步任务中表现不佳，尤其是对关键步骤的敏感性导致失败。现有方法（如PRMs）计算成本高且难以扩展。

Method: 提出RRO方法，通过维持奖励上升趋势动态扩展搜索空间，高效捕获高质量数据。

Result: 在WebShop和InterCode-SQL基准测试中，RRO表现优异且探索成本显著降低。

Conclusion: RRO为多步任务中的过程监督提供了一种高效且低成本的方法，具有实际应用潜力。

Abstract: Large language models (LLMs) have exhibited extraordinary performance in a
variety of tasks while it remains challenging for them to solve complex
multi-step tasks as agents. In practice, agents sensitive to the outcome of
certain key steps which makes them likely to fail the task because of a subtle
mistake in the planning trajectory. Recent approaches resort to calibrating the
reasoning process through reinforcement learning. They reward or penalize every
reasoning step with process supervision, as known as Process Reward Models
(PRMs). However, PRMs are difficult and costly to scale up with a large number
of next action candidates since they require extensive computations to acquire
the training data through the per-step trajectory exploration. To mitigate this
issue, we focus on the relative reward trend across successive reasoning steps
and propose maintaining an increasing reward in the collected trajectories for
process supervision, which we term Reward Rising Optimization (RRO).
Specifically, we incrementally augment the process supervision until
identifying a step exhibiting positive reward differentials, i.e. rising
rewards, relative to its preceding iteration. This method dynamically expands
the search space for the next action candidates, efficiently capturing
high-quality data. We provide mathematical groundings and empirical results on
the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO
achieves superior performance while requiring much less exploration cost.

</details>


### [42] [MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science](https://arxiv.org/abs/2505.20740)
*Xiangyu Zhao,Wanghan Xu,Bo Liu,Yuhao Zhou,Fenghua Ling,Ben Fei,Xiaoyu Yue,Lei Bai,Wenlong Zhang,Xiao-Ming Wu*

Main category: cs.AI

TL;DR: MSEarth是一个多模态科学基准，针对地球科学领域的复杂推理任务，填补了研究生级别基准的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在地球科学领域的应用缺乏反映复杂推理和领域特定知识的基准。

Method: 通过从高质量科学出版物中收集数据，构建包含五大地球圈层（大气、冰冻圈、水圈、岩石圈、生物圈）的7K多张图片及其丰富标注的基准。

Result: MSEarth支持多种任务（如图片标注、选择题、开放式推理），为科学推理任务提供了高保真资源。

Conclusion: MSEarth填补了研究生级别基准的空白，促进了多模态大语言模型在科学推理中的发展。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
unlocked new opportunities to tackle complex scientific challenges. Despite
this progress, their application in addressing earth science problems,
especially at the graduate level, remains underexplored. A significant barrier
is the absence of benchmarks that capture the depth and contextual complexity
of geoscientific reasoning. Current benchmarks often rely on synthetic datasets
or simplistic figure-caption pairs, which do not adequately reflect the
intricate reasoning and domain-specific insights required for real-world
scientific applications. To address these gaps, we introduce MSEarth, a
multimodal scientific benchmark curated from high-quality, open-access
scientific publications. MSEarth encompasses the five major spheres of Earth
science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,
featuring over 7K figures with refined captions. These captions are crafted
from the original figure captions and enriched with discussions and reasoning
from the papers, ensuring the benchmark captures the nuanced reasoning and
knowledge-intensive content essential for advanced scientific tasks. MSEarth
supports a variety of tasks, including scientific figure captioning, multiple
choice questions, and open-ended reasoning challenges. By bridging the gap in
graduate-level benchmarks, MSEarth provides a scalable and high-fidelity
resource to enhance the development and evaluation of MLLMs in scientific
reasoning. The benchmark is publicly available to foster further research and
innovation in this field. Resources related to this benchmark can be found at
https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.

</details>


### [43] [Can Agents Fix Agent Issues?](https://arxiv.org/abs/2505.20749)
*Alfin Wijaya Rahardja,Junwei Liu,Weitong Chen,Zhenpeng Chen,Yiling Lou*

Main category: cs.AI

TL;DR: 论文探讨了基于LLM的代理系统维护的挑战，构建了AGENTISSUE-BENCH基准测试，并发现现有SE代理在解决代理系统问题时的效果有限。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理系统在多个领域的广泛应用，其维护成本高且问题解决困难，需要研究如何自动解决代理系统的问题。

Method: 通过手动分析201个真实代理问题，构建了包含50个任务的AGENTISSUE-BENCH基准，并评估了现有SE代理的性能。

Result: 现有SE代理在AGENTISSUE-BENCH上的解决率仅为3.33% - 12.67%，表明其效果有限。

Conclusion: 代理系统的维护具有独特性，需要进一步研究开发更先进的SE代理以解决问题。

Abstract: LLM-based agent systems are emerging as a new software paradigm and have been
widely adopted across diverse domains such as medicine, robotics, and
programming. However, maintaining these systems requires substantial effort, as
they are inevitably prone to bugs and continually evolve to meet changing
external requirements. Therefore, automatically resolving agent issues (i.e.,
bug reports or feature requests) is a crucial and challenging task. While
recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in
addressing issues in traditional software systems, it remains unclear how
effectively they can resolve real-world issues in agent systems, which differ
significantly from traditional software. To fill this gap, we first manually
analyze 201 real-world agent issues and identify common categories of agent
issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a
reproducible benchmark comprising 50 agent issue resolution tasks (each with an
executable environment and failure-triggering tests). We further evaluate
state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited
effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results
underscore the unique challenges of maintaining agent systems compared to
traditional software, highlighting the need for further research to develop
advanced SE agents for resolving agent issues. Data and code are available at
https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .

</details>


### [44] [MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization](https://arxiv.org/abs/2505.20820)
*Hyomin Kim,Yunhui Jang,Sungsoo Ahn*

Main category: cs.AI

TL;DR: MT-Mol是一个多智能体框架，利用工具引导的推理和角色专业化LLM代理进行分子优化，在PMO-1K基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在分子优化方面潜力巨大，但结构化推理、可解释性和工具支持的优化尚未充分探索。

Method: MT-Mol结合RDKit工具，分为五个领域，由专家代理管理，通过多智能体协作实现分子优化。

Result: 在PMO-1K基准测试的23项任务中，17项表现最优。

Conclusion: MT-Mol框架展示了工具引导的多智能体协作在分子优化中的高效性和潜力。

Abstract: Large language models (LLMs) have large potential for molecular optimization,
as they can gather external chemistry tools and enable collaborative
interactions to iteratively refine molecular candidates. However, this
potential remains underexplored, particularly in the context of structured
reasoning, interpretability, and comprehensive tool-grounded molecular
optimization. To address this gap, we introduce MT-Mol, a multi-agent framework
for molecular optimization that leverages tool-guided reasoning and
role-specialized LLM agents. Our system incorporates comprehensive RDKit tools,
categorized into five distinct domains: structural descriptors, electronic and
topological features, fragment-based functional groups, molecular
representations, and miscellaneous chemical properties. Each category is
managed by an expert analyst agent, responsible for extracting task-relevant
tools and enabling interpretable, chemically grounded feedback. MT-Mol produces
molecules with tool-aligned and stepwise reasoning through the interaction
between the analyst agents, a molecule-generating scientist, a reasoning-output
verifier, and a reviewer agent. As a result, we show that our framework shows
the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.

</details>


### [45] [Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving](https://arxiv.org/abs/2505.20869)
*Kuo Zhou,Lu Zhang*

Main category: cs.AI

TL;DR: 论文提出MATH-VF框架，通过Formalizer和Critic验证LLM生成的数学问题解决方案的正确性，并支持错误修正。


<details>
  <summary>Details</summary>
Motivation: LLM在解决数学问题时可能产生逻辑或计算错误，需一种方法验证和修正这些错误。

Method: MATH-VF框架包含Formalizer（将自然语言解转为形式化内容）和Critic（结合外部工具验证并反馈错误）。

Result: 在MATH500和ProcessBench基准测试中表现优于现有方法。

Conclusion: MATH-VF能有效验证和修正LLM生成的数学解，提升准确性。

Abstract: Large Language Models (LLMs) have demonstrated formidable capabilities in
solving mathematical problems, yet they may still commit logical reasoning and
computational errors during the problem-solving process. Thus, this paper
proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for
formally verifying the correctness of the solutions generated by large language
models. Our framework first utilizes a Formalizer which employs an LLM to
translate a natural language solution into a formal context. Afterward, our
Critic (which integrates various external tools such as a Computer Algebra
System and an SMT solver) evaluates the correctness of each statement within
the formal context, and when a statement is incorrect, our Critic provides
corrective feedback. We empirically investigate the effectiveness of MATH-VF in
two scenarios: 1) Verification: MATH-VF is utilized to determine the
correctness of a solution to a given problem. 2) Refinement: When MATH-VF
identifies errors in the solution generated by an LLM-based solution generator
for a given problem, it submits the corrective suggestions proposed by the
Critic to the solution generator to regenerate the solution. We evaluate our
framework on widely used mathematical benchmarks: MATH500 and ProcessBench,
demonstrating the superiority of our approach over existing approaches.

</details>


### [46] [Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment](https://arxiv.org/abs/2505.20889)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zhenliang Ma*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度强化学习的框架，将静态系统最优交通分配问题转化为单智能体任务，通过推荐个性化路线实现系统总旅行时间最小化。


<details>
  <summary>Details</summary>
Motivation: 研究个性化路线推荐是否能集体实现系统最优交通分配，以提升个体旅行体验和运营效率。

Method: 采用MSA引导的深度Q学习算法，将传统交通分配方法的迭代结构融入强化学习训练过程。

Result: 在Braess网络中收敛到理论最优解，在OW网络中仅偏离0.35%。

Conclusion: 通过基于学习的顺序分配，为个体路由行为与系统效率之间的桥梁提供了理论和实践方法。

Abstract: Modern navigation systems and shared mobility platforms increasingly rely on
personalized route recommendations to improve individual travel experience and
operational efficiency. However, a key question remains: can such sequential,
personalized routing decisions collectively lead to system-optimal (SO) traffic
assignment? This paper addresses this question by proposing a learning-based
framework that reformulates the static SO traffic assignment problem as a
single-agent deep reinforcement learning (RL) task. A central agent
sequentially recommends routes to travelers as origin-destination (OD) demands
arrive, to minimize total system travel time. To enhance learning efficiency
and solution quality, we develop an MSA-guided deep Q-learning algorithm that
integrates the iterative structure of traditional traffic assignment methods
into the RL training process. The proposed approach is evaluated on both the
Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent
converges to the theoretical SO solution in the Braess network and achieves
only a 0.35% deviation in the OW network. Further ablation studies demonstrate
that the route action set's design significantly impacts convergence speed and
final performance, with SO-informed route sets leading to faster learning and
better outcomes. This work provides a theoretically grounded and practically
relevant approach to bridging individual routing behavior with system-level
efficiency through learning-based sequential assignment.

</details>


### [47] [Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs](https://arxiv.org/abs/2505.20948)
*Yisen Gao,Jiaxin Bai,Tianshi Zheng,Qingyun Sun,Ziwei Zhang,Jianxin Li,Yangqiu Song,Xingcheng Fu*

Main category: cs.AI

TL;DR: 论文提出了一种可控逻辑假设生成框架CtrlHGen，用于知识图谱上的溯因推理，解决了假设空间塌缩和假设过度敏感问题。


<details>
  <summary>Details</summary>
Motivation: 现有溯因推理方法在大型知识图谱上生成冗余或无关假设，缺乏可控性，限制了实际应用。

Method: 提出两阶段训练框架（监督学习+强化学习），通过子逻辑分解的数据增强策略和语义奖励机制（Dice、Overlap分数）优化生成过程。

Result: 在三个基准数据集上，CtrlHGen在控制条件和语义相似性方面均优于基线方法。

Conclusion: CtrlHGen框架显著提升了溯因推理的可控性和实用性。

Abstract: Abductive reasoning in knowledge graphs aims to generate plausible logical
hypotheses from observed entities, with broad applications in areas such as
clinical diagnosis and scientific discovery. However, due to a lack of
controllability, a single observation may yield numerous plausible but
redundant or irrelevant hypotheses on large-scale knowledge graphs. To address
this limitation, we introduce the task of controllable hypothesis generation to
improve the practical utility of abductive reasoning. This task faces two key
challenges when controlling for generating long and complex logical hypotheses:
hypothesis space collapse and hypothesis oversensitivity. To address these
challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation
framework for abductive reasoning over knowledge graphs, trained in a two-stage
paradigm including supervised learning and subsequent reinforcement learning.
To mitigate hypothesis space collapse, we design a dataset augmentation
strategy based on sub-logical decomposition, enabling the model to learn
complex logical structures by leveraging semantic patterns in simpler
components. To address hypothesis oversensitivity, we incorporate smoothed
semantic rewards including Dice and Overlap scores, and introduce a
condition-adherence reward to guide the generation toward user-specified
control constraints. Extensive experiments on three benchmark datasets
demonstrate that our model not only better adheres to control conditions but
also achieves superior semantic similarity performance compared to baselines.

</details>


### [48] [Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking](https://arxiv.org/abs/2505.21045)
*Lingyi Cai,Ruichen Zhang,Changyuan Zhao,Yu Zhang,Jiawen Kang,Dusit Niyato,Tao Jiang,Xuemin Shen*

Main category: cs.AI

TL;DR: 论文探讨了如何利用大语言模型（LLMs）增强强化学习（RL）以解决低空经济网络（LAENet）中的决策、资源和环境挑战。


<details>
  <summary>Details</summary>
Motivation: LAENet面临复杂决策、资源限制和环境不确定性，传统RL方法在泛化性、奖励设计和模型稳定性上存在不足，LLMs为RL提供了新的改进机会。

Method: 提出将LLMs集成到RL中的框架，利用LLMs的生成、上下文理解和结构化推理能力，将其作为信息处理器、奖励设计器、决策者和生成器。

Result: 通过案例研究展示了LLMs设计奖励函数以提升RL在LAENet中的学习性能。

Conclusion: 总结了LLMs增强RL的潜力，并讨论了未来研究方向。

Abstract: Low-Altitude Economic Networking (LAENet) aims to support diverse flying
applications below 1,000 meters by deploying various aerial vehicles for
flexible and cost-effective aerial networking. However, complex
decision-making, resource constraints, and environmental uncertainty pose
significant challenges to the development of the LAENet. Reinforcement learning
(RL) offers a potential solution in response to these challenges but has
limitations in generalization, reward design, and model stability. The
emergence of large language models (LLMs) offers new opportunities for RL to
mitigate these limitations. In this paper, we first present a tutorial about
integrating LLMs into RL by using the capacities of generation, contextual
understanding, and structured reasoning of LLMs. We then propose an
LLM-enhanced RL framework for the LAENet in terms of serving the LLM as
information processor, reward designer, decision-maker, and generator.
Moreover, we conduct a case study by using LLMs to design a reward function to
improve the learning performance of RL in the LAENet. Finally, we provide a
conclusion and discuss future work.

</details>


### [49] [Agent-Environment Alignment via Automated Interface Generation](https://arxiv.org/abs/2505.21055)
*Kaiming Liu,Xuanyu Lei,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出ALIGN框架，通过自动对齐接口缓解LLM代理与环境之间的不匹配问题，显著提升代理性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理在交互决策任务中表现出色，但代理与环境之间的接口不匹配（agent-environment misalignment）成为性能瓶颈，现有研究对此关注不足。

Method: 提出ALIGN框架，通过自动生成的接口丰富环境信息和逐步观察，无需修改代理逻辑或环境代码。

Result: 实验表明，ALIGN在多个领域（如具身任务、网页导航和工具使用）中性能提升显著，ALFWorld任务成功率最高提升45.67%。

Conclusion: ALIGN框架有效缓解了代理与环境的不匹配问题，且具有跨代理架构和LLM骨干的泛化能力。

Abstract: Large language model (LLM) agents have shown impressive reasoning
capabilities in interactive decision-making tasks. These agents interact with
environment through intermediate interfaces, such as predefined action spaces
and interaction rules, which mediate the perception and action. However,
mismatches often happen between the internal expectations of the agent
regarding the influence of its issued actions and the actual state transitions
in the environment, a phenomenon referred to as \textbf{agent-environment
misalignment}. While prior work has invested substantially in improving agent
strategies and environment design, the critical role of the interface still
remains underexplored. In this work, we empirically demonstrate that
agent-environment misalignment poses a significant bottleneck to agent
performance. To mitigate this issue, we propose \textbf{ALIGN}, an
\underline{A}uto-A\underline{l}igned \underline{I}nterface
\underline{G}e\underline{n}eration framework that alleviates the misalignment
by enriching the interface. Specifically, the ALIGN-generated interface
enhances both the static information of the environment and the step-wise
observations returned to the agent. Implemented as a lightweight wrapper, this
interface achieves the alignment without modifying either the agent logic or
the environment code. Experiments across multiple domains including embodied
tasks, web navigation and tool-use, show consistent performance improvements,
with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile,
ALIGN-generated interface can generalize across different agent architectures
and LLM backbones without interface regeneration. Code and experimental results
are available at https://github.com/THUNLP-MT/ALIGN.

</details>


### [50] [Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning](https://arxiv.org/abs/2505.21067)
*Xiao Hu,Xingyu Lu,Liyuan Mao,YiFan Zhang,Tianke Zhang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 通过仅使用920个示例的简单蒸馏方法，显著优于需要更多数据和计算成本的零强化学习方法，提升模型的灵活推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何高效提升大型语言模型的推理能力，减少对大量数据和计算资源的依赖。

Method: 采用基于基础模型的简单蒸馏方法，分析模型输出中的词频和行为模式。

Result: 蒸馏模型在灵活推理、多视角思维和元认知意识方面表现更优，优于零强化学习模型。

Conclusion: 蒸馏方法能更高效地提升模型的推理能力，尤其是在复杂问题解决中表现突出。

Abstract: Reinforcement learning (RL) has played an important role in improving the
reasoning ability of large language models (LLMs). Some studies apply RL
directly to \textit{smaller} base models (known as zero-RL) and also achieve
notable progress. However, in this paper, we show that using only 920 examples,
a simple distillation method based on the base model can clearly outperform
zero-RL, which typically requires much more data and computational cost. By
analyzing the token frequency in model outputs, we find that the distilled
model shows more flexible reasoning. It uses anthropomorphic tokens and logical
connectors much more often than the zero-RL model. Further analysis reveals
that distillation enhances the presence of two advanced cognitive behaviors:
Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent
occurrences of these two advanced cognitive behaviors give rise to flexible
reasoning, which is essential for solving complex reasoning problems, while
zero-RL fails to significantly boost the frequency of these behaviors.

</details>


### [51] [Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation](https://arxiv.org/abs/2505.21106)
*Zhengyang Ji,Yifan Jia,Shang Gao,Yutao Yue*

Main category: cs.AI

TL;DR: 该论文提出了一种解释性框架，结合信息流分析和多轮对话评估，以理解大型视觉语言模型（LVLMs）中社会偏见的起源。研究发现，模型在处理不同人群图像时存在信息利用的系统性差异，且文本模态中也存在偏见模式。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务中表现突出，但其存在显著的社会偏见，现有研究主要关注检测和量化偏见，而对其内在机制缺乏深入理解。

Method: 通过信息流分析识别模型推理过程中高贡献的图像标记，并设计多轮对话机制评估这些标记是否编码敏感信息。同时从文本模态角度补充分析。

Result: 实验表明，LVLMs在处理不同人群图像时存在信息利用的系统性差异，且文本模态的语义表示也显示出偏见模式。

Conclusion: 社会偏见根植于模型的内部推理动态中，研究为跨模态的偏见形成提供了新的解释。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress in
multimodal tasks, yet they also exhibit notable social biases. These biases
often manifest as unintended associations between neutral concepts and
sensitive human attributes, leading to disparate model behaviors across
demographic groups. While existing studies primarily focus on detecting and
quantifying such biases, they offer limited insight into the underlying
mechanisms within the models. To address this gap, we propose an explanatory
framework that combines information flow analysis with multi-round dialogue
evaluation, aiming to understand the origin of social bias from the perspective
of imbalanced internal information utilization. Specifically, we first identify
high-contribution image tokens involved in the model's reasoning process for
neutral questions via information flow analysis. Then, we design a multi-turn
dialogue mechanism to evaluate the extent to which these key tokens encode
sensitive information. Extensive experiments reveal that LVLMs exhibit
systematic disparities in information usage when processing images of different
demographic groups, suggesting that social bias is deeply rooted in the model's
internal reasoning dynamics. Furthermore, we complement our findings from a
textual modality perspective, showing that the model's semantic representations
already display biased proximity patterns, thereby offering a cross-modal
explanation of bias formation.

</details>


### [52] [Interpretable DNFs](https://arxiv.org/abs/2505.21212)
*Martin C. Cooper,Imane Bousdira,Clément Carbonnel*

Main category: cs.AI

TL;DR: 论文研究了可解释的DNF分类器，特别是k-DNF及其补集也能表示为k-DNF的情况，比较了深度k决策树和新型嵌套k-DNF模型，发现后者在可解释性和准确性上具有优势。


<details>
  <summary>Details</summary>
Motivation: 探讨如何设计可解释的分类器，尤其是DNF公式在解释正负决策时的表现。

Method: 研究k-DNF及其补集的可表达性，比较深度k决策树和嵌套k-DNF模型。

Result: 实验表明，嵌套k-DNF在可解释性和准确性上优于决策树。

Conclusion: 嵌套k-DNF是一种有潜力的可解释分类器替代方案。

Abstract: A classifier is considered interpretable if each of its decisions has an
explanation which is small enough to be easily understood by a human user. A
DNF formula can be seen as a binary classifier $\kappa$ over boolean domains.
The size of an explanation of a positive decision taken by a DNF $\kappa$ is
bounded by the size of the terms in $\kappa$, since we can explain a positive
decision by giving a term of $\kappa$ that evaluates to true. Since both
positive and negative decisions must be explained, we consider that
interpretable DNFs are those $\kappa$ for which both $\kappa$ and
$\overline{\kappa}$ can be expressed as DNFs composed of terms of bounded size.
In this paper, we study the family of $k$-DNFs whose complements can also be
expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision
trees and nested $k$-DNFs, a novel family of models. Experiments indicate that
nested $k$-DNFs are an interesting alternative to decision trees in terms of
interpretability and accuracy.

</details>


### [53] [XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration](https://arxiv.org/abs/2505.21279)
*Shaoqing Zhang,Kehai Chen,Zhuosheng Zhang,Rumei Li,Rongxiang Weng,Yang Xiang,Liqiang Nie,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为XBOUND的新评估方法，用于更细粒度地评估设备控制代理（DC agents）的性能，通过计算探索度量来界定其能力边界。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法（如逐步动作准确性和任务成功率）无法提供微观层面的错误分析，难以满足实际应用需求。

Method: 提出XBOUND评估方法，基于探索度量评估DC agents对单个状态的掌握能力，并使用从Android Control测试数据生成的伪情节树数据集。

Result: 对OS-Atlas和UI-TARS系列进行了全面评估，揭示了其在五个常见任务中的整体和具体表现，并指出了当前缺陷。

Conclusion: XBOUND方法为DC agents的性能评估提供了新视角，有助于发现潜在问题并推动改进。

Abstract: Recent advancements in vision-language models (VLMs) have spurred increased
interest in Device-Control Agents (DC agents), such as utilizing in-the-wild
device control to manage graphical user interfaces. Conventional methods for
assessing the capabilities of DC agents, such as computing step-wise action
accuracy and overall task success rates, provide a macroscopic view of DC
agents' performance; however, they fail to offer microscopic insights into
potential errors that may occur in real-world applications. Conducting a
finer-grained performance evaluation of DC agents presents significant
challenges. This study introduces a new perspective on evaluation methods for
DC agents by proposing the XBOUND evaluation method, which employs the
calculation of a novel Explore Metric to delineate the capability boundaries of
DC agents. Compared to previous evaluation methods, XBOUND focuses on
individual states to assess the proficiency of DC agents in mastering these
states. Furthermore, we have developed a ``pseudo'' episode tree dataset
derived from Android Control test data. Utilizing this dataset and XBOUND, we
comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the
overall and specific performance across five common tasks. Additionally, we
select representative cases to highlight the current deficiencies and
limitations inherent in both series. Code is available at
https://github.com/sqzhang-lazy/XBOUND.

</details>


### [54] [RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models](https://arxiv.org/abs/2505.21281)
*Yue Zhang,Zhiliang Tian,Shicheng Zhou,Haiyang Wang,Wenqing Hou,Yuying Liu,Xuechen Zhao,Minlie Huang,Ye Wang,Bin Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于一阶逻辑（FOL）和对比学习（CL）的规则增强法律判决预测框架，通过自适应调整法律逻辑提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有法律判决预测模型忽视法律推理逻辑，且逻辑僵化难以适应复杂案例。

Method: 采用三阶段方法：FOL初始化判决规则，CACL动态优化规则，最终预测判决。

Result: 在两个公开数据集上表现优于现有方法。

Conclusion: 提出的框架有效提升了法律判决预测的性能和适应性。

Abstract: Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing
semantic-enhanced LJP models integrate judicial precedents and legal knowledge
for high performance. But they neglect legal reasoning logic, a critical
component of legal judgments requiring rigorous logical analysis. Although some
approaches utilize legal reasoning logic for high-quality predictions, their
logic rigidity hinders adaptation to case-specific logical frameworks,
particularly in complex cases that are lengthy and detailed. This paper
proposes a rule-enhanced legal judgment prediction framework based on
first-order logic (FOL) formalism and comparative learning (CL) to develop an
adaptive adjustment mechanism for legal judgment logic and further enhance
performance in LJP. Inspired by the process of human exam preparation, our
method follows a three-stage approach: first, we initialize judgment rules
using the FOL formalism to capture complex reasoning logic accurately; next, we
propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize
the judgment rules through a quiz consisting of confusable cases; finally, we
utilize the optimized judgment rules to predict legal judgments. Experimental
results on two public datasets show superior performance across all metrics.
The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.

</details>


### [55] [Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework](https://arxiv.org/abs/2505.21291)
*Saman Marandi,Yu-Shu Hu,Mohammad Modarres*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱（KG）和大型语言模型（LLM）的新型诊断框架，用于高可靠性系统（如核电站）的诊断，通过功能建模和动态主逻辑（DML）模型实现高效诊断。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法在复杂系统中表现不佳，功能建模更具吸引力，因此开发了结合KG和LLM的框架以提高诊断效率和准确性。

Method: 采用DML模型的功能建模原则，结合两个LLM组件：一个用于从系统文档自动构建DML逻辑，另一个用于交互式诊断。生成的逻辑编码为KG-DML，支持分层故障推理。

Result: 在辅助给水系统的案例研究中，框架表现出超过90%的关键元素准确性，工具和参数提取一致，适用于安全关键诊断。

Conclusion: 该框架通过KG和LLM的结合，显著提升了复杂系统的诊断能力，具有实际应用潜力。

Abstract: In this paper, we present a novel diagnostic framework that integrates
Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system
diagnostics in high-reliability systems such as nuclear power plants.
Traditional diagnostic modeling struggles when systems become too complex,
making functional modeling a more attractive approach. Our approach introduces
a diagnostic framework grounded in the functional modeling principles of the
Dynamic Master Logic (DML) model. It incorporates two coordinated LLM
components, including an LLM-based workflow for automated construction of DML
logic from system documentation and an LLM agent that facilitates interactive
diagnostics. The generated logic is encoded into a structured KG, referred to
as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or
operational data can also be incorporated to refine the model's precision and
diagnostic depth. In the interaction phase, users submit natural language
queries, which are interpreted by the LLM agent. The agent selects appropriate
tools for structured reasoning, including upward and downward propagation
across the KG-DML. Rather than embedding KG content into every prompt, the LLM
agent distinguishes between diagnostic and interpretive tasks. For diagnostics,
the agent selects and executes external tools that perform structured KG
reasoning. For general queries, a Graph-based Retrieval-Augmented Generation
(Graph-RAG) approach is used, retrieving relevant KG segments and embedding
them into the prompt to generate natural explanations. A case study on an
auxiliary feedwater system demonstrated the framework's effectiveness, with
over 90% accuracy in key elements and consistent tool and argument extraction,
supporting its use in safety-critical diagnostics.

</details>


### [56] [Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations](https://arxiv.org/abs/2505.21318)
*Hao Li,He Cao,Bin Feng,Yanjun Shao,Xiangru Tang,Zhiyuan Yan,Li Yuan,Yonghong Tian,Yu Li*

Main category: cs.AI

TL;DR: ChemCoTBench是一个将分子结构理解与算术操作相结合的推理框架，旨在解决化学领域的复杂任务，如分子优化和反应预测。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在化学领域的系统推理能力未被充分开发，现有基准测试过于简单，无法满足复杂任务的需求。

Method: 通过将分子转化视为模块化的“化学操作”，框架实现了逐步推理，类似于数学证明的逻辑。

Result: 在分子属性优化和化学反应预测两个高影响力任务上进行了评估，提供了标注数据集和基线评估。

Conclusion: ChemCoTBench填补了抽象推理方法与实际化学发现之间的空白，为AI驱动的科学创新奠定了基础。

Abstract: While large language models (LLMs) with Chain-of-Thought (CoT) reasoning
excel in mathematics and coding, their potential for systematic reasoning in
chemistry, a domain demanding rigorous structural analysis for real-world tasks
like drug design and reaction engineering, remains untapped. Current benchmarks
focus on simple knowledge retrieval, neglecting step-by-step reasoning required
for complex tasks such as molecular optimization and reaction prediction. To
address this, we introduce ChemCoTBench, a reasoning framework that bridges
molecular structure understanding with arithmetic-inspired operations,
including addition, deletion, and substitution, to formalize chemical
problem-solving into transparent, step-by-step workflows. By treating molecular
transformations as modular "chemical operations", the framework enables
slow-thinking reasoning, mirroring the logic of mathematical proofs while
grounding solutions in real-world chemical constraints. We evaluate models on
two high-impact tasks: Molecular Property Optimization and Chemical Reaction
Prediction. These tasks mirror real-world challenges while providing structured
evaluability. By providing annotated datasets, a reasoning taxonomy, and
baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning
methods and practical chemical discovery, establishing a foundation for
advancing LLMs as tools for AI-driven scientific innovation.

</details>


### [57] [Assured Autonomy with Neuro-Symbolic Perception](https://arxiv.org/abs/2505.21322)
*R. Spencer Hallyburton,Miroslav Pajic*

Main category: cs.AI

TL;DR: 论文提出了一种神经符号感知范式（NeuSPaPer），通过结合对象检测和场景图生成（SGG）实现深度场景理解，以提升AI在安全关键领域的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在物理信息系统中缺乏安全保障，需要结合符号结构以增强可靠性和情境感知能力。

Method: 提出NeuSPaPer框架，利用基础模型离线提取知识，结合实时SGG算法，构建结构化关系图。

Result: 通过物理模拟器和真实数据集验证，SGG能有效连接低层感知与高层推理。

Conclusion: NeuSPaPer为物理信息系统中的可信自主性提供了基础，推动了上下文感知AI的发展。

Abstract: Many state-of-the-art AI models deployed in cyber-physical systems (CPS),
while highly accurate, are simply pattern-matchers.~With limited security
guarantees, there are concerns for their reliability in safety-critical and
contested domains. To advance assured AI, we advocate for a paradigm shift that
imbues data-driven perception models with symbolic structure, inspired by a
human's ability to reason over low-level features and high-level context. We
propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how
joint object detection and scene graph generation (SGG) yields deep scene
understanding.~Powered by foundation models for offline knowledge extraction
and specialized SGG algorithms for real-time deployment, we design a framework
leveraging structured relational graphs that ensures the integrity of
situational awareness in autonomy. Using physics-based simulators and
real-world datasets, we demonstrate how SGG bridges the gap between low-level
sensor perception and high-level reasoning, establishing a foundation for
resilient, context-aware AI and advancing trusted autonomy in CPS.

</details>


### [58] [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327)
*Jiakang Yuan,Tianshuo Peng,Yilei Jiang,Yiting Lu,Renrui Zhang,Kaituo Feng,Chaoyou Fu,Tao Chen,Lei Bai,Bo Zhang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 论文介绍了MME-Reasoning基准，用于全面评估多模态大语言模型（MLLMs）的逻辑推理能力，发现现有模型在综合推理能力上存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能全面评估MLLMs的逻辑推理能力，缺乏明确的推理类型分类和对推理能力的清晰理解。

Method: 提出MME-Reasoning基准，涵盖归纳、演绎和溯因三种推理类型，并精心设计问题以有效评估推理能力。

Result: 评估显示当前最先进的MLLMs在综合逻辑推理能力上表现有限，且在不同推理类型中存在性能不平衡。

Conclusion: 研究揭示了当前MLLMs在多样逻辑推理场景中的关键局限性和性能不平衡，为推理能力的理解和评估提供了系统见解。

Abstract: Logical reasoning is a fundamental aspect of human intelligence and an
essential capability for multimodal large language models (MLLMs). Despite the
significant advancement in multimodal reasoning, existing benchmarks fail to
comprehensively evaluate their reasoning abilities due to the lack of explicit
categorization for logical reasoning types and an unclear understanding of
reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive
benchmark designed to evaluate the reasoning ability of MLLMs, which covers all
three types of reasoning (i.e., inductive, deductive, and abductive) in its
questions. We carefully curate the data to ensure that each question
effectively evaluates reasoning ability rather than perceptual skills or
knowledge breadth, and extend the evaluation protocols to cover the evaluation
of diverse questions. Our evaluation reveals substantial limitations of
state-of-the-art MLLMs when subjected to holistic assessments of logical
reasoning capabilities. Even the most advanced MLLMs show limited performance
in comprehensive logical reasoning, with notable performance imbalances across
reasoning types. In addition, we conducted an in-depth analysis of approaches
such as ``thinking mode'' and Rule-based RL, which are commonly believed to
enhance reasoning abilities. These findings highlight the critical limitations
and performance imbalances of current MLLMs in diverse logical reasoning
scenarios, providing comprehensive and systematic insights into the
understanding and evaluation of reasoning capabilities.

</details>


### [59] [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
*Aidan Peppin,Julia Kreutzer,Alice Schoenauer Sebag,Kelly Marchisio,Beyza Ermis,John Dang,Samuel Cahyawijaya,Shivalika Singh,Seraphina Goldfarb-Tarrant,Viraat Aryabumi,Aakanksha,Wei-Yin Ko,Ahmet Üstün,Matthias Gallé,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TL;DR: 论文探讨了AI中语言差距的挑战及其对全球AI安全的影响，提出了解决建议。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力提升，但非主流语言的能力与安全性仍存在巨大差距，需解决这一语言差距及其带来的安全问题。

Method: 分析了语言差距存在的原因及其扩大趋势，探讨了其对全球AI安全的影响。

Result: 指出了解决语言差距的障碍，并建议通过支持多语言数据集创建、透明度和研究来应对安全问题。

Conclusion: 呼吁政策与治理领域的工作者采取行动，缩小语言差距，提升全球AI安全性。

Abstract: Despite advances in large language model capabilities in recent years, a
large gap remains in their capabilities and safety performance for many
languages beyond a relatively small handful of globally dominant languages.
This paper provides researchers, policymakers and governance experts with an
overview of key challenges to bridging the "language gap" in AI and minimizing
safety risks across languages. We provide an analysis of why the language gap
in AI exists and grows, and how it creates disparities in global AI safety. We
identify barriers to address these challenges, and recommend how those working
in policy and governance can help address safety concerns associated with the
language gap by supporting multilingual dataset creation, transparency, and
research.

</details>


### [60] [A Structured Unplugged Approach for Foundational AI Literacy in Primary Education](https://arxiv.org/abs/2505.21398)
*Maria Cristina Carrisi,Mirko Marras,Sara Vergallo*

Main category: cs.AI

TL;DR: 论文提出了一种针对小学生的结构化AI素养教学方法，通过结合数学核心概念，提升学生对AI的理解和应用能力。实证研究表明，该方法有效提高了学生的术语理解、逻辑推理和评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育过于工具化，缺乏对核心概念的理解，导致学生容易产生误解和偏见。研究旨在通过结构化方法提升小学生的AI素养。

Method: 提出基于数学核心概念的结构化教学方法，通过实证研究（31名五年级学生）评估其效果，采用后测和满意度调查。

Result: 学生在术语理解、逻辑推理和评估能力方面有显著提升，同时对AI决策过程及其局限性有了更深理解。

Conclusion: 该方法不仅有效提升了小学生的AI素养，还因其趣味性和实用性受到学生欢迎，为AI基础教育提供了可行方案。

Abstract: Younger generations are growing up in a world increasingly shaped by
intelligent technologies, making early AI literacy crucial for developing the
skills to critically understand and navigate them. However, education in this
field often emphasizes tool-based learning, prioritizing usage over
understanding the underlying concepts. This lack of knowledge leaves
non-experts, especially children, prone to misconceptions, unrealistic
expectations, and difficulties in recognizing biases and stereotypes. In this
paper, we propose a structured and replicable teaching approach that fosters
foundational AI literacy in primary students, by building upon core
mathematical elements closely connected to and of interest in primary
curricula, to strengthen conceptualization, data representation, classification
reasoning, and evaluation of AI. To assess the effectiveness of our approach,
we conducted an empirical study with thirty-one fifth-grade students across two
classes, evaluating their progress through a post-test and a satisfaction
survey. Our results indicate improvements in terminology understanding and
usage, features description, logical reasoning, and evaluative skills, with
students showing a deeper comprehension of decision-making processes and their
limitations. Moreover, the approach proved engaging, with students particularly
enjoying activities that linked AI concepts to real-world reasoning. Materials:
https://github.com/tail-unica/ai-literacy-primary-ed.

</details>


### [61] [MRSD: Multi-Resolution Skill Discovery for HRL Agents](https://arxiv.org/abs/2505.21410)
*Shashank Sharma,Janina Hoffmann,Vinay Namboodiri*

Main category: cs.AI

TL;DR: 论文提出了一种多分辨率技能发现（MRSD）框架，通过并行学习不同时间分辨率的技能编码器，提升分层强化学习（HRL）的效率。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现方法通常局限于每个任务单一技能，而人类能同时使用多粒度技能。MRSD受此启发，旨在实现更灵活的控制策略。

Method: MRSD框架并行学习多分辨率技能编码器，高层管理器动态选择技能，适应不同任务需求。

Result: 在DeepMind Control Suite任务中，MRSD表现优于现有技能发现和HRL方法，收敛更快且性能更高。

Conclusion: 多分辨率技能的集成显著提升了HRL的效率和适应性，为开发更通用的智能体提供了新思路。

Abstract: Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.

</details>


### [62] [Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs](https://arxiv.org/abs/2505.21419)
*Yifan Wang,Kenneth P. Birman*

Main category: cs.AI

TL;DR: ARCA是一种多模态RAG LLM系统，通过结合AI模式匹配能力和自然多模态接口，简化云应用问题的识别与解决，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 云托管应用和服务的复杂性导致性能或功能问题难以快速定位和解决，需要更高效的工具。

Method: 结合现代AI的模式匹配能力和多模态RAG LLM接口，开发了ARCA系统。

Result: 逐步评估显示ARCA优于现有最先进方案。

Conclusion: ARCA通过多模态RAG LLM和AI模式匹配，有效简化了云应用问题的识别与解决。

Abstract: Today's cloud-hosted applications and services are complex systems, and a
performance or functional instability can have dozens or hundreds of potential
root causes. Our hypothesis is that by combining the pattern matching
capabilities of modern AI tools with a natural multi-modal RAG LLM interface,
problem identification and resolution can be simplified. ARCA is a new
multi-modal RAG LLM system that targets this domain. Step-wise evaluations show
that ARCA outperforms state-of-the-art alternatives.

</details>


### [63] [Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning](https://arxiv.org/abs/2505.21427)
*Xianling Mu,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 提出了一种基于记忆增强大型语言模型（LLM）的透明且数据高效的投资决策框架，通过上下文学习（ICL）实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 早期初创企业投资风险高且数据稀缺，传统机器学习方法依赖大量标注数据且不透明，难以解释和改进。

Method: 采用自然语言策略嵌入LLM提示，结合少量样本学习和上下文学习循环，实现透明且可迭代优化的决策逻辑。

Result: 系统预测初创企业成功的准确率远超基准，比随机概率高20倍，比顶级风投公司高7.1倍。

Conclusion: 该方法为高风险投资提供了一种透明、高效且可解释的决策工具。

Abstract: Early-stage startup investment is a high-risk endeavor characterized by
scarce data and uncertain outcomes. Traditional machine learning approaches
often require large, labeled datasets and extensive fine-tuning, yet remain
opaque and difficult for domain experts to interpret or improve. In this paper,
we propose a transparent and data-efficient investment decision framework
powered by memory-augmented large language models (LLMs) using in-context
learning (ICL). Central to our method is a natural language policy embedded
directly into the LLM prompt, enabling the model to apply explicit reasoning
patterns and allowing human experts to easily interpret, audit, and iteratively
refine the logic. We introduce a lightweight training process that combines
few-shot learning with an in-context learning loop, enabling the LLM to update
its decision policy iteratively based on structured feedback. With only minimal
supervision and no gradient-based optimization, our system predicts startup
success far more accurately than existing benchmarks. It is over 20x more
precise than random chance, which succeeds 1.9% of the time. It is also 7.1x
more precise than the typical 5.6% success rate of top-tier venture capital
(VC) firms.

</details>


### [64] [Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming](https://arxiv.org/abs/2505.21486)
*Yang Yang,Jiemin Wu,Yutao Yue*

Main category: cs.AI

TL;DR: 论文提出了一种结合多智能体系统（基于LLMs）和归纳逻辑编程（ILP）的新框架，用于自动化生成鲁棒的假设。


<details>
  <summary>Details</summary>
Motivation: 解决传统ILP依赖预定义符号结构和纯LLM方法对噪声敏感的问题，实现自动化、可解释的假设生成。

Method: 通过LLM智能体从原始文本数据中自动定义符号词汇和关系模板（语言偏置），指导ILP求解器生成可解释规则。

Result: 在多样化的挑战性场景中验证了其优越性能。

Conclusion: 为自动化、可解释且可验证的假设生成开辟了新途径。

Abstract: Automating robust hypothesis generation in open environments is pivotal for
AI cognition. We introduce a novel framework integrating a multi-agent system,
powered by Large Language Models (LLMs), with Inductive Logic Programming
(ILP). Our system's LLM agents autonomously define a structured symbolic
vocabulary (predicates) and relational templates , i.e., \emph{language bias}
directly from raw textual data. This automated symbolic grounding (the
construction of the language bias), traditionally an expert-driven bottleneck
for ILP, then guides the transformation of text into facts for an ILP solver,
which inductively learns interpretable rules. This approach overcomes
traditional ILP's reliance on predefined symbolic structures and the
noise-sensitivity of pure LLM methods. Extensive experiments in diverse,
challenging scenarios validate superior performance, paving a new path for
automated, explainable, and verifiable hypothesis generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
*Lorenzo Baraldi,Davide Bucciarelli,Federico Betti,Marcella Cornia,Lorenzo Baraldi,Nicu Sebe,Rita Cucchiara*

Main category: cs.CV

TL;DR: DICE是一个基于多模态大语言模型的图像编辑评估工具，通过检测差异和评估编辑一致性，显著提升了与人类判断的相关性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑评估指标与人类判断和可解释性脱节，亟需一种更有效的评估方法。

Method: DICE由差异检测器和一致性评估器组成，基于自监督、蒸馏和全监督策略训练的多模态大语言模型。

Result: 实验表明DICE能有效识别一致性编辑，并与人类判断高度相关。

Conclusion: DICE为图像编辑评估提供了更可靠的解决方案，代码和模型已公开。

Abstract: Instruction-based image editing models offer increased personalization
opportunities in generative tasks. However, properly evaluating their results
is challenging, and most of the existing metrics lag in terms of alignment with
human judgment and explainability. To tackle these issues, we introduce DICE
(DIfference Coherence Estimator), a model designed to detect localized
differences between the original and the edited image and to assess their
relevance to the given modification request. DICE consists of two key
components: a difference detector and a coherence estimator, both built on an
autoregressive Multimodal Large Language Model (MLLM) and trained using a
strategy that leverages self-supervision, distillation from inpainting
networks, and full supervision. Through extensive experiments, we evaluate each
stage of our pipeline, comparing different MLLMs within the proposed framework.
We demonstrate that DICE effectively identifies coherent edits, effectively
evaluating images generated by different editing models with a strong
correlation with human judgment. We publicly release our source code, models,
and data.

</details>


### [66] [ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking](https://arxiv.org/abs/2505.20381)
*Sijia Chen,Yanqiu Yu,En Yu,Wenbing Tao*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务ReaMOT，专注于基于推理的多目标跟踪，并构建了ReaMOT Challenge基准和ReaTrack框架。


<details>
  <summary>Details</summary>
Motivation: 现有的RMOT任务在复杂推理语言指令下表现不佳，需要更挑战性的任务和评估方法。

Method: 构建了ReaMOT Challenge基准，包含多难度推理语言指令和图像对；提出ReaTrack框架，基于大视觉语言模型和SAM2。

Result: ReaTrack框架在ReaMOT Challenge基准上表现有效。

Conclusion: ReaMOT任务和ReaTrack框架为基于推理的多目标跟踪提供了新方向和基准。

Abstract: Referring Multi-object tracking (RMOT) is an important research field in
computer vision. Its task form is to guide the models to track the objects that
conform to the language instruction. However, the RMOT task commonly requires
clear language instructions, such methods often fail to work when complex
language instructions with reasoning characteristics appear. In this work, we
propose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT).
ReaMOT is a more challenging task that requires accurate reasoning about
objects that match the language instruction with reasoning characteristic and
tracking the objects' trajectories. To advance the ReaMOT task and evaluate the
reasoning capabilities of tracking models, we construct ReaMOT Challenge, a
reasoning-based multi-object tracking benchmark built upon 12 datasets.
Specifically, it comprises 1,156 language instructions with reasoning
characteristic, 423,359 image-language pairs, and 869 diverse scenes, which is
divided into three levels of reasoning difficulty. In addition, we propose a
set of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose
ReaTrack, a training-free framework for reasoning-based multi-object tracking
based on large vision-language models (LVLM) and SAM2, as a baseline for the
ReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark
demonstrate the effectiveness of our ReaTrack framework.

</details>


### [67] [RetroMotion: Retrocausal Motion Forecasting Models are Instructable](https://arxiv.org/abs/2505.20414)
*Royden Wagner,Omer Sahin Tas,Felix Hauser,Marlon Steiner,Dominik Strutz,Abhishek Vivekanandan,Carlos Fernandez,Christoph Stiller*

Main category: cs.CV

TL;DR: 该论文提出了一种多任务学习方法，用于运动预测，结合了逆向信息流，实现了在Waymo和Argoverse 2数据集上的先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决场景约束和交互行为导致的运动预测复杂性差异问题。

Method: 使用Transformer模型生成联合轨迹分布，通过重新编码边缘分布和成对建模，引入逆向信息流。

Result: 在Waymo Interaction Prediction数据集上达到最优结果，并泛化至Argoverse 2数据集。

Conclusion: 该方法不仅能预测运动，还能通过轨迹修改接受指令，适应场景上下文。

Abstract: Motion forecasts of road users (i.e., agents) vary in complexity as a
function of scene constraints and interactive behavior. We address this with a
multi-task learning method for motion forecasting that includes a retrocausal
flow of information. The corresponding tasks are to forecast (1) marginal
trajectory distributions for all modeled agents and (2) joint trajectory
distributions for interacting agents. Using a transformer model, we generate
the joint distributions by re-encoding marginal distributions followed by
pairwise modeling. This incorporates a retrocausal flow of information from
later points in marginal trajectories to earlier points in joint trajectories.
Per trajectory point, we model positional uncertainty using compressed
exponential power distributions. Notably, our method achieves state-of-the-art
results in the Waymo Interaction Prediction dataset and generalizes well to the
Argoverse 2 dataset. Additionally, our method provides an interface for issuing
instructions through trajectory modifications. Our experiments show that
regular training of motion forecasting leads to the ability to follow
goal-based instructions and to adapt basic directional instructions to the
scene context. Code: https://github.com/kit-mrt/future-motion

</details>


### [68] [MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness](https://arxiv.org/abs/2505.20426)
*Yunlong Tang,Pinxin Liu,Mingqian Feng,Zhangyun Tan,Rui Mao,Chao Huang,Jing Bi,Yunzhong Xiao,Susan Liang,Hang Hua,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Chenliang Xu*

Main category: cs.CV

TL;DR: MMPerspective是首个专门评估多模态大语言模型（MLLMs）对透视几何理解的基准测试，包含10个任务和5,083个问答对，揭示了模型在感知、推理和鲁棒性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型是否具备透视几何理解能力，填补现有研究的空白。

Method: 设计MMPerspective基准测试，包含2,711张真实和合成图像，通过10个任务评估模型的透视感知、推理和鲁棒性。

Result: 43个先进MLLMs在表面感知任务上表现良好，但在组合推理和空间一致性方面存在显著不足。

Conclusion: MMPerspective为诊断和提升视觉语言系统的空间理解能力提供了重要工具。

Abstract: Understanding perspective is fundamental to human visual perception, yet the
extent to which multimodal large language models (MLLMs) internalize
perspective geometry remains unclear. We introduce MMPerspective, the first
benchmark specifically designed to systematically evaluate MLLMs' understanding
of perspective through 10 carefully crafted tasks across three complementary
dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark
comprises 2,711 real-world and synthetic image instances with 5,083
question-answer pairs that probe key capabilities, such as vanishing point
perception and counting, perspective type reasoning, line relationship
understanding in 3D space, invariance to perspective-preserving
transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art
MLLMs, we uncover significant limitations: while models demonstrate competence
on surface-level perceptual tasks, they struggle with compositional reasoning
and maintaining spatial consistency under perturbations. Our analysis further
reveals intriguing patterns between model architecture, scale, and perspective
capabilities, highlighting both robustness bottlenecks and the benefits of
chain-of-thought prompting. MMPerspective establishes a valuable testbed for
diagnosing and advancing spatial understanding in vision-language systems.
Resources available at: https://yunlong10.github.io/MMPerspective/

</details>


### [69] [DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data](https://arxiv.org/abs/2505.20460)
*Ruqi Wu,Xinjie Wang,Liu Liu,Chunle Guo,Jiaxiong Qiu,Chongyi Li,Lichao Huang,Zhizhong Su,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: DIPO是一个新颖的框架，通过双图像输入生成可控的3D关节物体，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决单图像方法在预测物体运动信息上的不足，双图像输入提供更可靠的运动指导。

Method: 提出双图像扩散模型和基于Chain-of-Thought的图推理器，并开发自动化数据集扩展工具LEGO-Art。

Result: DIPO在静止和关节状态下均显著优于基线，PM-X数据集增强了泛化能力。

Conclusion: DIPO和PM-X数据集为可控3D物体生成提供了高效解决方案，代码和数据集将公开。

Abstract: We present DIPO, a novel framework for the controllable generation of
articulated 3D objects from a pair of images: one depicting the object in a
resting state and the other in an articulated state. Compared to the
single-image approach, our dual-image input imposes only a modest overhead for
data collection, but at the same time provides important motion information,
which is a reliable guide for predicting kinematic relationships between parts.
Specifically, we propose a dual-image diffusion model that captures
relationships between the image pair to generate part layouts and joint
parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph
reasoner that explicitly infers part connectivity relationships. To further
improve robustness and generalization on complex articulated objects, we
develop a fully automated dataset expansion pipeline, name LEGO-Art, that
enriches the diversity and complexity of PartNet-Mobility dataset. We propose
PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by
rendered images, URDF annotations, and textual descriptions. Extensive
experiments demonstrate that DIPO significantly outperforms existing baselines
in both the resting state and the articulated state, while the proposed PM-X
dataset further enhances generalization to diverse and structurally complex
articulated objects. Our code and dataset will be released to the community
upon publication.

</details>


### [70] [CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.20469)
*Lei Tian,Xiaomin Li,Liqian Ma,Hefei Huang,Zirui Zheng,Hao Yin,Taiqing Li,Huchuan Lu,Xu Jia*

Main category: cs.CV

TL;DR: 论文提出CCL-LGS框架，通过多视角语义线索解决3D语义理解中的视角不一致问题，提升3D高斯语义场质量。


<details>
  <summary>Details</summary>
Motivation: 现有依赖2D先验的方法因遮挡、图像模糊和视角变化导致语义不一致，影响3D语义重建质量。

Method: 结合零样本跟踪器对齐SAM生成的2D掩码，利用CLIP提取多视角语义编码，并通过对比码书学习模块优化特征。

Result: 实验表明CCL-LGS优于现有方法，有效解决语义冲突并保持类别区分性。

Conclusion: CCL-LGS通过多视角一致性监督显著提升了3D语义理解的质量。

Abstract: Recent advances in 3D reconstruction techniques and vision-language models
have fueled significant progress in 3D semantic understanding, a capability
critical to robotics, autonomous driving, and virtual/augmented reality.
However, methods that rely on 2D priors are prone to a critical challenge:
cross-view semantic inconsistencies induced by occlusion, image blur, and
view-dependent variations. These inconsistencies, when propagated via
projection supervision, deteriorate the quality of 3D Gaussian semantic fields
and introduce artifacts in the rendered outputs. To mitigate this limitation,
we propose CCL-LGS, a novel framework that enforces view-consistent semantic
supervision by integrating multi-view semantic cues. Specifically, our approach
first employs a zero-shot tracker to align a set of SAM-generated 2D masks and
reliably identify their corresponding categories. Next, we utilize CLIP to
extract robust semantic encodings across views. Finally, our Contrastive
Codebook Learning (CCL) module distills discriminative semantic features by
enforcing intra-class compactness and inter-class distinctiveness. In contrast
to previous methods that directly apply CLIP to imperfect masks, our framework
explicitly resolves semantic conflicts while preserving category
discriminability. Extensive experiments demonstrate that CCL-LGS outperforms
previous state-of-the-art methods. Our project page is available at
https://epsilontl.github.io/CCL-LGS/.

</details>


### [71] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/abs/2505.20471)
*Chenghao Qian,Wenjing Li,Yuhu Guo,Gustav Markkula*

Main category: cs.CV

TL;DR: WeatherEdit是一种新颖的天气编辑管道，用于在3D场景中生成具有可控类型和强度的逼真天气效果。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶模拟提供多样且可控的恶劣天气效果。

Method: 分为天气背景编辑和天气粒子构建两部分，使用扩散模型和4D高斯场技术。

Result: 在多个驾驶数据集上验证了生成多样且可控天气效果的能力。

Conclusion: WeatherEdit在自动驾驶模拟中具有潜力。

Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for
generating realistic weather effects with controllable types and severity in 3D
scenes. Our approach is structured into two key components: weather background
editing and weather particle construction. For weather background editing, we
introduce an all-in-one adapter that integrates multiple weather styles into a
single pretrained diffusion model, enabling the generation of diverse weather
effects in 2D image backgrounds. During inference, we design a Temporal-View
(TV-) attention mechanism that follows a specific order to aggregate temporal
and spatial information, ensuring consistent editing across multi-frame and
multi-view images. To construct the weather particles, we first reconstruct a
3D scene using the edited images and then introduce a dynamic 4D Gaussian field
to generate snowflakes, raindrops and fog in the scene. The attributes and
dynamics of these particles are precisely controlled through physical-based
modelling and simulation, ensuring realistic weather representation and
flexible severity adjustments. Finally, we integrate the 4D Gaussian field with
the 3D scene to render consistent and highly realistic weather effects.
Experiments on multiple driving datasets demonstrate that WeatherEdit can
generate diverse weather effects with controllable condition severity,
highlighting its potential for autonomous driving simulation in adverse
weather. See project page: https://jumponthemoon.github.io/w-edit

</details>


### [72] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/abs/2505.20498)
*Dongyu Luo,Kelin Yu,Amir-Hossein Shahidzadeh,Cornelia Fermüller,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: ControlTac是一个两阶段可控框架，通过参考触觉图像、接触力和接触位置生成逼真的触觉图像，用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决触觉数据采集成本高、现有方法生成不真实且难以迁移的问题。

Method: 提出ControlTac框架，利用物理先验作为控制输入生成触觉图像。

Result: 实验证明ControlTac能有效增强触觉数据集并提升下游任务性能。

Conclusion: ControlTac在真实实验中验证了其实用性。

Abstract: Vision-based tactile sensing has been widely used in perception,
reconstruction, and robotic manipulation. However, collecting large-scale
tactile data remains costly due to the localized nature of sensor-object
interactions and inconsistencies across sensor instances. Existing approaches
to scaling tactile data, such as simulation and free-form tactile generation,
often suffer from unrealistic output and poor transferability to downstream
tasks.To address this, we propose ControlTac, a two-stage controllable
framework that generates realistic tactile images conditioned on a single
reference tactile image, contact force, and contact position. With those
physical priors as control input, ControlTac generates physically plausible and
varied tactile images that can be used for effective data augmentation. Through
experiments on three downstream tasks, we demonstrate that ControlTac can
effectively augment tactile datasets and lead to consistent gains. Our three
real-world experiments further validate the practical utility of our approach.
Project page: https://dongyuluo.github.io/controltac.

</details>


### [73] [Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](https://arxiv.org/abs/2505.20507)
*Elias Arbash,Ahmed Jamal Afifi,Ymane Belahsen,Margret Fuchs,Pedram Ghamisi,Paul Scheunders,Richard Gloaguen*

Main category: cs.CV

TL;DR: 论文提出了一个名为Electrolyzers-HSI的多模态基准数据集，用于加速电解器材料的分类，支持可持续回收。


<details>
  <summary>Details</summary>
Motivation: 解决全球可持续回收的挑战，需要自动化、快速且准确的材料检测系统，以支持循环经济和绿色协议。

Method: 数据集包含RGB图像和HSI数据立方体，评估了多种ML和DL方法，包括Vision Transformer和Multimodal Fusion Transformer。

Result: 数据集包含420万像素向量和42万标记像素，支持非侵入式光谱分析和材料分类。

Conclusion: 数据集和代码公开可用，促进智能和可持续电子废物回收的广泛应用。

Abstract: The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.

</details>


### [74] [CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic](https://arxiv.org/abs/2505.20510)
*Yuxuan Sun,Yixuan Si,Chenglu Zhu,Kai Zhang,Zhongyi Shui,Bowen Ding,Tao Lin,Lin Yang*

Main category: cs.CV

TL;DR: CPathAgent是一种基于代理的模型，模拟病理学家的诊断逻辑，通过多阶段训练策略实现多尺度理解，显著提升诊断报告的详细性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学方法未能模拟病理学家的诊断逻辑，无法系统性地从低倍镜到高倍镜逐步分析图像。

Method: 提出CPathAgent，通过自主执行放大/缩小和导航操作模拟病理学家的推理过程，采用多阶段训练策略统一补丁级、区域级和全切片级能力。

Result: CPathAgent在三个尺度的基准测试中均优于现有方法，并构建了首个用于大区域分析的专家验证基准PathMMU-HR²。

Conclusion: CPathAgent为计算病理学的未来发展提供了有前景的方向，验证了基于代理的诊断方法的有效性。

Abstract: Recent advances in computational pathology have led to the emergence of
numerous foundation models. However, these approaches fail to replicate the
diagnostic process of pathologists, as they either simply rely on
general-purpose encoders with multi-instance learning for classification or
directly apply multimodal models to generate reports from images. A significant
limitation is their inability to emulate the diagnostic logic employed by
pathologists, who systematically examine slides at low magnification for
overview before progressively zooming in on suspicious regions to formulate
comprehensive diagnoses. To address this gap, we introduce CPathAgent, an
innovative agent-based model that mimics pathologists' reasoning processes by
autonomously executing zoom-in/out and navigation operations across pathology
images based on observed visual features. To achieve this, we develop a
multi-stage training strategy unifying patch-level, region-level, and
whole-slide capabilities within a single model, which is essential for
mimicking pathologists, who require understanding and reasoning capabilities
across all three scales. This approach generates substantially more detailed
and interpretable diagnostic reports compared to existing methods, particularly
for huge region understanding. Additionally, we construct an expert-validated
PathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical
intermediate scale between patches and whole slides, as diagnosticians
typically examine several key regions rather than entire slides at once.
Extensive experiments demonstrate that CPathAgent consistently outperforms
existing approaches across three scales of benchmarks, validating the
effectiveness of our agent-based diagnostic approach and highlighting a
promising direction for the future development of computational pathology.

</details>


### [75] [A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models](https://arxiv.org/abs/2505.20512)
*Tangzheng Lian,Oya Celiktutan*

Main category: cs.CV

TL;DR: 提出了一种无需人口统计标签的特征级偏差评估框架，并通过统计模块确保结果的显著性，揭示了FER模型中的显著人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型的偏差分析受限于人口统计标签的缺乏或伪标签的失真，且缺乏统计显著性验证。

Method: 提出特征级偏差评估框架，引入统计模块验证显著性，分析年龄、性别和种族等敏感属性。

Result: 实验表明，该方法比依赖伪标签的方法更有效，揭示了FER模型中显著的人口统计偏差。

Conclusion: 该方法为FER模型的公平性评估提供了新视角，并指导选择更公平的网络架构。

Abstract: Recent studies on fairness have shown that Facial Expression Recognition
(FER) models exhibit biases toward certain visually perceived demographic
groups. However, the limited availability of human-annotated demographic labels
in public FER datasets has constrained the scope of such bias analysis. To
overcome this limitation, some prior works have resorted to pseudo-demographic
labels, which may distort bias evaluation results. Alternatively, in this
paper, we propose a feature-level bias evaluation framework for evaluating
demographic biases in FER models under the setting where demographic labels are
unavailable in the test set. Extensive experiments demonstrate that our method
more effectively evaluates demographic biases compared to existing approaches
that rely on pseudo-demographic labels. Furthermore, we observe that many
existing studies do not include statistical testing in their bias evaluations,
raising concerns that some reported biases may not be statistically significant
but rather due to randomness. To address this issue, we introduce a
plug-and-play statistical module to ensure the statistical significance of
biased evaluation results. A comprehensive bias analysis based on the proposed
module is then conducted across three sensitive attributes (age, gender, and
race), seven facial expressions, and multiple network architectures on a
large-scale dataset, revealing the prominent demographic biases in FER and
providing insights on selecting a fairer network architecture.

</details>


### [76] [MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning](https://arxiv.org/abs/2505.20513)
*Wenhao Gu,Li Gu,Ching Yee Suen,Yang Wang*

Main category: cs.CV

TL;DR: 提出了一种高效的个性化手写文本识别框架，通过提示调优和自监督损失，仅需更新少量参数即可适应不同书写风格，显著减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统手写文本识别方法在测试时缺乏对书写者个性化风格的适应能力，且现有方法需要标注数据或参数调优效率低。

Method: 将个性化问题建模为提示调优，结合自监督损失和元学习优化提示初始化，仅更新不到1%的参数。

Result: 在RIMES和IAM基准测试中表现优于现有方法，参数使用量减少20倍。

Conclusion: 该方法为资源受限场景下的个性化手写文本识别提供了高效可靠的解决方案。

Abstract: Recent advancements in handwritten text recognition (HTR) have enabled the
effective conversion of handwritten text to digital formats. However, achieving
robust recognition across diverse writing styles remains challenging.
Traditional HTR methods lack writer-specific personalization at test time due
to limitations in model architecture and training strategies. Existing attempts
to bridge this gap, through gradient-based meta-learning, still require labeled
examples and suffer from parameter-inefficient fine-tuning, leading to
substantial computational and memory overhead. To overcome these challenges, we
propose an efficient framework that formulates personalization as prompt
tuning, incorporating an auxiliary image reconstruction task with a
self-supervised loss to guide prompt adaptation with unlabeled test-time
examples. To ensure self-supervised loss effectively minimizes text recognition
error, we leverage meta-learning to learn the optimal initialization of the
prompts. As a result, our method allows the model to efficiently capture unique
writing styles by updating less than 1% of its parameters and eliminating the
need for time-intensive annotation processes. We validate our approach on the
RIMES and IAM Handwriting Database benchmarks, where it consistently
outperforms previous state-of-the-art methods while using 20x fewer parameters.
We believe this represents a significant advancement in personalized
handwritten text recognition, paving the way for more reliable and practical
deployment in resource-constrained scenarios.

</details>


### [77] [MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance](https://arxiv.org/abs/2505.20525)
*Aniket Roy,Maitreya Suin,Ketul Shah,Rama Chellappa*

Main category: cs.CV

TL;DR: MultLFG是一种无需训练的多LoRA适配器融合框架，通过频域指导实现自适应融合，显著提升多概念生成的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在无需训练的情况下难以有效融合多个LoRA适配器，尤其是在复杂视觉元素组合中。

Method: MultLFG采用基于时间步和频域子带的自适应融合策略，选择性激活相关LoRA适配器。

Result: 实验表明，MultLFG在ComposLoRA基准上显著提升了组合保真度和图像质量，优于现有方法。

Conclusion: MultLFG通过频域指导实现了更优的多LoRA融合效果，为多概念生成提供了新思路。

Abstract: Low-Rank Adaptation (LoRA) has gained prominence as a computationally
efficient method for fine-tuning generative models, enabling distinct visual
concept synthesis with minimal overhead. However, current methods struggle to
effectively merge multiple LoRA adapters without training, particularly in
complex compositions involving diverse visual elements. We introduce MultLFG, a
novel framework for training-free multi-LoRA composition that utilizes
frequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike
existing methods that uniformly aggregate concept-specific LoRAs, MultLFG
employs a timestep and frequency subband adaptive fusion strategy, selectively
activating relevant LoRAs based on content relevance at specific timesteps and
frequency bands. This frequency-sensitive guidance not only improves spatial
coherence but also provides finer control over multi-LoRA composition, leading
to more accurate and consistent results. Experimental evaluations on the
ComposLoRA benchmark reveal that MultLFG substantially enhances compositional
fidelity and image quality across various styles and concept sets,
outperforming state-of-the-art baselines in multi-concept generation tasks.
Code will be released.

</details>


### [78] [Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey](https://arxiv.org/abs/2505.20540)
*Md Rashidunnabi,Kailash Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: 这篇论文综述了因果推理在视频行人重识别（Re-ID）中的应用，分析了传统方法的局限性，并提出基于因果推理的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频行人重识别方法依赖表面相关性（如服装、背景或光照），难以泛化到不同领域、视角和时间变化。因果推理提供了一种更可靠的方法。

Method: 论文提出了一种新的分类法，涵盖生成解耦、领域不变建模和因果变换器等方法，并引入了因果特定的鲁棒性评估指标。

Result: 综述展示了因果推理在行人重识别中的潜力，并提出了实际应用中的挑战（如可扩展性、公平性、隐私等）。

Conclusion: 论文为因果视频行人重识别奠定了基础，并指出了未来研究方向，如结合高效架构和自监督学习。

Abstract: Video-based person re-identification (Re-ID) remains brittle in real-world
deployments despite impressive benchmark performance. Most existing models rely
on superficial correlations such as clothing, background, or lighting that fail
to generalize across domains, viewpoints, and temporal variations. This survey
examines the emerging role of causal reasoning as a principled alternative to
traditional correlation-based approaches in video-based Re-ID. We provide a
structured and critical analysis of methods that leverage structural causal
models, interventions, and counterfactual reasoning to isolate
identity-specific features from confounding factors. The survey is organized
around a novel taxonomy of causal Re-ID methods that spans generative
disentanglement, domain-invariant modeling, and causal transformers. We review
current evaluation metrics and introduce causal-specific robustness measures.
In addition, we assess practical challenges of scalability, fairness,
interpretability, and privacy that must be addressed for real-world adoption.
Finally, we identify open problems and outline future research directions that
integrate causal modeling with efficient architectures and self-supervised
learning. This survey aims to establish a coherent foundation for causal
video-based person Re-ID and to catalyze the next phase of research in this
rapidly evolving domain.

</details>


### [79] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.20569)
*Jihoon Lee,Min Song*

Main category: cs.CV

TL;DR: RVCD（检索视觉对比解码）是一种新方法，通过利用正负图像在logit级别抑制目标幻觉（OH），显著优于现有解码方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型取得显著进展，但目标幻觉（OH）问题仍然存在。

Method: RVCD利用正负图像在logit级别进行对比解码，无需额外模型训练。

Result: RVCD在抑制OH方面表现优于现有解码方法。

Conclusion: RVCD为抑制目标幻觉提供了一种有效且无需额外训练的解决方案。

Abstract: Despite significant advancements in Large Vision-Language Models, Object
Hallucination (OH) remains a persistent challenge. Building upon prior studies
on contrastive decoding that address this issue without requiring additional
model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an
advanced method to suppress OH. RVCD leverages both negative and positive
images at the logit level, explicitly referencing AI-generated images designed
to represent a single concept. Our approach demonstrates substantial
improvements over existing decoding-based methods.

</details>


### [80] [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
*Wei Pang,Kevin Qinghong Lin,Xiangru Jian,Xi He,Philip Torr*

Main category: cs.CV

TL;DR: 论文提出了首个学术海报生成基准和指标套件，并开发了PosterAgent多智能体流程，显著提升了海报生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决学术海报生成中长上下文文档压缩为视觉连贯页面的挑战。

Method: 引入基准和指标套件，提出PosterAgent多智能体流程（Parser、Planner、Painter-Commenter）。

Result: PosterAgent在多项指标上优于现有系统，生成成本低至$0.005。

Conclusion: 为下一代全自动海报生成模型指明了方向，代码和数据集已开源。

Abstract: Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.

</details>


### [81] [Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting](https://arxiv.org/abs/2505.20582)
*Yizhou Zhao,Chunjiang Liu,Haoyu Chen,Bhiksha Raj,Min Xu,Tadas Baltrusaitis,Mitch Rundle,HsiangTao Wu,Kamran Ghasedi*

Main category: cs.CV

TL;DR: Total-Editing是一个统一的肖像编辑框架，结合了面部重演和肖像重光照，通过神经辐射场和变形场实现高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的面部重演和肖像重光照方法通常独立处理，缺乏协同效应，限制了编辑的灵活性和质量。

Method: 设计了具有固有分解能力的神经辐射场解码器，并结合基于移动最小二乘的变形场，实现几何一致性和光照感知的统一。

Result: 框架显著提升了肖像编辑的质量和真实感，支持多源应用如光照转移和肖像动画。

Conclusion: Total-Editing通过统一框架实现了更灵活、高质量的肖像编辑，为未来研究提供了新方向。

Abstract: Face reenactment and portrait relighting are essential tasks in portrait
editing, yet they are typically addressed independently, without much synergy.
Most face reenactment methods prioritize motion control and multiview
consistency, while portrait relighting focuses on adjusting shading effects. To
take advantage of both geometric consistency and illumination awareness, we
introduce Total-Editing, a unified portrait editing framework that enables
precise control over appearance, motion, and lighting. Specifically, we design
a neural radiance field decoder with intrinsic decomposition capabilities. This
allows seamless integration of lighting information from portrait images or HDR
environment maps into synthesized portraits. We also incorporate a moving least
squares based deformation field to enhance the spatiotemporal coherence of
avatar motion and shading effects. With these innovations, our unified
framework significantly improves the quality and realism of portrait editing
results. Further, the multi-source nature of Total-Editing supports more
flexible applications, such as illumination transfer from one portrait to
another, or portrait animation with customized backgrounds.

</details>


### [82] [OmniIndoor3D: Comprehensive Indoor 3D Reconstruction](https://arxiv.org/abs/2505.20610)
*Xiaobao Wei,Xiaoan Zhang,Hao Wang,Qingpo Wuwu,Ming Lu,Wenzhao Zheng,Shanghang Zhang*

Main category: cs.CV

TL;DR: OmniIndoor3D是一个基于高斯表示的室内3D重建框架，通过RGB-D相机实现外观、几何和全景重建。通过联合优化和轻量级MLP，解决了3DGS几何精度不足的问题，并在多个数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法主要用于逼真渲染，缺乏高精度几何重建能力，而室内场景需要全面的3D理解以支持机器人导航。

Method: 结合RGB-D图像生成粗略3D重建，初始化3D高斯并指导训练；引入轻量级MLP调整几何属性；提出基于全景先验的高斯分布优化策略。

Result: 在多个数据集上实现了外观、几何和全景重建的最优性能。

Conclusion: OmniIndoor3D填补了室内3D重建的关键空白，为机器人导航提供了准确和鲁棒的解决方案。

Abstract: We propose a novel framework for comprehensive indoor 3D reconstruction using
Gaussian representations, called OmniIndoor3D. This framework enables accurate
appearance, geometry, and panoptic reconstruction of diverse indoor scenes
captured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized
for photorealistic rendering, it lacks the precise geometry critical for
high-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines
multiple RGB-D images to create a coarse 3D reconstruction, which is then used
to initialize the 3D Gaussians and guide the 3DGS training. To decouple the
optimization conflict between appearance and geometry, we introduce a
lightweight MLP that adjusts the geometric properties of 3D Gaussians. The
introduced lightweight MLP serves as a low-pass filter for geometry
reconstruction and significantly reduces noise in indoor scenes. To improve the
distribution of Gaussian primitives, we propose a densification strategy guided
by panoptic priors to encourage smoothness on planar surfaces. Through the
joint optimization of appearance, geometry, and panoptic reconstruction,
OmniIndoor3D provides comprehensive 3D indoor scene understanding, which
facilitates accurate and robust robotic navigation. We perform thorough
evaluations across multiple datasets, and OmniIndoor3D achieves
state-of-the-art results in appearance, geometry, and panoptic reconstruction.
We believe our work bridges a critical gap in indoor 3D reconstruction. The
code will be released at: https://ucwxb.github.io/OmniIndoor3D/

</details>


### [83] [Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation](https://arxiv.org/abs/2505.20611)
*Zenghao Zheng,Lianping Yang,Jinshan Pan,Hegui Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的拓扑融合框架，通过骨骼感知模块和增强的卷积结构，解决了Mamba模型在3D人体姿态估计中的局限性，显著降低了计算成本并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: Transformer在3D人体姿态估计中因自注意力机制复杂度高而面临计算挑战，而Mamba模型虽能高效处理长序列，但对拓扑结构数据的处理能力不足。

Method: 设计了骨骼感知模块推断骨骼向量方向与长度，增强Mamba的卷积结构以捕捉局部关节关系，并引入时空细化模块建模时空关系。

Result: 在Human3.6M和MPI-INF-3DHP数据集上实验表明，该方法显著降低计算成本并提高准确性。

Conclusion: 通过融合骨骼拓扑结构，有效解决了Mamba模型的局限性，为3D人体姿态估计提供了高效且准确的解决方案。

Abstract: Transformer-based methods for 3-D human pose estimation face significant
computational challenges due to the quadratic growth of self-attention
mechanism complexity with sequence length. Recently, the Mamba model has
substantially reduced computational overhead and demonstrated outstanding
performance in modeling long sequences by leveraging state space model (SSM).
However, the ability of SSM to process sequential data is not suitable for 3-D
joint sequences with topological structures, and the causal convolution
structure in Mamba also lacks insight into local joint relationships. To
address these issues, we propose the Mamba-Driven Topology Fusion framework in
this paper. Specifically, the proposed Bone Aware Module infers the direction
and length of bone vectors in the spherical coordinate system, providing
effective topological guidance for the Mamba model in processing joint
sequences. Furthermore, we enhance the convolutional structure within the Mamba
model by integrating forward and backward graph convolutional network, enabling
it to better capture local joint dependencies. Finally, we design a
Spatiotemporal Refinement Module to model both temporal and spatial
relationships within the sequence. Through the incorporation of skeletal
topology, our approach effectively alleviates Mamba's limitations in capturing
human structural relationships. We conduct extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results
show that the proposed method greatly reduces computational cost while
achieving higher accuracy. Ablation studies further demonstrate the
effectiveness of each proposed module. The code and models will be released.

</details>


### [84] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
*Peter Robicheaux,Matvei Popov,Anish Madan,Isaac Robinson,Joseph Nelson,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 论文提出通过多模态指令对齐提升视觉语言模型（VLMs）在新概念上的泛化能力，并发布了Roboflow100-VL数据集用于评估。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在常见对象上表现良好，但在分布外类别和任务上泛化能力不足，需改进对齐方法。

Method: 提出用少量视觉示例和丰富文本描述对齐VLMs，并构建Roboflow100-VL数据集进行多场景评估。

Result: 实验显示GroundingDINO等模型在医学影像数据集上零样本准确率低于2%，需少样本对齐。

Conclusion: 少样本概念对齐是提升VLMs泛化能力的关键，Roboflow100-VL为未来研究提供了基准。

Abstract: Vision-language models (VLMs) trained on internet-scale data achieve
remarkable zero-shot detection performance on common objects like car, truck,
and pedestrian. However, state-of-the-art models still struggle to generalize
to out-of-distribution classes, tasks and imaging modalities not typically
found in their pre-training. Rather than simply re-training VLMs on more visual
data, we argue that one should align VLMs to new concepts with annotation
instructions containing a few visual examples and rich textual descriptions. To
this end, we introduce Roboflow100-VL, a large-scale collection of 100
multi-modal object detection datasets with diverse concepts not commonly found
in VLM pre-training. We evaluate state-of-the-art models on our benchmark in
zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing
for comparison across data regimes. Notably, we find that VLMs like
GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on
challenging medical imaging datasets within Roboflow100-VL, demonstrating the
need for few-shot concept alignment. Our code and dataset are available at
https://github.com/roboflow/rf100-vl/ and
https://universe.roboflow.com/rf100-vl/

</details>


### [85] [Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea](https://arxiv.org/abs/2505.20615)
*Omid Halimi Milani,Ahmet Enis Cetin,Bharati Prasad*

Main category: cs.CV

TL;DR: 该研究提出了一种结合离散余弦变换（DCT）和迁移学习的深度学习方法，用于预测阻塞性睡眠呼吸暂停（OSA）患者在五年内发展为高血压的风险，取得了72.88%的AUC。


<details>
  <summary>Details</summary>
Motivation: OSA是高血压的重要风险因素，但预测其五年内发展为高血压仍具挑战性。研究旨在利用多导睡眠图信号和深度学习提高预测准确性。

Method: 通过DCT将多导睡眠图信号转换为频域表示，结合预训练的2D神经网络（如MobileNet、EfficientNet和ResNet），并引入DCT层以增强特征学习和抗噪能力。

Result: 模型在EfficientNet中深度截断处放置DCT层，取得了72.88%的最佳AUC。

Conclusion: 频域特征提取和迁移学习的结合对有限医疗数据集的高血压风险预测有效。

Abstract: Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,
primarily due to intermittent hypoxia and sleep fragmentation. Predicting
whether individuals with OSA will develop hypertension within five years
remains a complex challenge. This study introduces a novel deep learning
approach that integrates Discrete Cosine Transform (DCT)-based transfer
learning to enhance prediction accuracy. We are the first to incorporate all
polysomnography signals together for hypertension prediction, leveraging their
collective information to improve model performance. Features were extracted
from these signals and transformed into a 2D representation to utilize
pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet
variants. To further improve feature learning, we introduced a DCT layer, which
transforms input features into a frequency-based representation, preserving
essential spectral information, decorrelating features, and enhancing
robustness to noise. This frequency-domain approach, coupled with transfer
learning, is especially beneficial for limited medical datasets, as it
leverages rich representations from pre-trained networks to improve
generalization. By strategically placing the DCT layer at deeper truncation
depths within EfficientNet, our model achieved a best area under the curve
(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature
extraction and transfer learning in predicting hypertension risk in OSA
patients over a five-year period.

</details>


### [86] [OccLE: Label-Efficient 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.20617)
*Naiyu Fang,Zheyuan Zhou,Fayao Liu,Xulei Yang,Jiacheng Wei,Lemiao Qiu,Guosheng Lin*

Main category: cs.CV

TL;DR: OccLE是一种标签高效的3D语义占用预测方法，通过解耦语义和几何学习任务，结合2D基础模型和半监督学习，仅需10%的体素标注即可达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的全监督或性能有限的自监督，OccLE旨在通过标签高效的方式解决这一问题。

Method: 解耦语义和几何学习任务，利用2D基础模型生成伪标签，结合图像和LiDAR输入进行半监督几何学习，通过Dual Mamba融合特征网格。

Result: 在仅10%体素标注的情况下，OccLE在SemanticKITTI验证集上达到16.59%的mIoU。

Conclusion: OccLE提供了一种高效且性能优越的3D语义占用预测解决方案。

Abstract: 3D semantic occupancy prediction offers an intuitive and efficient scene
understanding and has attracted significant interest in autonomous driving
perception. Existing approaches either rely on full supervision, which demands
costly voxel-level annotations, or on self-supervision, which provides limited
guidance and yields suboptimal performance. To address these challenges, we
propose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes
images and LiDAR as inputs and maintains high performance with limited voxel
annotations. Our intuition is to decouple the semantic and geometric learning
tasks and then fuse the learned feature grids from both tasks for the final
semantic occupancy prediction. Therefore, the semantic branch distills 2D
foundation model to provide aligned pseudo labels for 2D and 3D semantic
learning. The geometric branch integrates image and LiDAR inputs in cross-plane
synergy based on their inherency, employing semi-supervision to enhance
geometry learning. We fuse semantic-geometric feature grids through Dual Mamba
and incorporate a scatter-accumulated projection to supervise unannotated
prediction with aligned pseudo labels. Experiments show that OccLE achieves
competitive performance with only 10% of voxel annotations, reaching a mIoU of
16.59% on the SemanticKITTI validation set.

</details>


### [87] [ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation](https://arxiv.org/abs/2505.20626)
*Yohai Mazuz,Janna Bruner,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过操纵注意力矩阵实现文本对齐和主题一致性，同时保持风格多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持主题一致性和风格多样性之间存在矛盾，且依赖大规模微调或优化，难以泛化。

Method: 通过从锚定图像获取Queries和Keys，从非锚定图像获取Values，并在自注意力机制中添加跨图像组件，同时对齐Value矩阵的统计量。

Result: 实验表明，该方法有效解耦风格与主题外观，生成多样风格下文本对齐且主题一致的图像。

Conclusion: 该方法无需训练，成功解决了风格与主题一致性的矛盾，具有广泛适用性。

Abstract: In text-to-image models, consistent character generation is the task of
achieving text alignment while maintaining the subject's appearance across
different prompts. However, since style and appearance are often entangled, the
existing methods struggle to preserve consistent subject characteristics while
adhering to varying style prompts. Current approaches for consistent
text-to-image generation typically rely on large-scale fine-tuning on curated
image sets or per-subject optimization, which either fail to generalize across
prompts or do not align well with textual descriptions. Meanwhile,
training-free methods often fail to maintain subject consistency across
different styles. In this work, we introduce a training-free method that
achieves both style alignment and subject consistency. The attention matrices
are manipulated such that Queries and Keys are obtained from the anchor
image(s) that are used to define the subject, while the Values are imported
from a parallel copy that is not subject-anchored. Additionally, cross-image
components are added to the self-attention mechanism by expanding the Key and
Value matrices. To do without shifting from the target style, we align the
statistics of the Value matrices. As is demonstrated in a comprehensive battery
of qualitative and quantitative experiments, our method effectively decouples
style from subject appearance and enables faithful generation of text-aligned
images with consistent characters across diverse styles.

</details>


### [88] [Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training](https://arxiv.org/abs/2505.20629)
*Bolin Lai,Sangmin Lee,Xu Cao,Xiang Li,James M. Rehg*

Main category: cs.CV

TL;DR: FlexTI2V是一种无需训练的文本-图像到视频生成方法，通过灵活的视觉条件处理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在资源消耗和视觉条件灵活性上的局限性。

Method: 采用随机块交换策略和动态控制机制，将视觉特征融入视频生成过程。

Result: 实验表明，FlexTI2V在性能上显著优于其他无需训练的方法。

Conclusion: FlexTI2V为可控视频生成提供了高效且灵活的解决方案。

Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable
video generation using both semantic and visual conditions. Most existing
methods typically add visual conditions to text-to-video (T2V) foundation
models by finetuning, which is costly in resources and only limited to a few
predefined conditioning settings. To tackle this issue, we introduce a unified
formulation for TI2V generation with flexible visual conditioning. Furthermore,
we propose an innovative training-free approach, dubbed FlexTI2V, that can
condition T2V foundation models on an arbitrary amount of images at arbitrary
positions. Specifically, we firstly invert the condition images to noisy
representation in a latent space. Then, in the denoising process of T2V models,
our method uses a novel random patch swapping strategy to incorporate visual
features into video representations through local image patches. To balance
creativity and fidelity, we use a dynamic control mechanism to adjust the
strength of visual conditioning to each video frame. Extensive experiments
validate that our method surpasses previous training-free image conditioning
methods by a notable margin. We also show more insights of our method by
detailed ablation study and analysis.

</details>


### [89] [TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone](https://arxiv.org/abs/2505.20637)
*Ana M. Cabanas,Alma Pedro,Domingo Mery*

Main category: cs.CV

TL;DR: 研究比较了两种肤色分类方法（ITA和基于$L^*$-$H^*$的方法）在面部情感分析系统中的公平性表现，发现肤色较深的群体代表性不足且存在公平性差异，$L^*$-$H^*$方法更稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨肤色分类方法对公平性评估的影响，尤其是肤色较深群体的代表性不足问题。

Method: 比较ITA和$L^*$-$H^*$方法，使用AffectNet和MobileNet模型评估公平性，并通过Grad-CAM分析模型注意力模式。

Result: 肤色较深群体代表性不足（约2%），公平性指标（F1-score和TPR）存在显著差异，$L^*$-$H^*$方法更稳定。

Conclusion: 肤色测量方法的选择对公平性评估至关重要，ITA可能低估肤色较深群体的差异，建议采用$L^*$-$H^*$方法。

Abstract: Understanding how facial affect analysis (FAA) systems perform across
different demographic groups requires reliable measurement of sensitive
attributes such as ancestry, often approximated by skin tone, which itself is
highly influenced by lighting conditions. This study compares two objective
skin tone classification methods: the widely used Individual Typology Angle
(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and
Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness
across skin tone groups defined by each method. Results reveal a severe
underrepresentation of dark skin tones ($\sim 2 \%$), alongside fairness
disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While
ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$
method yields more consistent subgrouping and enables clearer diagnostics
through metrics such as Equal Opportunity. Grad-CAM analysis further highlights
differences in model attention patterns by skin tone, suggesting variation in
feature encoding. To support future mitigation efforts, we also propose a
modular fairness-aware pipeline that integrates perceptual skin tone
estimation, model interpretability, and fairness evaluation. These findings
emphasize the relevance of skin tone measurement choices in fairness assessment
and suggest that ITA-based evaluations may overlook disparities affecting
darker-skinned individuals.

</details>


### [90] [Open-Det: An Efficient Learning Framework for Open-Ended Detection](https://arxiv.org/abs/2505.20639)
*Guiping Cao,Tao Wang,Wenjian Huang,Xiangyuan Lan,Jianguo Zhang,Dongmei Jiang*

Main category: cs.CV

TL;DR: Open-Det是一种高效的开集目标检测框架，通过重构检测器和名称生成器、引入视觉-语言对齐机制和联合损失函数，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有开集目标检测模型（如GenerateU）需要大规模数据集训练、收敛慢且性能有限，Open-Det旨在解决这些问题。

Method: Open-Det包含重构的检测器和名称生成器、视觉-语言对齐机制（V-to-L和L-to-V）、提示蒸馏器和掩码对齐损失，以及联合损失函数。

Result: Open-Det仅需1.5%的训练数据、20.8%的训练epoch和更少的GPU资源，性能比GenerateU提升1.0%（APr）。

Conclusion: Open-Det通过高效设计和优化，显著提升了开集目标检测的训练效率和性能。

Abstract: Open-Ended object Detection (OED) is a novel and challenging task that
detects objects and generates their category names in a free-form manner,
without requiring additional vocabularies during inference. However, the
existing OED models, such as GenerateU, require large-scale datasets for
training, suffer from slow convergence, and exhibit limited performance. To
address these issues, we present a novel and efficient Open-Det framework,
consisting of four collaborative parts. Specifically, Open-Det accelerates
model training in both the bounding box and object name generation process by
reconstructing the Object Detector and the Object Name Generator. To bridge the
semantic gap between Vision and Language modalities, we propose a
Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms,
incorporating with the Prompts Distiller to transfer knowledge from the VLM
into VL-prompts, enabling accurate object name generation for the LLM. In
addition, we design a Masked Alignment Loss to eliminate contradictory
supervision and introduce a Joint Loss to enhance classification, resulting in
more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of
the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs.
149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher
performance (+1.0% in APr). The source codes are available at:
https://github.com/Med-Process/Open-Det.

</details>


### [91] [IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios](https://arxiv.org/abs/2505.20640)
*Yifan Li,Yuhang Chen,Anh Dao,Lichi Li,Zhongyi Cai,Zhen Tan,Tianlong Chen,Yu Kong*

Main category: cs.CV

TL;DR: IndustryEQA是首个专注于工业仓库安全场景的EQA基准测试，提供高保真视频和丰富标注，旨在推动更安全、实用的EQA研究。


<details>
  <summary>Details</summary>
Motivation: 现有EQA基准主要关注家庭环境，忽略了工业场景的安全性和推理需求，限制了代理在工业应用中的评估。

Method: 基于NVIDIA Isaac Sim平台构建，包含高保真视频、多样化工业资产和危险场景，提供六类标注和额外推理评估。

Result: 包含1344个问答对（971来自小型仓库，373来自大型仓库），并提出综合评估框架和基线模型。

Conclusion: IndustryEQA旨在推动EQA研究向更安全、实用的工业场景发展，基准和代码已开源。

Abstract: Existing Embodied Question Answering (EQA) benchmarks primarily focus on
household environments, often overlooking safety-critical aspects and reasoning
processes pertinent to industrial settings. This drawback limits the evaluation
of agent readiness for real-world industrial applications. To bridge this, we
introduce IndustryEQA, the first benchmark dedicated to evaluating embodied
agent capabilities within safety-critical warehouse scenarios. Built upon the
NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory
videos featuring diverse industrial assets, dynamic human agents, and carefully
designed hazardous situations inspired by real-world safety guidelines. The
benchmark includes rich annotations covering six categories: equipment safety,
human safety, object recognition, attribute recognition, temporal
understanding, and spatial understanding. Besides, it also provides extra
reasoning evaluation based on these categories. Specifically, it comprises 971
question-answer pairs generated from small warehouse and 373 pairs from large
ones, incorporating scenarios with and without human. We further propose a
comprehensive evaluation framework, including various baseline models, to
assess their general perception and reasoning abilities in industrial
environments. IndustryEQA aims to steer EQA research towards developing more
robust, safety-aware, and practically applicable embodied agents for complex
industrial environments. Benchmark and codes are available.

</details>


### [92] [See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction](https://arxiv.org/abs/2505.20641)
*Yuan Wu,Zhiqiang Yan,Yigong Zhang,Xiang Li,ian Yang*

Main category: cs.CV

TL;DR: LIAR是一个新框架，通过学习光照相关表示来解决夜间场景下的3D空间占用预测问题，通过选择性低光增强和光照感知组件提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法在白天表现良好，但在夜间因能见度低和光照条件复杂而表现不佳，LIAR旨在解决这一问题。

Method: LIAR引入选择性低光增强（SLLIE）和两个光照感知组件（2D-IGS和3D-IDP），分别处理局部曝光不足和过度曝光。

Result: 在真实和合成数据集上的实验表明，LIAR在夜间场景下表现优越。

Conclusion: LIAR通过光照相关表示显著提升了夜间3D占用预测的性能，代码和模型已开源。

Abstract: Occupancy prediction aims to estimate the 3D spatial distribution of occupied
regions along with their corresponding semantic labels. Existing vision-based
methods perform well on daytime benchmarks but struggle in nighttime scenarios
due to limited visibility and challenging lighting conditions. To address these
challenges, we propose \textbf{LIAR}, a novel framework that learns
illumination-affined representations. LIAR first introduces Selective Low-light
Image Enhancement (SLLIE), which leverages the illumination priors from daytime
scenes to adaptively determine whether a nighttime image is genuinely dark or
sufficiently well-lit, enabling more targeted global enhancement. Building on
the illumination maps generated by SLLIE, LIAR further incorporates two
illumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D
Illumination-driven Projection (3D-IDP), to respectively tackle local
underexposure and overexposure. Specifically, 2D-IGS modulates feature sampling
positions according to illumination maps, assigning larger offsets to darker
regions and smaller ones to brighter regions, thereby alleviating feature
degradation in underexposed areas. Subsequently, 3D-IDP enhances semantic
understanding in overexposed regions by constructing illumination intensity
fields and supplying refined residual queries to the BEV context refinement
process. Extensive experiments on both real and synthetic datasets demonstrate
the superior performance of LIAR under challenging nighttime scenarios. The
source code and pretrained models are available
\href{https://github.com/yanzq95/LIAR}{here}.

</details>


### [93] [HCQA-1.5 @ Ego4D EgoSchema Challenge 2025](https://arxiv.org/abs/2505.20644)
*Haoyu Zhang,Yisen Feng,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种改进的HCQA框架，通过多源聚合策略和置信度过滤机制提升视频问答的可靠性，最终在EgoSchema挑战赛中取得第三名。


<details>
  <summary>Details</summary>
Motivation: 提升第一人称视角视频问答中答案预测的可靠性。

Method: 扩展HCQA框架，引入多源聚合策略生成多样化预测，结合置信度过滤机制选择高置信度答案，并对低置信度情况采用细粒度推理模块进行优化。

Result: 在EgoSchema盲测集上达到77%准确率，优于去年的获胜方案和多数参赛团队。

Conclusion: 该方法显著提升了视频问答的准确性，代码已开源。

Abstract: In this report, we present the method that achieves third place for Ego4D
EgoSchema Challenge in CVPR 2025. To improve the reliability of answer
prediction in egocentric video question answering, we propose an effective
extension to the previously proposed HCQA framework. Our approach introduces a
multi-source aggregation strategy to generate diverse predictions, followed by
a confidence-based filtering mechanism that selects high-confidence answers
directly. For low-confidence cases, we incorporate a fine-grained reasoning
module that performs additional visual and contextual analysis to refine the
predictions. Evaluated on the EgoSchema blind test set, our method achieves 77%
accuracy on over 5,000 human-curated multiple-choice questions, outperforming
last year's winning solution and the majority of participating teams. Our code
will be added at https://github.com/Hyu-Zhang/HCQA.

</details>


### [94] [Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design](https://arxiv.org/abs/2505.20649)
*HsiaoYuan Hsu,Yuxin Peng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Scan-and-Print的补丁级数据总结与增强方法，用于解决AI海报设计中内容感知布局生成的高参数需求问题，显著提升了实时性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在感知背景图像时需要高参数数量，远超可用训练数据规模，导致模型实时性能和泛化能力受限。

Method: Scan-and-Print方法包括扫描（选择适合放置元素顶点的补丁）和打印（混合补丁和顶点以合成新样本），并引入基于顶点的布局表示。

Result: 实验表明，Scan-and-Print能生成视觉吸引的布局，质量达到SOTA，同时计算瓶颈减少95.2%。

Conclusion: Scan-and-Print方法有效解决了高参数需求问题，显著提升了布局生成的效率和效果。

Abstract: In AI-empowered poster design, content-aware layout generation is crucial for
the on-image arrangement of visual-textual elements, e.g., logo, text, and
underlay. To perceive the background images, existing work demanded a high
parameter count that far exceeds the size of available training data, which has
impeded the model's real-time performance and generalization ability. To
address these challenges, we proposed a patch-level data summarization and
augmentation approach, vividly named Scan-and-Print. Specifically, the scan
procedure selects only the patches suitable for placing element vertices to
perform fine-grained perception efficiently. Then, the print procedure mixes up
the patches and vertices across two image-layout pairs to synthesize over 100%
new samples in each epoch while preserving their plausibility. Besides, to
facilitate the vertex-level operations, a vertex-based layout representation is
introduced. Extensive experimental results on widely used benchmarks
demonstrated that Scan-and-Print can generate visually appealing layouts with
state-of-the-art quality while dramatically reducing computational bottleneck
by 95.2%.

</details>


### [95] [RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment](https://arxiv.org/abs/2505.20653)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Main category: cs.CV

TL;DR: 提出一种新的学习目标，通过梯度对齐增强深度伪造检测模型的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过额外模块防止过拟合，但可能阻碍经验风险最小化（ERM）目标的优化。

Method: 通过扰动模型参数对齐跨域梯度更新，保留域不变特征。

Result: 在多个深度伪造检测数据集上表现优于现有域泛化技术。

Conclusion: 梯度对齐策略有效提升模型跨域性能，且无需额外正则化。

Abstract: Recent advancements in domain generalization for deepfake detection have
attracted significant attention, with previous methods often incorporating
additional modules to prevent overfitting to domain-specific patterns. However,
such regularization can hinder the optimization of the empirical risk
minimization (ERM) objective, ultimately degrading model performance. In this
paper, we propose a novel learning objective that aligns generalization
gradient updates with ERM gradient updates. The key innovation is the
application of perturbations to model parameters, aligning the ascending points
across domains, which specifically enhances the robustness of deepfake
detection models to domain shifts. This approach effectively preserves
domain-invariant features while managing domain-specific characteristics,
without introducing additional regularization. Experimental results on multiple
challenging deepfake detection datasets demonstrate that our gradient alignment
strategy outperforms state-of-the-art domain generalization techniques,
confirming the efficacy of our method. The code is available at
https://github.com/Lynn0925/RoGA.

</details>


### [96] [Photography Perspective Composition: Towards Aesthetic Perspective Recommendation](https://arxiv.org/abs/2505.20655)
*Lujian Yao,Siming Zheng,Xinbin Yuan,Zhuoxuan Cai,Pu Wu,Jinwei Chen,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PPC的摄影视角构图方法，解决了传统2D裁剪方法的局限性，并通过自动化数据集构建、视频生成和视角质量评估模型实现了3D视角调整。


<details>
  <summary>Details</summary>
Motivation: 传统2D裁剪方法在场景主体排列不佳时效果有限，而专业摄影师常通过3D视角调整优化构图。本文旨在扩展这一艺术实践，解决数据稀缺和评估标准缺失的问题。

Method: 提出PPC方法，包括自动化构建数据集、生成视角变换视频和基于人类表现的视角质量评估模型。

Result: 实现了无需额外提示或相机轨迹的简洁方法，帮助普通用户提升构图技能。

Conclusion: PPC方法有效解决了传统构图的局限性，为摄影构图提供了新的视角调整工具。

Abstract: Traditional photography composition approaches are dominated by 2D
cropping-based methods. However, these methods fall short when scenes contain
poorly arranged subjects. Professional photographers often employ perspective
adjustment as a form of 3D recomposition, modifying the projected 2D
relationships between subjects while maintaining their actual spatial positions
to achieve better compositional balance. Inspired by this artistic practice, we
propose photography perspective composition (PPC), extending beyond traditional
cropping-based methods. However, implementing the PPC faces significant
challenges: the scarcity of perspective transformation datasets and undefined
assessment criteria for perspective quality. To address these challenges, we
present three key contributions: (1) An automated framework for building PPC
datasets through expert photographs. (2) A video generation approach that
demonstrates the transformation process from suboptimal to optimal
perspectives. (3) A perspective quality assessment (PQA) model constructed
based on human performance. Our approach is concise and requires no additional
prompt instructions or camera trajectories, helping and guiding ordinary users
to enhance their composition skills.

</details>


### [97] [DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving](https://arxiv.org/abs/2505.20665)
*Muxi Diao,Lele Yang,Hongbo Yin,Zhexu Wang,Yejie Wang,Daxin Tian,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: AutoDriveRL是一个统一的训练框架，将自动驾驶任务建模为四个核心任务的结构化推理过程，通过任务特定的奖励模型优化，训练出实时决策的VLM模型DriveRX。


<details>
  <summary>Details</summary>
Motivation: 传统端到端模型在复杂场景中泛化能力不足，现有视觉语言模型（VLMs）多依赖孤立模块和静态监督，无法支持多阶段决策。

Method: 将自动驾驶任务分解为四个核心任务，每个任务建模为视觉语言问答问题，并通过任务特定奖励模型优化。

Result: DriveRX在公共基准测试中表现优异，行为推理优于GPT-4o，且在复杂或损坏的驾驶条件下具有鲁棒性。

Conclusion: AutoDriveRL框架和DriveRX模型为自动驾驶研究提供了新的结构化推理方法，未来将开源支持进一步研究。

Abstract: Autonomous driving requires real-time, robust reasoning across perception,
prediction, planning, and behavior. However, conventional end-to-end models
fail to generalize in complex scenarios due to the lack of structured
reasoning. Recent vision-language models (VLMs) have been applied to driving
tasks, but they typically rely on isolated modules and static supervision,
limiting their ability to support multi-stage decision-making. We present
AutoDriveRL, a unified training framework that formulates autonomous driving as
a structured reasoning process over four core tasks. Each task is independently
modeled as a vision-language question-answering problem and optimized using
task-specific reward models, enabling fine-grained reinforcement signals at
different reasoning stages. Within this framework, we train DriveRX, a
cross-task reasoning VLM designed for real-time decision-making. DriveRX
achieves strong performance on a public benchmark, outperforming GPT-4o in
behavior reasoning and demonstrating robustness under complex or corrupted
driving conditions. Our analysis further highlights the impact of vision
encoder design and reward-guided reasoning compression. We will release the
AutoDriveRL framework and the DriveRX model to support future research.

</details>


### [98] [Contrastive Desensitization Learning for Cross Domain Face Forgery Detection](https://arxiv.org/abs/2505.20675)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Main category: cs.CV

TL;DR: 提出了一种新的跨域人脸伪造检测方法，通过对比去敏网络（CDN）降低误报率并提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸伪造检测方法在多域应用中误报率高，影响系统可用性。

Method: 基于对比去敏网络（CDN）和鲁棒去敏算法，通过学习真实人脸图像的域变换特征来捕获本质域特性。

Result: 在大规模基准数据集上的实验表明，该方法显著降低了误报率并提高了检测精度。

Conclusion: CDN方法在跨域人脸伪造检测中表现出色，具有理论上的鲁棒性和实际应用优势。

Abstract: In this paper, we propose a new cross-domain face forgery detection method
that is insensitive to different and possibly unseen forgery methods while
ensuring an acceptable low false positive rate. Although existing face forgery
detection methods are applicable to multiple domains to some degree, they often
come with a high false positive rate, which can greatly disrupt the usability
of the system. To address this issue, we propose an Contrastive Desensitization
Network (CDN) based on a robust desensitization algorithm, which captures the
essential domain characteristics through learning them from domain
transformation over pairs of genuine face images. One advantage of CDN lies in
that the learnt face representation is theoretical justified with regard to the
its robustness against the domain changes. Extensive experiments over
large-scale benchmark datasets demonstrate that our method achieves a much
lower false alarm rate with improved detection accuracy compared to several
state-of-the-art methods.

</details>


### [99] [Supervised Contrastive Learning for Ordinal Engagement Measurement](https://arxiv.org/abs/2505.20676)
*Sadaf Safa,Ali Abedi,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频的学生参与度测量方法，利用监督对比学习进行有序分类，解决了类别不平衡和顺序性问题，并在公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 学生参与度对教育项目至关重要，但传统方法难以解决类别不平衡和参与度顺序性问题。

Method: 从视频样本中提取情感和行为特征，结合监督对比学习框架和时序数据增强技术，训练有序分类器。

Result: 在DAiSEE数据集上验证了方法的有效性，能够稳健地分类学生参与度。

Conclusion: 该方法为虚拟学习环境中学生参与度的理解和提升提供了重要贡献。

Abstract: Student engagement plays a crucial role in the successful delivery of
educational programs. Automated engagement measurement helps instructors
monitor student participation, identify disengagement, and adapt their teaching
strategies to enhance learning outcomes effectively. This paper identifies two
key challenges in this problem: class imbalance and incorporating order into
engagement levels rather than treating it as mere categories. Then, a novel
approach to video-based student engagement measurement in virtual learning
environments is proposed that utilizes supervised contrastive learning for
ordinal classification of engagement. Various affective and behavioral features
are extracted from video samples and utilized to train ordinal classifiers
within a supervised contrastive learning framework (with a sequential
classifier as the encoder). A key step involves the application of diverse
time-series data augmentation techniques to these feature vectors, enhancing
model training. The effectiveness of the proposed method was evaluated using a
publicly available dataset for engagement measurement, DAiSEE, containing
videos of students who participated in virtual learning programs. The results
demonstrate the robust ability of the proposed method for the classification of
the engagement level. This approach promises a significant contribution to
understanding and enhancing student engagement in virtual learning
environments.

</details>


### [100] [Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors](https://arxiv.org/abs/2505.20680)
*Haodong Lu,Xinyu Zhang,Kristen Moore,Jason Xue,Lina Yao,Anton van den Hengel,Dong Gong*

Main category: cs.CV

TL;DR: 本文提出了一种基于增量提示调优的简洁持续学习方法（TPPT），通过文本原型引导视觉提示学习，充分利用CLIP的多模态结构，减少遗忘并提升新知识学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖复杂设计，未能充分利用CLIP的内在能力，本文旨在简化设计并提升性能。

Method: 提出TPPT方法，通过文本原型引导视觉提示学习（TPPT-V），并联合优化视觉和文本提示（TPPT-VT），引入关系多样性正则化防止嵌入空间崩溃。

Result: 实验表明TPPT能有效学习新知识并减少遗忘，优于现有方法。

Conclusion: TPPT通过利用CLIP的内在指导能力，为持续学习提供了一种简洁高效的解决方案。

Abstract: Continual learning (CL) enables deep networks to acquire new knowledge while
avoiding catastrophic forgetting. The powerful generalization ability of
pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training
(CLIP) model, has inspired a range of CL methods targeting new and specialized
tasks, providing rich multi-modal embeddings that support lightweight,
incremental prompt tuning. Existing methods often rely on complex designs built
upon specific assumptions, such as intricate regularization schemes for prompt
pools, specialized routing mechanisms, or multi-stage incrementations, that
introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's
intrinsic capabilities. In this paper, we propose a concise CL approach for
CLIP based on incremental prompt tuning that fully exploits its multi-modal
structure and the stability of textual representations. Our method, Textual
Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely
as static classifiers, as in existing methods, but as stable anchors to guide
the learning of visual prompts, thereby shaping the embedding space (i.e.,
TPPT-V). We show that our bidirectional supervision strategy enables more
effective learning of new knowledge while reducing forgetting. To further close
the vision-language gap during CL, we jointly optimizes visual and textual
prompts (i.e., TPPT-VT). We also introduce a relational diversity
regularization on the textual anchors to prevent embedding space collapse and
mitigate correlated forgetting. Extensive experiments and analyses demonstrate
the effectiveness of our proposed approach, highlighting the benefits of
leveraging CLIP's intrinsic guidance for continual adaptation.

</details>


### [101] [VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images](https://arxiv.org/abs/2505.20687)
*Mingxuan Sun,Juntao Jiang,Zhiqiang Yang,Shenao Kong,Jiamin Qi,Jianru Shang,Shuangling Luo,Wanfa Sun,Tianyi Wang,Yanqi Wang,Qixuan Wang,Tingjian Dai,Tianxiang Chen,Jinming Zhang,Xuerui Zhang,Yuepeng He,Pengcheng Fu,Qiu Guan,Shizheng Zhou,Yanbo Yu,Qigui Jiang,Teng Zhou,Liuyong Shi,Hong Yan*

Main category: cs.CV

TL;DR: 本文总结了2023年“Vision Meets Algae”挑战赛，旨在提升高通量微藻细胞检测技术，吸引了369支团队参与，提供了1000张图像数据集，并展示了前十名方法的关键成果。


<details>
  <summary>Details</summary>
Motivation: 微藻因其多样性和复杂条件难以检测，挑战赛旨在通过计算机视觉技术解决这一问题，促进生态研究和科技进步。

Method: 挑战赛提供了包含六类微藻的1000张图像数据集，任务包括小目标检测、运动模糊处理和复杂背景应对。

Result: 前十名团队的方法为解决微藻检测中的挑战提供了有效方案，显著提升了检测精度。

Conclusion: 该挑战赛为微藻研究和计算机视觉技术的结合提供了重要平台，推动了生态理解和科技发展。

Abstract: Microalgae, vital for ecological balance and economic sectors, present
challenges in detection due to their diverse sizes and conditions. This paper
summarizes the second "Vision Meets Algae" (VisAlgae 2023) Challenge, aiming to
enhance high-throughput microalgae cell detection. The challenge, which
attracted 369 participating teams, includes a dataset of 1000 images across six
classes, featuring microalgae of varying sizes and distinct features.
Participants faced tasks such as detecting small targets, handling motion blur,
and complex backgrounds. The top 10 methods, outlined here, offer insights into
overcoming these challenges and maximizing detection accuracy. This
intersection of algae research and computer vision offers promise for
ecological understanding and technological advancement. The dataset can be
accessed at: https://github.com/juntaoJianggavin/Visalgae2023/.

</details>


### [102] [Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets](https://arxiv.org/abs/2505.20694)
*Xulin Gu,Xinhao Zhong,Zhixing Wei,Yimin Zhou,Shuoyang Sun,Bin Chen,Hongpeng Wang,Yuan Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新的单级视频数据集蒸馏框架，通过优化合成视频以减少计算成本并保留时间动态。


<details>
  <summary>Details</summary>
Motivation: 视频数据的高维度和时间复杂性使得视频数据集蒸馏具有挑战性，现有方法计算成本高且难以保留时间动态。

Method: 提出了一种时间显著性引导的过滤机制，利用帧间差异指导蒸馏过程，保留信息性时间线索并抑制冗余。

Result: 在标准视频基准测试中，该方法实现了最先进的性能，缩小了真实与蒸馏视频数据之间的差距。

Conclusion: 该方法为视频数据集压缩提供了一种可扩展的解决方案。

Abstract: Dataset distillation (DD) has emerged as a powerful paradigm for dataset
compression, enabling the synthesis of compact surrogate datasets that
approximate the training utility of large-scale ones. While significant
progress has been achieved in distilling image datasets, extending DD to the
video domain remains challenging due to the high dimensionality and temporal
complexity inherent in video data. Existing video distillation (VD) methods
often suffer from excessive computational costs and struggle to preserve
temporal dynamics, as na\"ive extensions of image-based approaches typically
lead to degraded performance. In this paper, we propose a novel uni-level video
dataset distillation framework that directly optimizes synthetic videos with
respect to a pre-trained model. To address temporal redundancy and enhance
motion preservation, we introduce a temporal saliency-guided filtering
mechanism that leverages inter-frame differences to guide the distillation
process, encouraging the retention of informative temporal cues while
suppressing frame-level redundancy. Extensive experiments on standard video
benchmarks demonstrate that our method achieves state-of-the-art performance,
bridging the gap between real and distilled video data and offering a scalable
solution for video dataset compression.

</details>


### [103] [Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation](https://arxiv.org/abs/2505.20704)
*Zixuan Hu,Yichun Hu,Xiaotong Li,Shixiang Tang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 提出ReCAP方法，通过区域置信度优化解决WTTA中的噪声优化问题，显著提升适应效率。


<details>
  <summary>Details</summary>
Motivation: 现有WTTA方法主要关注样本选择策略，忽视了优化问题，尤其是熵最小化框架在噪声优化中的局限性。

Method: 提出ReCAP方法，包括概率区域建模方案和有限到无限的渐近逼近，将区域置信度转化为可处理的代理。

Result: ReCAP在多种数据集和场景中表现优于现有方法。

Conclusion: ReCAP通过区域集成方法显著提升了WTTA的适应效率，解决了传统方法的局限性。

Abstract: Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to
unseen domains under extreme data scarcity and multiple shifts. Previous
approaches mainly focused on sample selection strategies, while overlooking the
fundamental problem on underlying optimization. Initially, we critically
analyze the widely-adopted entropy minimization framework in WTTA and uncover
its significant limitations in noisy optimization dynamics that substantially
hinder adaptation efficiency. Through our analysis, we identify region
confidence as a superior alternative to traditional entropy, however, its
direct optimization remains computationally prohibitive for real-time
applications. In this paper, we introduce a novel region-integrated method
ReCAP that bypasses the lengthy process. Specifically, we propose a
probabilistic region modeling scheme that flexibly captures semantic changes in
embedding space. Subsequently, we develop a finite-to-infinite asymptotic
approximation that transforms the intractable region confidence into a
tractable and upper-bounded proxy. These innovations significantly unlock the
overlooked potential dynamics in local region in a concise solution. Our
extensive experiments demonstrate the consistent superiority of ReCAP over
existing methods across various datasets and wild scenarios.

</details>


### [104] [Hierarchical Instruction-aware Embodied Visual Tracking](https://arxiv.org/abs/2505.20710)
*Kui Wu,Hao Chen,Churan Wang,Fakhri Karray,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Main category: cs.CV

TL;DR: HIEVT提出了一种分层指令感知的视觉跟踪方法，通过空间目标桥接用户指令与动作生成，解决了现有模型在推理速度和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型（如LLMs、VLMs、VLAs）在用户为中心的视觉跟踪任务中面临推理速度慢或泛化性差的问题，需要一种更高效的解决方案。

Method: HIEVT通过LLM-based语义-空间目标对齐器将指令转化为空间目标，再通过RL-based自适应目标对齐策略生成动作。

Result: 在十百万轨迹的训练和多样环境测试中，HIEVT表现出强大的鲁棒性和泛化能力。

Conclusion: HIEVT成功解决了用户指令与低层动作之间的鸿沟，适用于复杂环境和动态目标。

Abstract: User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for
reinforcement learning-based models due to the substantial gap between
high-level user instructions and low-level agent actions. While recent
advancements in language models (e.g., LLMs, VLMs, VLAs) have improved
instruction comprehension, these models face critical limitations in either
inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To
address these challenges, we propose \textbf{Hierarchical Instruction-aware
Embodied Visual Tracking (HIEVT)} agent, which bridges instruction
comprehension and action generation using \textit{spatial goals} as
intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal
Aligner} to translate diverse human instructions into spatial goals that
directly annotate the desired spatial position. Then the \textit{RL-based
Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to
position the target as specified by the spatial goal. To benchmark UC-EVT
tasks, we collect over ten million trajectories for training and evaluate
across one seen environment and nine unseen challenging environments. Extensive
experiments and real-world deployments demonstrate the robustness and
generalizability of HIEVT across diverse environments, varying target dynamics,
and complex instruction combinations. The complete project is available at
https://sites.google.com/view/hievt.

</details>


### [105] [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
*Fuwen Luo,Shengfeng Lou,Chi Chen,Ziyue Wang,Chenliang Li,Weizhou Shen,Jiyue Guo,Peng Li,Ming Yan,Ji Zhang,Fei Huang,Yang Liu*

Main category: cs.CV

TL;DR: MUSEG是一种基于强化学习的方法，通过引入时间戳感知的多段接地技术，提升多模态大语言模型（MLLMs）在视频中的细粒度时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视频时间推理方面表现不足，现有强化学习方法效果有限，需要更有效的方法。

Method: 提出MUSEG方法，通过多段接地和时间戳感知技术，结合分阶段奖励的强化学习训练策略。

Result: 在时间接地和时间敏感视频问答任务中，MUSEG显著优于现有方法，且泛化能力强。

Conclusion: MUSEG通过改进的时间推理方法，为MLLMs在视频理解中的时间推理提供了有效解决方案。

Abstract: Video temporal understanding is crucial for multimodal large language models
(MLLMs) to reason over events in videos. Despite recent advances in general
video understanding, current MLLMs still struggle with fine-grained temporal
reasoning. While reinforcement learning (RL) has been explored to address this
issue recently, existing RL approaches remain limited in effectiveness. In this
work, we propose MUSEG, a novel RL-based method that enhances temporal
understanding by introducing timestamp-aware multi-segment grounding. MUSEG
enables MLLMs to align queries with multiple relevant video segments, promoting
more comprehensive temporal reasoning. To facilitate effective learning, we
design a customized RL training recipe with phased rewards that progressively
guides the model toward temporally grounded reasoning. Extensive experiments on
temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG
significantly outperforms existing methods and generalizes well across diverse
temporal understanding scenarios. View our project at
https://github.com/THUNLP-MT/MUSEG.

</details>


### [106] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models](https://arxiv.org/abs/2505.20718)
*Kui Wu,Shuhang Xu,Hao Chen,Churan Wang,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Main category: cs.CV

TL;DR: 提出一种结合视觉语言模型（VLM）的自改进框架，提升主动视觉跟踪（EVT）在失败恢复中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决当前主动视觉跟踪系统在跟踪失败后恢复能力不足的问题。

Method: 结合现成的主动跟踪方法和VLM的推理能力，快速视觉策略用于正常跟踪，失败时激活VLM推理，并引入记忆增强的自反思机制。

Result: 实验显示性能显著提升，成功率分别提高72%（RL方法）和220%（PID方法）。

Conclusion: 首次将VLM推理整合到EVT中，为动态非结构化环境中的机器人应用提供重要进展。

Abstract: We introduce a novel self-improving framework that enhances Embodied Visual
Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of
current active visual tracking systems in recovering from tracking failure. Our
approach combines the off-the-shelf active tracking methods with VLMs'
reasoning capabilities, deploying a fast visual policy for normal tracking and
activating VLM reasoning only upon failure detection. The framework features a
memory-augmented self-reflection mechanism that enables the VLM to
progressively improve by learning from past experiences, effectively addressing
VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate
significant performance improvements, with our framework boosting success rates
by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based
methods in challenging environments. This work represents the first integration
of VLM-based reasoning to assist EVT agents in proactive failure recovery,
offering substantial advances for real-world robotic applications that require
continuous target monitoring in dynamic, unstructured environments. Project
website: https://sites.google.com/view/evt-recovery-assistant.

</details>


### [107] [LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation](https://arxiv.org/abs/2505.20723)
*Pascal Zwick,Nils Friederich,Maximilian Beichter,Lennart Hilbert,Ralf Mikut,Oliver Bringmann*

Main category: cs.CV

TL;DR: LeDiFlow通过改进Flow Matching的先验分布，减少了ODE求解器的步骤，显著提升了图像生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）的迭代特性导致高质量图像生成效率低下，Flow Matching（FM）作为一种替代方法，但其基于高斯先验的路径对ODE求解器不友好。

Method: 提出LeDiFlow，通过辅助模型学习更适合的先验分布，初始化ODE求解器，从而减少求解步骤。结合SOTA Transformer架构和潜在空间采样。

Result: LeDiFlow在像素空间推理速度提升3.75倍，潜在空间模型图像质量提升1.32倍（CMMD指标）。

Conclusion: LeDiFlow显著优化了FM模型的效率和生成质量，适用于消费级工作站。

Abstract: Enhancing the efficiency of high-quality image generation using Diffusion
Models (DMs) is a significant challenge due to the iterative nature of the
process. Flow Matching (FM) is emerging as a powerful generative modeling
paradigm based on a simulation-free training objective instead of a score-based
one used in DMs. Typical FM approaches rely on a Gaussian distribution prior,
which induces curved, conditional probability paths between the prior and
target data distribution. These curved paths pose a challenge for the Ordinary
Differential Equation (ODE) solver, requiring a large number of inference calls
to the flow prediction network. To address this issue, we present Learned
Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for
training FM-based image generation models using a better-suited prior
distribution learned via a regression-based auxiliary model. By initializing
the ODE solver with a prior closer to the target data distribution, LeDiFlow
enables the learning of more computationally tractable probability paths. These
paths directly translate to fewer solver steps needed for high-quality image
generation at inference time. Our method utilizes a State-Of-The-Art (SOTA)
transformer architecture combined with latent space sampling and can be trained
on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably
outperforms the respective FM baselines. For instance, when operating directly
on pixels, our model accelerates inference by up to 3.75x compared to the
corresponding pixel-space baseline. Simultaneously, our latent FM model
enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy
(CMMD) metric against its respective baseline.

</details>


### [108] [Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2505.20729)
*Xiangyu Sun,Runnan Chen,Mingming Gong,Dong Xu,Tongliang Liu*

Main category: cs.CV

TL;DR: 论文提出Intern-GS方法，利用视觉基础模型增强稀疏视图高斯溅射，实现高质量场景重建。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图场景重建因数据有限导致信息不完整，现有方法效果不佳。

Method: Intern-GS利用视觉基础模型指导3D高斯溅射的初始化和优化过程，通过DUSt3R生成密集高斯点云，并在优化中预测深度和外观以补全缺失信息。

Result: 实验表明，Intern-GS在多个数据集（如LLFF、DTU等）上达到最先进的渲染质量。

Conclusion: Intern-GS通过结合视觉基础模型，有效解决了稀疏视图重建的挑战。

Abstract: Sparse-view scene reconstruction often faces significant challenges due to
the constraints imposed by limited observational data. These limitations result
in incomplete information, leading to suboptimal reconstructions using existing
methodologies. To address this, we present Intern-GS, a novel approach that
effectively leverages rich prior knowledge from vision foundation models to
enhance the process of sparse-view Gaussian Splatting, thereby enabling
high-quality scene reconstruction. Specifically, Intern-GS utilizes vision
foundation models to guide both the initialization and the optimization process
of 3D Gaussian splatting, effectively addressing the limitations of sparse
inputs. In the initialization process, our method employs DUSt3R to generate a
dense and non-redundant gaussian point cloud. This approach significantly
alleviates the limitations encountered by traditional structure-from-motion
(SfM) methods, which often struggle under sparse-view constraints. During the
optimization process, vision foundation models predict depth and appearance for
unobserved views, refining the 3D Gaussians to compensate for missing
information in unseen regions. Extensive experiments demonstrate that Intern-GS
achieves state-of-the-art rendering quality across diverse datasets, including
both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and
Temples.

</details>


### [109] [MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition](https://arxiv.org/abs/2505.20744)
*Hao Zhang,Zhan Zhuang,Xuehao Wang,Xiaodong Yang,Yu Zhang*

Main category: cs.CV

TL;DR: MoPFormer是一个自监督框架，通过将传感器信号转换为语义化的运动基元，并利用Transformer架构提升可解释性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴传感器在人类活动识别（HAR）中可解释性差和跨数据集泛化能力不足的问题。

Method: 分为两阶段：1）将传感器信号分割并量化为运动基元；2）通过上下文感知嵌入模块和Transformer编码器处理。

Result: 在六个HAR基准测试中表现优于现有方法，并显著提升跨数据集性能。

Conclusion: MoPFormer通过捕捉基础运动模式，提升了可解释性和泛化能力。

Abstract: Human Activity Recognition (HAR) with wearable sensors is challenged by
limited interpretability, which significantly impacts cross-dataset
generalization. To address this challenge, we propose Motion-Primitive
Transformer (MoPFormer), a novel self-supervised framework that enhances
interpretability by tokenizing inertial measurement unit signals into
semantically meaningful motion primitives and leverages a Transformer
architecture to learn rich temporal representations. MoPFormer comprises
two-stages. first stage is to partition multi-channel sensor streams into short
segments and quantizing them into discrete "motion primitive" codewords, while
the second stage enriches those tokenized sequences through a context-aware
embedding module and then processes them with a Transformer encoder. The
proposed MoPFormer can be pre-trained using a masked motion-modeling objective
that reconstructs missing primitives, enabling it to develop robust
representations across diverse sensor configurations. Experiments on six HAR
benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art
methods but also successfully generalizes across multiple datasets. Most
importantly, the learned motion primitives significantly enhance both
interpretability and cross-dataset performance by capturing fundamental
movement patterns that remain consistent across similar activities regardless
of dataset origin.

</details>


### [110] [MVTN: Learning Multi-View Transformations for 3D Understanding](https://arxiv.org/abs/2212.13462)
*Abdullah Hamdi,Faisal AlZahrani,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: 论文提出了一种名为MVTN的多视图变换网络，通过学习最优视角点来改进3D形状识别，实现了端到端训练，并在多个基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图技术在3D形状识别中表现优异，但其视角点是固定的，限制了性能。为了解决这一问题，作者提出通过学习视角点来优化识别效果。

Method: 引入了MVTN，利用可微分渲染确定最优视角点，并将其集成到一个自适应多视图流程中，支持3D网格和点云的渲染。

Result: 在ModelNet40、ScanObjectNN和ShapeNet Core55等基准测试中，MVTN实现了最先进的分类和形状检索性能，并对遮挡表现出更强的鲁棒性。

Conclusion: MVTN通过学习视角点显著提升了3D形状识别的性能，同时作者发布了MVTorch库以支持进一步研究。

Abstract: Multi-view projection techniques have shown themselves to be highly effective
in achieving top-performing results in the recognition of 3D shapes. These
methods involve learning how to combine information from multiple view-points.
However, the camera view-points from which these views are obtained are often
fixed for all shapes. To overcome the static nature of current multi-view
techniques, we propose learning these view-points. Specifically, we introduce
the Multi-View Transformation Network (MVTN), which uses differentiable
rendering to determine optimal view-points for 3D shape recognition. As a
result, MVTN can be trained end-to-end with any multi-view network for 3D shape
classification. We integrate MVTN into a novel adaptive multi-view pipeline
that is capable of rendering both 3D meshes and point clouds. Our approach
demonstrates state-of-the-art performance in 3D classification and shape
retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).
Further analysis indicates that our approach exhibits improved robustness to
occlusion compared to other methods. We also investigate additional aspects of
MVTN, such as 2D pretraining and its use for segmentation. To support further
research in this area, we have released MVTorch, a PyTorch library for 3D
understanding and generation using multi-view projections.

</details>


### [111] [Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models](https://arxiv.org/abs/2505.20753)
*Yufei Zhan,Hongyin Zhao,Yousong Zhu,Shurong Zheng,Fan Yang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种统一的视觉推理机制Griffon-R，通过模拟人类的理解-思考-回答过程，提升大型多模态模型（LMMs）在复杂组合推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在组合推理任务中表现不足，缺乏任务特定的高级能力，阻碍其成为真正通用的视觉模型。

Method: 引入人类式的理解-思考-回答流程，单次推理完成复杂任务，无需多次推断或外部工具。同时构建了334K视觉指令样本数据集。

Result: Griffon-R在VSR、CLEVR等复杂视觉推理基准上表现优异，同时在MMBench、ScienceQA等多模态任务中提升能力。

Conclusion: Griffon-R通过统一机制和丰富数据集，显著提升了LMMs的视觉推理能力，为通用视觉模型的发展提供了新方向。

Abstract: Large Multimodal Models (LMMs) have recently demonstrated remarkable visual
understanding performance on both vision-language and vision-centric tasks.
However, they often fall short in integrating advanced, task-specific
capabilities for compositional reasoning, which hinders their progress toward
truly competent general vision models. To address this, we present a unified
visual reasoning mechanism that enables LMMs to solve complicated compositional
problems by leveraging their intrinsic capabilities (e.g. grounding and visual
understanding capabilities). Different from the previous shortcut learning
mechanism, our approach introduces a human-like
understanding-thinking-answering process, allowing the model to complete all
steps in a single pass forwarding without the need for multiple inferences or
external tools. This design bridges the gap between foundational visual
capabilities and general question answering, encouraging LMMs to generate
faithful and traceable responses for complex visual reasoning. Meanwhile, we
curate 334K visual instruction samples covering both general scenes and
text-rich scenes and involving multiple foundational visual capabilities. Our
trained model, Griffon-R, has the ability of end-to-end automatic
understanding, self-thinking, and reasoning answers. Comprehensive experiments
show that Griffon-R not only achieves advancing performance on complex visual
reasoning benchmarks including VSR and CLEVR, but also enhances multimodal
capabilities across various benchmarks like MMBench and ScienceQA. Data,
models, and codes will be release at
https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.

</details>


### [112] [PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding](https://arxiv.org/abs/2505.20759)
*Ansel Blume,Jeonghwan Kim,Hyeonjeong Ha,Elen Chatikyan,Xiaomeng Jin,Khanh Duy Nguyen,Nanyun Peng,Kai-Wei Chang,Derek Hoiem,Heng Ji*

Main category: cs.CV

TL;DR: PARTONOMY是一个用于像素级部分定位的LMM基准测试，揭示了现有LMM在部分定位能力上的不足，并提出了改进模型PLUM。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物体由特定部分组成，但现有LMM难以完成部分定位任务，因此需要新的基准和改进方法。

Method: 通过构建PARTONOMY数据集（包含862个部分标签和534个物体标签），并提出PLUM模型（采用span tagging和反馈循环机制）。

Result: 实验显示现有LMM表现不佳（如LISA-13B仅5.9% gIoU），而PLUM在多个任务上优于现有模型。

Conclusion: PLUM为LMM的细粒度视觉理解提供了新方向，解决了部分定位的关键问题。

Abstract: Real-world objects are composed of distinctive, object-specific parts.
Identifying these parts is key to performing fine-grained, compositional
reasoning-yet, large multimodal models (LMMs) struggle to perform this
seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM
benchmark designed for pixel-level part grounding. We construct PARTONOMY from
existing part datasets and our own rigorously annotated set of images,
encompassing 862 part labels and 534 object labels for evaluation. Unlike
existing datasets that simply ask models to identify generic parts, PARTONOMY
uses specialized concepts (e.g., agricultural airplane), and challenges models
to compare objects' parts, consider part-whole relationships, and justify
textual predictions with visual segmentations. Our experiments demonstrate
significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only
5.9% gIoU), highlighting a critical gap in their part grounding abilities. We
note that existing segmentation-enabled LMMs (segmenting LMMs) have two key
architectural shortcomings: they use special [SEG] tokens not seen during
pretraining which induce distribution shift, and they discard predicted
segmentations instead of using past predictions to guide future ones. To
address these deficiencies, we train several part-centric LMMs and propose
PLUM, a novel segmenting LMM that uses span tagging instead of segmentation
tokens and that conditions on prior predictions in a feedback loop. We find
that pretrained PLUM outperforms existing segmenting LMMs on reasoning
segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM
finetuned on our proposed Explanatory Part Segmentation task is competitive
with segmenting LMMs trained on significantly more segmentation data. Our work
opens up new avenues towards enabling fine-grained, grounded visual
understanding in LMMs.

</details>


### [113] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/abs/2505.12703)
*Jiabin Chen,Haiping Wang,Jinpeng Li,Yuan Liu,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: SpatialLLM是一种无需训练或专家干预的统一语言模型，通过结构化场景描述直接处理复杂城市空间任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖地理分析工具或领域专家的问题，提供更灵活的城市空间智能分析方案。

Method: 通过构建详细的结构化场景描述，直接利用预训练LLM进行零样本空间任务分析。

Result: 实验表明，预训练LLM能准确感知空间分布信息，实现零样本高级空间任务（如城市规划、交通管理等）。

Conclusion: SpatialLLM为城市智能分析提供了新视角，多领域知识、上下文长度和推理能力是关键影响因素。

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>


### [114] [ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval](https://arxiv.org/abs/2505.20764)
*Eric Xing,Pranavi Kolouju,Robert Pless,Abby Stylianou,Nathan Jacobs*

Main category: cs.CV

TL;DR: ConText-CIR框架通过Text Concept-Consistency损失和合成数据生成管道，提升了组合图像检索任务的性能，并在多个基准数据集上实现了新的最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法在图像和文本修改的表示上表现不佳，导致性能不理想。

Method: 提出ConText-CIR框架，采用Text Concept-Consistency损失函数，并通过合成数据生成管道增强训练数据。

Result: 在CIRR和CIRCO等基准数据集上实现了监督和零样本设置下的最优性能。

Conclusion: ConText-CIR通过改进表示学习和数据生成，显著提升了组合图像检索任务的性能。

Abstract: Composed image retrieval (CIR) is the task of retrieving a target image
specified by a query image and a relative text that describes a semantic
modification to the query image. Existing methods in CIR struggle to accurately
represent the image and the text modification, resulting in subpar performance.
To address this limitation, we introduce a CIR framework, ConText-CIR, trained
with a Text Concept-Consistency loss that encourages the representations of
noun phrases in the text modification to better attend to the relevant parts of
the query image. To support training with this loss function, we also propose a
synthetic data generation pipeline that creates training data from existing CIR
datasets or unlabeled images. We show that these components together enable
stronger performance on CIR tasks, setting a new state-of-the-art in composed
image retrieval in both the supervised and zero-shot settings on multiple
benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,
and our new datasets are available at https://github.com/mvrl/ConText-CIR.

</details>


### [115] [MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning](https://arxiv.org/abs/2505.20772)
*Hongjia Liu,Rongzhen Zhao,Haohan Chen,Joni Pajarinen*

Main category: cs.CV

TL;DR: MetaSlot是一种改进的Slot Attention变体，通过动态调整对象数量、去重和噪声注入，提升了对象中心学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对象中心学习方法因固定数量的槽导致对象表示不完整，MetaSlot旨在解决这一问题。

Method: MetaSlot通过维护对象原型代码本、去重和噪声注入优化Slot Attention。

Result: 在多个数据集和任务中，MetaSlot显著提升了性能并生成更可解释的槽表示。

Conclusion: MetaSlot是一种通用且高效的Slot Attention改进方法，适用于现有对象中心学习架构。

Abstract: Learning object-level, structured representations is widely regarded as a key
to better generalization in vision and underpins the design of next-generation
Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)
methods adopt Slot Attention or its variants to iteratively aggregate objects'
super-pixels into a fixed set of query feature vectors, termed slots. However,
their reliance on a static slot count leads to an object being represented as
multiple parts when the number of objects varies. We introduce MetaSlot, a
plug-and-play Slot Attention variant that adapts to variable object counts.
MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset
by vector-quantizing the resulting slot representations; (ii) removes duplicate
slots from the traditionally aggregated slots by quantizing them with the
codebook; and (iii) injects progressively weaker noise into the Slot Attention
iterations to accelerate and stabilize the aggregation. MetaSlot is a general
Slot Attention variant that can be seamlessly integrated into existing OCL
architectures. Across multiple public datasets and tasks--including object
discovery and recognition--models equipped with MetaSlot achieve significant
performance gains and markedly interpretable slot representations, compared
with existing Slot Attention variants.

</details>


### [116] [TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs](https://arxiv.org/abs/2505.20777)
*Zhehan Kan,Yanlin Liu,Kun Yin,Xinghua Jiang,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Qingmin Liao,Wenming Yang*

Main category: cs.CV

TL;DR: TACO是一种新型强化学习算法，用于解决多模态推理中的问题，如推理与答案不一致、模型不稳定和数据效率低。通过引入Think-Answer Consistency、Rollback Resample Strategy和自适应学习计划，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理方法存在推理与答案不一致、模型不稳定和数据效率低的问题，需要一种更有效的解决方案。

Method: 提出TACO算法，结合GRPO，引入Think-Answer Consistency、Rollback Resample Strategy和自适应学习计划，优化推理和计算效率。

Result: 在REC和VQA任务中，TACO显著提升了性能，尤其在长链推理和数据效率方面表现突出。

Conclusion: TACO通过创新方法解决了多模态推理中的关键问题，为未来研究提供了新方向。

Abstract: DeepSeek R1 has significantly advanced complex reasoning for large language
models (LLMs). While recent methods have attempted to replicate R1's reasoning
capabilities in multimodal settings, they face limitations, including
inconsistencies between reasoning and final answers, model instability and
crashes during long-chain exploration, and low data learning efficiency. To
address these challenges, we propose TACO, a novel reinforcement learning
algorithm for visual reasoning. Building on Generalized Reinforcement Policy
Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly
couples reasoning with answer consistency to ensure answers are grounded in
thoughtful reasoning. We also introduce the Rollback Resample Strategy, which
adaptively removes problematic samples and reintroduces them to the sampler,
enabling stable long-chain exploration and future learning opportunities.
Additionally, TACO employs an adaptive learning schedule that focuses on
moderate difficulty samples to optimize data efficiency. Furthermore, we
propose the Test-Time-Resolution-Scaling scheme to address performance
degradation due to varying resolutions during reasoning while balancing
computational overhead. Extensive experiments on in-distribution and
out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning
LVLMs leads to significant performance improvements.

</details>


### [117] [Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks](https://arxiv.org/abs/2505.20782)
*Taïga Gonçalves,Tomo Miyazaki,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种跨域多目标攻击方法（CD-MTA），能生成误导图像分类器到任意目标类别的对抗样本，包括未见过的类别，解决了传统方法依赖训练数据和类嵌入的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统目标攻击方法每模型仅支持一个目标类别，需昂贵重训练；现有多目标攻击方法依赖训练数据和类嵌入，导致数据泄露和泛化能力差。

Method: CD-MTA采用基于图像的条件输入和类无关损失，消除对类语义的依赖，实现跨数据集和未见类别的泛化。

Result: 在ImageNet等八个数据集上，CD-MTA在标准和跨域场景中均优于现有方法，且无需访问黑盒模型的训练数据。

Conclusion: CD-MTA通过创新设计解决了多目标攻击的泛化和数据依赖问题，展现了在跨域场景中的优越性能。

Abstract: We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for
generating adversarial examples that mislead image classifiers toward any
target class, including those not seen during training. Traditional targeted
attacks are limited to one class per model, requiring expensive retraining for
each target. Multi-targeted attacks address this by introducing a perturbation
generator with a conditional input to specify the target class. However,
existing methods are constrained to classes observed during training and
require access to the black-box model's training data--introducing a form of
data leakage that undermines realistic evaluation in practical black-box
scenarios. We identify overreliance on class embeddings as a key limitation,
leading to overfitting and poor generalization to unseen classes. To address
this, CD-MTA replaces class-level supervision with an image-based conditional
input and introduces class-agnostic losses that align the perturbed and target
images in the feature space. This design removes dependence on class semantics,
thereby enabling generalization to unseen classes across datasets. Experiments
on ImageNet and seven other datasets show that CD-MTA outperforms prior
multi-targeted attacks in both standard and cross-domain settings--without
accessing the black-box model's training data.

</details>


### [118] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/abs/2505.20789)
*Yang Zheng,Wen Li,Zhaoqiang Liu*

Main category: cs.CV

TL;DR: 论文提出了两种新方法DMILO和DMILO-PGD，以解决扩散模型在逆问题中的计算负担和收敛问题。DMILO通过中间层优化减轻内存负担，DMILO-PGD结合投影梯度下降提升收敛性。实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统逆问题方法依赖手工先验，难以捕捉复杂数据。扩散模型虽表现优异，但存在计算负担和收敛问题。

Method: 提出DMILO（中间层优化）和DMILO-PGD（结合投影梯度下降），扩展扩散模型范围并优化收敛。

Result: 在多种图像数据集上验证，性能显著优于现有方法。

Conclusion: DMILO和DMILO-PGD有效解决了扩散模型在逆问题中的常见挑战。

Abstract: Inverse problems (IPs) involve reconstructing signals from noisy
observations. Traditional approaches often rely on handcrafted priors, which
can fail to capture the complexity of real-world data. The advent of
pre-trained generative models has introduced new paradigms, offering improved
reconstructions by learning rich priors from data. Among these, diffusion
models (DMs) have emerged as a powerful framework, achieving remarkable
reconstruction performance across numerous IPs. However, existing DM-based
methods frequently encounter issues such as heavy computational demands and
suboptimal convergence. In this work, building upon the idea of the recent work
DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approach under appropriate conditions and validate its superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.

</details>


### [119] [Rendering-Aware Reinforcement Learning for Vector Graphics Generation](https://arxiv.org/abs/2505.20793)
*Juan A. Rodriguez,Haotian Zhang,Abhay Puri,Aarash Feizi,Rishav Pramanik,Pascal Wichmann,Arnab Mondal,Mohammad Reza Samsami,Rabiul Awal,Perouz Taslakian,Spandana Gella,Sai Rajeswar,David Vazquez,Christopher Pal,Marco Pedersoli*

Main category: cs.CV

TL;DR: 论文提出RLRF方法，通过强化学习利用渲染反馈优化SVG生成，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有VLM方法在生成SVG时因未观察渲染图像而难以保证准确性和效率。

Method: 采用强化学习，通过比较渲染输出与原图计算奖励，优化SVG生成。

Result: RLRF显著优于监督微调，生成更准确、高效且语义连贯的SVG。

Conclusion: RLRF为高质量SVG生成提供了有效解决方案，具有强泛化能力。

Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing
visual designs as interpretable code. Recent advances in vision-language models
(VLMs) have enabled high-quality SVG generation by framing the problem as a
code generation task and leveraging large-scale pretraining. VLMs are
particularly suitable for this task as they capture both global semantics and
fine-grained visual patterns, while transferring knowledge across vision,
natural language, and code domains. However, existing VLM approaches often
struggle to produce faithful and efficient SVGs because they never observe the
rendered images during training. Although differentiable rendering for
autoregressive SVG code generation remains unavailable, rendered outputs can
still be compared to original inputs, enabling evaluative feedback suitable for
reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from
Rendering Feedback), an RL method that enhances SVG generation in
autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an
input image, the model generates SVG roll-outs that are rendered and compared
to the original image to compute a reward. This visual fidelity feedback guides
the model toward producing more accurate, efficient, and semantically coherent
SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common
failure modes and enabling precise, high-quality SVG generation with strong
structural understanding and generalization.

</details>


### [120] [Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis](https://arxiv.org/abs/2505.20808)
*Bo-Kai Ruan,Zi-Xiang Ni,Bo-Lun Huang,Teng-Fang Hsiao,Hong-Han Shuai*

Main category: cs.CV

TL;DR: RAP框架通过将罕见概念生成视为潜在因果路径，利用频繁提示近似罕见提示，动态切换提示并作为二阶去噪机制，显著提升了扩散模型在罕见概念生成上的表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成罕见概念时表现不佳，RAP旨在通过理论分析和动态提示切换解决这一问题。

Method: RAP将罕见概念生成视为潜在因果路径，利用频繁提示近似罕见提示，动态切换提示并作为二阶去噪机制。

Result: RAP在多种扩散模型上显著提升了罕见概念生成效果，优于基线方法。

Conclusion: RAP通过理论驱动的动态提示切换和二阶去噪机制，有效提升了扩散模型在罕见概念生成上的能力。

Abstract: Diffusion models have shown strong capabilities in high-fidelity image
generation but often falter when synthesizing rare concepts, i.e., prompts that
are infrequently observed in the training distribution. In this paper, we
introduce RAP, a principled framework that treats rare concept generation as
navigating a latent causal path: a progressive, model-aligned trajectory
through the generative space from frequent concepts to rare targets. Rather
than relying on heuristic prompt alternation, we theoretically justify that
rare prompt guidance can be approximated by semantically related frequent
prompts. We then formulate prompt switching as a dynamic process based on score
similarity, enabling adaptive stage transitions. Furthermore, we reinterpret
prompt alternation as a second-order denoising mechanism, promoting smooth
semantic progression and coherent visual synthesis. Through this causal lens,
we align input scheduling with the model's internal generative dynamics.
Experiments across diverse diffusion backbones demonstrate that RAP
consistently enhances rare concept generation, outperforming strong baselines
in both automated evaluations and human studies.

</details>


### [121] [Frame-Level Captions for Long Video Generation with Complex Multi Scenes](https://arxiv.org/abs/2505.20827)
*Guangcong Zheng,Jianlong Yuan,Bo Wang,Haoyang Huang,Guoqing Ma,Nan Duan*

Main category: cs.CV

TL;DR: 提出一种新方法，通过帧级标注和注意力机制解决长视频生成中的误差累积和多场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在生成长视频时存在误差累积和多场景适应性不足的问题。

Method: 提出帧级标注和Frame-Level Attention Mechanism，结合Diffusion Forcing训练。

Result: 在VBench 2.0基准测试中表现优异，能生成高质量长视频。

Conclusion: 新方法有效解决了复杂场景下的长视频生成问题，计划开源数据集和模型。

Abstract: Generating long videos that can show complex stories, like movie scenes from
scripts, has great promise and offers much more than short clips. However,
current methods that use autoregression with diffusion models often struggle
because their step-by-step process naturally leads to a serious error
accumulation (drift). Also, many existing ways to make long videos focus on
single, continuous scenes, making them less useful for stories with many events
and changes. This paper introduces a new approach to solve these problems.
First, we propose a novel way to annotate datasets at the frame-level,
providing detailed text guidance needed for making complex, multi-scene long
videos. This detailed guidance works with a Frame-Level Attention Mechanism to
make sure text and video match precisely. A key feature is that each part
(frame) within these windows can be guided by its own distinct text prompt. Our
training uses Diffusion Forcing to provide the model with the ability to handle
time flexibly. We tested our approach on difficult VBench 2.0 benchmarks
("Complex Plots" and "Complex Landscapes") based on the WanX2.1-T2V-1.3B model.
The results show our method is better at following instructions in complex,
changing scenes and creates high-quality long videos. We plan to share our
dataset annotation methods and trained models with the research community.
Project page: https://zgctroy.github.io/frame-level-captions .

</details>


### [122] [Causality-Driven Infrared and Visible Image Fusion](https://arxiv.org/abs/2505.20830)
*Linli Ma,Suzhen Lin,Jianchao Zeng,Zanxia Jin,Yanbo Wang,Fengyuan Li,Yubing Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果关系的图像融合方法，通过构建因果图消除数据集场景偏差的影响，并设计了BAFFM模块提升融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了数据集场景偏差对模型训练的影响，导致模型学习到虚假相关性，限制了融合性能。

Method: 从因果关系角度重新审视图像融合任务，构建因果图消除偏差影响，提出BAFFM模块消除混淆因素干扰。

Result: 在三个标准数据集上的实验表明，该方法显著优于现有红外和可见光图像融合方法。

Conclusion: 通过因果分析和BAFFM模块，有效提升了图像融合性能，解决了场景偏差问题。

Abstract: Image fusion aims to combine complementary information from multiple source
images to generate more comprehensive scene representations. Existing methods
primarily rely on the stacking and design of network architectures to enhance
the fusion performance, often ignoring the impact of dataset scene bias on
model training. This oversight leads the model to learn spurious correlations
between specific scenes and fusion weights under conventional likelihood
estimation framework, thereby limiting fusion performance. To solve the above
problems, this paper first re-examines the image fusion task from the causality
perspective, and disentangles the model from the impact of bias by constructing
a tailored causal graph to clarify the causalities among the variables in image
fusion task. Then, the Back-door Adjustment based Feature Fusion Module (BAFFM)
is proposed to eliminate confounder interference and enable the model to learn
the true causal effect. Finally, Extensive experiments on three standard
datasets prove that the proposed method significantly surpasses
state-of-the-art methods in infrared and visible image fusion.

</details>


### [123] [Fully Spiking Neural Networks for Unified Frame-Event Object Tracking](https://arxiv.org/abs/2505.20834)
*Jingjun Yang,Liangwei Fan,Jinpu Zhang,Xiangkai Lian,Hui Shen,Dewen Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SpikeFET的全脉冲框架-事件跟踪框架，通过结合卷积局部特征提取和基于Transformer的全局建模，高效融合图像和事件流数据，解决了现有方法计算开销大和事件流信息提取效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前融合图像和事件流的方法虽然性能高，但计算开销大且难以高效提取事件流的稀疏异步信息，未能充分利用事件驱动脉冲范式的能效优势。

Method: 提出SpikeFET框架，结合卷积局部特征提取和Transformer全局建模；引入随机拼图模块（RPM）消除卷积填充导致的平移不变性退化；提出时空正则化（STR）策略解决不对称特征导致的相似性度量退化。

Result: 在多个基准测试中，SpikeFET表现出优于现有方法的跟踪精度，同时显著降低功耗，实现了性能与效率的最佳平衡。

Conclusion: SpikeFET框架通过创新设计解决了现有方法的局限性，为高效、稳健的视觉目标跟踪提供了新思路。

Abstract: The integration of image and event streams offers a promising approach for
achieving robust visual object tracking in complex environments. However,
current fusion methods achieve high performance at the cost of significant
computational overhead and struggle to efficiently extract the sparse,
asynchronous information from event streams, failing to leverage the
energy-efficient advantages of event-driven spiking paradigms. To address this
challenge, we propose the first fully Spiking Frame-Event Tracking framework
called SpikeFET. This network achieves synergistic integration of convolutional
local feature extraction and Transformer-based global modeling within the
spiking paradigm, effectively fusing frame and event data. To overcome the
degradation of translation invariance caused by convolutional padding, we
introduce a Random Patchwork Module (RPM) that eliminates positional bias
through randomized spatial reorganization and learnable type encoding while
preserving residual structures. Furthermore, we propose a Spatial-Temporal
Regularization (STR) strategy that overcomes similarity metric degradation from
asymmetric features by enforcing spatio-temporal consistency among temporal
template features in latent space. Extensive experiments across multiple
benchmarks demonstrate that the proposed framework achieves superior tracking
accuracy over existing methods while significantly reducing power consumption,
attaining an optimal balance between performance and efficiency. The code will
be released.

</details>


### [124] [ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient](https://arxiv.org/abs/2505.20858)
*Jason Chui,Daniel Cremers*

Main category: cs.CV

TL;DR: ProBA是一种新的概率化Bundle Adjustment方法，通过建模和传播2D观测与3D场景结构的不确定性，无需相机位姿或焦距的先验知识即可优化。


<details>
  <summary>Details</summary>
Motivation: 传统BA方法需要准确的初始估计和已知相机内参，限制了其在不确定或未知情况下的适用性。

Method: 使用3D高斯模型替代点状地标，引入不确定性感知的重投影损失，并通过Bhattacharyya系数强制几何一致性。

Result: ProBA在具有挑战性的真实场景中优于传统方法，减少了对初始化和已知内参的依赖。

Conclusion: ProBA提升了SLAM系统在非结构化环境中的实用性。

Abstract: Classical Bundle Adjustment (BA) methods require accurate initial estimates
for convergence and typically assume known camera intrinsics, which limits
their applicability when such information is uncertain or unavailable. We
propose a novel probabilistic formulation of BA (ProBA) that explicitly models
and propagates uncertainty in both the 2D observations and the 3D scene
structure, enabling optimization without any prior knowledge of camera poses or
focal length. Our method uses 3D Gaussians instead of point-like landmarks and
we introduce uncertainty-aware reprojection losses by projecting the 3D
Gaussians onto the 2D image space, and enforce geometric consistency across
multiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap
between their corresponding Gaussian distributions. This probabilistic
framework leads to more robust and reliable optimization, even in the presence
of outliers in the correspondence set, reducing the likelihood of converging to
poor local minima. Experimental results show that \textit{ProBA} outperforms
traditional methods in challenging real-world conditions. By removing the need
for strong initialization and known intrinsics, ProBA enhances the practicality
of SLAM systems deployed in unstructured environments.

</details>


### [125] [Exploring Timeline Control for Facial Motion Generation](https://arxiv.org/abs/2505.20861)
*Yifeng Ma,Jinwei Qi,Chaonan Ji,Peng Zhang,Bang Zhang,Zhidong Deng,Liefeng Bo*

Main category: cs.CV

TL;DR: 论文提出了一种新的面部动作生成控制信号——时间线控制，相比音频和文本信号，时间线能提供更精细的控制。通过扩散模型和ChatGPT辅助，实现了高精度和自然的面部动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统音频和文本信号对面部动作生成的控制不够精细，时间线控制能提供更精确的动作和时序控制。

Method: 1. 使用Toeplitz逆协方差聚类标注面部动作时间区间；2. 提出基于扩散的生成模型；3. 利用ChatGPT将文本转换为时间线。

Result: 实验表明，方法能准确标注面部动作区间，并生成与时间线精确对齐的自然面部动作。

Conclusion: 时间线控制为面部动作生成提供了更精细和灵活的控制方式，结合扩散模型和文本转换工具，实现了高质量的结果。

Abstract: This paper introduces a new control signal for facial motion generation:
timeline control. Compared to audio and text signals, timelines provide more
fine-grained control, such as generating specific facial motions with precise
timing. Users can specify a multi-track timeline of facial actions arranged in
temporal intervals, allowing precise control over the timing of each action. To
model the timeline control capability, We first annotate the time intervals of
facial actions in natural facial motion sequences at a frame-level granularity.
This process is facilitated by Toeplitz Inverse Covariance-based Clustering to
minimize human labor. Based on the annotations, we propose a diffusion-based
generation model capable of generating facial motions that are natural and
accurately aligned with input timelines. Our method supports text-guided motion
generation by using ChatGPT to convert text into timelines. Experimental
results show that our method can annotate facial action intervals with
satisfactory accuracy, and produces natural facial motions accurately aligned
with timelines.

</details>


### [126] [AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding](https://arxiv.org/abs/2505.20862)
*Chaeyoung Jung,Youngjoon Jang,Joon Son Chung*

Main category: cs.CV

TL;DR: 提出了一种名为AVCD的新型解码框架，用于抑制多模态大语言模型中的幻觉问题，通过动态识别和掩蔽主导模态，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（AV-LLMs）中的幻觉问题复杂，涉及音频、视频和语言的跨模态交互，现有方法难以适应。

Method: 提出AVCD框架，利用注意力分布动态识别主导模态并掩蔽，同时引入熵引导的自适应解码以提高效率。

Result: 在AVHBench数据集上，AVCD显著提升了模型性能，VideoLLaMA2和video-SALMONN的准确率分别提高了6%和11%。

Conclusion: AVCD是一种无需训练的解码框架，能有效抑制多模态幻觉，具有强鲁棒性和泛化能力。

Abstract: Hallucination remains a major challenge in multimodal large language models
(MLLMs). To address this, various contrastive decoding (CD) methods have been
proposed that contrasts original logits with hallucinated logits generated from
perturbed inputs. While CD has shown promise in vision-language models (VLMs),
it is not well-suited for AV-LLMs, where hallucinations often emerge from both
unimodal and cross-modal combinations involving audio, video, and language.
These intricate interactions call for a more adaptive and modality-aware
decoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding
(AVCD)-a novel, training-free decoding framework designed to model trimodal
interactions and suppress modality-induced hallucinations in AV-LLMs. Unlike
previous CD methods in VLMs that corrupt a fixed modality, AVCD leverages
attention distributions to dynamically identify less dominant modalities and
applies attentive masking to generate perturbed output logits. To support CD in
a trimodal setting, we also reformulate the original CD framework to jointly
handle audio, visual, and textual inputs. Finally, to improve efficiency, we
introduce entropy-guided adaptive decoding, which selectively skips unnecessary
decoding steps based on the model's confidence in its predictions. Extensive
experiments demonstrate that AVCD consistently outperforms existing decoding
methods. Especially, on the AVHBench dataset, it improves accuracy by 6% for
VideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and
generalizability.

</details>


### [127] [In Context Learning with Vision Transformers: Case Study](https://arxiv.org/abs/2505.20872)
*Antony Zhao,Alex Proshkin,Fergal Hennessy,Francesco Crivelli*

Main category: cs.CV

TL;DR: 论文探讨了大型Transformer模型在图像空间中学习复杂函数（如卷积神经网络）的能力，扩展了其在上下文学习中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在图像空间中学习复杂函数的能力，以验证其在上下文学习中的潜力。

Method: 通过实验分析Transformer模型在图像空间中对复杂函数（如卷积神经网络）的上下文学习能力。

Result: 模型展示了在图像空间中学习复杂函数的潜力。

Conclusion: Transformer模型在图像空间中的上下文学习能力值得进一步研究。

Abstract: Large transformer models have been shown to be capable of performing
in-context learning. By using examples in a prompt as well as a query, they are
capable of performing tasks such as few-shot, one-shot, or zero-shot learning
to output the corresponding answer to this query. One area of interest to us is
that these transformer models have been shown to be capable of learning the
general class of certain functions, such as linear functions and small 2-layer
neural networks, on random data (Garg et al, 2023). We aim to extend this to
the image space to analyze their capability to in-context learn more complex
functions on the image space, such as convolutional neural networks and other
methods.

</details>


### [128] [Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models](https://arxiv.org/abs/2505.20873)
*Chaeyoung Jung,Youngjoon Jang,Jongmin Choi,Joon Son Chung*

Main category: cs.CV

TL;DR: 论文提出Fork-Merge Decoding（FMD）方法，通过推理阶段的分支-合并策略解决音频-视觉大语言模型（AV-LLMs）中的模态偏差问题，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 当前AV-LLMs中音频和视频特征联合处理可能导致模态偏差，即模型过度依赖某一模态。

Method: FMD在推理时先分别处理音频和视频输入（分支阶段），再合并隐藏状态进行联合推理（合并阶段）。

Result: 在VideoLLaMA2和video-SALMONN模型上，FMD在音频、视频及联合任务中均表现提升。

Conclusion: FMD通过推理阶段干预有效提升了多模态理解的鲁棒性。

Abstract: The goal of this work is to enhance balanced multimodal understanding in
audio-visual large language models (AV-LLMs) by addressing modality bias
without requiring additional training. In current AV-LLMs, audio and video
features are typically processed jointly in the decoder. While this strategy
facilitates unified multimodal understanding, it may introduce modality bias,
where the model tends to over-rely on one modality due to imbalanced training
signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet
effective inference-time strategy that requires no additional training or
architectural modifications. FMD first performs modality-specific reasoning by
processing audio-only and video-only inputs through the early decoder layers (a
fork phase), and then merges the resulting hidden states for joint reasoning in
the remaining layers (a merge phase). This approach promotes balanced modality
contributions and leverages complementary information across modalities. We
evaluate our method on two representative AV-LLMs, VideoLLaMA2 and
video-SALMONN, using three benchmark datasets. Experimental results demonstrate
consistent performance improvements on tasks focused on audio, video, and
combined audio-visual reasoning, demonstrating the effectiveness of
inference-time interventions for robust multimodal understanding.

</details>


### [129] [Stereo Radargrammetry Using Deep Learning from Airborne SAR Images](https://arxiv.org/abs/2505.20876)
*Tatsuya Sasayama,Shintaro Ito,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的立体雷达测量方法，用于处理机载SAR图像，通过数据集创建和微调，提升了图像质量和测量精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在SAR图像处理中缺乏公开数据集，且易受几何图像调制影响。

Method: 创建SAR图像数据集，微调深度学习图像对应方法，避免地面投影并分块处理图像。

Result: 实验表明，该方法比传统方法具有更广范围和更高精度的海拔测量能力。

Conclusion: 该方法有效解决了SAR图像处理中的问题，提升了测量性能。

Abstract: In this paper, we propose a stereo radargrammetry method using deep learning
from airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods
are considered to suffer less from geometric image modulation, while there is
no public SAR image dataset used to train such methods.We create a SAR image
dataset and perform fine-tuning of a deep learning-based image correspondence
method.The proposed method suppresses the degradation of image quality by pixel
interpolation without ground projection of the SAR image and divides the SAR
image into patches for processing, which makes it possible to apply deep
learning.Through a set of experiments, we demonstrate that the proposed method
exhibits a wider range and more accurate elevation measurements compared to
conventional methods.

</details>


### [130] [YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation](https://arxiv.org/abs/2505.20884)
*Weichao Pan,Bohan Xu,Xu Wang,Chengze Lv,Shuoyang Wang,Zhenke Duan*

Main category: cs.CV

TL;DR: 提出了一种名为YOLO-FireAD的新模型，通过注意力引导的倒置残差块和双池化下采样融合块，解决了动态环境中火灾检测的特征提取和信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的火灾检测面临光照变化干扰、误检或漏检等问题，现有YOLO模型在特征提取和信息保留方面存在局限。

Method: YOLO-FireAD引入两个核心创新：注意力引导的倒置残差块（AIR）和双池化下采样融合块（DPDF），分别用于增强火灾特征和保留多尺度模式。

Result: 在两个公开数据集上的评估显示，模型参数量减少（1.45M，比YOLOv8n低51.8%），计算量降低（4.6G，比YOLOv8n低43.2%），mAP75比主流实时检测模型高1.3-5.5%。

Conclusion: YOLO-FireAD在效率和准确性上均优于现有模型，适用于动态环境中的火灾检测。

Abstract: Fire detection in dynamic environments faces continuous challenges, including
the interference of illumination changes, many false detections or missed
detections, and it is difficult to achieve both efficiency and accuracy. To
address the problem of feature extraction limitation and information loss in
the existing YOLO-based models, this study propose You Only Look Once for Fire
Detection with Attention-guided Inverted Residual and Dual-pooling Downscale
Fusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted
Residual Block (AIR) integrates hybrid channel-spatial attention with inverted
residuals to adaptively enhance fire features and suppress environmental noise;
(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns
through learnable fusion of max-average pooling outputs, mitigating small-fire
detection failures. Extensive evaluation on two public datasets shows the
efficient performance of our model. Our proposed model keeps the sum amount of
parameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),
and mAP75 is higher than the mainstream real-time object detection models
YOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants
1.3-5.5%.

</details>


### [131] [Frequency Composition for Compressed and Domain-Adaptive Neural Networks](https://arxiv.org/abs/2505.20890)
*Yoojin Kwon,Hongjun Suh,Wooseok Lee,Taesik Gong,Songyi Han,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: CoDA框架通过频率组合统一了模型压缩和域适应，结合量化感知训练和测试时适应，显著提升了压缩模型在域转移下的性能。


<details>
  <summary>Details</summary>
Motivation: 现代设备上的神经网络应用需在资源受限且域转移不可预测的条件下运行，但现有工作分别处理压缩和域适应问题，未解决两者的结合挑战。

Method: CoDA框架在训练时使用低频分量进行量化感知训练，测试时利用全频信息进行源无关的域适应，将高频分量作为域特定线索。

Result: 在CIFAR10-C和ImageNet-C基准测试中，CoDA显著提升了压缩模型的准确性，分别比全精度基线提高了7.96%和5.37%。

Conclusion: CoDA成功统一了模型压缩和域适应，为资源受限设备上的神经网络应用提供了高效解决方案。

Abstract: Modern on-device neural network applications must operate under resource
constraints while adapting to unpredictable domain shifts. However, this
combined challenge-model compression and domain adaptation-remains largely
unaddressed, as prior work has tackled each issue in isolation: compressed
networks prioritize efficiency within a fixed domain, whereas large, capable
models focus on handling domain shifts. In this work, we propose CoDA, a
frequency composition-based framework that unifies compression and domain
adaptation. During training, CoDA employs quantization-aware training (QAT)
with low-frequency components, enabling a compressed model to selectively learn
robust, generalizable features. At test time, it refines the compact model in a
source-free manner (i.e., test-time adaptation, TTA), leveraging the
full-frequency information from incoming data to adapt to target domains while
treating high-frequency components as domain-specific cues. LFC are aligned
with the trained distribution, while HFC unique to the target distribution are
solely utilized for batch normalization. CoDA can be integrated synergistically
into existing QAT and TTA methods. CoDA is evaluated on widely used
domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various
model architectures. With significant compression, it achieves accuracy
improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the
full-precision TTA baseline.

</details>


### [132] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
*Pingrui Zhang,Yifei Su,Pengyuan Wu,Dong An,Li Zhang,Zhigang Wang,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于语言形式的自适应想象方法（ATD），通过双分支自引导想象策略，结合大语言模型（LLM），在视觉与语言导航任务中实现了更高效和可靠的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉与语言导航（VLN）任务中，部分可观测性导致感知与语言对齐困难。现有方法依赖视觉合成，计算成本高且冗余。

Method: 提出自适应文本想象器（ATD），采用类似人脑左右半球的架构，左脑负责逻辑整合，右脑负责未来场景的想象预测。仅微调Q-former以激活LLM的领域知识，并引入交叉交互机制规范想象输出。

Result: 在R2R基准测试中，ATD以更少的参数实现了最先进的性能。

Conclusion: ATD通过语言形式的自适应想象，显著提升了导航任务的效率和可靠性。

Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by
following natural instructions under partial observability, making it difficult
to align perception with language. Recent methods mitigate this by imagining
future scenes, yet they rely on vision-based synthesis, leading to high
computational cost and redundant details. To this end, we propose to adaptively
imagine key environmental semantics via \textit{language} form, enabling a more
reliable and efficient strategy. Specifically, we introduce a novel Adaptive
Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a
large language model (LLM). ATD is designed with a human-like left-right brain
architecture, where the left brain focuses on logical integration, and the
right brain is responsible for imaginative prediction of future scenes. To
achieve this, we fine-tune only the Q-former within both brains to efficiently
activate domain-specific knowledge in the LLM, enabling dynamic updates of
logical reasoning and imagination during navigation. Furthermore, we introduce
a cross-interaction mechanism to regularize the imagined outputs and inject
them into a navigation expert module, allowing ATD to jointly exploit both the
reasoning capacity of the LLM and the expertise of the navigation model. We
conduct extensive experiments on the R2R benchmark, where ATD achieves
state-of-the-art performance with fewer parameters. The code is
\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

</details>


### [133] [HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion](https://arxiv.org/abs/2505.20904)
*Guanghu Xie,Yonglong Zhang,Zhiduo Jiang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.CV

TL;DR: HTMNet是一种结合Transformer、CNN和Mamba架构的混合模型，用于解决透明和反射物体深度信息缺失的问题，在多数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 透明和反射物体导致深度传感器信息不完整，影响机器人感知和操作任务。

Method: 采用双分支Transformer-CNN编码器，结合Transformer-Mamba多尺度融合模块，并引入基于自注意力机制和状态空间模型的多模态融合模块。

Result: 在多个公开数据集上验证了模型的SOTA性能。

Conclusion: HTMNet在透明物体深度补全领域展示了Mamba架构的潜力，并通过多尺度融合模块有效整合特征。

Abstract: Transparent and reflective objects pose significant challenges for depth
sensors, resulting in incomplete depth information that adversely affects
downstream robotic perception and manipulation tasks. To address this issue, we
propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba
architectures. The encoder is constructed based on a dual-branch
Transformer-CNN framework, while the multi-scale fusion module leverages a
Transformer-Mamba architecture, which also serves as the foundation for the
decoder design. We introduce a novel multimodal fusion module grounded in
self-attention mechanisms and state space models, marking the first application
of the Mamba architecture in the field of transparent object depth completion
and revealing its promising potential. Additionally, we design an innovative
multi-scale fusion module for the decoder that combines channel attention,
spatial attention, and multi-scale feature extraction techniques to effectively
integrate multi-scale features through a down-fusion strategy. Extensive
evaluations on multiple public datasets demonstrate that our model achieves
state-of-the-art(SOTA) performance, validating the effectiveness of our
approach.

</details>


### [134] [Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects](https://arxiv.org/abs/2505.20909)
*Wei Li,Hebei Li,Yansong Peng,Siying Wu,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: LCP-Diffusion模型通过动态-静态互补视觉细化模块和双重布局控制机制，实现了无需调整的高保真个性化图像生成与布局控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本到图像生成中缺乏精确的布局控制，且未充分利用动态特征提升保真度。

Method: 提出LCP-Diffusion模型，结合动态-静态互补视觉细化模块和双重布局控制机制。

Result: 实验验证LCP-Diffusion在身份保持和布局控制方面表现优异。

Conclusion: 该模型首次实现了用户“在任何地方创造任何东西”的目标。

Abstract: Diffusion models have significantly advanced text-to-image generation, laying
the foundation for the development of personalized generative frameworks.
However, existing methods lack precise layout controllability and overlook the
potential of dynamic features of reference subjects in improving fidelity. In
this work, we propose Layout-Controllable Personalized Diffusion
(LCP-Diffusion) model, a novel framework that integrates subject identity
preservation with flexible layout guidance in a tuning-free approach. Our model
employs a Dynamic-Static Complementary Visual Refining module to
comprehensively capture the intricate details of reference subjects, and
introduces a Dual Layout Control mechanism to enforce robust spatial control
across both training and inference stages. Extensive experiments validate that
LCP-Diffusion excels in both identity preservation and layout controllability.
To the best of our knowledge, this is a pioneering work enabling users to
"create anything anywhere".

</details>


### [135] [Geometry-Editable and Appearance-Preserving Object Compositon](https://arxiv.org/abs/2505.20914)
*Jianman Lin,Haojie Li,Chunmei Qing,Zhijing Yang,Liang Lin,Tianshui Chen*

Main category: cs.CV

TL;DR: DGAD模型通过解耦几何编辑和外观保留，结合语义嵌入和交叉注意力机制，实现精确的几何编辑和外观一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅编码高层语义线索，丢失细粒度外观细节，DGAD旨在解决这一问题。

Method: 利用CLIP/DINO提取语义嵌入和外观特征，通过预训练扩散模型和交叉注意力机制实现解耦编辑。

Result: 在公开基准测试中验证了DGAD的有效性。

Conclusion: DGAD在几何编辑和外观保留方面表现优异。

Abstract: General object composition (GOC) aims to seamlessly integrate a target object
into a background scene with desired geometric properties, while simultaneously
preserving its fine-grained appearance details. Recent approaches derive
semantic embeddings and integrate them into advanced diffusion models to enable
geometry-editable generation. However, these highly compact embeddings encode
only high-level semantic cues and inevitably discard fine-grained appearance
details. We introduce a Disentangled Geometry-editable and
Appearance-preserving Diffusion (DGAD) model that first leverages semantic
embeddings to implicitly capture the desired geometric transformations and then
employs a cross-attention retrieval mechanism to align fine-grained appearance
features with the geometry-edited representation, facilitating both precise
geometry editing and faithful appearance preservation in object composition.
Specifically, DGAD builds on CLIP/DINO-derived and reference networks to
extract semantic embeddings and appearance-preserving representations, which
are then seamlessly integrated into the encoding and decoding pipelines in a
disentangled manner. We first integrate the semantic embeddings into
pre-trained diffusion models that exhibit strong spatial reasoning capabilities
to implicitly capture object geometry, thereby facilitating flexible object
manipulation and ensuring effective editability. Then, we design a dense
cross-attention mechanism that leverages the implicitly learned object geometry
to retrieve and spatially align appearance features with their corresponding
regions, ensuring faithful appearance consistency. Extensive experiments on
public benchmarks demonstrate the effectiveness of the proposed DGAD framework.

</details>


### [136] [HuMoCon: Concept Discovery for Human Motion Understanding](https://arxiv.org/abs/2505.20920)
*Qihang Fang,Chengcheng Tang,Bugra Tekin,Shugao Ma,Yanchao Yang*

Main category: cs.CV

TL;DR: HuMoCon是一个用于高级人类行为分析的运动视频理解框架，通过多模态编码器提取语义特征，解决了特征对齐和高频信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 解决运动概念发现中的多模态特征对齐不足和高频信息丢失问题。

Method: 结合视频上下文理解和运动建模，引入速度重建机制以减少时间平滑。

Result: 在标准基准测试中显著优于现有方法，实现了有效的运动概念发现。

Conclusion: HuMoCon为人类运动理解提供了高效框架，并将开源代码。

Abstract: We present HuMoCon, a novel motion-video understanding framework designed for
advanced human behavior analysis. The core of our method is a human motion
concept discovery framework that efficiently trains multi-modal encoders to
extract semantically meaningful and generalizable features. HuMoCon addresses
key challenges in motion concept discovery for understanding and reasoning,
including the lack of explicit multi-modality feature alignment and the loss of
high-frequency information in masked autoencoding frameworks. Our approach
integrates a feature alignment strategy that leverages video for contextual
understanding and motion for fine-grained interaction modeling, further with a
velocity reconstruction mechanism to enhance high-frequency feature expression
and mitigate temporal over-smoothing. Comprehensive experiments on standard
benchmarks demonstrate that HuMoCon enables effective motion concept discovery
and significantly outperforms state-of-the-art methods in training large models
for human motion understanding. We will open-source the associated code with
our paper.

</details>


### [137] [Good Enough: Is it Worth Improving your Label Quality?](https://arxiv.org/abs/2505.20928)
*Alexander Jaus,Zdravko Marinov,Constantin Seibold,Simon Reiß,Jens Kleesiek,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 研究评估了医学图像分割中标签质量的影响，发现高质量标签对域内性能有提升，但对预训练影响较小。


<details>
  <summary>Details</summary>
Motivation: 探讨提高医学图像分割标签质量的成本效益，明确其实际价值。

Method: 使用nnU-Net、TotalSegmentator和MedSAM生成多版本伪标签CT数据集，系统评估标签质量的影响。

Result: 高质量标签提升域内性能，但对预训练影响有限；低于阈值时收益不明显。

Conclusion: 研究为决定是否投入资源提高标签质量提供了依据。

Abstract: Improving label quality in medical image segmentation is costly, but its
benefits remain unclear. We systematically evaluate its impact using multiple
pseudo-labeled versions of CT datasets, generated by models like nnU-Net,
TotalSegmentator, and MedSAM. Our results show that while higher-quality labels
improve in-domain performance, gains remain unclear if below a small threshold.
For pre-training, label quality has minimal impact, suggesting that models
rather transfer general concepts than detailed annotations. These findings
provide guidance on when improving label quality is worth the effort.

</details>


### [138] [QwT-v2: Practical, Effective and Efficient Post-Training Quantization](https://arxiv.org/abs/2505.20932)
*Ningyuan Tang,Minghao Fu,Hao Yu,Jianxin Wu*

Main category: cs.CV

TL;DR: QwT-v2是一种改进的网络量化方法，通过轻量级的通道仿射补偿模块（CWAC），解决了QwT的兼容性和额外开销问题，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 网络量化是减少深度神经网络资源消耗的实用方法，但现有方法（如QwT）存在兼容性差和额外开销大的问题。

Method: 提出QwT-v2，采用CWAC模块，减少参数和计算量，并提升硬件兼容性。

Result: QwT-v2在减少开销的同时，精度与QwT相当或更高，且兼容多数硬件平台。

Conclusion: QwT-v2是一种高效、兼容性强的网络量化改进方法。

Abstract: Network quantization is arguably one of the most practical network
compression approaches for reducing the enormous resource consumption of modern
deep neural networks. They usually require diverse and subtle design choices
for specific architecture and tasks. Instead, the QwT method is a simple and
general approach which introduces lightweight additional structures to improve
quantization. But QwT incurs extra parameters and latency. More importantly,
QwT is not compatible with many hardware platforms. In this paper, we propose
QwT-v2, which not only enjoys all advantages of but also resolves major defects
of QwT. By adopting a very lightweight channel-wise affine compensation (CWAC)
module, QwT-v2 introduces significantly less extra parameters and computations
compared to QwT, and at the same time matches or even outperforms QwT in
accuracy. The compensation module of QwT-v2 can be integrated into quantization
inference engines with little effort, which not only effectively removes the
extra costs but also makes it compatible with most existing hardware platforms.

</details>


### [139] [ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation](https://arxiv.org/abs/2505.20935)
*Sanghyun Jo,Wooyeol Lee,Ziseok Lee,Kyungsu Kim*

Main category: cs.CV

TL;DR: ISAC是一种无需训练的方法，通过实例优先建模解决多实例场景中对象合并或遗漏的问题，显著提升多实例生成准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在多实例场景中表现不佳，容易合并或遗漏对象，ISAC旨在解决这一问题。

Method: ISAC采用实例优先建模方法，通过层次化树状提示机制解耦多对象实例，并分别对齐语义标签。

Result: ISAC在多类和多实例准确率上分别达到52%和83%，无需外部模型支持。

Conclusion: ISAC通过实例优先建模有效解决了多实例生成问题，具有显著优势。

Abstract: Text-to-image diffusion models excel at generating single-instance scenes but
struggle with multi-instance scenarios, often merging or omitting objects.
Unlike previous training-free approaches that rely solely on semantic-level
guidance without addressing instance individuation, our training-free method,
Instance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete
instance formation and semantic entanglement through an instance-first modeling
approach. This enables ISAC to effectively leverage a hierarchical,
tree-structured prompt mechanism, disentangling multiple object instances and
individually aligning them with their corresponding semantic labels. Without
employing any external models, ISAC achieves up to 52% average multi-class
accuracy and 83% average multi-instance accuracy by effectively forming
disentangled instances. The code will be made available upon publication.

</details>


### [140] [PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter](https://arxiv.org/abs/2505.20941)
*Yaohua Zha,Yanzi Wang,Hang Guo,Jinpeng Wang,Tao Dai,Bin Chen,Zhihao Ouyang,Xue Yuerong,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 论文提出Point Mamba Adapter (PMA)，通过整合预训练模型中间层的互补信息，提升点云理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用预训练模型的最终输出，忽略了中间层的丰富信息，限制了模型潜力。

Method: 提出PMA，构建有序特征序列并利用Mamba融合多层级语义；引入G2PG动态优化空间顺序。

Result: 在多个点云数据集上验证了PMA的有效性，显著提升了点云理解能力。

Conclusion: PMA通过融合中间层特征，为点云理解提供了新思路，代码已开源。

Abstract: Applying pre-trained models to assist point cloud understanding has recently
become a mainstream paradigm in 3D perception. However, existing application
strategies are straightforward, utilizing only the final output of the
pre-trained model for various task heads. It neglects the rich complementary
information in the intermediate layer, thereby failing to fully unlock the
potential of pre-trained models. To overcome this limitation, we propose an
orthogonal solution: Point Mamba Adapter (PMA), which constructs an ordered
feature sequence from all layers of the pre-trained model and leverages Mamba
to fuse all complementary semantics, thereby promoting comprehensive point
cloud understanding. Constructing this ordered sequence is non-trivial due to
the inherent isotropy of 3D space. Therefore, we further propose a
geometry-constrained gate prompt generator (G2PG) shared across different
layers, which applies shared geometric constraints to the output gates of the
Mamba and dynamically optimizes the spatial order, thus enabling more effective
integration of multi-layer information. Extensive experiments conducted on
challenging point cloud datasets across various tasks demonstrate that our PMA
elevates the capability for point cloud understanding to a new level by fusing
diverse complementary intermediate features. Code is available at
https://github.com/zyh16143998882/PMA.

</details>


### [141] [DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.20951)
*Naiyu Fang,Zheyuan Zhou,Kang Wang,Ruibo Li,Lemiao Qiu,Shuyou Zhang,Zhe Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: DSOcc通过深度感知和语义辅助提升基于相机的3D语义占用预测，联合推断占用状态和类别，利用非学习方法计算软占用置信度，结合图像特征增强深度感知，并通过多帧融合提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式占用状态推断，导致特征分配错误且样本不足，影响占用类别推断的学习。

Method: 联合推断占用状态和类别，通过非学习方法计算软占用置信度并与图像特征结合，同时利用训练好的图像语义分割和多帧融合辅助类别推断。

Result: 在SemanticKITTI数据集上，DSOcc在基于相机的方法中达到最优性能。

Conclusion: DSOcc通过深度感知和语义辅助有效解决了现有方法的局限性，提升了3D语义占用预测的性能。

Abstract: Camera-based 3D semantic occupancy prediction offers an efficient and
cost-effective solution for perceiving surrounding scenes in autonomous
driving. However, existing works rely on explicit occupancy state inference,
leading to numerous incorrect feature assignments, and insufficient samples
restrict the learning of occupancy class inference. To address these
challenges, we propose leveraging Depth awareness and Semantic aid to boost
camera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform
occupancy state and occupancy class inference, where soft occupancy confidence
is calculated through non-learning method and multiplied with image features to
make the voxel representation aware of depth, enabling adaptive implicit
occupancy state inference. Rather than focusing on improving feature learning,
we directly utilize well-trained image semantic segmentation and fuse multiple
frames with their occupancy probabilities to aid occupancy class inference,
thereby enhancing robustness. Experimental results demonstrate that DSOcc
achieves state-of-the-art performance on the SemanticKITTI dataset among
camera-based methods.

</details>


### [142] [OrienText: Surface Oriented Textual Image Generation](https://arxiv.org/abs/2505.20958)
*Shubham Singh Paliwal,Arushi Jain,Monika Sharma,Vikram Jamwal,Lovekesh Vig*

Main category: cs.CV

TL;DR: OrienText方法通过利用区域特定的表面法线作为条件输入，改进了文本到图像生成模型在复杂表面上准确嵌入文本的能力。


<details>
  <summary>Details</summary>
Motivation: 电子商务等领域中，图像中的文本对营销等活动至关重要，但现有文本到图像生成模型在复杂表面上准确嵌入文本时表现不佳。

Method: 提出OrienText方法，利用区域特定的表面法线作为条件输入，指导文本在图像中的准确渲染和定向。

Result: 在自建数据集上验证了OrienText的有效性，并对比了现有方法。

Conclusion: OrienText能够显著提升文本在复杂表面上的生成准确性和定向效果。

Abstract: Textual content in images is crucial in e-commerce sectors, particularly in
marketing campaigns, product imaging, advertising, and the entertainment
industry. Current text-to-image (T2I) generation diffusion models, though
proficient at producing high-quality images, often struggle to incorporate text
accurately onto complex surfaces with varied perspectives, such as angled views
of architectural elements like buildings, banners, or walls. In this paper, we
introduce the Surface Oriented Textual Image Generation (OrienText) method,
which leverages region-specific surface normals as conditional input to T2I
generation diffusion model. Our approach ensures accurate rendering and correct
orientation of the text within the image context. We demonstrate the
effectiveness of the OrienText method on a self-curated dataset of images and
compare it against the existing textual image generation methods.

</details>


### [143] [RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes](https://arxiv.org/abs/2505.20967)
*Jiarui Zhang,Zhihao Li,Chong Wang,Bihan Wen*

Main category: cs.CV

TL;DR: RF4D是一种基于毫米波雷达的神经场框架，专为动态户外场景的新视角合成设计，通过引入时间信息和特征级流模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经场方法在恶劣天气下表现脆弱，而毫米波雷达对环境变化具有鲁棒性，但其与神经场的结合尚未充分探索。动态场景需要时空建模。

Method: RF4D框架显式加入时间信息，提出特征级流模块预测时间偏移，并设计雷达功率渲染公式以提升精度。

Result: 在公开雷达数据集上，RF4D在新视角合成质量和占用估计准确性上表现优异，尤其在动态场景中改进显著。

Conclusion: RF4D通过结合雷达鲁棒性和时空建模，显著提升了动态户外场景的神经场性能。

Abstract: Neural fields (NFs) have demonstrated remarkable performance in scene
reconstruction, powering various tasks such as novel view synthesis. However,
existing NF methods relying on RGB or LiDAR inputs often exhibit severe
fragility to adverse weather, particularly when applied in outdoor scenarios
like autonomous driving. In contrast, millimeter-wave radar is inherently
robust to environmental changes, while unfortunately, its integration with NFs
remains largely underexplored. Besides, as outdoor driving scenarios frequently
involve moving objects, making spatiotemporal modeling essential for temporally
consistent novel view synthesis. To this end, we introduce RF4D, a radar-based
neural field framework specifically designed for novel view synthesis in
outdoor dynamic scenes. RF4D explicitly incorporates temporal information into
its representation, significantly enhancing its capability to model moving
objects. We further introduce a feature-level flow module that predicts latent
temporal offsets between adjacent frames, enforcing temporal coherence in
dynamic scene modeling. Moreover, we propose a radar-specific power rendering
formulation closely aligned with radar sensing physics, improving synthesis
accuracy and interoperability. Extensive experiments on public radar datasets
demonstrate the superior performance of RF4D in terms of radar measurement
synthesis quality and occupancy estimation accuracy, achieving especially
pronounced improvements in dynamic outdoor scenarios.

</details>


### [144] [DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization](https://arxiv.org/abs/2505.20975)
*Shamil Ayupov,Maksim Nakhodnov,Anastasia Yaschenko,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的个性化扩散模型方法，通过生成合成配对数据集优化概念保真度和文本对齐，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 解决个性化扩散模型中概念保真度与上下文对齐的平衡问题。

Method: 利用外部质量指标生成合成配对数据集，进行DPO式训练，支持灵活调整图像保真度与文本对齐的权衡。

Result: 在多步训练中优于基线方法，收敛速度和输出质量均有提升。

Conclusion: 该方法在各种架构和微调技术中均表现出有效性，代码已开源。

Abstract: Personalized diffusion models have shown remarkable success in Text-to-Image
(T2I) generation by enabling the injection of user-defined concepts into
diverse contexts. However, balancing concept fidelity with contextual alignment
remains a challenging open problem. In this work, we propose an RL-based
approach that leverages the diverse outputs of T2I models to address this
issue. Our method eliminates the need for human-annotated scores by generating
a synthetic paired dataset for DPO-like training using external quality
metrics. These better-worse pairs are specifically constructed to improve both
concept fidelity and prompt adherence. Moreover, our approach supports flexible
adjustment of the trade-off between image fidelity and textual alignment.
Through multi-step training, our approach outperforms a naive baseline in
convergence speed and output quality. We conduct extensive qualitative and
quantitative analysis, demonstrating the effectiveness of our method across
various architectures and fine-tuning techniques. The source code can be found
at https://github.com/ControlGenAI/DreamBoothDPO.

</details>


### [145] [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
*Cainan Davidson,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: 论文提出RefAV方法，利用视觉语言模型（VLMs）从自动驾驶车辆日志中挖掘关键场景，并发布了一个大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统场景挖掘方法效率低且易错，需要改进以更高效地识别安全关键场景。

Method: 通过视觉语言模型（VLMs）检测和定位日志中描述的复杂场景，并引入RefAV数据集支持研究。

Result: 实验表明，直接使用现有VLMs效果不佳，说明场景挖掘具有独特挑战。

Conclusion: RefAV为场景挖掘提供了新方法和数据集，未来需进一步优化模型。

Abstract: Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal
data localized to HD maps during normal fleet testing. However, identifying
interesting and safety-critical scenarios from uncurated driving logs remains a
significant challenge. Traditional scenario mining techniques are error-prone
and prohibitively time-consuming, often relying on hand-crafted structured
queries. In this work, we revisit spatio-temporal scenario mining through the
lens of recent vision-language models (VLMs) to detect whether a described
scenario occurs in a driving log and, if so, precisely localize it in both time
and space. To address this problem, we introduce RefAV, a large-scale dataset
of 10,000 diverse natural language queries that describe complex multi-agent
interactions relevant to motion planning derived from 1000 driving logs in the
Argoverse 2 Sensor dataset. We evaluate several referential multi-object
trackers and present an empirical analysis of our baselines. Notably, we find
that naively repurposing off-the-shelf VLMs yields poor performance, suggesting
that scenario mining presents unique challenges. Our code and dataset are
available at https://github.com/CainanD/RefAV/ and
https://argoverse.github.io/user-guide/tasks/scenario_mining.html

</details>


### [146] [Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos](https://arxiv.org/abs/2505.20985)
*Mustafa İzzet Muştu,Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: 研究探讨了人脸交换技术在视频隐私保护中的潜力，证明其能在保持数据质量的同时有效隐藏身份。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉数据需求与隐私法规的冲突促使研究匿名化方法，以在不严重降低数据质量的情况下隐藏个人身份。

Method: 通过评估时间一致性、匿名强度和视觉保真度，研究人脸交换技术的效果。

Result: 人脸交换技术能生成一致的面部过渡并有效隐藏身份。

Conclusion: 人脸交换技术适用于隐私保护视频应用，为未来匿名化研究奠定了基础。

Abstract: The increasing demand for large-scale visual data, coupled with strict
privacy regulations, has driven research into anonymization methods that hide
personal identities without seriously degrading data quality. In this paper, we
explore the potential of face swapping methods to preserve privacy in video
data. Through extensive evaluations focusing on temporal consistency, anonymity
strength, and visual fidelity, we find that face swapping techniques can
produce consistent facial transitions and effectively hide identities. These
results underscore the suitability of face swapping for privacy-preserving
video applications and lay the groundwork for future advancements in
anonymization focused face-swapping models.

</details>


### [147] [Facial Attribute Based Text Guided Face Anonymization](https://arxiv.org/abs/2505.21002)
*Mustafa İzzet Muştu,Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的面部匿名化流程，利用扩散修复模型生成自然但无法识别的面部图像，以解决隐私合规数据集收集的难题。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉应用中处理大量含个人信息的视觉数据时，隐私保护成为关键问题。现有隐私法规要求个人数据需经同意处理，限制了高质量数据集的收集。

Method: 采用三阶段流程：RetinaNet进行面部检测，VGG-Face提取特征，BrushNet扩散模型生成自然但匿名化的面部图像，结合文本提示控制面部属性。

Result: 生成的面部图像自然且无法识别个体，符合隐私法规，为计算机视觉研究提供了合规数据集。

Conclusion: 该方法无需训练GAN，利用扩散模型高效生成匿名化图像，解决了隐私与数据质量之间的矛盾。

Abstract: The increasing prevalence of computer vision applications necessitates
handling vast amounts of visual data, often containing personal information.
While this technology offers significant benefits, it should not compromise
privacy. Data privacy regulations emphasize the need for individual consent for
processing personal data, hindering researchers' ability to collect
high-quality datasets containing the faces of the individuals. This paper
presents a deep learning-based face anonymization pipeline to overcome this
challenge. Unlike most of the existing methods, our method leverages recent
advancements in diffusion-based inpainting models, eliminating the need for
training Generative Adversarial Networks. The pipeline employs a three-stage
approach: face detection with RetinaNet, feature extraction with VGG-Face, and
realistic face generation using the state-of-the-art BrushNet diffusion model.
BrushNet utilizes the entire image, face masks, and text prompts specifying
desired facial attributes like age, ethnicity, gender, and expression. This
enables the generation of natural-looking images with unrecognizable
individuals, facilitating the creation of privacy-compliant datasets for
computer vision research.

</details>


### [148] [Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains](https://arxiv.org/abs/2505.21010)
*Sabbir Ahmed,Mamshad Nayeem Rizve,Abdullah Al Arafat,Jacqueline Liu,Rahim Hossain,Mohaiminul Al Nahian,Adnan Siraj Rakin*

Main category: cs.CV

TL;DR: 论文提出了一种名为统一对齐协议（UAP）的新框架，用于解决半监督联邦学习（SSFL）中的领域泛化问题，通过两阶段训练过程提升模型在未见领域的表现。


<details>
  <summary>Details</summary>
Motivation: 传统SSFL假设训练和测试阶段的数据分布相同，但实际中领域偏移频繁发生，因此需要增强SSFL的泛化能力。

Method: UAP包含两阶段训练：服务器模型学习并对齐特征分布，随后客户端利用服务器特征分布进行特征对齐。

Result: 在多个模型架构和标准领域泛化数据集上的实验表明，UAP在SSFL中实现了最先进的泛化性能。

Conclusion: UAP通过创新的两阶段训练方法，显著提升了SSFL在领域泛化中的实用性。

Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over
conventional Federated Learning in many real-world applications. Due to the
practical limitation of limited labeled data on the client side, SSFL considers
that participating clients train with unlabeled data, and only the central
server has the necessary resources to access limited labeled data, making it an
ideal fit for real-world applications (e.g., healthcare). However, traditional
SSFL assumes that the data distributions in the training phase and testing
phase are the same. In practice, however, domain shifts frequently occur,
making it essential for SSFL to incorporate generalization capabilities and
enhance their practicality. The core challenge is improving model
generalization to new, unseen domains while the client participate in SSFL.
However, the decentralized setup of SSFL and unsupervised client training
necessitates innovation to achieve improved generalization across domains. To
achieve this, we propose a novel framework called the Unified Alignment
Protocol (UAP), which consists of an alternating two-stage training process.
The first stage involves training the server model to learn and align the
features with a parametric distribution, which is subsequently communicated to
clients without additional communication overhead. The second stage proposes a
novel training algorithm that utilizes the server feature distribution to align
client features accordingly. Our extensive experiments on standard domain
generalization benchmark datasets across multiple model architectures reveal
that proposed UAP successfully achieves SOTA generalization performance in SSFL
setting.

</details>


### [149] [FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/abs/2505.21032)
*Nils Neukirch,Johanna Vielhaben,Nils Strodthoff*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的方法，用于高保真地映射特征空间到输入空间，以提升深度神经网络内部表示的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的内部表示难以解释，现有方法通常依赖粗略近似，无法满足高保真需求。

Method: 使用预训练的高保真条件扩散模型，结合空间分辨特征图，以概率方式学习特征空间到输入空间的映射。

Result: 在多种预训练图像分类器（如CNN和ViT）上展示了出色的重建能力，并通过定性比较和鲁棒性分析验证了方法的有效性。

Conclusion: 该方法为计算机视觉模型的特征空间理解提供了广泛潜力，可用于概念可视化或特征空间复合性研究。

Abstract: Internal representations are crucial for understanding deep neural networks,
such as their properties and reasoning patterns, but remain difficult to
interpret. While mapping from feature space to input space aids in interpreting
the former, existing approaches often rely on crude approximations. We propose
using a conditional diffusion model - a pretrained high-fidelity diffusion
model conditioned on spatially resolved feature maps - to learn such a mapping
in a probabilistic manner. We demonstrate the feasibility of this approach
across various pretrained image classifiers from CNNs to ViTs, showing
excellent reconstruction capabilities. Through qualitative comparisons and
robustness analysis, we validate our method and showcase possible applications,
such as the visualization of concept steering in input space or investigations
of the composite nature of the feature space. This approach has broad potential
for improving feature space understanding in computer vision models.

</details>


### [150] [RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](https://arxiv.org/abs/2505.21036)
*Aiyue Chen,Bin Dong,Jingru Li,Jing Lin,Yiwu Yao,Gongyi Wang*

Main category: cs.CV

TL;DR: RainFusion是一种无需训练的新型稀疏注意力方法，通过利用视觉数据中的稀疏性加速注意力计算，同时保持视频质量，实现了2倍以上的计算速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中计算成本高，尤其是3D注意力机制占用了大量资源，因此需要一种高效的方法来加速计算。

Method: RainFusion通过识别视频生成中的三种稀疏模式（空间、时间和纹理模式），并利用自适应识别模块（ARM）在线确定稀疏模式，无需额外训练即可集成到现有模型中。

Result: 实验表明，RainFusion在注意力计算上实现了2倍以上的加速，且对视频质量影响极小（VBench分数仅下降0.2%）。

Conclusion: RainFusion是一种高效、通用的方法，能够显著提升视频生成模型的效率，同时保持生成质量。

Abstract: Video generation using diffusion models is highly computationally intensive,
with 3D attention in Diffusion Transformer (DiT) models accounting for over
80\% of the total computational resources. In this work, we introduce {\bf
RainFusion}, a novel training-free sparse attention method that exploits
inherent sparsity nature in visual data to accelerate attention computation
while preserving video quality. Specifically, we identify three unique sparse
patterns in video generation attention calculations--Spatial Pattern, Temporal
Pattern and Textural Pattern. The sparse pattern for each attention head is
determined online with negligible overhead (\textasciitilde\,0.2\%) with our
proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed
{\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated
into state-of-the-art 3D-attention video generation models without additional
training or calibration. We evaluate our method on leading open-sourced models
including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its
broad applicability and effectiveness. Experimental results show that
RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation
while maintaining video quality, with only a minimal impact on VBench scores
(-0.2\%).

</details>


### [151] [Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing](https://arxiv.org/abs/2505.21049)
*Dehao Wang,Haohang Zhu,Yiwen Xu,Kaiqi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种结合目标检测和单目深度估计的鲁棒性道路坑洼面积估计框架，通过改进的ACSH-YOLOv8模型和MBTP方法提升检测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 道路坑洼对驾驶安全和舒适性构成严重威胁，现有视觉方法因相机角度变化和平坦路面假设限制，在复杂环境中表现不佳。

Method: 提出ACSH-YOLOv8模型增强特征提取，结合BoT-SORT跟踪和DepthAnything V2生成深度图，利用MBTP方法估计坑洼面积，并通过CDKF优化结果一致性。

Result: ACSH-YOLOv8的AP(50)达到76.6%，优于YOLOv8 7.6%，CDKF优化提升了预测的鲁棒性。

Conclusion: 该方法显著提升了坑洼检测和面积估计的精度与实用性，适用于复杂现实环境。

Abstract: Road potholes pose a serious threat to driving safety and comfort, making
their detection and assessment a critical task in fields such as autonomous
driving. When driving vehicles, the operators usually avoid large potholes and
approach smaller ones at reduced speeds to ensure safety. Therefore, accurately
estimating pothole area is of vital importance. Most existing vision-based
methods rely on distance priors to construct geometric models. However, their
performance is susceptible to variations in camera angles and typically relies
on the assumption of a flat road surface, potentially leading to significant
errors in complex real-world environments. To address these problems, a robust
pothole area estimation framework that integrates object detection and
monocular depth estimation in a video stream is proposed in this paper. First,
to enhance pothole feature extraction and improve the detection of small
potholes, ACSH-YOLOv8 is proposed with ACmix module and the small object
detection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,
while DepthAnything V2 generates depth maps for each frame. With the obtained
depth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel
(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter
based on Confidence and Distance (CDKF) is developed to maintain consistency of
estimation results across consecutive frames. The results show that ACSH-YOLOv8
model achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.
Through CDKF optimization across consecutive frames, pothole predictions become
more robust, thereby enhancing the method's practical applicability.

</details>


### [152] [Advancing high-fidelity 3D and Texture Generation with 2.5D latents](https://arxiv.org/abs/2505.21050)
*Xin Yang,Jiantao Lin,Yingjie Xu,Haodong Li,Yingcong Chen*

Main category: cs.CV

TL;DR: 提出了一种联合生成3D几何和纹理的新框架，通过2.5D潜在表示实现高质量3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法中3D几何和纹理生成分离，导致结果不连贯，需统一表示提升性能。

Method: 整合多视角RGB、法线和坐标图像为2.5D潜在表示，利用预训练2D基础模型生成2.5D，再通过轻量级解码器转为3D。

Result: 模型在生成高质量3D对象及几何条件纹理生成上显著优于现有方法。

Conclusion: 联合生成框架有效提升3D生成质量与连贯性。

Abstract: Despite the availability of large-scale 3D datasets and advancements in 3D
generative models, the complexity and uneven quality of 3D geometry and texture
data continue to hinder the performance of 3D generation techniques. In most
existing approaches, 3D geometry and texture are generated in separate stages
using different models and non-unified representations, frequently leading to
unsatisfactory coherence between geometry and texture. To address these
challenges, we propose a novel framework for joint generation of 3D geometry
and texture. Specifically, we focus in generate a versatile 2.5D
representations that can be seamlessly transformed between 2D and 3D. Our
approach begins by integrating multiview RGB, normal, and coordinate images
into a unified representation, termed as 2.5D latents. Next, we adapt
pre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing
both text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D
refiner-decoder framework that efficiently generates detailed 3D
representations from 2.5D images. Extensive experiments demonstrate that our
model not only excels in generating high-quality 3D objects with coherent
structure and color from text and image inputs but also significantly
outperforms existing methods in geometry-conditioned texture generation.

</details>


### [153] [Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles](https://arxiv.org/abs/2505.21060)
*Peng Wang,Xiang Liu,Peidong Liu*

Main category: cs.CV

TL;DR: 提出了一种快速3D场景风格化方法，通过分支架构分离结构和外观，实现高效且多视角一致的风格化。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格化方法需要密集输入和计算密集型优化，效率低且难以保持多视角一致性。

Method: 采用分支架构分离结构建模和外观着色，结合身份损失预训练模型，实现快速风格化。

Result: 在多种数据集上验证，新方法在风格化质量、多视角一致性和效率上优于现有方法。

Conclusion: 该方法高效且高质量地实现了3D场景风格化，为实时应用提供了可能。

Abstract: Stylizing 3D scenes instantly while maintaining multi-view consistency and
faithfully resembling a style image remains a significant challenge. Current
state-of-the-art 3D stylization methods typically involve computationally
intensive test-time optimization to transfer artistic features into a
pretrained 3D representation, often requiring dense posed input images. In
contrast, leveraging recent advances in feed-forward reconstruction models, we
demonstrate a novel approach to achieve direct 3D stylization in less than a
second using unposed sparse-view scene images and an arbitrary style image. To
address the inherent decoupling between reconstruction and stylization, we
introduce a branched architecture that separates structure modeling and
appearance shading, effectively preventing stylistic transfer from distorting
the underlying 3D scene structure. Furthermore, we adapt an identity loss to
facilitate pre-training our stylization model through the novel view synthesis
task. This strategy also allows our model to retain its original reconstruction
capabilities while being fine-tuned for stylization. Comprehensive evaluations,
using both in-domain and out-of-domain datasets, demonstrate that our approach
produces high-quality stylized 3D content that achieve a superior blend of
style and scene appearance, while also outperforming existing methods in terms
of multi-view consistency and efficiency.

</details>


### [154] [LPOI: Listwise Preference Optimization for Vision Language Models](https://arxiv.org/abs/2505.21061)
*Fatemeh Pesaran Zadeh,Yoojin Oh,Gunhee Kim*

Main category: cs.CV

TL;DR: LPOI是一种针对大型视觉语言模型（VLM）的对象感知列表偏好优化方法，通过掩码关键对象并插值生成渐进完整的图像序列，有效减少幻觉并保持视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLHF和DPO）容易过拟合文本信息或加剧幻觉，而负样本增强仅部分解决问题。列表偏好优化因复杂性和成本未被用于VLM。

Method: LPOI通过掩码图像中的关键对象，并在正负样本间插值掩码区域，生成渐进完整的图像序列，训练模型按对象可见性排序。

Result: 在MMHalBench、AMBER和Object HalBench上的实验表明，LPOI在减少幻觉和提升VLM性能上优于现有方法。

Conclusion: LPOI无需额外标注，通过自动构建排序列表，显著减少幻觉，是VLM偏好优化的有效方法。

Abstract: Aligning large VLMs with human preferences is a challenging task, as methods
like RLHF and DPO often overfit to textual information or exacerbate
hallucinations. Although augmenting negative image samples partially addresses
these pitfalls, no prior work has employed listwise preference optimization for
VLMs, due to the complexity and cost of constructing listwise image samples. In
this work, we propose LPOI, the first object-aware listwise preference
optimization developed for reducing hallucinations in VLMs. LPOI identifies and
masks a critical object in the image, and then interpolates the masked region
between the positive and negative images to form a sequence of incrementally
more complete images. The model is trained to rank these images in ascending
order of object visibility, effectively reducing hallucinations while retaining
visual fidelity. LPOI requires no extra annotations beyond standard pairwise
preference data, as it automatically constructs the ranked lists through object
masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and
Object HalBench confirm that LPOI outperforms existing preference optimization
methods in reducing hallucinations and enhancing VLM performance. We make the
code available at https://github.com/fatemehpesaran310/lpoi.

</details>


### [155] [Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals](https://arxiv.org/abs/2505.21062)
*Davide Lobba,Fulvio Sanguigni,Bin Ren,Marcella Cornia,Rita Cucchiara,Nicu Sebe*

Main category: cs.CV

TL;DR: 本文提出了一种名为TEMU-VTOFF的新架构，用于解决虚拟试脱（VTOFF）任务中的两大挑战，并通过多模态输入和多类别设置显著提升了生成图像的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 虚拟试脱（VTOFF）任务旨在从穿着服装的个体照片中生成标准化的服装产品图像，但现有方法在分离服装特征和适用性上存在局限。

Method: 提出TEMU-VTOFF架构，采用双DiT主干和修改的多模态注意力机制，支持图像、文本和掩码等多模态输入，并引入对齐模块优化细节。

Result: 在VITON-HD和Dress Code数据集上的实验表明，TEMU-VTOFF在视觉质量和目标服装保真度上均达到最新水平。

Conclusion: TEMU-VTOFF为VTOFF任务提供了高效解决方案，尤其在多类别服装和复杂场景下表现优异。

Abstract: While virtual try-on (VTON) systems aim to render a garment onto a target
person image, this paper tackles the novel task of virtual try-off (VTOFF),
which addresses the inverse problem: generating standardized product images of
garments from real-world photos of clothed individuals. Unlike VTON, which must
resolve diverse pose and style variations, VTOFF benefits from a consistent and
well-defined output format -- typically a flat, lay-down-style representation
of the garment -- making it a promising tool for data generation and dataset
enhancement. However, existing VTOFF approaches face two major limitations: (i)
difficulty in disentangling garment features from occlusions and complex poses,
often leading to visual artifacts, and (ii) restricted applicability to
single-category garments (e.g., upper-body clothes only), limiting
generalization. To address these challenges, we present Text-Enhanced
MUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a
dual DiT-based backbone with a modified multimodal attention mechanism for
robust garment feature extraction. Our architecture is designed to receive
garment information from multiple modalities like images, text, and masks to
work in a multi-category setting. Finally, we propose an additional alignment
module to further refine the generated visual details. Experiments on VITON-HD
and Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the
VTOFF task, significantly improving both visual quality and fidelity to the
target garments.

</details>


### [156] [Minute-Long Videos with Dual Parallelisms](https://arxiv.org/abs/2505.21070)
*Zeqing Wang,Bowen Zheng,Xingyi Yang,Yuecong Xu,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为DualParal的分布式推理策略，通过并行化时间帧和模型层来降低DiT视频扩散模型的长视频处理延迟和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决DiT视频扩散模型在生成长视频时的高延迟和高内存成本问题。

Method: 采用块级去噪方案，将帧块序列通过管道处理，GPU异步计算和通信；引入特征缓存和协调噪声初始化策略优化性能。

Result: 在8×RTX 4090 GPU上，生成长达1,025帧的视频，延迟降低6.54倍，内存成本降低1.48倍。

Conclusion: DualParal策略实现了高效、无伪影且无限长的视频生成。

Abstract: Diffusion Transformer (DiT)-based video diffusion models generate
high-quality videos at scale but incur prohibitive processing latency and
memory costs for long videos. To address this, we propose a novel distributed
inference strategy, termed DualParal. The core idea is that, instead of
generating an entire video on a single GPU, we parallelize both temporal frames
and model layers across GPUs. However, a naive implementation of this division
faces a key limitation: since diffusion models require synchronized noise
levels across frames, this implementation leads to the serialization of
original parallelisms. We leverage a block-wise denoising scheme to handle
this. Namely, we process a sequence of frame blocks through the pipeline with
progressively decreasing noise levels. Each GPU handles a specific block and
layer subset while passing previous results to the next GPU, enabling
asynchronous computation and communication. To further optimize performance, we
incorporate two key enhancements. Firstly, a feature cache is implemented on
each GPU to store and reuse features from the prior block as context,
minimizing inter-GPU communication and redundant computation. Secondly, we
employ a coordinated noise initialization strategy, ensuring globally
consistent temporal dynamics by sharing initial noise patterns across GPUs
without extra resource costs. Together, these enable fast, artifact-free, and
infinitely long video generation. Applied to the latest diffusion transformer
video generator, our method efficiently produces 1,025-frame videos with up to
6.54$\times$ lower latency and 1.48$\times$ lower memory cost on 8$\times$RTX
4090 GPUs.

</details>


### [157] [DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding](https://arxiv.org/abs/2505.21076)
*Weihao Xuan,Junjue Wang,Heli Qi,Zihang Chen,Zhuo Zheng,Yanfei Zhong,Junshi Xia,Naoto Yokoya*

Main category: cs.CV

TL;DR: DVL-Suite是一个用于长期城市动态分析的框架，包含DVL-Bench和DVL-Instruct两部分，旨在提升多模态大语言模型在长期地球观测中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在长期地球观测分析中表现有限，主要集中于单时相或双时相影像。

Method: 提出DVL-Suite框架，包含15,063张高分辨率多时相影像，覆盖42个美国大城市，并分为DVL-Bench（7个任务）和DVL-Instruct（指令调优数据集）。

Result: 评估了17个先进模型，发现其在长期时序理解和定量分析中的不足，并开发了DVLChat基线模型。

Conclusion: DVL-Suite填补了长期城市动态分析的空白，并通过DVLChat展示了语言交互在城市理解中的潜力。

Abstract: Multimodal large language models have demonstrated remarkable capabilities in
visual understanding, but their application to long-term Earth observation
analysis remains limited, primarily focusing on single-temporal or bi-temporal
imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework
for analyzing long-term urban dynamics through remote sensing imagery. Our
suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42
megacities in the U.S. from 2005 to 2023, organized into two components:
DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding
tasks, from fundamental change detection (pixel-level) to quantitative analyses
(regional-level) and comprehensive urban narratives (scene-level), capturing
diverse urban dynamics including expansion/transformation patterns, disaster
assessment, and environmental challenges. We evaluate 17 state-of-the-art
multimodal large language models and reveal their limitations in long-term
temporal understanding and quantitative analysis. These challenges motivate the
creation of DVL-Instruct, a specialized instruction-tuning dataset designed to
enhance models' capabilities in multi-temporal Earth observation. Building upon
this dataset, we develop DVLChat, a baseline model capable of both image-level
question-answering and pixel-level segmentation, facilitating a comprehensive
understanding of city dynamics through language interactions.

</details>


### [158] [Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts](https://arxiv.org/abs/2505.21079)
*Yue Zhang,Yingzhao Jian,Hehe Fan,Yi Yang,Roger Zimmermann*

Main category: cs.CV

TL;DR: Uni3D-MoE是一种基于稀疏混合专家（MoE）的多模态大语言模型，旨在通过动态选择专家实现自适应3D多模态融合，提升3D场景理解的完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅使用单一或有限的3D模态，导致场景表示不完整且解释准确性降低，同时不同查询类型依赖不同模态，统一处理效果不佳。

Method: Uni3D-MoE整合多种3D模态（如RGB、深度图像、BEV地图、点云和体素表示），并采用稀疏MoE框架中的可学习路由机制，动态选择专家处理多模态标记。

Result: 在标准3D场景理解基准和专用数据集上的广泛评估证明了Uni3D-MoE的有效性。

Conclusion: Uni3D-MoE通过自适应多模态融合显著提升了3D场景理解的性能，为复杂任务提供了灵活解决方案。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
demonstrated considerable potential for comprehensive 3D scene understanding.
However, existing approaches typically utilize only one or a limited subset of
3D modalities, resulting in incomplete representations of 3D scenes and reduced
interpretive accuracy. Furthermore, different types of queries inherently
depend on distinct modalities, indicating that uniform processing of all
modality tokens may fail to effectively capture query-specific context. To
address these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts
(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.
Specifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,
including multi-view RGB and depth images, bird's-eye-view (BEV) maps, point
clouds, and voxel representations. At its core, our framework employs a
learnable routing mechanism within the sparse MoE-based large language model,
dynamically selecting appropriate experts at the token level. Each expert
specializes in processing multimodal tokens based on learned modality
preferences, thus facilitating flexible collaboration tailored to diverse
task-specific requirements. Extensive evaluations on standard 3D scene
understanding benchmarks and specialized datasets demonstrate the efficacy of
Uni3D-MoE.

</details>


### [159] [DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response](https://arxiv.org/abs/2505.21089)
*Junjue Wang,Weihao Xuan,Heli Qi,Zhihao Liu,Kunyi Liu,Yuhan Wu,Hongruixuan Chen,Jian Song,Junshi Xia,Zhuo Zheng,Naoto Yokoya*

Main category: cs.CV

TL;DR: 论文提出了一个全球规模的遥感视觉语言数据集DisasterM3，用于灾害评估与响应，填补了复杂灾害场景下VLM应用的空白。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（VLM）在复杂灾害场景（如多样灾害类型、地理区域和卫星传感器）中面临挑战，缺乏专门的数据集支持。

Method: 构建了DisasterM3数据集，包含26,988对双时相卫星图像和123k指令对，涵盖多灾害、多传感器和多任务特性。

Result: 评估了14个通用和遥感VLM，发现现有模型在灾害任务上表现不佳；通过微调4个VLM，实现了任务性能的稳定提升。

Conclusion: DisasterM3数据集和微调方法显著提升了VLM在灾害场景中的表现，具有跨传感器和跨灾害的泛化能力。

Abstract: Large vision-language models (VLMs) have made great achievements in Earth
vision. However, complex disaster scenes with diverse disaster types,
geographic regions, and satellite sensors have posed new challenges for VLM
applications. To fill this gap, we curate a remote sensing vision-language
dataset (DisasterM3) for global-scale disaster assessment and response.
DisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction
pairs across 5 continents, with three characteristics: 1) Multi-hazard:
DisasterM3 involves 36 historical disaster events with significant impacts,
which are categorized into 10 common natural and man-made disasters.
2)Multi-sensor: Extreme weather during disasters often hinders optical sensor
imaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery
for post-disaster scenes. 3) Multi-task: Based on real-world scenarios,
DisasterM3 includes 9 disaster-related visual perception and reasoning tasks,
harnessing the full potential of VLM's reasoning ability with progressing from
disaster-bearing body recognition to structural damage assessment and object
relational reasoning, culminating in the generation of long-form disaster
reports. We extensively evaluated 14 generic and remote sensing VLMs on our
benchmark, revealing that state-of-the-art models struggle with the disaster
tasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,
and damage object counting insensitivity. Focusing on these issues, we
fine-tune four VLMs using our dataset and achieve stable improvements across
all tasks, with robust cross-sensor and cross-disaster generalization
capabilities.

</details>


### [160] [Instance Data Condensation for Image Super-Resolution](https://arxiv.org/abs/2505.21099)
*Tianhao Peng,Ho Man Kwan,Yuxuan Jiang,Ge Gao,Fan Zhang,Xiaozhong Xu,Shan Liu,David Bull*

Main category: cs.CV

TL;DR: 提出了一种针对图像超分辨率（ISR）的实例数据压缩（IDC）框架，通过随机局部傅里叶特征提取和多级特征分布匹配，实现了10%压缩率的DIV2K数据集，性能媲美甚至优于原始数据集。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像超分辨率依赖大数据集训练，计算和存储成本高；数据压缩技术在高层次计算机视觉任务中表现良好，但尚未充分应用于ISR。

Method: 采用随机局部傅里叶特征提取和多级特征分布匹配，实现实例级数据压缩。

Result: 压缩后的数据集（10%体积）在训练多种ISR模型时表现与原始数据集相当或更好，且训练稳定性高。

Conclusion: IDC框架首次展示了10%数据量的压缩数据集在ISR任务中的优异性能，代码和数据集已开源。

Abstract: Deep learning based image Super-Resolution (ISR) relies on large training
datasets to optimize model generalization; this requires substantial
computational and storage resources during training. While dataset condensation
has shown potential in improving data efficiency and privacy for high-level
computer vision tasks, it has not yet been fully exploited for ISR. In this
paper, we propose a novel Instance Data Condensation (IDC) framework
specifically for ISR, which achieves instance-level data condensation through
Random Local Fourier Feature Extraction and Multi-level Feature Distribution
Matching. This aims to optimize feature distributions at both global and local
levels and obtain high-quality synthesized training content with fine detail.
This framework has been utilized to condense the most commonly used training
dataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic
dataset offers comparable or (in certain cases) even better performance
compared to the original full dataset and excellent training stability when
used to train various popular ISR models. To the best of our knowledge, this is
the first time that a condensed/synthetic dataset (with a 10% data volume) has
demonstrated such performance. The source code and the synthetic dataset have
been made available at https://github.com/.

</details>


### [161] [Differentiable Solver Search for Fast Diffusion Sampling](https://arxiv.org/abs/2505.21114)
*Shuai Wang,Zexian Li,Qipeng zhang,Tianhui Song,Xubin Li,Tiezheng Ge,Bo Zheng,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的可微分求解器搜索算法，用于优化扩散模型的求解器，显著提升了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成质量上表现优异，但计算成本高昂。现有的ODE求解器依赖次优的t相关拉格朗日插值，限制了性能。

Method: 通过分析时间步长和求解器系数的紧凑搜索空间，提出了一种可微分求解器搜索算法，以找到更优的求解器。

Result: 在ImageNet256上，使用10步采样时，SiT-XL/2和FlowDCN-XL/2的FID分别达到2.40和2.35，DiT-XL/2达到2.33，显著优于传统求解器。

Conclusion: 搜索到的求解器在多种模型架构、分辨率和模型大小上表现出通用性，为扩散模型的效率提升提供了新方向。

Abstract: Diffusion models have demonstrated remarkable generation quality but at the
cost of numerous function evaluations. Recently, advanced ODE-based solvers
have been developed to mitigate the substantial computational demands of
reverse-diffusion solving under limited sampling steps. However, these solvers,
heavily inspired by Adams-like multistep methods, rely solely on t-related
Lagrange interpolation. We show that t-related Lagrange interpolation is
suboptimal for diffusion model and reveal a compact search space comprised of
time steps and solver coefficients. Building on our analysis, we propose a
novel differentiable solver search algorithm to identify more optimal solver.
Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and
FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256
with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of
2.33 with only 10 steps. Notably, our searched solver outperforms traditional
solvers by a significant margin. Moreover, our searched solver demonstrates
generality across various model architectures, resolutions, and model sizes.

</details>


### [162] [ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction](https://arxiv.org/abs/2505.21117)
*Adeela Islam,Stefano Fiorini,Stuart James,Pietro Morerio,Alessio Del Bue*

Main category: cs.CV

TL;DR: ReassembleNet通过轮廓关键点和图神经网络降低复杂性，提升多模态特征整合，在旋转和平移误差上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在可扩展性、多模态和实际应用中的局限性，特别是在复杂几何形状和真实世界问题中。

Method: 使用轮廓关键点表示输入片段，通过图神经网络选择关键点，结合多模态特征，并采用扩散式姿态估计恢复原始结构。

Result: 在旋转和平移误差上分别提升了55%和86%。

Conclusion: ReassembleNet在复杂场景中表现出色，显著提升了重组任务的性能。

Abstract: The task of reassembly is a significant challenge across multiple domains,
including archaeology, genomics, and molecular docking, requiring the precise
placement and orientation of elements to reconstruct an original structure. In
this work, we address key limitations in state-of-the-art Deep Learning methods
for reassembly, namely i) scalability; ii) multimodality; and iii) real-world
applicability: beyond square or simple geometric shapes, realistic and complex
erosion, or other real-world problems. We propose ReassembleNet, a method that
reduces complexity by representing each input piece as a set of contour
keypoints and learning to select the most informative ones by Graph Neural
Networks pooling inspired techniques. ReassembleNet effectively lowers
computational complexity while enabling the integration of features from
multiple modalities, including both geometric and texture data. Further
enhanced through pretraining on a semi-synthetic dataset. We then apply
diffusion-based pose estimation to recover the original structure. We improve
on prior methods by 55% and 86% for RMSE Rotation and Translation,
respectively.

</details>


### [163] [FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention](https://arxiv.org/abs/2505.21144)
*Sergey Karpukhin,Vadim Titov,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出FastFace框架，解决预训练ID适配器在蒸馏加速扩散模型中的无训练适应问题，通过改进分类器自由指导和注意力机制提升身份相似性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有身份保留适配器需与基础扩散模型联合训练，导致推理速度慢，FastFace旨在实现无训练适应。

Method: 重新设计分类器自由指导和注意力机制，用于少步风格生成和解耦块中的注意力操作。

Result: 提出FastFace框架，提升身份相似性和保真度，并开发解耦的公共评估协议。

Conclusion: FastFace为身份保留适配器提供高效无训练适应方案，具有实际应用潜力。

Abstract: In latest years plethora of identity-preserving adapters for a personalized
generation with diffusion models have been released. Their main disadvantage is
that they are dominantly trained jointly with base diffusion models, which
suffer from slow multi-step inference. This work aims to tackle the challenge
of training-free adaptation of pretrained ID-adapters to diffusion models
accelerated via distillation - through careful re-design of classifier-free
guidance for few-step stylistic generation and attention manipulation
mechanisms in decoupled blocks to improve identity similarity and fidelity, we
propose universal FastFace framework. Additionally, we develop a disentangled
public evaluation protocol for id-preserving adapters.

</details>


### [164] [RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images](https://arxiv.org/abs/2505.21152)
*Xurui Li,Zhonesheng Jiang,Tingxuan Ai,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为RoBiS的鲁棒无监督异常检测框架，通过Swin-Cropping预处理、数据增强和自适应二值化策略，显著提升了在MVTec AD 2基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂真实场景（如MVTec AD 2）中性能下降的问题。

Method: 包括Swin-Cropping预处理、数据增强（噪声和光照模拟）和自适应二值化策略（结合传统统计方法和MEBin）。

Result: 在Test_private和Test_private_mixed上分别实现了29.2%和29.82%的SegF1提升。

Conclusion: RoBiS框架显著提高了无监督异常检测的鲁棒性和性能。

Abstract: Robust unsupervised anomaly detection (AD) in real-world scenarios is an
important task. Current methods exhibit severe performance degradation on the
MVTec AD 2 benchmark due to its complex real-world challenges. To solve this
problem, we propose a robust framework RoBiS, which consists of three core
modules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to
preserve the information of small anomalies through overlapping window
cropping. (2) The data augmentation of noise addition and lighting simulation
is carried out on the training data to improve the robustness of AD model. We
use INP-Former as our baseline, which could generate better results on the
various sub-images. (3) The traditional statistical-based binarization strategy
(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),
for joint adaptive binarization. Then, SAM is further employed to refine the
segmentation results. Compared with some methods reported by the MVTec AD 2,
our RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on
Test_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on
Test_private_mixed. Code is available at https://github.com/xrli-U/RoBiS.

</details>


### [165] [Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model](https://arxiv.org/abs/2505.21179)
*Dar-Yen Chen,Hmrishav Bandyopadhyay,Kai Zou,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 论文提出了一种名为NAG的高效、无需训练的负向引导机制，用于解决扩散模型中负向引导在少步采样中的挑战，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中负向引导在少步采样时失效的问题，尤其是在CFG方法表现不佳的情况下。

Method: 提出NAG方法，通过注意力空间的L1归一化和细化实现负向引导，适用于多种架构和模态。

Result: 实验表明NAG在文本对齐、保真度和人类感知质量上均有提升，且用户研究显示其输出更受欢迎。

Conclusion: NAG作为一种无需训练的通用插件，为现代扩散框架提供了高效的负向引导解决方案。

Abstract: Negative guidance -- explicitly suppressing unwanted attributes -- remains a
fundamental challenge in diffusion models, particularly in few-step sampling
regimes. While Classifier-Free Guidance (CFG) works well in standard settings,
it fails under aggressive sampling step compression due to divergent
predictions between positive and negative branches. We present Normalized
Attention Guidance (NAG), an efficient, training-free mechanism that applies
extrapolation in attention space with L1-based normalization and refinement.
NAG restores effective negative guidance where CFG collapses while maintaining
fidelity. Unlike existing approaches, NAG generalizes across architectures
(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,
video), functioning as a \textit{universal} plug-in with minimal computational
overhead. Through extensive experimentation, we demonstrate consistent
improvements in text alignment (CLIP Score), fidelity (FID, PFID), and
human-perceived quality (ImageReward). Our ablation studies validate each
design component, while user studies confirm significant preference for
NAG-guided outputs. As a model-agnostic inference-time approach requiring no
retraining, NAG provides effortless negative guidance for all modern diffusion
frameworks -- pseudocode in the Appendix!

</details>


### [166] [Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion](https://arxiv.org/abs/2505.21181)
*Yayin Zheng,Chen Wan,Zihong Guo,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗攻击框架FSA，通过结合频域和空域变换，显著提升了对抗样本在黑盒模型中的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在机器学习模型安全中是一个重要挑战，尤其是黑盒防御策略下。现有方法主要集中在空域，而忽略了频域的作用。

Method: FSA框架包含两个关键技术：高频增强（通过傅里叶变换和选择性放大）和分层梯度融合（多尺度梯度分解与融合）。

Result: 实验表明，FSA在多个黑盒模型上优于现有方法，攻击成功率平均提升23.6%。

Conclusion: FSA通过频域和空域的结合，显著提升了对抗攻击的效果，为黑盒防御提供了新的挑战。

Abstract: Adversarial attacks have become a significant challenge in the security of
machine learning models, particularly in the context of black-box defense
strategies. Existing methods for enhancing adversarial transferability
primarily focus on the spatial domain. This paper presents Frequency-Space
Attack (FSA), a new adversarial attack framework that effectively integrates
frequency-domain and spatial-domain transformations. FSA combines two key
techniques: (1) High-Frequency Augmentation, which applies Fourier transform
with frequency-selective amplification to diversify inputs and emphasize the
critical role of high-frequency components in adversarial attacks, and (2)
Hierarchical-Gradient Fusion, which merges multi-scale gradient decomposition
and fusion to capture both global structures and fine-grained details,
resulting in smoother perturbations. Our experiment demonstrates that FSA
consistently outperforms state-of-the-art methods across various black-box
models. Notably, our proposed FSA achieves an average attack success rate
increase of 23.6% compared with BSR (CVPR 2024) on eight black-box defense
models.

</details>


### [167] [Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling](https://arxiv.org/abs/2505.21187)
*Hesam Araghi,Jan van Gemert,Nergis Tomen*

Main category: cs.CV

TL;DR: 本文系统评估了六种硬件友好的子采样方法对事件视频分类任务的影响，并提出了一种基于密度的子采样方法，提高了稀疏区域的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高事件率给数据传输和处理带来挑战，现有子采样方法对下游视觉任务的影响尚未充分研究。

Method: 使用卷积神经网络在多个基准数据集上评估六种子采样方法，并提出一种基于密度的子采样方法。

Result: 基于密度的子采样方法在稀疏区域提高了分类准确性，同时分析了影响子采样性能的关键因素。

Conclusion: 研究结果为平衡数据效率和任务准确性的硬件高效子采样策略提供了见解。

Abstract: Event cameras offer high temporal resolution and power efficiency, making
them well-suited for edge AI applications. However, their high event rates
present challenges for data transmission and processing. Subsampling methods
provide a practical solution, but their effect on downstream visual tasks
remains underexplored. In this work, we systematically evaluate six
hardware-friendly subsampling methods using convolutional neural networks for
event video classification on various benchmark datasets. We hypothesize that
events from high-density regions carry more task-relevant information and are
therefore better suited for subsampling. To test this, we introduce a simple
causal density-based subsampling method, demonstrating improved classification
accuracy in sparse regimes. Our analysis further highlights key factors
affecting subsampling performance, including sensitivity to hyperparameters and
failure cases in scenarios with large event count variance. These findings
provide insights for utilization of hardware-efficient subsampling strategies
that balance data efficiency and task accuracy. The code for this paper will be
released at: https://github.com/hesamaraghi/event-camera-subsampling-methods.

</details>


### [168] [Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models](https://arxiv.org/abs/2505.21200)
*Xudong Tan,Yaoxin Yang,Peng Ye,Jialin Zheng,Bizhe Bai,Xinyi Wang,Jia Hao,Tao Chen*

Main category: cs.CV

TL;DR: FlashVLA是一种无需训练、即插即用的加速框架，通过动作重用和视觉令牌选择优化Vision-Language-Action模型的推理效率，显著降低计算开销和延迟。


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action模型的高推理成本限制了实时部署和边缘应用，FlashVLA旨在解决这一问题。

Method: 提出动作重用机制和视觉令牌选择策略，减少冗余计算。

Result: 在LIBERO基准测试中，FlashVLA将FLOPs降低55.7%，延迟减少36.0%，任务成功率仅下降0.7%。

Conclusion: FlashVLA有效实现了轻量级、低延迟的VLA推理，无需重新训练。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm for
general-purpose robot control through natural language instructions. However,
their high inference cost-stemming from large-scale token computation and
autoregressive decoding-poses significant challenges for real-time deployment
and edge applications. While prior work has primarily focused on architectural
optimization, we take a different perspective by identifying a dual form of
redundancy in VLA models: (i) high similarity across consecutive action steps,
and (ii) substantial redundancy in visual tokens. Motivated by these
observations, we propose FlashVLA, the first training-free and plug-and-play
acceleration framework that enables action reuse in VLA models. FlashVLA
improves inference efficiency through a token-aware action reuse mechanism that
avoids redundant decoding across stable action steps, and an information-guided
visual token selection strategy that prunes low-contribution tokens. Extensive
experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%
and latency by 36.0%, with only a 0.7% drop in task success rate. These results
demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency
VLA inference without retraining.

</details>


### [169] [Sci-Fi: Symmetric Constraint for Frame Inbetweening](https://arxiv.org/abs/2505.21205)
*Liuhan Chen,Xiaodong Cun,Xiaoyu Li,Xianyi He,Shenghai Yuan,Jie Chen,Ying Shan,Li Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sci-Fi的新框架，通过改进的机制对称约束起始帧和结束帧，解决了现有方法中因不对称控制导致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入结束帧约束时沿用起始帧的机制，导致约束强度不对称，可能引发运动不一致或外观崩溃。

Method: 提出Sci-Fi框架，引入轻量级模块EF-Net，专门编码结束帧并扩展为时序自适应特征，以增强结束帧的约束强度。

Result: 实验证明Sci-Fi能生成更和谐的过渡效果，优于其他基线方法。

Conclusion: Sci-Fi通过对称约束机制有效提升了帧间合成的质量。

Abstract: Frame inbetweening aims to synthesize intermediate video sequences
conditioned on the given start and end frames. Current state-of-the-art methods
mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)
by incorporating end-frame constraints via directly fine-tuning or omitting
training. We identify a critical limitation in their design: Their injections
of the end-frame constraint usually utilize the same mechanism that originally
imposed the start-frame (single image) constraint. However, since the original
I2V-DMs are adequately trained for the start-frame condition in advance,
naively introducing the end-frame constraint by the same mechanism with much
less (even zero) specialized training probably can't make the end frame have a
strong enough impact on the intermediate content like the start frame. This
asymmetric control strength of the two frames over the intermediate content
likely leads to inconsistent motion or appearance collapse in generated frames.
To efficiently achieve symmetric constraints of start and end frames, we
propose a novel framework, termed Sci-Fi, which applies a stronger injection
for the constraint of a smaller training scale. Specifically, it deals with the
start-frame constraint as before, while introducing the end-frame constraint by
an improved mechanism. The new mechanism is based on a well-designed
lightweight module, named EF-Net, which encodes only the end frame and expands
it into temporally adaptive frame-wise features injected into the I2V-DM. This
makes the end-frame constraint as strong as the start-frame constraint,
enabling our Sci-Fi to produce more harmonious transitions in various
scenarios. Extensive experiments prove the superiority of our Sci-Fi compared
with other baselines.

</details>


### [170] [Is Hyperbolic Space All You Need for Medical Anomaly Detection?](https://arxiv.org/abs/2505.21228)
*Alvaro Gonzalez-Jimenez,Simone Lionetti,Ludovic Amruthalingam,Philippe Gottfrois,Fabian Gröger,Marc Pouly,Alexander A. Navarini*

Main category: cs.CV

TL;DR: 论文提出了一种将特征投影到双曲空间的方法，用于医学异常检测，优于传统欧几里得空间方法。


<details>
  <summary>Details</summary>
Motivation: 医学异常检测面临数据可用性和标注限制的挑战，传统欧几里得空间方法无法有效捕捉特征的层次关系。

Method: 将特征表示投影到双曲空间，基于置信度聚合特征，并分类为健康或异常样本。

Result: 双曲空间方法在多个医学基准数据集上表现优于欧几里得方法，AUROC分数更高，且对参数变化和少样本场景具有鲁棒性。

Conclusion: 双曲空间是医学异常检测的有力替代方案，具有潜在的应用价值。

Abstract: Medical anomaly detection has emerged as a promising solution to challenges
in data availability and labeling constraints. Traditional methods extract
features from different layers of pre-trained networks in Euclidean space;
however, Euclidean representations fail to effectively capture the hierarchical
relationships within these features, leading to suboptimal anomaly detection
performance. We propose a novel yet simple approach that projects feature
representations into hyperbolic space, aggregates them based on confidence
levels, and classifies samples as healthy or anomalous. Our experiments
demonstrate that hyperbolic space consistently outperforms Euclidean-based
frameworks, achieving higher AUROC scores at both image and pixel levels across
multiple medical benchmark datasets. Additionally, we show that hyperbolic
space exhibits resilience to parameter variations and excels in few-shot
scenarios, where healthy images are scarce. These findings underscore the
potential of hyperbolic space as a powerful alternative for medical anomaly
detection. The project website can be found at
https://hyperbolic-anomalies.github.io

</details>


### [171] [Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning](https://arxiv.org/abs/2505.21231)
*Lintao Xu,Yinghao Wang,Chaohui Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MoDOT的网络，联合估计深度和遮挡边界（OB），通过CASM模块和OBDCL损失函数提升性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 遮挡边界（OB）和单目深度估计（MDE）相互提供几何线索，联合估计可以提升场景理解和3D重建能力。

Method: 提出MoDOT网络，结合CASM模块（跨注意力多尺度卷积）和OBDCL损失函数（遮挡感知损失），联合优化深度和OB估计。

Result: 在合成数据集和NYUD-v2上达到SOTA，深度迁移结果与竞争对手相当，同时保持几何保真度。

Conclusion: 联合估计深度和OB具有显著优势，MoDOT的设计有效，代码和模型将开源以支持未来研究。

Abstract: Occlusion Boundary Estimation (OBE) identifies boundaries arising from both
inter-object occlusions and self-occlusion within individual objects,
distinguishing intrinsic object edges from occlusion-induced contours to
improve scene understanding and 3D reconstruction capacity. This is closely
related to Monocular Depth Estimation (MDE), which infers depth from a single
image, as occlusion boundaries provide critical geometric cues for resolving
depth ambiguities, while depth priors can conversely refine occlusion reasoning
in complex scenes. In this paper, we propose a novel network, MoDOT, that first
jointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale
strip convolution module, leverages mid-level OB features to significantly
enhance depth prediction. Additionally, we introduce an occlusion-aware loss
function, OBDCL, which encourages sharper and more accurate depth boundaries.
Extensive experiments on both real and synthetic datasets demonstrate the
mutual benefits of jointly estimating depth and OB, and highlight the
effectiveness of our model design. Our method achieves the state-of-the-art
(SOTA) on both our proposed synthetic datasets and one popular real dataset,
NYUD-v2, significantly outperforming multi-task baselines. Besides, without
domain adaptation, results on real-world depth transfer are comparable to the
competitors, while preserving sharp occlusion boundaries for geometric
fidelity. We will release our code, pre-trained models, and datasets to support
future research in this direction.

</details>


### [172] [CROP: Contextual Region-Oriented Visual Token Pruning](https://arxiv.org/abs/2505.21233)
*Jiawei Guo,Feifei Zhai,Pu Jian,Qianrun Wei,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出CROP框架，通过定位和剪枝两步压缩视觉令牌，解决VLM中冗余视觉令牌导致的高内存和计算需求问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLM-based VQA方法处理整张图像时产生大量冗余视觉令牌，增加内存和计算负担。

Method: CROP框架分两步：1) 定位输入查询相关的上下文区域；2) 采用PLC（预LLM压缩）和ILP（内部LLM剪枝）策略剪枝令牌。

Result: 在多种VQA任务中，CROP显著优于现有视觉令牌剪枝方法，达到SOTA性能。

Conclusion: CROP有效减少冗余视觉令牌，提升VQA任务性能，代码和数据集将公开。

Abstract: Current VLM-based VQA methods often process entire images, leading to
excessive visual tokens that include redundant information irrelevant to the
posed question. This abundance of unnecessary image details creates numerous
visual tokens, drastically increasing memory and computational requirements in
VLMs. To address this, we propose Contextual Region-Oriented Visual Token
Pruning (CROP), a novel framework to compress visual tokens through a two-step
process: Localization and Pruning. Specifically, CROP first employs an
efficient model to identify the contextual region relevant to the input query.
Subsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM
Compression (PLC), which adaptively compresses different image regions with
varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that
prunes tokens within early LLM layers guided by the identified contextual
region. Extensive experiments on a wide range of VQA tasks demonstrate that
CROP significantly outperforms existing visual token pruning methods and
achieves state-of-the-art performance. Our code and datasets will be made
available.

</details>


### [173] [3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin](https://arxiv.org/abs/2505.21238)
*Jieyu Yuan,Yujun Li,Yuanlin Zhang,Chunle Guo,Xiongxin Tang,Ruixing Wang,Chongyi Li*

Main category: cs.CV

TL;DR: 提出了一种基于物理的框架，通过高斯建模分离水下物体外观与水体效应，结合距离引导优化策略，提升水下场景重建的渲染质量和物理准确性。


<details>
  <summary>Details</summary>
Motivation: 水下场景重建因复杂的光-介质相互作用面临独特挑战，传统体积渲染假设在非均匀传播介质中失效，3D高斯泼溅（3DGS）在此环境下表现不佳。

Method: 提出外观嵌入和显式介质表示（后向散射和衰减），结合距离引导优化策略（伪深度图监督、深度正则化和尺度惩罚项）。

Result: 实验表明，该方法在渲染质量和场景恢复准确性上显著优于现有方法。

Conclusion: 通过物理建模和优化策略，实现了高质量的新视角合成和物理准确的水下场景恢复。

Abstract: Novel view synthesis for underwater scene reconstruction presents unique
challenges due to complex light-media interactions. Optical scattering and
absorption in water body bring inhomogeneous medium attenuation interference
that disrupts conventional volume rendering assumptions of uniform propagation
medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering
capabilities, it struggles with underwater inhomogeneous environments where
scattering media introduce artifacts and inconsistent appearance. In this
study, we propose a physics-based framework that disentangles object appearance
from water medium effects through tailored Gaussian modeling. Our approach
introduces appearance embeddings, which are explicit medium representations for
backscatter and attenuation, enhancing scene consistency. In addition, we
propose a distance-guided optimization strategy that leverages pseudo-depth
maps as supervision with depth regularization and scale penalty terms to
improve geometric fidelity. By integrating the proposed appearance and medium
modeling components via an underwater imaging model, our approach achieves both
high-quality novel view synthesis and physically accurate scene restoration.
Experiments demonstrate our significant improvements in rendering quality and
restoration accuracy over existing methods. The project page is available at
\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR

</details>


### [174] [Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation](https://arxiv.org/abs/2505.21258)
*Changguanng Wu,Jiangxin Dong,Chengjian Li,Jinhui Tang*

Main category: cs.CV

TL;DR: Plenodium是一种高效的三维表示框架，能够同时建模物体和参与介质，通过球谐编码结合方向和位置信息，实现高精度的水下场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有介质表示仅依赖视角相关建模，无法准确处理水下场景，因此提出一种结合方向和位置信息的新表示方法。

Method: 提出伪深度高斯补充增强COLMAP点云，并开发深度排序正则化损失优化场景几何和深度图一致性。

Result: 在真实水下数据集上显著提升3D重建效果，并通过模拟数据集验证水下场景恢复能力。

Conclusion: Plenodium在水下场景重建中表现出色，代码和数据集已开源。

Abstract: We present Plenodium (plenoptic medium), an effective and efficient 3D
representation framework capable of jointly modeling both objects and
participating media. In contrast to existing medium representations that rely
solely on view-dependent modeling, our novel plenoptic medium representation
incorporates both directional and positional information through spherical
harmonics encoding, enabling highly accurate underwater scene reconstruction.
To address the initialization challenge in degraded underwater environments, we
propose the pseudo-depth Gaussian complementation to augment COLMAP-derived
point clouds with robust depth priors. In addition, a depth ranking regularized
loss is developed to optimize the geometry of the scene and improve the ordinal
consistency of the depth maps. Extensive experiments on real-world underwater
datasets demonstrate that our method achieves significant improvements in 3D
reconstruction. Furthermore, we conduct a simulated dataset with ground truth
and the controllable scattering medium to demonstrate the restoration
capability of our method in underwater scenarios. Our code and dataset are
available at https://plenodium.github.io/.

</details>


### [175] [DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution](https://arxiv.org/abs/2505.21262)
*M. Akin Yilmaz,Ahmet Bilican,A. Murat Tekalp*

Main category: cs.CV

TL;DR: DiMoSR提出了一种轻量级单图像超分辨率（SISR）架构，通过调制增强特征表示，结合多分支扩张卷积以高效捕获上下文信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 轻量级SISR中重建质量与模型效率的平衡是关键挑战，现有注意力机制主导的方法需要探索替代架构。

Method: 提出DiMoSR架构，利用多分支扩张卷积增强特征表示，同时保持计算效率。

Result: DiMoSR在多个基准数据集上优于现有轻量级方法，PSNR和SSIM指标更优，计算复杂度相当或更低。

Conclusion: DiMoSR验证了特征调制与注意力机制的互补性，为高效网络设计提供了新思路。

Abstract: Balancing reconstruction quality versus model efficiency remains a critical
challenge in lightweight single image super-resolution (SISR). Despite the
prevalence of attention mechanisms in recent state-of-the-art SISR approaches
that primarily emphasize or suppress feature maps, alternative architectural
paradigms warrant further exploration. This paper introduces DiMoSR (Dilated
Modulation Super-Resolution), a novel architecture that enhances feature
representation through modulation to complement attention in lightweight SISR
networks. The proposed approach leverages multi-branch dilated convolutions to
capture rich contextual information over a wider receptive field while
maintaining computational efficiency. Experimental results demonstrate that
DiMoSR outperforms state-of-the-art lightweight methods across diverse
benchmark datasets, achieving superior PSNR and SSIM metrics with comparable or
reduced computational complexity. Through comprehensive ablation studies, this
work not only validates the effectiveness of DiMoSR but also provides critical
insights into the interplay between attention mechanisms and feature modulation
to guide future research in efficient network design. The code and model
weights to reproduce our results are available at:
https://github.com/makinyilmaz/DiMoSR

</details>


### [176] [Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands](https://arxiv.org/abs/2505.21269)
*Eva Gmelich Meijling,Roberto Del Prete,Arnoud Visser*

Main category: cs.CV

TL;DR: 本文提出了一种结合监督学习和自监督学习的方法，用于湿地土地覆盖分类，解决了高分辨率卫星影像标注数据稀缺的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 湿地土地覆盖分类对环境监测和生态系统管理至关重要，但高分辨率卫星影像的标注数据稀缺，限制了监督学习的效果。

Method: 采用U-Net模型，结合监督学习和自监督学习（SSL）预训练，利用Sentinel-2影像在荷兰六个湿地区域进行训练和测试。

Result: 基线模型准确率为85.26%，自监督学习预训练后提升至88.23%。高分辨率影像提供了更清晰的边界和细节。

Conclusion: 该方法有效解决了标注数据稀缺问题，并公开了一个适用于湿地分类的Sentinel-2数据集。

Abstract: Accurate wetland land-cover classification is essential for environmental
monitoring, biodiversity assessment, and sustainable ecosystem management.
However, the scarcity of annotated data, especially for high-resolution
satellite imagery, poses a significant challenge for supervised learning
approaches. To tackle this issue, this study presents a methodology for wetland
land-cover segmentation and classification that adopts both supervised and
self-supervised learning (SSL). We train a U-Net model from scratch on
Sentinel-2 imagery across six wetland regions in the Netherlands, achieving a
baseline model accuracy of 85.26%.
  Addressing the limited availability of labeled data, the results show that
SSL pretraining with an autoencoder can improve accuracy, especially for the
high-resolution imagery where it is more difficult to obtain labeled data,
reaching an accuracy of 88.23%.
  Furthermore, we introduce a framework to scale manually annotated
high-resolution labels to medium-resolution inputs. While the quantitative
performance between resolutions is comparable, high-resolution imagery provides
significantly sharper segmentation boundaries and finer spatial detail.
  As part of this work, we also contribute a curated Sentinel-2 dataset with
Dynamic World labels, tailored for wetland classification tasks and made
publicly available.

</details>


### [177] [Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2505.21309)
*Zenghao Zheng,Lianping Yang,Hegui Zhu,Mingrui Ye*

Main category: cs.CV

TL;DR: 提出了一种基于谱压缩变换（SCT）和线姿态图（LPG）的双流网络架构，用于高效3D人体姿态估计，减少冗余并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Transformer在3D姿态估计中因自注意力的二次复杂度导致计算成本高，且现有方法未能有效消除序列冗余。

Method: 引入SCT通过离散余弦变换压缩序列长度，提出LPG补充结构信息，设计双流网络建模空间关节关系和运动轨迹。

Result: 在Human3.6M和MPI-INF-3DHP数据集上达到SOTA性能（MPJPE 37.7mm），计算效率显著提升。

Conclusion: SCT和LPG有效减少冗余并提升性能，双流网络设计进一步优化了模型表现。

Abstract: Transformer-based 3D human pose estimation methods suffer from high
computational costs due to the quadratic complexity of self-attention with
respect to sequence length. Additionally, pose sequences often contain
significant redundancy between frames. However, recent methods typically fail
to improve model capacity while effectively eliminating sequence redundancy. In
this work, we introduce the Spectral Compression Transformer (SCT) to reduce
sequence length and accelerate computation. The SCT encoder treats hidden
features between blocks as Temporal Feature Signals (TFS) and applies the
Discrete Cosine Transform, a Fourier transform-based technique, to determine
the spectral components to be retained. By filtering out certain high-frequency
noise components, SCT compresses the sequence length and reduces redundancy. To
further enrich the input sequence with prior structural information, we propose
the Line Pose Graph (LPG) based on line graph theory. The LPG generates
skeletal position information that complements the input 2D joint positions,
thereby improving the model's performance. Finally, we design a dual-stream
network architecture to effectively model spatial joint relationships and the
compressed motion trajectory within the pose sequence. Extensive experiments on
two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our
model achieves state-of-the-art performance with improved computational
efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE
of 37.7mm while maintaining a low computational cost. Furthermore, we perform
ablation studies on each module to assess its effectiveness. The code and
models will be released.

</details>


### [178] [Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism](https://arxiv.org/abs/2505.21316)
*Enam Ahmed Taufik,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 提出了一种结合Mid Point Normalization（MPN）和注意力机制的两阶段方法，显著提升了植物病害检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 植物病害检测面临标记数据稀缺和复杂环境因素的挑战，需要更智能的预处理和特征优化方法。

Method: 采用MPN进行图像预处理，结合Squeeze-and-Excitation（SE）注意力机制动态优化特征表示，并集成到U-Net架构中。

Result: 分类任务达到93%准确率，目标类F1分数完美；分割任务Dice分数72.44%，IoU 58.54%，显著优于基线方法。

Conclusion: 该方法不仅精度高，还计算高效，适合实际计算机视觉应用。

Abstract: Enhancing plant disease detection from leaf imagery remains a persistent
challenge due to scarce labeled data and complex contextual factors. We
introduce a transformative two-stage methodology, Mid Point Normalization (MPN)
for intelligent image preprocessing, coupled with sophisticated attention
mechanisms that dynamically recalibrate feature representations. Our
classification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks,
achieves remarkable 93% accuracy while maintaining exceptional class-wise
balance. The perfect F1 score attained for our target class exemplifies
attention's power in adaptive feature refinement. For segmentation tasks, we
seamlessly integrate identical attention blocks within U-Net architecture using
MPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice
score and 58.54% IoU, substantially outperforming baseline implementations.
Beyond superior accuracy metrics, our approach yields computationally
efficient, lightweight architectures perfectly suited for real-world computer
vision applications.

</details>


### [179] [MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on](https://arxiv.org/abs/2505.21325)
*Guangyuan Li,Siming Zheng,Hao Zhang,Jinwei Chen,Junsheng Luan,Binkai Ou,Lei Zhao,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicTryOn提出了一种基于视频扩散Transformer的视频虚拟试穿框架，解决了现有方法在时空一致性和服装内容保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频虚拟试穿方法在时空一致性、服装细节重建和动态一致性方面存在局限，影响了试穿效果的逼真度和稳定性。

Method: 采用扩散Transformer替代U-Net，结合全自注意力建模时空一致性；设计了粗到细的服装保留策略，并在去噪阶段引入多条件；还提出了掩码感知损失优化服装区域保真度。

Result: 在图像和视频试穿数据集上的实验表明，MagicTryOn在综合评估中优于现有SOTA方法，并能泛化到实际场景。

Conclusion: MagicTryOn通过改进架构和策略，显著提升了视频虚拟试穿的效果，具有实际应用潜力。

Abstract: Video Virtual Try-On (VVT) aims to simulate the natural appearance of
garments across consecutive video frames, capturing their dynamic variations
and interactions with human body motion. However, current VVT methods still
face challenges in terms of spatiotemporal consistency and garment content
preservation. First, they use diffusion models based on the U-Net, which are
limited in their expressive capability and struggle to reconstruct complex
details. Second, they adopt a separative modeling approach for spatial and
temporal attention, which hinders the effective capture of structural
relationships and dynamic consistency across frames. Third, their expression of
garment details remains insufficient, affecting the realism and stability of
the overall synthesized results, especially during human motion. To address the
above challenges, we propose MagicTryOn, a video virtual try-on framework built
upon the large-scale video diffusion Transformer.We replace the U-Net
architecture with a diffusion Transformer and combine full self-attention to
jointly model the spatiotemporal consistency of videos. We design a
coarse-to-fine garment preservation strategy. The coarse strategy integrates
garment tokens during the embedding stage, while the fine strategy incorporates
multiple garment-based conditions, such as semantics, textures, and contour
lines during the denoising stage. Moreover, we introduce a mask-aware loss to
further optimize garment region fidelity. Extensive experiments on both image
and video try-on datasets demonstrate that our method outperforms existing SOTA
methods in comprehensive evaluations and generalizes to in-the-wild scenarios.

</details>


### [180] [MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](https://arxiv.org/abs/2505.21333)
*Yang Shi,Huanqian Wang,Wulin Xie,Huanyao Zhang,Lijie Zhao,Yi-Fan Zhang,Xinfeng Li,Chaoyou Fu,Zhuoer Wen,Wenting Liu,Zhuoran Zhang,Xinlong Chen,Bohan Zeng,Sihan Yang,Yuanxing Zhang,Pengfei Wan,Haotian Wang,Wenjing Yang*

Main category: cs.CV

TL;DR: 论文介绍了MME-VideoOCR基准测试，用于评估多模态大语言模型（MLLMs）在视频OCR任务中的表现，发现现有模型在需要时空推理或跨帧信息整合的任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在静态图像OCR中表现良好，但在视频OCR中因运动模糊、时间变化等因素效果显著下降，需更全面的基准测试指导模型训练。

Method: 提出MME-VideoOCR基准，包含10类任务、25个具体任务和44种场景，涵盖1,464个视频和2,000个标注问答对，评估了18种MLLMs。

Result: 最佳模型（Gemini-2.5 Pro）准确率仅73.7%，模型在单帧任务中表现良好，但在需要时空推理或跨帧整合的任务中表现有限。

Conclusion: 高分辨率视觉输入和足够的时间覆盖对动态视频OCR至关重要，现有MLLMs需改进以应对复杂视频场景。

Abstract: Multimodal Large Language Models (MLLMs) have achieved considerable accuracy
in Optical Character Recognition (OCR) from static images. However, their
efficacy in video OCR is significantly diminished due to factors such as motion
blur, temporal variations, and visual effects inherent in video content. To
provide clearer guidance for training practical MLLMs, we introduce the
MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR
application scenarios. MME-VideoOCR features 10 task categories comprising 25
individual tasks and spans 44 diverse scenarios. These tasks extend beyond text
recognition to incorporate deeper comprehension and reasoning of textual
content within videos. The benchmark consists of 1,464 videos with varying
resolutions, aspect ratios, and durations, along with 2,000 meticulously
curated, manually annotated question-answer pairs. We evaluate 18
state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing
model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained
analysis indicates that while existing MLLMs demonstrate strong performance on
tasks where relevant texts are contained within a single or few frames, they
exhibit limited capability in effectively handling tasks that demand holistic
video comprehension. These limitations are especially evident in scenarios that
require spatio-temporal reasoning, cross-frame information integration, or
resistance to language prior bias. Our findings also highlight the importance
of high-resolution visual input and sufficient temporal coverage for reliable
OCR in dynamic video scenarios.

</details>


### [181] [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)
*Kele Shao,Keda Tao,Can Qin,Haoxuan You,Yang Sui,Huan Wang*

Main category: cs.CV

TL;DR: HoliTom是一个无需训练的全新视频令牌合并框架，通过全局冗余感知时间分割和时空合并，显著减少视频令牌，同时结合内-LLM令牌相似性合并方法，实现了高效的计算性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM令牌剪枝方法存在计算效率低下的问题，内-LLM剪枝方法在浅层引入计算开销，外-LLM剪枝方法忽视全局时间动态性，导致次优的时空压缩效果。

Method: 提出HoliTom框架，采用外-LLM剪枝（全局冗余感知时间分割和时空合并）和内-LLM令牌相似性合并方法，减少90%以上视觉令牌。

Result: 在LLaVA-OneVision-7B上，计算成本降至6.9% FLOPs，保持99.1%原始性能，TTFT减少2.28倍，解码吞吐量加速1.32倍。

Conclusion: HoliTom通过结合内外-LLM剪枝策略，显著提升视频LLM的计算效率，同时保持高性能。

Abstract: Video large language models (video LLMs) excel at video comprehension but
face significant computational inefficiency due to redundant video tokens.
Existing token pruning methods offer solutions. However, approaches operating
within the LLM (inner-LLM pruning), such as FastV, incur intrinsic
computational overhead in shallow layers. In contrast, methods performing token
pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy
within individual frames or limited temporal windows, neglecting the crucial
global temporal dynamics and correlations across longer video sequences. This
leads to sub-optimal spatio-temporal reduction and does not leverage video
compressibility fully. Crucially, the synergistic potential and mutual
influence of combining these strategies remain unexplored. To further reduce
redundancy, we introduce HoliTom, a novel training-free holistic token merging
framework. HoliTom employs outer-LLM pruning through global redundancy-aware
temporal segmentation, followed by spatial-temporal merging to reduce visual
tokens by over 90%, significantly alleviating the LLM's computational burden.
Complementing this, we introduce a robust inner-LLM token similarity-based
merging approach, designed for superior performance and compatibility with
outer-LLM pruning. Evaluations demonstrate our method's promising
efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational
costs to 6.9% of FLOPs while maintaining 99.1% of the original performance.
Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a
1.32x acceleration in decoding throughput, highlighting the practical benefits
of our integrated pruning approach for efficient video LLMs inference.

</details>


### [182] [Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning](https://arxiv.org/abs/2505.21338)
*Katarzyna Filus,Mateusz Żarski*

Main category: cs.CV

TL;DR: 论文提出Deep Similarity Inspector (DSI)框架，研究深度视觉网络如何发展相似性感知及其与语义相似性的对齐。实验发现CNN和ViT在训练中经历三个阶段（初始相似性激增、细化、稳定），并观察到错误细化现象。


<details>
  <summary>Details</summary>
Motivation: 研究深度视觉网络中相似性感知的形成及其与语义相似性的关系，填补该领域的研究空白。

Method: 引入DSI框架，系统分析CNN和ViT在训练过程中相似性感知的发展阶段和特点。

Result: CNN和ViT在训练中经历三个阶段，且两者存在明显差异，同时观察到错误细化现象。

Conclusion: DSI框架揭示了深度视觉网络相似性感知的发展规律，为未来研究提供了新视角。

Abstract: Similarity manifests in various forms, including semantic similarity that is
particularly important, serving as an approximation of human object
categorization based on e.g. shared functionalities and evolutionary traits. It
also offers practical advantages in computational modeling via lexical
structures such as WordNet with constant and interpretable similarity. As in
the domain of deep vision, there is still not enough focus on the phenomena
regarding the similarity perception emergence. We introduce Deep Similarity
Inspector (DSI) -- a systematic framework to inspect how deep vision networks
develop their similarity perception and its alignment with semantic similarity.
Our experiments show that both Convolutional Neural Networks' (CNNs) and Vision
Transformers' (ViTs) develop a rich similarity perception during training with
3 phases (initial similarity surge, refinement, stabilization), with clear
differences between CNNs and ViTs. Besides the gradual mistakes elimination,
the mistakes refinement phenomenon can be observed.

</details>


### [183] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/abs/2505.21357)
*Wenyuan Li,Shunlin Liang,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Husheng Fang,Zhenwei Shi*

Main category: cs.CV

TL;DR: AgriFM是一种专为农业作物测绘设计的遥感基础模型，通过同步时空特征提取和动态解码器架构，显著提升了作物测绘的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型在作物测绘中表现不佳，主要因为固定的时空窗口或忽视时间信息。AgriFM旨在解决这些问题。

Method: 采用改进的Video Swin Transformer架构，同步时空下采样，结合多源卫星数据（MODIS、Landsat-8/9、Sentinel-2）进行预训练。

Result: AgriFM在多种下游任务中表现优于传统深度学习和现有遥感基础模型。

Conclusion: AgriFM通过高效的时空特征提取和多源数据融合，为农业作物测绘提供了更优的解决方案。

Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale
spatiotemporal patterns, where spatial scales range from individual field
textures to landscape-level context, and temporal scales capture both
short-term phenological transitions and full growing-season dynamics.
Transformer-based remote sensing foundation models (RSFMs) offer promising
potential for crop mapping due to their innate ability for unified
spatiotemporal processing. However, current RSFMs remain suboptimal for crop
mapping: they either employ fixed spatiotemporal windows that ignore the
multi-scale nature of crop systems or completely disregard temporal information
by focusing solely on spatial patterns. To bridge these gaps, we present
AgriFM, a multi-source remote sensing foundation model specifically designed
for agricultural crop mapping. Our approach begins by establishing the
necessity of simultaneous hierarchical spatiotemporal feature extraction,
leading to the development of a modified Video Swin Transformer architecture
where temporal down-sampling is synchronized with spatial scaling operations.
This modified backbone enables efficient unified processing of long time-series
satellite inputs. AgriFM leverages temporally rich data streams from three
satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is
pre-trained on a global representative dataset comprising over 25 million image
samples supervised by land cover products. The resulting framework incorporates
a versatile decoder architecture that dynamically fuses these learned
spatiotemporal representations, supporting diverse downstream tasks.
Comprehensive evaluations demonstrate AgriFM's superior performance over
conventional deep learning approaches and state-of-the-art general-purpose
RSFMs across all downstream tasks. Codes will be available at
urlhttps://github.com/flyakon/AgriFM.

</details>


### [184] [YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration](https://arxiv.org/abs/2505.21370)
*Xinyuan Wang,Lian Peng,Xiangcheng Li,Yilin He,KinTak U*

Main category: cs.CV

TL;DR: YOLO-SPCI是一种基于YOLOv8的改进检测框架，通过引入轻量级SPCI模块增强多尺度特征表示，在遥感图像检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中的目标检测面临尺度变化大、目标密集和背景复杂等挑战，现有检测器缺乏多尺度特征优化机制。

Method: 提出YOLO-SPCI框架，包含SPCI模块（SSG、PFM、CDM），嵌入YOLOv8的P3和P5阶段，优化特征表示。

Result: 在NWPU VHR-10数据集上，YOLO-SPCI性能优于现有先进检测器。

Conclusion: YOLO-SPCI通过多尺度特征优化显著提升了遥感图像目标检测性能。

Abstract: Object detection in remote sensing imagery remains a challenging task due to
extreme scale variation, dense object distributions, and cluttered backgrounds.
While recent detectors such as YOLOv8 have shown promising results, their
backbone architectures lack explicit mechanisms to guide multi-scale feature
refinement, limiting performance on high-resolution aerial data. In this work,
we propose YOLO-SPCI, an attention-enhanced detection framework that introduces
a lightweight Selective-Perspective-Class Integration (SPCI) module to improve
feature representation. The SPCI module integrates three components: a
Selective Stream Gate (SSG) for adaptive regulation of global feature flow, a
Perspective Fusion Module (PFM) for context-aware multi-scale integration, and
a Class Discrimination Module (CDM) to enhance inter-class separability. We
embed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,
enabling effective refinement while preserving compatibility with the original
neck and head. Experiments on the NWPU VHR-10 dataset demonstrate that
YOLO-SPCI achieves superior performance compared to state-of-the-art detectors.

</details>


### [185] [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/abs/2505.21374)
*Junhao Cheng,Yuying Ge,Teng Wang,Yixiao Ge,Jing Liao,Ying Shan*

Main category: cs.CV

TL;DR: Video-Holmes是一个新的视频推理基准，旨在评估多模态大语言模型（MLLMs）在复杂视频推理中的表现，发现现有模型在信息整合和线索连接方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估视觉感知和基础能力，无法全面反映真实世界推理的复杂性，因此需要设计更接近人类推理过程的评测标准。

Method: 基于270部悬疑短片的1,837个问题，设计了7个任务，要求模型主动定位并整合分散在不同视频片段中的视觉线索。

Result: 现有MLLMs在视觉感知上表现良好，但在信息整合和线索连接上表现不佳，最佳模型Gemini-2.5-Pro准确率仅为45%。

Conclusion: Video-Holmes可作为多模态推理的“Holmes测试”，推动模型更接近人类推理能力，并突显该领域的挑战。

Abstract: Recent advances in CoT reasoning and RL post-training have been reported to
enhance video reasoning capabilities of MLLMs. This progress naturally raises a
question: can these models perform complex video reasoning in a manner
comparable to human experts? However, existing video benchmarks primarily
evaluate visual perception and grounding abilities, with questions that can be
answered based on explicit prompts or isolated visual cues. Such benchmarks do
not fully capture the intricacies of real-world reasoning, where humans must
actively search for, integrate, and analyze multiple clues before reaching a
conclusion. To address this issue, we present Video-Holmes, a benchmark
inspired by the reasoning process of Sherlock Holmes, designed to evaluate the
complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837
questions derived from 270 manually annotated suspense short films, which spans
seven carefully designed tasks. Each task is constructed by first identifying
key events and causal relationships within films, and then designing questions
that require models to actively locate and connect multiple relevant visual
clues scattered across different video segments. Our comprehensive evaluation
of state-of-the-art MLLMs reveals that, while these models generally excel at
visual perception, they encounter substantial difficulties with integrating
information and often miss critical clues. For example, the best-performing
model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models
scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for
multimodal reasoning, motivating models to reason more like humans and
emphasizing the ongoing challenges in this field. The benchmark is released in
https://github.com/TencentARC/Video-Holmes.

</details>


### [186] [GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution](https://arxiv.org/abs/2505.21375)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Di Wang,Haotian Wang,Zonghao Guo,Zefan Wang,Boqi Shan,Long Lan,Yulin Wang,Hongzhen Wang,Wenjing Yang,Bo Du,Jing Zhang*

Main category: cs.CV

TL;DR: 论文提出GeoLLaVA-8K模型，通过新数据集和优化策略解决超高分辨率遥感图像处理中的数据和计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像数据稀缺且处理时存在token爆炸问题，限制了多模态基础模型的应用。

Method: 引入SuperRS-VQA和HighRS-VQA数据集，并提出背景token修剪和锚定token选择策略以减少计算负担。

Result: GeoLLaVA-8K模型在XLRS-Bench上达到新SOTA，能处理8K×8K分辨率输入。

Conclusion: 新数据集和优化策略有效解决了超高分辨率遥感图像处理的挑战，推动了相关领域的发展。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data
for Earth observation but pose challenges for existing multimodal foundation
models due to two key bottlenecks: (1) limited availability of UHR training
data, and (2) token explosion caused by the large image size. To address data
scarcity, we introduce SuperRS-VQA (avg. 8,376$\times$8,376) and HighRS-VQA
(avg. 2,000$\times$1,912), the highest-resolution vision-language datasets in
RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,
our pilot studies reveal significant redundancy in RS images: crucial
information is concentrated in a small subset of object-centric tokens, while
pruning background tokens (e.g., ocean or forest) can even improve performance.
Motivated by these findings, we propose two strategies: Background Token
Pruning and Anchored Token Selection, to reduce the memory footprint while
preserving key semantics.Integrating these techniques, we introduce
GeoLLaVA-8K, the first RS-focused multimodal large language model capable of
handling inputs up to 8K$\times$8K resolution, built on the LLaVA framework.
Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art
on the XLRS-Bench.

</details>


### [187] [Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility](https://arxiv.org/abs/2505.21377)
*Yidi Li,Jun Xiao,Zhengda Lu,Yiqun Wang,Haiyong Jiang*

Main category: cs.CV

TL;DR: Dream3DVG是一种新颖的文本到矢量图形生成方法，支持任意视角查看、渐进细节优化和视角相关遮挡感知。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示与矢量图形之间的领域差距，提供更一致的指导和渐进细节控制。

Method: 采用双分支优化框架，包括3D高斯泼溅优化分支和3D矢量图形优化分支，并引入可见性感知渲染模块。

Result: 在3D草图和3D图标上展示了方法在细节抽象、跨视角一致性和遮挡感知笔画剔除方面的优越性。

Conclusion: Dream3DVG在生成矢量图形时表现出色，尤其在细节控制和遮挡处理方面具有优势。

Abstract: This work presents a novel text-to-vector graphics generation approach,
Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail
optimization, and view-dependent occlusion awareness. Our approach is a
dual-branch optimization framework, consisting of an auxiliary 3D Gaussian
Splatting optimization branch and a 3D vector graphics optimization branch. The
introduced 3DGS branch can bridge the domain gaps between text prompts and
vector graphics with more consistent guidance. Moreover, 3DGS allows for
progressive detail control by scheduling classifier-free guidance, facilitating
guiding vector graphics with coarse shapes at the initial stages and finer
details at later stages. We also improve the view-dependent occlusions by
devising a visibility-awareness rendering module. Extensive results on 3D
sketches and 3D iconographies, demonstrate the superiority of the method on
different abstraction levels of details, cross-view consistency, and
occlusion-aware stroke culling.

</details>


### [188] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/abs/2505.21381)
*Linshuang Diao,Dayong Ren,Sensen Song,Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba通过改进扫描路径和语义掩码策略，提升了点云自监督学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有PointMamba方法依赖复杂标记排序和随机掩码，破坏了空间连续性和局部语义相关性。

Method: 提出Zigzag扫描路径和语义孪生掩码策略（SMS），增强空间连续性和局部语义建模。

Result: 在多个下游任务中表现优异，如ShapeNetPart分割提升1.59% mIoU，ModelNet40分类提升0.4%准确率。

Conclusion: ZigzagPointMamba显著提升了点云自监督学习的性能，适用于多种下游任务。

Abstract: State Space models (SSMs) such as PointMamba enable efficient feature
extraction for point cloud self-supervised learning with linear complexity,
outperforming Transformers in computational efficiency. However, existing
PointMamba-based methods depend on complex token ordering and random masking,
which disrupt spatial continuity and local semantic correlations. We propose
ZigzagPointMamba to tackle these challenges. The core of our approach is a
simple zigzag scan path that globally sequences point cloud tokens, enhancing
spatial continuity by preserving the proximity of spatially adjacent point
tokens. Nevertheless, random masking undermines local semantic modeling in
self-supervised learning. To address this, we introduce a Semantic-Siamese
Masking Strategy (SMS), which masks semantically similar tokens to facilitate
reconstruction by integrating local features of original and similar tokens.
This overcomes the dependence on isolated local features and enables robust
global semantic modeling. Our pre-trained ZigzagPointMamba weights
significantly improve downstream tasks, achieving a 1.59% mIoU gain on
ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for
classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for
the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of
ScanObjectNN. The code is available at:
https://anonymous.4open.science/r/ZigzagPointMamba-1800/

</details>


### [189] [Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios](https://arxiv.org/abs/2505.21387)
*Xihong Yang,Siwei Wang,Fangdi Wang,Jiaqi Jin,Suyuan Liu,Yue Liu,En Zhu,Xinwang Liu,Yueming Jin*

Main category: cs.CV

TL;DR: AIRMVC是一个新的多视图聚类框架，通过自动识别和校正噪声数据，提高了在噪声场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多视图数据常受噪声影响，现有方法假设视图干净，导致性能下降。

Method: 将噪声识别建模为异常识别问题（GMM），设计混合校正策略，并引入噪声鲁棒对比机制。

Result: 在六个基准数据集上，AIRMVC在噪声场景下优于现有算法。

Conclusion: AIRMVC能有效丢弃噪声信息，提升下游任务性能，代码已开源。

Abstract: Leveraging the powerful representation learning capabilities, deep multi-view
clustering methods have demonstrated reliable performance by effectively
integrating multi-source information from diverse views in recent years. Most
existing methods rely on the assumption of clean views. However, noise is
pervasive in real-world scenarios, leading to a significant degradation in
performance. To tackle this problem, we propose a novel multi-view clustering
framework for the automatic identification and rectification of noisy data,
termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly
identification problem using GMM. We then design a hybrid rectification
strategy to mitigate the adverse effects of noisy data based on the
identification results. Furthermore, we introduce a noise-robust contrastive
mechanism to generate reliable representations. Additionally, we provide a
theoretical proof demonstrating that these representations can discard noisy
information, thereby improving the performance of downstream tasks. Extensive
experiments on six benchmark datasets demonstrate that AIRMVC outperforms
state-of-the-art algorithms in terms of robustness in noisy scenarios. The code
of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.

</details>


### [190] [Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning](https://arxiv.org/abs/2505.21420)
*Jinbao Wang,Hanzhe Liang,Can Gao,Chenxi Hu,Jie Zhou,Yunkang Cao,Linlin Shen,Weiming Shen*

Main category: cs.CV

TL;DR: 提出了一种名为Mentor3AD的新方法，通过多模态导师学习提升3D异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用多模态互补信息改进3D异常检测，通过导师学习进一步区分正常与异常特征。

Method: 设计了融合模块（MFM）生成导师特征，指导模块（MGM）支持跨模态重建，投票模块（VM）生成最终异常分数。

Result: 在MVTec 3D-AD和Eyecandies数据集上验证了方法的有效性。

Conclusion: Mentor3AD通过多模态导师学习显著提升了3D异常检测性能。

Abstract: Multimodal feature reconstruction is a promising approach for 3D anomaly
detection, leveraging the complementary information from dual modalities. We
further advance this paradigm by utilizing multi-modal mentor learning, which
fuses intermediate features to further distinguish normal from feature
differences. To address these challenges, we propose a novel method called
Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared
features of different modalities, Mentor3AD can extract more effective features
and guide feature reconstruction, ultimately improving detection performance.
Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges
features extracted from RGB and 3D modalities to create a mentor feature.
Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate
cross-modal reconstruction, supported by the mentor feature. Lastly, we
introduce a Voting Module (VM) to more accurately generate the final anomaly
score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies
have verified the effectiveness of the proposed method.

</details>


### [191] [OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers](https://arxiv.org/abs/2505.21448)
*Ziqiao Peng,Jiwen Liu,Haoxian Zhang,Xiaoqiang Liu,Songlin Tang,Pengfei Wan,Di Zhang,Hongyan Liu,Jun He*

Main category: cs.CV

TL;DR: OmniSync是一个通用的唇同步框架，通过无掩码训练和动态时空分类器自由引导机制，显著提升了唇同步的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有唇同步方法依赖参考帧和掩码修复，限制了其对身份一致性、姿态变化、面部遮挡和风格化内容的鲁棒性。音频信号的条件较弱，原始视频的唇形泄漏也会影响同步质量。

Method: OmniSync采用无掩码训练范式，使用扩散Transformer模型直接编辑帧，无需显式掩码。提出基于流匹配的渐进噪声初始化保持姿态和身份一致性，并开发动态时空分类器自由引导机制（DS-CFG）增强音频条件。

Result: OmniSync在视觉质量和唇同步准确性上显著优于现有方法，在真实和AI生成视频中均表现优异。

Conclusion: OmniSync通过创新方法解决了唇同步中的关键挑战，为多样化视觉场景提供了高效解决方案。

Abstract: Lip synchronization is the task of aligning a speaker's lip movements in
video with corresponding speech audio, and it is essential for creating
realistic, expressive video content. However, existing methods often rely on
reference frames and masked-frame inpainting, which limit their robustness to
identity consistency, pose variations, facial occlusions, and stylized content.
In addition, since audio signals provide weaker conditioning than visual cues,
lip shape leakage from the original video will affect lip sync quality. In this
paper, we present OmniSync, a universal lip synchronization framework for
diverse visual scenarios. Our approach introduces a mask-free training paradigm
using Diffusion Transformer models for direct frame editing without explicit
masks, enabling unlimited-duration inference while maintaining natural facial
dynamics and preserving character identity. During inference, we propose a
flow-matching-based progressive noise initialization to ensure pose and
identity consistency, while allowing precise mouth-region editing. To address
the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal
Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance
strength over time and space. We also establish the AIGC-LipSync Benchmark, the
first evaluation suite for lip synchronization in diverse AI-generated videos.
Extensive experiments demonstrate that OmniSync significantly outperforms prior
methods in both visual quality and lip sync accuracy, achieving superior
results in both real-world and AI-generated videos.

</details>


### [192] [Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations](https://arxiv.org/abs/2505.21454)
*Yue Li Du,Ben Alexander,Mikhail Antonenka,Rohan Mahadev,Hao-yu Wu,Dmitry Kislyuk*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉产品图（VPG）的实时检索系统，用于检索语义相似但视觉不同的内容，并结合上下文和互补产品推荐。


<details>
  <summary>Details</summary>
Motivation: 解决视觉搜索系统中检索语义相似但视觉不同内容的问题，并提供上下文和互补产品推荐。

Method: 利用高性能基础设施和先进计算机视觉模型构建VPG，改进对象检测、视觉嵌入等核心模块。

Result: 系统在端到端人类相关性评估中达到78.8%的极相似@1，模块参与率为6%。

Conclusion: VPG技术已在Pinterest的“Ways to Style It”模块中部署，展示了其实际应用价值。

Abstract: Retrieving semantically similar but visually distinct contents has been a
critical capability in visual search systems. In this work, we aim to tackle
this problem with Visual Product Graph (VPG), leveraging high-performance
infrastructure for storage and state-of-the-art computer vision models for
image understanding. VPG is built to be an online real-time retrieval system
that enables navigation from individual products to composite scenes containing
those products, along with complementary recommendations. Our system not only
offers contextual insights by showcasing how products can be styled in a
context, but also provides recommendations for complementary products drawn
from these inspirations. We discuss the essential components for building the
Visual Product Graph, along with the core computer vision model improvements
across object detection, foundational visual embeddings, and other visual
signals. Our system achieves a 78.8% extremely similar@1 in end-to-end human
relevance evaluations, and a 6% module engagement rate. The "Ways to Style It"
module, powered by the Visual Product Graph technology, is deployed in
production at Pinterest.

</details>


### [193] [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/abs/2505.21457)
*Muzhi Zhu,Hao Zhong,Canyu Zhao,Zongze Du,Zheng Huang,Mingyu Liu,Hao Chen,Cheng Zou,Jingdong Chen,Ming Yang,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的框架ACTIVE-O3，旨在为多模态大语言模型（MLLMs）赋予主动感知能力，并通过全面基准测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在机器人系统中作为规划和决策模块受到广泛关注，但其主动感知能力的研究仍不足。本文旨在填补这一空白。

Method: 提出ACTIVE-O3框架，基于GRPO的强化学习方法，训练MLLMs实现高效主动感知。

Result: ACTIVE-O3在通用和领域特定任务中表现优异，并展示了零样本推理能力。

Conclusion: ACTIVE-O3为MLLMs的主动感知研究提供了简单工具和评估标准，推动了该领域的未来发展。

Abstract: Active vision, also known as active perception, refers to the process of
actively selecting where and how to look in order to gather task-relevant
information. It is a critical component of efficient perception and
decision-making in humans and advanced embodied agents. Recently, the use of
Multimodal Large Language Models (MLLMs) as central planning and
decision-making modules in robotic systems has gained extensive attention.
However, despite the importance of active perception in embodied intelligence,
there is little to no exploration of how MLLMs can be equipped with or learn
active perception capabilities. In this paper, we first provide a systematic
definition of MLLM-based active perception tasks. We point out that the
recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a
special case of active perception; however, it still suffers from low search
efficiency and inaccurate region selection. To address these issues, we propose
ACTIVE-O3, a purely reinforcement learning based training framework built on
top of GRPO, designed to equip MLLMs with active perception capabilities. We
further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across
both general open-world tasks, such as small-object and dense object grounding,
and domain-specific scenarios, including small object detection in remote
sensing and autonomous driving, as well as fine-grained interactive
segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot
reasoning abilities on the V* Benchmark, without relying on any explicit
reasoning data. We hope that our work can provide a simple codebase and
evaluation protocol to facilitate future research on active perception in
MLLMs.

</details>


### [194] [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
*Bozhou Li,Wentao Zhang*

Main category: cs.CV

TL;DR: 论文提出ID-Align方法，通过重新排序位置ID解决高分辨率与缩略图令牌间交互问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法同时编码高分辨率和缩略图图像会生成大量令牌，结合RoPE的长距离衰减特性，阻碍了令牌间及文本与图像的交互。

Method: 提出ID-Align，通过让高分辨率令牌继承对应缩略图令牌的位置ID，并限制位置索引的过度扩展。

Result: 在LLaVA-Next框架下实验，ID-Align在MMBench关系推理任务上提升6.09%，并在多个基准测试中表现优异。

Conclusion: ID-Align有效解决了令牌交互问题，显著提升了模型性能。

Abstract: Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)
performance is to encode both the high-resolution version and the thumbnail of
an image simultaneously. While effective, this method generates a large number
of image tokens. When combined with the widely used Rotary Position Embedding
(RoPE), its long-term decay property hinders the interaction between
high-resolution tokens and thumbnail tokens, as well as between text and image.
To address these issues, we propose ID-Align, which alleviates these problems
by reordering position IDs. In this method, high-resolution tokens inherit IDs
from their corresponding thumbnail token while constraining the overexpansion
of positional indices. Our experiments conducted within the LLaVA-Next
framework demonstrate that ID-Align achieves significant improvements,
including a 6.09% enhancement on MMBench's relation reasoning tasks and notable
gains across multiple benchmarks. Our code is available at the following link:
https://github.com/zooblastlbz/ID-Align.

</details>


### [195] [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: CAAC框架通过视觉标记校准和自适应注意力重缩放，有效减少大型视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现优异，但常出现幻觉问题，即错误描述图像中不存在的对象或属性。现有方法在开放性和长文本生成场景中难以保持准确性。

Method: CAAC框架通过两步解决：视觉标记校准（VTC）平衡视觉标记的注意力，自适应注意力重缩放（AAR）基于模型置信度强化视觉基础。

Result: 在CHAIR、AMBER和POPE基准测试中，CAAC优于基线方法，尤其在长文本生成中显著减少幻觉。

Conclusion: CAAC通过置信度驱动的调整，在多模态任务中实现了更一致的视觉对齐，有效减少幻觉。

Abstract: Large vision-language models (LVLMs) achieve impressive performance on
multimodal tasks but often suffer from hallucination, and confidently describe
objects or attributes not present in the image. Current inference-time
interventions, while training-free, struggle to maintain accuracy in open-ended
and long-form generation scenarios. We introduce the Confidence-Aware Attention
Calibration (CAAC) framework to address this challenge by targeting two key
biases: spatial perception bias, which distributes attention disproportionately
across image tokens, and modality bias, which shifts focus from visual to
textual inputs over time. CAAC employs a two-step approach: Visual-Token
Calibration (VTC) to balance attention across visual tokens, and Adaptive
Attention Re-Scaling (AAR) to reinforce visual grounding based on the model's
confidence. This confidence-driven adjustment ensures consistent visual
alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks
demonstrate that CAAC outperforms baselines, particularly in long-form
generations, effectively reducing hallucination.

</details>


### [196] [DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction](https://arxiv.org/abs/2505.21473)
*Yiheng Liu,Liao Qu,Huichao Zhang,Xu Wang,Yi Jiang,Yiming Gao,Hu Ye,Xian Li,Shuai Wang,Daniel K. Du,Shu Cheng,Zehuan Yuan,Xinglong Wu*

Main category: cs.CV

TL;DR: DetailFlow是一种从粗到细的1D自回归图像生成方法，通过渐进式细节预测策略生成高质量图像，显著减少token数量并提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有自回归图像生成方法（如VAR/VQGAN）需要大量token且生成效率低，DetailFlow旨在通过更自然的1D token序列和并行推理机制解决这些问题。

Method: DetailFlow采用分辨率感知的token序列，通过渐进式降质图像监督学习，结合并行推理和自我校正机制，实现高效生成。

Result: 在ImageNet 256x256基准测试中，DetailFlow仅用128个token达到2.96 gFID，优于VAR（3.3 FID）和FlexVAR（3.05 FID），且生成速度快2倍。

Conclusion: DetailFlow在生成质量和效率上优于现有方法，为自回归图像生成提供了更高效的解决方案。

Abstract: This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image
generation method that models images through a novel next-detail prediction
strategy. By learning a resolution-aware token sequence supervised with
progressively degraded images, DetailFlow enables the generation process to
start from the global structure and incrementally refine details. This
coarse-to-fine 1D token sequence aligns well with the autoregressive inference
mechanism, providing a more natural and efficient way for the AR model to
generate complex visual content. Our compact 1D AR model achieves high-quality
image synthesis with significantly fewer tokens than previous approaches, i.e.
VAR/VQGAN. We further propose a parallel inference mechanism with
self-correction that accelerates generation speed by approximately 8x while
reducing accumulation sampling error inherent in teacher-forcing supervision.
On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128
tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require
680 tokens in their AR models. Moreover, due to the significantly reduced token
count and parallel inference mechanism, our method runs nearly 2x faster
inference speed compared to VAR and FlexVAR. Extensive experimental results
demonstrate DetailFlow's superior generation quality and efficiency compared to
existing state-of-the-art methods.

</details>


### [197] [Policy Optimized Text-to-Image Pipeline Design](https://arxiv.org/abs/2505.21478)
*Uri Gadot,Rinon Gal,Yftah Ziser,Gal Chechik,Shie Mannor*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习的框架，用于优化文本到图像生成的多组件流程，解决了现有方法计算成本高和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成的多组件流程设计需要大量专业知识，且现有自动化方法计算成本高、泛化能力不足。

Method: 采用强化学习框架，训练奖励模型预测图像质量分数，结合两阶段训练策略（初始词汇训练和GRPO优化）及无分类器引导增强技术。

Result: 实验表明，该方法能生成更多样化的流程，并显著提升图像质量。

Conclusion: 提出的框架有效解决了现有方法的局限性，为文本到图像生成流程的自动化设计提供了新思路。

Abstract: Text-to-image generation has evolved beyond single monolithic models to
complex multi-component pipelines. These combine fine-tuned generators,
adapters, upscaling blocks and even editing steps, leading to significant
improvements in image quality. However, their effective design requires
substantial expertise. Recent approaches have shown promise in automating this
process through large language models (LLMs), but they suffer from two critical
limitations: extensive computational requirements from generating images with
hundreds of predefined pipelines, and poor generalization beyond memorized
training examples. We introduce a novel reinforcement learning-based framework
that addresses these inefficiencies. Our approach first trains an ensemble of
reward models capable of predicting image quality scores directly from
prompt-workflow combinations, eliminating the need for costly image generation
during training. We then implement a two-phase training strategy: initial
workflow vocabulary training followed by GRPO-based optimization that guides
the model toward higher-performing regions of the workflow space. Additionally,
we incorporate a classifier-free guidance based enhancement technique that
extrapolates along the path between the initial and GRPO-tuned models, further
improving output quality. We validate our approach through a set of
comparisons, showing that it can successfully create new flows with greater
diversity and lead to superior image quality compared to existing baselines.

</details>


### [198] [MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation](https://arxiv.org/abs/2505.21483)
*Kerui Ren,Jiayang Bai,Linning Xu,Lihan Jiang,Jiangmiao Pang,Mulin Yu,Bo Dai*

Main category: cs.CV

TL;DR: MV-CoLight是一个两阶段框架，用于在2D图像和3D场景中实现光照一致的对象合成，解决了多视角一致性和复杂场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单图像场景或固有分解技术上存在局限性，难以处理多视角一致性、复杂场景和多样化光照条件。

Method: 采用新颖的前馈架构直接建模光照和阴影，避免基于扩散方法的迭代偏差，并使用Hilbert曲线映射将2D输入与3D高斯场景表示对齐。

Result: 实验表明，MV-CoLight在标准基准和真实场景中均实现了最先进的和谐效果，展现了框架的鲁棒性和广泛泛化能力。

Conclusion: MV-CoLight通过高效的两阶段框架，显著提升了光照一致对象合成的适用性和性能。

Abstract: Object compositing offers significant promise for augmented reality (AR) and
embodied intelligence applications. Existing approaches predominantly focus on
single-image scenarios or intrinsic decomposition techniques, facing challenges
with multi-view consistency, complex scenes, and diverse lighting conditions.
Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based
methods, have enhanced consistency but are limited by scalability, heavy data
requirements, or prolonged reconstruction time per scene. To broaden its
applicability, we introduce MV-CoLight, a two-stage framework for
illumination-consistent object compositing in both 2D images and 3D scenes. Our
novel feed-forward architecture models lighting and shadows directly, avoiding
the iterative biases of diffusion-based methods. We employ a Hilbert
curve-based mapping to align 2D image inputs with 3D Gaussian scene
representations seamlessly. To facilitate training and evaluation, we further
introduce a large-scale 3D compositing dataset. Experiments demonstrate
state-of-the-art harmonized results across standard benchmarks and our dataset,
as well as casually captured real-world scenes demonstrate the framework's
robustness and wide generalization.

</details>


### [199] [Be Decisive: Noise-Induced Layouts for Multi-Subject Generation](https://arxiv.org/abs/2505.21488)
*Omer Dahary,Yehonathan Cohen,Or Patashnik,Kfir Aberman,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 提出一种新方法，通过预测与初始噪声对齐的空间布局并逐步优化，解决多主体生成中的主体泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在多主体生成中存在主体泄漏问题，外部布局控制常与模型先验冲突。

Method: 利用小型神经网络预测并优化基于初始噪声的空间布局，避免与外部布局冲突。

Result: 实验表明，该方法在文本-图像对齐和多主体生成稳定性上优于现有布局引导技术。

Conclusion: 该方法有效解决了主体泄漏问题，同时保持了模型的多样性。

Abstract: Generating multiple distinct subjects remains a challenge for existing
text-to-image diffusion models. Complex prompts often lead to subject leakage,
causing inaccuracies in quantities, attributes, and visual features. Preventing
leakage among subjects necessitates knowledge of each subject's spatial
location. Recent methods provide these spatial locations via an external layout
control. However, enforcing such a prescribed layout often conflicts with the
innate layout dictated by the sampled initial noise, leading to misalignment
with the model's prior. In this work, we introduce a new approach that predicts
a spatial layout aligned with the prompt, derived from the initial noise, and
refines it throughout the denoising process. By relying on this noise-induced
layout, we avoid conflicts with externally imposed layouts and better preserve
the model's prior. Our method employs a small neural network to predict and
refine the evolving noise-induced layout at each denoising step, ensuring clear
boundaries between subjects while maintaining consistency. Experimental results
show that this noise-aligned strategy achieves improved text-image alignment
and more stable multi-subject generation compared to existing layout-guided
techniques, while preserving the rich diversity of the model's original
distribution.

</details>


### [200] [Frame In-N-Out: Unbounded Controllable Image-to-Video Generation](https://arxiv.org/abs/2505.21491)
*Boyang Wang,Xuweiyi Chen,Matheus Gadelha,Zezhou Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于用户指定运动轨迹的图像到视频生成方法，解决了视频生成中的可控性、时间一致性和细节合成问题。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中可控性、时间一致性和细节合成的关键挑战，特别是通过探索Frame In和Frame Out这一电影技术。

Method: 提出了一种新的半自动数据集、评估协议，以及一种高效的基于Diffusion Transformer的身份保持运动可控视频生成架构。

Result: 实验表明，该方法显著优于现有基线。

Conclusion: 通过用户控制的运动轨迹，实现了高质量的视频生成，为视频生成领域提供了新的解决方案。

Abstract: Controllability, temporal coherence, and detail synthesis remain the most
critical challenges in video generation. In this paper, we focus on a commonly
used yet underexplored cinematic technique known as Frame In and Frame Out.
Specifically, starting from image-to-video generation, users can control the
objects in the image to naturally leave the scene or provide breaking new
identity references to enter the scene, guided by user-specified motion
trajectory. To support this task, we introduce a new dataset curated
semi-automatically, a comprehensive evaluation protocol targeting this setting,
and an efficient identity-preserving motion-controllable video Diffusion
Transformer architecture. Our evaluation shows that our proposed approach
significantly outperforms existing baselines.

</details>


### [201] [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment](https://arxiv.org/abs/2505.21494)
*Xiaojun Jia,Sensen Gao,Simeng Qin,Tianyu Pang,Chao Du,Yihao Huang,Xinfeng Li,Yiming Li,Bo Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出了一种基于特征最优对齐的针对性可迁移对抗攻击方法（FOA-Attack），通过全局和局部特征对齐提升对抗样本的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过全局特征对齐实现针对性攻击，但忽略了局部信息，导致迁移能力不足，尤其是对闭源模型。

Method: 在全局层面引入余弦相似度损失对齐粗粒度特征；在局部层面利用聚类提取紧凑局部模式，并通过最优传输问题实现细粒度特征对齐。此外，采用动态集成模型加权策略。

Result: 实验表明，该方法优于现有技术，尤其在闭源MLLMs上的迁移能力显著提升。

Conclusion: FOA-Attack通过全局和局部特征的最优对齐，显著提高了对抗样本的迁移能力，尤其在闭源模型中表现突出。

Abstract: Multimodal large language models (MLLMs) remain vulnerable to transferable
adversarial examples. While existing methods typically achieve targeted attacks
by aligning global features-such as CLIP's [CLS] token-between adversarial and
target samples, they often overlook the rich local information encoded in patch
tokens. This leads to suboptimal alignment and limited transferability,
particularly for closed-source models. To address this limitation, we propose a
targeted transferable adversarial attack method based on feature optimal
alignment, called FOA-Attack, to improve adversarial transfer capability.
Specifically, at the global level, we introduce a global feature loss based on
cosine similarity to align the coarse-grained features of adversarial samples
with those of target samples. At the local level, given the rich local
representations within Transformers, we leverage clustering techniques to
extract compact local patterns to alleviate redundant local features. We then
formulate local feature alignment between adversarial and target samples as an
optimal transport (OT) problem and propose a local clustering optimal transport
loss to refine fine-grained feature alignment. Additionally, we propose a
dynamic ensemble model weighting strategy to adaptively balance the influence
of multiple models during adversarial example generation, thereby further
improving transferability. Extensive experiments across various models
demonstrate the superiority of the proposed method, outperforming
state-of-the-art methods, especially in transferring to closed-source MLLMs.
The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.

</details>


### [202] [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
*Dingming Li,Hongxing Li,Zixuan Wang,Yuchen Yan,Hang Zhang,Siqi Chen,Guiyang Hou,Shengpei Jiang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: 论文提出ViewSpatial-Bench，首个针对多视角空间定位识别的综合基准，揭示了现有视觉语言模型在跨视角空间推理中的局限性，并通过微调显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在跨视角空间推理中存在局限性，尤其是从非自我中心视角（如人类视角）进行推理时表现不佳。

Method: 引入ViewSpatial-Bench基准，包含五种任务类型，并开发自动化3D标注流程生成精确方向标签。通过微调模型在多视角空间数据集上提升性能。

Result: 模型在自我中心视角任务中表现尚可，但在人类视角任务中准确性下降。微调后整体性能提升46.24%。

Conclusion: 该工作为具身AI系统的空间智能提供了关键基准，并证明建模3D空间关系能增强视觉语言模型的空间理解能力。

Abstract: Vision-language models (VLMs) have demonstrated remarkable capabilities in
understanding and reasoning about visual content, but significant challenges
persist in tasks requiring cross-viewpoint understanding and spatial reasoning.
We identify a critical limitation: current VLMs excel primarily at egocentric
spatial reasoning (from the camera's perspective) but fail to generalize to
allocentric viewpoints when required to adopt another entity's spatial frame of
reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark
designed specifically for multi-viewpoint spatial localization recognition
evaluation across five distinct task types, supported by an automated 3D
annotation pipeline that generates precise directional labels. Comprehensive
evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant
performance disparity: models demonstrate reasonable performance on
camera-perspective tasks but exhibit reduced accuracy when reasoning from a
human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,
we achieve an overall performance improvement of 46.24% across tasks,
highlighting the efficacy of our approach. Our work establishes a crucial
benchmark for spatial intelligence in embodied AI systems and provides
empirical evidence that modeling 3D spatial relationships enhances VLMs'
corresponding spatial comprehension capabilities.

</details>


### [203] [Vision Transformers with Self-Distilled Registers](https://arxiv.org/abs/2505.21501)
*Yinjie Chen,Zipeng Yan,Chong Zhou,Bo Dai,Andrew F. Luo*

Main category: cs.CV

TL;DR: 论文提出了一种名为PH-Reg的高效自蒸馏方法，用于为预训练的ViT模型添加寄存器令牌，以减少异常令牌的影响，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: ViT在处理视觉任务时会出现与局部语义不符的异常令牌，影响性能。现有方法需重新训练模型，但预训练ViT规模大，重新训练不现实。

Method: 提出PH-Reg方法，通过自蒸馏将寄存器令牌集成到现有ViT中。教师网络保持不变，学生网络添加随机初始化的寄存器令牌，并通过测试时增强生成无异常的密集嵌入来优化学生网络的部分权重。

Result: PH-Reg有效减少了异常令牌数量，在零样本和线性探测任务中提升了学生ViT的分割和深度预测性能。

Conclusion: PH-Reg是一种无需重新训练的高效方法，能够显著改善ViT的性能，适用于大规模预训练模型。

Abstract: Vision Transformers (ViTs) have emerged as the dominant architecture for
visual processing tasks, demonstrating excellent scalability with increased
training data and model size. However, recent work has identified the emergence
of artifact tokens in ViTs that are incongruous with the local semantics. These
anomalous tokens degrade ViT performance in tasks that require fine-grained
localization or structural coherence. An effective mitigation of this issue is
to the addition of register tokens to ViTs, which implicitly "absorb" the
artifact term during training. Given the availability of various large-scale
pre-trained ViTs, in this paper we aim at equipping them with such register
tokens without the need of re-training them from scratch, which is infeasible
considering their size. Specifically, we propose Post Hoc Registers (PH-Reg),
an efficient self-distillation method that integrates registers into an
existing ViT without requiring additional labeled data and full retraining.
PH-Reg initializes both teacher and student networks from the same pre-trained
ViT. The teacher remains frozen and unmodified, while the student is augmented
with randomly initialized register tokens. By applying test-time augmentation
to the teacher's inputs, we generate denoised dense embeddings free of
artifacts, which are then used to optimize only a small subset of unlocked
student weights. We show that our approach can effectively reduce the number of
artifact tokens, improving the segmentation and depth prediction of the student
ViT under zero-shot and linear probing.

</details>


### [204] [Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis](https://arxiv.org/abs/2505.21502)
*Yipengjing Sun,Chenyang Wang,Shunyuan Zheng,Zonglin Li,Shengping Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: GRGS是一个通用的、可重光照的3D高斯框架，用于在多样光照条件下实现高保真的人类新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖逐角色优化或忽略物理约束，GRGS旨在通过前馈、全监督策略解决这些问题。

Method: 采用光照感知几何细化（LGR）模块重建光照无关几何，基于高质量几何的物理基础神经渲染（PGNR）模块结合神经预测与物理着色。

Result: GRGS在视觉质量、几何一致性和跨角色及光照条件的泛化性上表现优异。

Conclusion: GRGS通过创新的模块和训练方案，实现了高效且高质量的3D高斯表示。

Abstract: We propose GRGS, a generalizable and relightable 3D Gaussian framework for
high-fidelity human novel view synthesis under diverse lighting conditions.
Unlike existing methods that rely on per-character optimization or ignore
physical constraints, GRGS adopts a feed-forward, fully supervised strategy
that projects geometry, material, and illumination cues from multi-view 2D
observations into 3D Gaussian representations. Specifically, to reconstruct
lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement
(LGR) module trained on synthetically relit data to predict accurate depth and
surface normals. Based on the high-quality geometry, a Physically Grounded
Neural Rendering (PGNR) module is further proposed to integrate neural
prediction with physics-based shading, supporting editable relighting with
shadows and indirect illumination. Besides, we design a 2D-to-3D projection
training scheme that leverages differentiable supervision from ambient
occlusion, direct, and indirect lighting maps, which alleviates the
computational cost of explicit ray tracing. Extensive experiments demonstrate
that GRGS achieves superior visual quality, geometric consistency, and
generalization across characters and lighting conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [205] [FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](https://arxiv.org/abs/2505.20353)
*Dong Liu,Jiayi Zhang,Yifan Li,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.LG

TL;DR: FastCache通过隐藏状态缓存和压缩框架加速DiT推理，减少计算冗余，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiT）计算密集，FastCache旨在通过利用模型内部表示的冗余性提高效率。

Method: FastCache采用空间感知的令牌选择机制和transformer级缓存策略，减少冗余计算。

Result: 实验表明FastCache显著降低延迟和内存使用，同时保持最佳生成质量。

Conclusion: FastCache是一种高效且有效的DiT加速方法，代码已开源。

Abstract: Diffusion Transformers (DiT) are powerful generative models but remain
computationally intensive due to their iterative structure and deep transformer
stacks. To alleviate this inefficiency, we propose FastCache, a
hidden-state-level caching and compression framework that accelerates DiT
inference by exploiting redundancy within the model's internal representations.
FastCache introduces a dual strategy: (1) a spatial-aware token selection
mechanism that adaptively filters redundant tokens based on hidden state
saliency, and (2) a transformer-level cache that reuses latent activations
across timesteps when changes are statistically insignificant. These modules
work jointly to reduce unnecessary computation while preserving generation
fidelity through learnable linear approximation. Theoretical analysis shows
that FastCache maintains bounded approximation error under a
hypothesis-testing-based decision rule. Empirical evaluations across multiple
DiT variants demonstrate substantial reductions in latency and memory usage,
with best generation output quality compared to other cache methods, as
measured by FID and t-FID. Code implementation of FastCache is available on
GitHub at https://github.com/NoakLiu/FastCache-xDiT.

</details>


### [206] [FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design](https://arxiv.org/abs/2505.20300)
*Chenxi Wu,Juan Diego Toscano,Khemraj Shukla,Yingjie Chen,Ali Shahmohammadi,Edward Raymond,Thomas Toupy,Neda Nazemifard,Charles Papageorgiou,George Em Karniadakis*

Main category: cs.LG

TL;DR: FMEnets是一个基于物理信息的机器学习框架，用于设计和分析非理想活塞流反应器，通过整合多尺度网络模型实现正向和逆向问题求解。


<details>
  <summary>Details</summary>
Motivation: 解决非理想活塞流反应器的设计和分析问题，结合物理方程与机器学习，提高计算效率和准确性。

Method: FMEnets整合Navier-Stokes方程、物质平衡和能量平衡，采用三个互联子网络和独立优化器，支持正向预测和逆向参数推断。

Result: FMEnets在三种反应场景中表现优异，相对误差低于2.5%，FME-KANs对噪声更鲁棒。

Conclusion: FMEnets为反应器设计和优化提供了高效的计算方法，并探索了物理方程与实验数据结合的新途径。

Abstract: We propose FMEnets, a physics-informed machine learning framework for the
design and analysis of non-ideal plug flow reactors. FMEnets integrates the
fundamental governing equations (Navier-Stokes for fluid flow, material balance
for reactive species transport, and energy balance for temperature
distribution) into a unified multi-scale network model. The framework is
composed of three interconnected sub-networks with independent optimizers that
enable both forward and inverse problem-solving. In the forward mode, FMEnets
predicts velocity, pressure, species concentrations, and temperature profiles
using only inlet and outlet information. In the inverse mode, FMEnets utilizes
sparse multi-residence-time measurements to simultaneously infer unknown
kinetic parameters and states. FMEnets can be implemented either as FME-PINNs,
which employ conventional multilayer perceptrons, or as FME-KANs, based on
Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the
critical role of the FMEnets architecture in achieving accurate predictions.
Specifically, FME-KANs are more robust to noise than FME-PINNs, although both
representations are comparable in accuracy and speed in noise-free conditions.
The proposed framework is applied to three different sets of reaction scenarios
and is compared with finite element simulations. FMEnets effectively captures
the complex interactions, achieving relative errors less than 2.5% for the
unknown kinetic parameters. The new network framework not only provides a
computationally efficient alternative for reactor design and optimization, but
also opens new avenues for integrating empirical correlations, limited and
noisy experimental data, and fundamental physical equations to guide reactor
design.

</details>


### [207] [PDFBench: A Benchmark for De novo Protein Design from Function](https://arxiv.org/abs/2505.20346)
*Jiahao Kuang,Nuowei Liu,Changzhi Sun,Tao Ji,Yuanbin Wu*

Main category: cs.LG

TL;DR: PDFBench是首个用于评估从头蛋白质设计的综合基准，支持两种任务，并提供22种指标以确保全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质设计方法依赖专有数据集和评估标准，缺乏公平比较和全面评估框架。

Method: 提出PDFBench基准，支持描述引导和关键词引导设计任务，涵盖22种评估指标。

Result: 评估了五种先进基线方法，揭示了其优缺点，并分析了指标间相关性。

Conclusion: PDFBench为功能驱动的从头蛋白质设计提供了统一框架，推动未来研究。

Abstract: In recent years, while natural language processing and multimodal learning
have seen rapid advancements, the field of de novo protein design has also
experienced significant growth. However, most current methods rely on
proprietary datasets and evaluation rubrics, making fair comparisons between
different approaches challenging. Moreover, these methods often employ
evaluation metrics that capture only a subset of the desired properties of
designed proteins, lacking a comprehensive assessment framework. To address
these, we introduce PDFBench, the first comprehensive benchmark for evaluating
de novo protein design from function. PDFBench supports two tasks:
description-guided design and keyword-guided design. To ensure fair and
multifaceted evaluation, we compile 22 metrics covering sequence plausibility,
structural fidelity, and language-protein alignment, along with measures of
novelty and diversity. We evaluate five state-of-the-art baselines, revealing
their respective strengths and weaknesses across tasks. Finally, we analyze
inter-metric correlations, exploring the relationships between four categories
of metrics, and offering guidelines for metric selection. PDFBench establishes
a unified framework to drive future advances in function-driven de novo protein
design.

</details>


### [208] [Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.20330)
*Yunfu Song,Zhijian Ou*

Main category: cs.LG

TL;DR: 论文分析了GANs和VAEs在半监督学习中的问题，提出了JRFs算法以解决模式缺失和覆盖问题，并在实验中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 发现GANs和VAEs在半监督学习中存在模式缺失和覆盖问题，且分类与生成之间存在冲突。

Method: 提出JRFs算法，构建深度无向生成模型。

Result: JRFs在合成实验中平衡了模式覆盖与缺失，并在MNIST等数据集上取得了与先进方法相当的分类结果和良好的生成效果。

Conclusion: JRFs是一种有效的半监督学习算法，解决了现有生成模型的问题。

Abstract: Our examination of deep generative models (DGMs) developed for
semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.
First, mode missing and mode covering phenomenons are observed in genertion
with GANs and VAEs. Second, there exists an awkward conflict between good
classification and good generation in SSL by employing directed generative
models. To address these problems, we formally present
joint-stochastic-approximation random fields (JRFs) -- a new family of
algorithms for building deep undirected generative models, with application to
SSL. It is found through synthetic experiments that JRFs work well in balancing
mode covering and mode missing, and match the empirical data distribution well.
Empirically, JRFs achieve good classification results comparable to the
state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in
SSL, and simultaneously perform good generation.

</details>


### [209] [Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling](https://arxiv.org/abs/2505.21452)
*Xiangxin Zhou,Mingyu Li,Yi Xiao,Jiahan Li,Dongyu Xue,Zaixiang Zheng,Jianzhu Ma,Quanquan Gu*

Main category: cs.LG

TL;DR: CpSDE是一种用于设计多样化环肽的生成模型，通过结合AtomSDE和ResRouter解决了数据稀缺和几何约束问题。


<details>
  <summary>Details</summary>
Motivation: 环肽在药物中具有稳定性与亲和力优势，但设计方法面临数据稀缺和几何约束等挑战。

Method: CpSDE由AtomSDE（基于谐波SDE的结构预测模型）和ResRouter（残基类型预测器）组成，通过交替采样迭代更新序列与结构。

Result: 实验表明，CpSDE设计的环肽具有可靠的稳定性和亲和力。

Conclusion: CpSDE克服了现有数据限制，能够高效设计多样化环肽。

Abstract: Cyclic peptides offer inherent advantages in pharmaceuticals. For example,
cyclic peptides are more resistant to enzymatic hydrolysis compared to linear
peptides and usually exhibit excellent stability and affinity. Although deep
generative models have achieved great success in linear peptide design, several
challenges prevent the development of computational methods for designing
diverse types of cyclic peptides. These challenges include the scarcity of 3D
structural data on target proteins and associated cyclic peptide ligands, the
geometric constraints that cyclization imposes, and the involvement of
non-canonical amino acids in cyclization. To address the above challenges, we
introduce CpSDE, which consists of two key components: AtomSDE, a generative
structure prediction model based on harmonic SDE, and ResRouter, a residue type
predictor. Utilizing a routed sampling algorithm that alternates between these
two models to iteratively update sequences and structures, CpSDE facilitates
the generation of cyclic peptides. By employing explicit all-atom and bond
modeling, CpSDE overcomes existing data limitations and is proficient in
designing a wide variety of cyclic peptides. Our experimental results
demonstrate that the cyclic peptides designed by our method exhibit reliable
stability and affinity.

</details>


### [210] [Decision Flow Policy Optimization](https://arxiv.org/abs/2505.20350)
*Jifeng Hu,Sili Huang,Siyuan Guo,Zhaogeng Liu,Li Shen,Lichao Sun,Hechang Chen,Yi Chang,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出了一种名为Decision Flow的统一框架，将多模态动作分布建模与策略优化结合，解决了传统方法中两者分离的问题，并在离线强化学习环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法中，生成模型作为行为模型与策略优化分离，阻碍了多模态分布拟合与策略改进的同步优化，限制了模型性能。

Method: 提出Decision Flow框架，将基于流的模型的行动生成过程视为流决策过程，每个行动生成步骤对应一个流决策，从而同步优化流策略并捕捉多模态动作分布。

Result: 在多个离线强化学习环境中，Decision Flow达到或匹配了当前最优性能。

Conclusion: Decision Flow通过统一框架解决了多模态动作分布建模与策略优化的分离问题，显著提升了模型性能。

Abstract: In recent years, generative models have shown remarkable capabilities across
diverse fields, including images, videos, language, and decision-making. By
applying powerful generative models such as flow-based models to reinforcement
learning, we can effectively model complex multi-modal action distributions and
achieve superior robotic control in continuous action spaces, surpassing the
limitations of single-modal action distributions with traditional
Gaussian-based policies. Previous methods usually adopt the generative models
as behavior models to fit state-conditioned action distributions from datasets,
with policy optimization conducted separately through additional policies using
value-based sample weighting or gradient-based updates. However, this
separation prevents the simultaneous optimization of multi-modal distribution
fitting and policy improvement, ultimately hindering the training of models and
degrading the performance. To address this issue, we propose Decision Flow, a
unified framework that integrates multi-modal action distribution modeling and
policy optimization. Specifically, our method formulates the action generation
procedure of flow-based models as a flow decision-making process, where each
action generation step corresponds to one flow decision. Consequently, our
method seamlessly optimizes the flow policy while capturing multi-modal action
distributions. We provide rigorous proofs of Decision Flow and validate the
effectiveness through extensive experiments across dozens of offline RL
environments. Compared with established offline RL baselines, the results
demonstrate that our method achieves or matches the SOTA performance.

</details>


### [211] [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2505.20355)
*Yeonjoon Jung,Daehyun Ahn,Hyungjun Kim,Taesu Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: GraLoRA改进LoRA的瓶颈问题，通过分块低秩适配提升性能，接近全微调效果。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中表现优异，但存在梯度纠缠和过拟合问题，限制了其性能。

Method: 提出Granular Low-Rank Adaptation (GraLoRA)，将权重矩阵分块并为每个子块配备低秩适配器。

Result: GraLoRA在代码生成和常识推理任务中显著优于LoRA，最高提升8.5% Pass@1。

Conclusion: GraLoRA是一种可扩展且鲁棒的参数高效微调解决方案，性能接近全微调。

Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient
fine-tuning (PEFT) of generative models, valued for its simplicity and
effectiveness. Despite recent enhancements, LoRA still suffers from a
fundamental limitation: overfitting when the bottleneck is widened. It performs
best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,
still falling short of full fine-tuning (FFT) performance. We identify the root
cause as LoRA's structural bottleneck, which introduces gradient entanglement
to the unrelated input channels and distorts gradient propagation. To address
this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)
that partitions weight matrices into sub-blocks, each with its own low-rank
adapter. With negligible computational or storage cost, GraLoRA overcomes
LoRA's limitations, effectively increases the representational capacity, and
more closely approximates FFT behavior. Experiments on code generation and
commonsense reasoning benchmarks show that GraLoRA consistently outperforms
LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on
HumanEval+. These improvements hold across model sizes and rank settings,
making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts
are available at https://github.com/SqueezeBits/GraLoRA.git

</details>


### [212] [Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning](https://arxiv.org/abs/2505.20882)
*Marc Damie,Edwige Cyffers*

Main category: cs.LG

TL;DR: Fedivertex是一个新的数据集，包含182个图，覆盖了Fediverse中的七个社交网络，每周爬取一次，持续14周。该数据集旨在为去中心化机器学习算法的基准测试提供真实的通信图拓扑。


<details>
  <summary>Details</summary>
Motivation: 去中心化机器学习中，学习动态依赖于通信图的拓扑结构，但现有图数据集多为固定时间点的商业社交网络数据，缺乏多样性。Fediverse（如Mastodon等）提供了一个开放且动态的替代方案。

Method: 通过每周爬取Fediverse中的七个社交网络，构建了182个图的Fedivertex数据集，并开发了配套的Python工具包。

Result: 数据集展示了Fediverse网络的动态变化，并支持多种任务，如新的“去联邦化”任务（模拟链接删除过程）。

Conclusion: Fedivertex为去中心化机器学习提供了更真实、动态的图数据集，填补了现有数据的不足。

Abstract: Decentralized machine learning - where each client keeps its own data locally
and uses its own computational resources to collaboratively train a model by
exchanging peer-to-peer messages - is increasingly popular, as it enables
better scalability and control over the data. A major challenge in this setting
is that learning dynamics depend on the topology of the communication graph,
which motivates the use of real graph datasets for benchmarking decentralized
algorithms. Unfortunately, existing graph datasets are largely limited to
for-profit social networks crawled at a fixed point in time and often collected
at the user scale, where links are heavily influenced by the platform and its
recommendation algorithms. The Fediverse, which includes several free and
open-source decentralized social media platforms such as Mastodon, Misskey, and
Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a
new dataset of 182 graphs, covering seven social networks from the Fediverse,
crawled weekly over 14 weeks. We release the dataset along with a Python
package to facilitate its use, and illustrate its utility on several tasks,
including a new defederation task, which captures a process of link deletion
observed on these networks.

</details>


### [213] [Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach](https://arxiv.org/abs/2505.20357)
*Jun Tian,He Wang,Jibo He,Yu Pan,Shuo Cao,Qingquan Jiang*

Main category: cs.LG

TL;DR: 提出了一种结合CNN和随机森林的混合架构，通过引入物理可解释的指标提升引力波检测性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决CNN在引力波检测中特征物理意义不明确的问题，提升模型的可解释性和检测性能。

Method: 结合CNN特征提取器和随机森林分类器，引入四个物理可解释指标（方差、信噪比、波形重叠和峰值振幅）辅助决策。

Result: 在长时程应变数据上，混合模型比基线CNN模型灵敏度提升21%，低信噪比信号检测能力显著改善。

Conclusion: 物理驱动的CNN特征后处理可提升引力波检测的效率和可解释性，弥合深度学习与领域知识的差距。

Abstract: Convolutional neural networks (CNNs) have become widely adopted in
gravitational wave (GW) detection pipelines due to their ability to
automatically learn hierarchical features from raw strain data. However, the
physical meaning of these learned features remains underexplored, limiting the
interpretability of such models. In this work, we propose a hybrid architecture
that combines a CNN-based feature extractor with a random forest (RF)
classifier to improve both detection performance and interpretability. Unlike
prior approaches that directly connect classifiers to CNN outputs, our method
introduces four physically interpretable metrics - variance, signal-to-noise
ratio (SNR), waveform overlap, and peak amplitude - computed from the final
convolutional layer. These are jointly used with the CNN output in the RF
classifier to enable more informed decision boundaries. Tested on long-duration
strain datasets, our hybrid model outperforms a baseline CNN model, achieving a
relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10
events per month. Notably, it also shows improved detection of low-SNR signals
(SNR $\le$ 10), which are especially vulnerable to misclassification in noisy
environments. Feature attribution via the RF model reveals that both
CNN-extracted and handcrafted features contribute significantly to
classification decisions, with learned variance and CNN outputs ranked among
the most informative. These findings suggest that physically motivated
post-processing of CNN feature maps can serve as a valuable tool for
interpretable and efficient GW detection, bridging the gap between deep
learning and domain knowledge.

</details>


### [214] [Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation](https://arxiv.org/abs/2505.20992)
*Meng Qin,Jiahong Liu,Irwin King*

Main category: cs.LG

TL;DR: 论文提出了一种基于图信号处理的随机特征聚合（RFA）方法，用于高效的身份和位置嵌入，无需训练即可实现高质量结果。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在捕捉图的拓扑属性（如节点身份或位置）时存在不明确性和效率问题，因此需要一种更高效且明确的方法。

Method: 通过图信号处理视角，利用高频和低频信息分别表征节点身份和位置，提出RFA方法，采用无参数GNN骨架，仅使用随机噪声输入，并通过单次前向传播生成嵌入。

Result: 实验表明，RFA的高通和低通滤波器变体分别能高效生成身份和位置嵌入，且在质量和效率上优于基线方法。

Conclusion: RFA在身份和位置嵌入任务中实现了质量与效率的更好平衡，为GNN研究提供了新的思路。

Abstract: Graph neural networks (GNNs), which capture graph structures via a feature
aggregation mechanism following the graph embedding framework, have
demonstrated a powerful ability to support various tasks. According to the
topology properties (e.g., structural roles or community memberships of nodes)
to be preserved, graph embedding can be categorized into identity and position
embedding. However, it is unclear for most GNN-based methods which property
they can capture. Some of them may also suffer from low efficiency and
scalability caused by several time- and space-consuming procedures (e.g.,
feature extraction and training). From a perspective of graph signal
processing, we find that high- and low-frequency information in the graph
spectral domain may characterize node identities and positions, respectively.
Based on this investigation, we propose random feature aggregation (RFA) for
efficient identity and position embedding, serving as an extreme ablation study
regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without
learnable parameters as its backbone, (ii) only uses random noises as inputs,
and (iii) derives embeddings via just one feed-forward propagation (FFP).
Inspired by degree-corrected spectral clustering, we further introduce a degree
correction mechanism to the GNN backbone. Surprisingly, our experiments
demonstrate that two variants of RFA with high- and low-pass filters can
respectively derive informative identity and position embeddings via just one
FFP (i.e., without any training). As a result, RFA can achieve a better
trade-off between quality and efficiency for both identity and position
embedding over various baselines.

</details>


### [215] [Risk-aware Direct Preference Optimization under Nested Risk Measure](https://arxiv.org/abs/2505.20359)
*Lijun Zhang,Lin Li,Yajie Qi,Huizhong Song,Yaodong Yang,Jun Wang,Wei Wei*

Main category: cs.LG

TL;DR: Ra-DPO是一种新的风险感知直接偏好优化方法，通过嵌套风险度量增强模型的风险意识，平衡对齐性能和模型漂移。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过KL散度约束模型偏差，不足以满足需要严格风险控制的应用场景。

Method: 提出Ra-DPO，利用嵌套风险度量构建风险感知优势函数，并将Bradley-Terry模型转换为token级表示，通过序列风险比抑制模型偏差。

Result: 在IMDb、Anthropic HH和AlpacaEval数据集上验证了Ra-DPO在平衡对齐性能和模型漂移方面的优越性。

Conclusion: Ra-DPO通过风险感知优化，有效提升了模型对齐性能并控制了风险。

Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human
values and intentions, maximizing the estimated reward can lead to superior
performance, but it also introduces potential risks due to deviations from the
reference model's intended behavior. Most existing methods typically introduce
KL divergence to constrain deviations between the trained model and the
reference model; however, this may not be sufficient in certain applications
that require tight risk control. In this paper, we introduce Risk-aware Direct
Preference Optimization (Ra-DPO), a novel approach that incorporates
risk-awareness by employing a class of nested risk measures. This approach
formulates a constrained risk-aware advantage function maximization problem and
then converts the Bradley-Terry model into a token-level representation. The
objective function maximizes the likelihood of the policy while suppressing the
deviation between a trained model and the reference model using a sequential
risk ratio, thereby enhancing the model's risk-awareness. Experimental results
across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and
AlpacaEval, demonstrate the proposed method's superior performance in balancing
alignment performance and model drift. Our code is opensourced at
https://github.com/zlj123-max/Ra-DPO.

</details>


### [216] [GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining](https://arxiv.org/abs/2505.20380)
*Simin Fan,Maria Ios Glarou,Martin Jaggi*

Main category: cs.LG

TL;DR: GRAPE是一种多源多目标任务的自适应预训练框架，通过动态调整领域权重和任务权重，优化预训练数据混合，以在多个目标任务上实现稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域重加权算法通常针对单一目标任务优化，导致模型在特定任务上过拟合，而在其他任务上性能下降。GRAPE旨在解决这一问题，实现多任务性能的均衡提升。

Method: GRAPE通过最小最大优化问题动态调整领域权重和任务权重，内层最大化利用DRO调整任务权重，外层最小化优化领域权重以最大化损失减少。

Result: 在ClimbLab和SlimPajama数据集上的实验表明，GRAPE在6个基准测试中推理性能优于基线方法，并在8种低资源语言上表现出色。

Conclusion: GRAPE通过多目标任务优化，显著提升了模型在多样化任务和语言上的性能，为预训练数据混合优化提供了有效解决方案。

Abstract: The performance of large language models (LLMs) across diverse downstream
applications is fundamentally governed by the quality and composition of their
pretraining corpora. Existing domain reweighting algorithms primarily optimize
data mixtures for a single target task, thereby resulting in models that
overfit to specialized objectives while exhibiting substantial performance
degradation on other benchmarks. This paper introduces Group Robust
Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target
domain reweighting framework designed to calibrate pretraining data mixtures
for robust performance across multiple target tasks simultaneously. GRAPE
dynamically adjusts sampling weights across source domains (domain weights)
while concurrently modulating task weights that quantify the relative
importance of each individual target task. This adaptive process prioritizes
tasks based on their learning difficulty throughout training. We formulate this
interleaved reweighting mechanism as a minimax optimization problem: The inner
maximization adjusts task weights leveraging group
distributed-robust-optimization (DRO), where those tasks demonstrating the
least improvement under the current data mixture are prioritized with higher
weights; The outer minimization then optimizes domain weights to maximize loss
reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama
datasets demonstrate that GRAPE consistently outperforms baseline methods in
terms of reasoning performance across 6 benchmarks. Furthermore, when applied
to multilingual targets, GRAPE effectively identifies optimal training mixtures
from mainstream languages, achieving superior language modeling capabilities
across 8 low-resource target languages.

</details>


### [217] [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/abs/2505.20435)
*Aideen Fay,Inés García-Redondo,Qiquan Wang,Haim Dubossarsky,Anthea Monod*

Main category: cs.LG

TL;DR: 论文提出使用持久同调（PH）分析语言模型在对抗条件下的潜在空间动态，揭示对抗攻击导致的结构压缩和特征变化。


<details>
  <summary>Details</summary>
Motivation: 研究对抗条件如何影响语言模型的高维激活空间，需要同时捕捉全局结构和局部细节的方法。

Method: 采用持久同调（PH）分析六种先进LLM在两种对抗攻击模式（后门微调和间接提示注入）下的多尺度潜在空间动态，并引入神经元级PH框架。

Result: 对抗条件导致潜在拓扑结构压缩，减少小尺度结构多样性，同时放大粗尺度主导特征，且这些拓扑特征在不同层、架构和模型大小中具有统计鲁棒性。

Conclusion: 持久同调为解释语言模型在分布偏移下的表征动态提供了统一且有原则的方法。

Abstract: Understanding how adversarial conditions affect language models requires
techniques that capture both global structure and local detail within
high-dimensional activation spaces. We propose persistent homology (PH), a tool
from topological data analysis, to systematically characterize multiscale
latent space dynamics in LLMs under two distinct attack modes -- backdoor
fine-tuning and indirect prompt injection. By analyzing six state-of-the-art
LLMs, we show that adversarial conditions consistently compress latent
topologies, reducing structural diversity at smaller scales while amplifying
dominant features at coarser ones. These topological signatures are
statistically robust across layers, architectures, model sizes, and align with
the emergence of adversarial effects deeper in the network. To capture
finer-grained mechanisms underlying these shifts, we introduce a neuron-level
PH framework that quantifies how information flows and transforms within and
across layers. Together, our findings demonstrate that PH offers a principled
and unifying approach to interpreting representational dynamics in LLMs,
particularly under distributional shift.

</details>


### [218] [The challenge of hidden gifts in multi-agent reinforcement learning](https://arxiv.org/abs/2505.20579)
*Dane Malenfant,Blake A. Richards*

Main category: cs.LG

TL;DR: 论文研究了多智能体强化学习（MARL）中“隐藏礼物”现象对信用分配的挑战，并通过一个简单的网格世界任务展示了现有算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨在多智能体环境中，当其他智能体的有益行为未被察觉时（“隐藏礼物”），如何有效分配信用的问题。

Method: 设计了一个简单的网格世界任务，其中智能体需共享一把钥匙以解锁各自的门并获取集体奖励。测试了多种RL算法，包括MARL算法和独立模型无关策略梯度算法。

Result: 现有MARL算法无法完成任务，而独立模型无关策略梯度算法在提供自身动作历史信息后可解决任务。通过引入修正项，进一步提高了独立算法的学习稳定性。

Conclusion: “隐藏礼物”现象增加了多智能体信用分配的难度，独立智能体的学习意识可以改善这一挑战。

Abstract: Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.

</details>


### [219] [HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models](https://arxiv.org/abs/2505.20444)
*Haoran Li,Yingjie Qin,Baoyuan Ou,Lai Xu,Ruiwen Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为HoPE的新型位置嵌入方法，旨在提升视觉语言模型（VLMs）在长上下文场景（如长视频）中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频任务中表现不佳，且缺乏对位置嵌入频率分配策略的理论分析。

Method: 提出HoPE，结合混合频率分配策略和动态时间缩放机制，以增强长上下文语义建模能力。

Result: 在四个视频基准测试中，HoPE显著优于现有方法。

Conclusion: HoPE有效提升了VLMs在长上下文任务中的性能，具有理论和实践意义。

Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal
tasks. However, their performance often deteriorates in long-context scenarios,
particularly long videos. While Rotary Position Embedding (RoPE) has been
widely adopted for length generalization in Large Language Models (LLMs),
extending vanilla RoPE to capture the intricate spatial-temporal dependencies
in videos remains an unsolved challenge. Existing methods typically allocate
different frequencies within RoPE to encode 3D positional information. However,
these allocation strategies mainly rely on heuristics, lacking in-depth
theoretical analysis. In this paper, we first study how different allocation
strategies impact the long-context capabilities of VLMs. Our analysis reveals
that current multimodal RoPEs fail to reliably capture semantic similarities
over extended contexts. To address this issue, we propose HoPE, a Hybrid of
Position Embedding designed to improve the long-context capabilities of VLMs.
HoPE introduces a hybrid frequency allocation strategy for reliable semantic
modeling over arbitrarily long context, and a dynamic temporal scaling
mechanism to facilitate robust learning and flexible inference across diverse
context lengths. Extensive experiments across four video benchmarks on long
video understanding and retrieval tasks demonstrate that HoPE consistently
outperforms existing methods, confirming its effectiveness. Code is available
at https://github.com/hrlics/HoPE.

</details>


### [220] [Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach](https://arxiv.org/abs/2505.20446)
*Tal Gonen,Itai Pemper,Ilan Naiman,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 本文提出了一种统一的扩散生成框架，用于在数据稀缺条件下生成高质量时间序列，并通过预训练和跨领域泛化实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 研究在数据稀缺条件下生成时间序列的挑战，填补现有生成模型在有限监督下性能理解的空白。

Method: 提出一种扩散生成框架，结合动态卷积层和数据集标记条件，通过大规模预训练学习通用时间表示。

Result: 模型在少样本和全数据集基准测试中均优于领域特定基线，展示了预训练和跨领域泛化的优势。

Conclusion: 本文呼吁社区重新审视少样本生成建模问题，并追求跨领域高效扩展的统一解决方案。

Abstract: Generative modeling of time series is a central challenge in time series
analysis, particularly under data-scarce conditions. Despite recent advances in
generative modeling, a comprehensive understanding of how state-of-the-art
generative models perform under limited supervision remains lacking. In this
work, we conduct the first large-scale study evaluating leading generative
models in data-scarce settings, revealing a substantial performance gap between
full-data and data-scarce regimes. To close this gap, we propose a unified
diffusion-based generative framework that can synthesize high-fidelity time
series across diverse domains using just a few examples. Our model is
pre-trained on a large, heterogeneous collection of time series datasets,
enabling it to learn generalizable temporal representations. It further
incorporates architectural innovations such as dynamic convolutional layers for
flexible channel adaptation and dataset token conditioning for domain-aware
generation. Without requiring abundant supervision, our unified model achieves
state-of-the-art performance in few-shot settings-outperforming domain-specific
baselines across a wide range of subset sizes. Remarkably, it also surpasses
all baselines even when tested on full datasets benchmarks, highlighting the
strength of pre-training and cross-domain generalization. We hope this work
encourages the community to revisit few-shot generative modeling as a key
problem in time series research and pursue unified solutions that scale
efficiently across domains. Code is available at
https://github.com/azencot-group/ImagenFew.

</details>


### [221] [Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes](https://arxiv.org/abs/2505.20452)
*Hao Zhao,Rong Pan*

Main category: cs.LG

TL;DR: 提出了一种结合主动学习和深度高斯过程的新算法，用于非平稳时间序列的多变点检测，通过谱分析和高效采样提升性能。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列中的多变点检测因模式多样而具有挑战性，需要一种能适应不同变化行为的鲁棒方法。

Method: 结合主动学习和深度高斯过程，利用谱分析识别潜在变化点，并通过主动学习优化采样策略。

Result: 在模拟和真实数据上的实验表明，该方法在检测精度和采样效率上优于现有技术。

Conclusion: 该方法为多变点检测提供了一种高效且适应性强的解决方案。

Abstract: Multiple change point (MCP) detection in non-stationary time series is
challenging due to the variety of underlying patterns. To address these
challenges, we propose a novel algorithm that integrates Active Learning (AL)
with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method
leverages spectral analysis to identify potential changes and employs AL to
strategically select new sampling points for improved efficiency. By
incorporating the modeling flexibility of DGPs with the change-identification
capabilities of spectral methods, our approach adapts to diverse spectral
change behaviors and effectively localizes multiple change points. Experiments
on both simulated and real-world data demonstrate that our method outperforms
existing techniques in terms of detection accuracy and sampling efficiency for
non-stationary time series.

</details>


### [222] [Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies](https://arxiv.org/abs/2505.21236)
*Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Ruan de Kock,Claude Formanek,Sasha Abramowitz,Oumayma Mahjoub,Wiem Khlifi,Simon Du Toit,Louay Ben Nessir,Refiloe Shabe,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Main category: cs.LG

TL;DR: 通过在推理阶段采用特定策略，多智能体强化学习性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现实中的复杂多智能体强化学习任务性能存在瓶颈，需突破。

Method: 在推理阶段利用时间和计算预算探索多次尝试，选择策略。

Result: 在17个任务中平均提升45%，最高126%，计算扩展性良好。

Conclusion: 推理阶段策略是突破多智能体强化学习性能瓶颈的关键。

Abstract: Reinforcement learning (RL) systems have countless applications, from
energy-grid management to protein design. However, such real-world scenarios
are often extremely difficult, combinatorial in nature, and require complex
coordination between multiple agents. This level of complexity can cause even
state-of-the-art RL systems, trained until convergence, to hit a performance
ceiling which they are unable to break out of with zero-shot inference.
Meanwhile, many digital or simulation-based applications allow for an inference
phase that utilises a specific time and compute budget to explore multiple
attempts before outputting a final solution. In this work, we show that such an
inference phase employed at execution time, and the choice of a corresponding
inference strategy, are key to breaking the performance ceiling observed in
complex multi-agent RL problems. Our main result is striking: we can obtain up
to a 126% and, on average, a 45% improvement over the previous state-of-the-art
across 17 tasks, using only a couple seconds of extra wall-clock time during
execution. We also demonstrate promising compute scaling properties, supported
by over 60k experiments, making it the largest study on inference strategies
for complex RL to date. Our experimental data and code are available at
https://sites.google.com/view/inf-marl.

</details>


### [223] [BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction](https://arxiv.org/abs/2505.20454)
*Reid Graves,Anthony Zhou,Amir Barati Farimani*

Main category: cs.LG

TL;DR: BlastOFormer是一种基于Transformer的替代模型，用于预测爆炸压力场，比传统方法更快更准确。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如经验模型和CFD模拟）在速度和准确性之间存在局限，无法满足复杂环境的需求。

Method: BlastOFormer采用SDF编码和网格到网格注意力架构，基于Transformer框架，训练数据来自blastFoam CFD求解器。

Result: BlastOFormer在R2分数（0.9516）和误差指标上表现最佳，推理速度比CFD快600,000倍。

Conclusion: BlastOFormer可作为复杂环境中实时爆炸压力估计的替代方案，优于传统方法。

Abstract: Accurate prediction of blast pressure fields is essential for applications in
structural safety, defense planning, and hazard mitigation. Traditional methods
such as empirical models and computational fluid dynamics (CFD) simulations
offer limited trade offs between speed and accuracy; empirical models fail to
capture complex interactions in cluttered environments, while CFD simulations
are computationally expensive and time consuming. In this work, we introduce
BlastOFormer, a novel Transformer based surrogate model for full field maximum
pressure prediction from arbitrary obstacle and charge configurations.
BlastOFormer leverages a signed distance function (SDF) encoding and a grid to
grid attention based architecture inspired by OFormer and Vision Transformer
(ViT) frameworks. Trained on a dataset generated using the open source
blastFoam CFD solver, our model outperforms convolutional neural networks
(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and
unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score
(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for
inference, more than 600,000 times faster than CFD simulations. Qualitative
visualizations and error analyses further confirm BlastOFormer's superior
spatial coherence and generalization capabilities. These results highlight its
potential as a real time alternative to conventional CFD approaches for blast
pressure estimation in complex environments.

</details>


### [224] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/abs/2505.20485)
*Abhijit Chunduru,Majid Morafah,Mahdi Morafah,Vishnu Pandi Chellapandi,Ang Li*

Main category: cs.LG

TL;DR: 本文通过实验分析发现现有联邦学习方法存在全局决策边界遗忘问题，并提出FedProj框架，通过服务器端知识转移损失和公共数据集记忆机制解决该问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据异构性对联邦学习带来挑战，现有方法缺乏对全局决策边界影响的深入理解。

Method: 提出FedProj框架，包括服务器端知识转移损失和公共数据集记忆机制，避免全局决策边界遗忘。

Result: 实验表明FedProj显著优于现有方法。

Conclusion: FedProj有效解决了全局决策边界遗忘问题，提升了联邦学习性能。

Abstract: The inevitable presence of data heterogeneity has made federated learning
very challenging. There are numerous methods to deal with this issue, such as
local regularization, better model fusion techniques, and data sharing. Though
effective, they lack a deep understanding of how data heterogeneity can affect
the global decision boundary. In this paper, we bridge this gap by performing
an experimental analysis of the learned decision boundary using a toy example.
Our observations are surprising: (1) we find that the existing methods suffer
from forgetting and clients forget the global decision boundary and only learn
the perfect local one, and (2) this happens regardless of the initial weights,
and clients forget the global decision boundary even starting from pre-trained
optimal weights. In this paper, we present FedProj, a federated learning
framework that robustly learns the global decision boundary and avoids its
forgetting during local training. To achieve better ensemble knowledge fusion,
we design a novel server-side ensemble knowledge transfer loss to further
calibrate the learned global decision boundary. To alleviate the issue of
learned global decision boundary forgetting, we further propose leveraging an
episodic memory of average ensemble logits on a public unlabeled dataset to
regulate the gradient updates at each step of local training. Experimental
results demonstrate that FedProj outperforms state-of-the-art methods by a
large margin.

</details>


### [225] [Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints](https://arxiv.org/abs/2505.20515)
*Avik Pal,Alan Edelman,Christopher Rackauckas*

Main category: cs.LG

TL;DR: 论文提出了一种名为PNODEs的方法，通过将ODE步骤投影到约束流形上，显式强制代数约束，解决了现有神经微分方程在复杂守恒物理系统建模中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有科学机器学习方法在结合数据驱动技术和机理建模时，难以有效处理硬约束，导致可扩展性和数值性能问题。

Method: 提出Manifold-Projected Neural ODEs (PNODEs)，基于半显式微分代数方程，通过投影步骤强制约束，并提供迭代和快速近似两种实现。

Result: PNODEs在六个基准问题上表现优于基线方法，平均约束违反误差低于$10^{-10}$，且运行时间更短。

Conclusion: 约束投影是一种简单有效的策略，可用于学习物理一致的长时程动力学。

Abstract: Despite the promise of scientific machine learning (SciML) in combining
data-driven techniques with mechanistic modeling, existing approaches for
incorporating hard constraints in neural differential equations (NDEs) face
significant limitations. Scalability issues and poor numerical properties
prevent these neural models from being used for modeling physical systems with
complicated conservation laws. We propose Manifold-Projected Neural ODEs
(PNODEs), a method that explicitly enforces algebraic constraints by projecting
each ODE step onto the constraint manifold. This framework arises naturally
from semi-explicit differential-algebraic equations (DAEs), and includes both a
robust iterative variant and a fast approximation requiring a single Jacobian
factorization. We further demonstrate that prior works on relaxation methods
are special cases of our approach. PNODEs consistently outperform baselines
across six benchmark problems achieving a mean constraint violation error below
$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to
other methods for a given level of error tolerance. These results show that
constraint projection offers a simple strategy for learning physically
consistent long-horizon dynamics.

</details>


### [226] [Towards Fully FP8 GEMM LLM Training at Scale](https://arxiv.org/abs/2505.20524)
*Alejandro Hernández-Cano,Dhia Garbaya,Imanol Schlag,Martin Jaggi*

Main category: cs.LG

TL;DR: 论文提出了一种新型LLM架构，首次支持FP8计算用于所有GEMM操作，实现了显著的吞吐量提升，同时保持了与BF16训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: FP8数据格式在LLM预训练中潜力巨大，但由于大规模训练中的稳定性问题，其应用受限。现有方法通常依赖次优的FP8内核或回退到高精度计算，牺牲了吞吐量增益。

Method: 设计了一种新型LLM架构，支持FP8计算用于所有GEMM操作，并通过减少大异常值激活来提升稳定性。同时提出了监控低精度训练的关键指标。

Result: 新架构实现了前所未有的吞吐量提升，尤其在规模化训练中，同时下游性能与标准BF16训练相当。

Conclusion: 该研究为FP8在LLM训练中的广泛应用提供了可行方案，同时通过稳定性优化和监控指标确保了长期训练的可靠性。

Abstract: Despite the significant potential of FP8 data formats for large language
model (LLM) pre-training, their adoption has been limited due to challenges in
maintaining stability at scale. Existing approaches often rely on suboptimal
fine-grained FP8 kernels or fall back to higher-precision matrix
multiplications (GEMMs) in sensitive components, such as attention projections,
compromising potential throughput gains. We introduce a new class of LLM
architectures that, for the first time, support FP8 computation for all GEMMs
within transformer blocks during both forward and backward passes. This enables
unprecedented throughput gains, particularly at scale, while matching the
downstream performance of standard BF16 training. Our architecture design
reduces large outlier activations, promoting stable long-term FP8 training. In
addition, we identify key metrics to monitor low-precision training and predict
potential future divergences.

</details>


### [227] [One-shot Robust Federated Learning of Independent Component Analysis](https://arxiv.org/abs/2505.20532)
*Dian Jin,Xin Bing,Yuqian Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于几何中位数和k均值聚类的鲁棒分布式ICA聚合框架，解决了异构场景下的排列模糊问题。


<details>
  <summary>Details</summary>
Motivation: 研究分布式和联邦ICA问题中的鲁棒一次性聚合方法，解决客户端估计中的排列模糊问题。

Method: 使用k均值聚类对客户端提供的估计器进行分组，然后在每个簇内用几何中位数聚合估计器。

Result: 理论证明在高异构场景下仍有效，仿真实验验证了方法的有效性。

Conclusion: 提出的方法在异构环境下表现鲁棒，为分布式ICA提供了一种有效解决方案。

Abstract: This paper investigates a general robust one-shot aggregation framework for
distributed and federated Independent Component Analysis (ICA) problem. We
propose a geometric median-based aggregation algorithm that leverages $k$-means
clustering to resolve the permutation ambiguity in local client estimations.
Our method first performs k-means to partition client-provided estimators into
clusters and then aggregates estimators within each cluster using the geometric
median. This approach provably remains effective even in highly heterogeneous
scenarios where at most half of the clients can observe only a minimal number
of samples. The key theoretical contribution lies in the combined analysis of
the geometric median's error bound-aided by sample quantiles-and the maximum
misclustering rates of the aforementioned solution of $k$-means. The
effectiveness of the proposed approach is further supported by simulation
studies conducted under various heterogeneous settings.

</details>


### [228] [Rotary Masked Autoencoders are Versatile Learners](https://arxiv.org/abs/2505.20535)
*Uros Zivanovic,Serafina Di Gioia,Andre Scaffidi,Martín de los Rios,Gabriella Contardo,Roberto Trotta*

Main category: cs.LG

TL;DR: RoMAE是一种基于Rotary Positional Embedding的改进版Masked Autoencoder，适用于不规则时间序列，无需专门架构即可在多模态数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理不规则时间序列时需要特殊架构，增加了计算复杂性和方法复杂性。RoMAE旨在避免这些限制，同时保持多模态性能。

Method: RoMAE扩展了Masked Autoencoder，利用Rotary Positional Embedding（RoPE）处理多维连续位置信息，无需时间序列专用架构。

Result: RoMAE在不规则时间序列、图像和音频等多种模态上表现优异，甚至超越专用时间序列架构。同时发现RoPE的相对位置属性在输入序列中加入学习嵌入时会失效。

Conclusion: RoMAE是一种高效且通用的方法，适用于不规则时间序列和多模态数据，同时揭示了RoPE在特定条件下的局限性。

Abstract: Applying Transformers to irregular time-series typically requires
specializations to their baseline architecture, which can result in additional
computational overhead and increased method complexity. We present the Rotary
Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional
Embedding (RoPE) method for continuous positions. RoMAE is an extension to the
Masked Autoencoder (MAE) that enables representation learning with
multidimensional continuous positional information while avoiding any
time-series-specific architectural specializations. We showcase RoMAE's
performance on a variety of modalities including irregular and multivariate
time-series, images, and audio, demonstrating that RoMAE surpasses specialized
time-series architectures on difficult datasets such as the DESC ELAsTiCC
Challenge while maintaining MAE's usual performance across other modalities. In
addition, we investigate RoMAE's ability to reconstruct the embedded continuous
positions, demonstrating that including learned embeddings in the input
sequence breaks RoPE's relative position property.

</details>


### [229] [A ZeNN architecture to avoid the Gaussian trap](https://arxiv.org/abs/2505.20553)
*Luís Carvalho,João L. Costa,José Mourão,Gonçalo Oliveira*

Main category: cs.LG

TL;DR: ZeNNs是一种新的神经网络架构，旨在解决传统多层感知机（MLPs）的不足，如无限宽度下的非参数性、点极限缺失、特征学习能力差等问题。


<details>
  <summary>Details</summary>
Motivation: 传统MLPs在无限宽度下存在非参数性、点极限缺失、特征学习能力差等问题，且有限宽度下学习高频特征效果不佳。

Method: ZeNNs基于三个谐波分析原则：1) 引入非学习权重；2) 添加频率因子；3) 选择正交激活函数。

Result: ZeNNs在无限宽度下具有点收敛性、非高斯性，并能进行特征学习；有限宽度下擅长学习低维域的高频特征。

Conclusion: ZeNNs有效解决了MLPs的缺陷，并在理论和实践中表现出色。

Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order
to overcome several shortcomings of standard multi-layer perceptrons (MLPs).
Namely, in the large width limit, MLPs are non-parametric, they do not have a
well-defined pointwise limit, they lose non-Gaussian attributes and become
unable to perform feature learning; moreover, finite width MLPs perform poorly
in learning high frequencies. The new ZeNN architecture is inspired by three
simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce
convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of
MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they
exhibit a rich asymptotic structure beyond Gaussianity, and perform feature
learning. Moreover, when appropriate activation functions are chosen, (finite
width) ZeNNs excel at learning high-frequency features of functions with low
dimensional domains.

</details>


### [230] [Learning a Pessimistic Reward Model in RLHF](https://arxiv.org/abs/2505.20556)
*Yinglun Xu,Hangoo Kang,Tarun Suresh,Yuxuan Wan,Gagandeep Singh*

Main category: cs.LG

TL;DR: 提出了一种名为PET的悲观奖励微调方法，用于在离线强化学习（RLHF）中防止奖励黑客行为。


<details>
  <summary>Details</summary>
Motivation: 传统奖励建模方法依赖KL正则化防止奖励黑客行为，但效果有限，PET旨在无需正则化即可解决此问题。

Method: 通过PET微调悲观奖励模型，优化策略时无需依赖正则化。

Result: 在TL;DR数据集上测试，PET能学习高质量策略且无需正则化，策略KL散度高但实际性能优异。

Conclusion: PET证明了悲观奖励模型在防止奖励黑客行为上的可行性，策略可贪婪优化悲观奖励而无须担心黑客行为。

Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to
learn a pessimistic reward model robust against reward hacking in offline
reinforcement learning from human feedback (RLHF). Traditional reward modeling
techniques in RLHF train an imperfect reward model, on which a KL
regularization plays a pivotal role in mitigating reward hacking when
optimizing a policy. Such an intuition-based method still suffers from reward
hacking, and the policies with large KL divergence from the dataset
distribution are excluded during learning. In contrast, we show that when
optimizing a policy on a pessimistic reward model fine-tuned through PET,
reward hacking can be prevented without relying on any regularization. We test
our methods on the standard TL;DR summarization dataset. We find that one can
learn a high-quality policy on our pessimistic reward without using any
regularization. Such a policy has a high KL divergence from the dataset
distribution while having high performance in practice. In summary, our work
shows the feasibility of learning a pessimistic reward model against reward
hacking. The agent can greedily search for the policy with a high pessimistic
reward without suffering from reward hacking.

</details>


### [231] [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
*Shenao Zhang,Yaqing Wang,Yinxiao Liu,Tianqi Liu,Peter Grabowski,Eugene Ie,Zhaoran Wang,Yunxuan Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯自适应强化学习（BARL）的方法，以解决传统马尔可夫强化学习在训练和测试阶段无法有效支持反思性探索的问题。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫强化学习在训练阶段限制了探索行为，且无法解释反思性推理在测试阶段的优势。作者希望通过贝叶斯框架优化探索与利用的平衡。

Method: 采用贝叶斯自适应强化学习框架，通过后验分布优化马尔可夫决策过程，指导模型根据观察结果动态切换策略。

Result: BARL在合成和数学推理任务中表现优于传统方法，提升了探索效率和token利用率。

Conclusion: BARL为反思性探索提供了理论支持，并在实践中验证了其有效性。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
exhibited strong reasoning capabilities and emergent reflective behaviors, such
as backtracking and error correction. However, conventional Markovian RL
confines exploration to the training phase to learn an optimal deterministic
policy and depends on the history contexts only through the current state.
Therefore, it remains unclear whether reflective reasoning will emerge during
Markovian RL training, or why they are beneficial at test time. To remedy this,
we recast reflective exploration within the Bayes-Adaptive RL framework, which
explicitly optimizes the expected return under a posterior distribution over
Markov decision processes. This Bayesian formulation inherently incentivizes
both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and
switch strategies based on the observed outcomes, offering principled guidance
on when and how the model should reflectively explore. Empirical results on
both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior
token efficiency with improved exploration effectiveness. Our code is available
at https://github.com/shenao-zhang/BARL.

</details>


### [232] [Bi-Level Unsupervised Feature Selection](https://arxiv.org/abs/2505.20563)
*Jingjing Liu,Xiansen Ju,Xianchao Xiu,Wanquan Liu*

Main category: cs.LG

TL;DR: 提出了一种新的双层无监督特征选择方法（BLUFS），结合聚类和特征级别，利用谱聚类和线性回归模型，并通过ℓ₂,₀范数约束选择特征。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法通常仅从单一视角建模，难以同时评估特征重要性和保留数据结构，限制了性能。

Method: BLUFS方法包括聚类级别（谱聚类生成伪标签和线性回归学习投影矩阵）和特征级别（ℓ₂,₀范数约束投影矩阵）。采用PAM算法求解模型。

Result: 在合成和真实数据集上的实验表明，BLUFS在聚类和分类任务中表现优越。

Conclusion: BLUFS是首个结合双层框架和ℓ₂,₀范数的方法，有效提升了无监督特征选择的性能。

Abstract: Unsupervised feature selection (UFS) is an important task in data
engineering. However, most UFS methods construct models from a single
perspective and often fail to simultaneously evaluate feature importance and
preserve their inherent data structure, thus limiting their performance. To
address this challenge, we propose a novel bi-level unsupervised feature
selection (BLUFS) method, including a clustering level and a feature level.
Specifically, at the clustering level, spectral clustering is used to generate
pseudo-labels for representing the data structure, while a continuous linear
regression model is developed to learn the projection matrix. At the feature
level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for
more effectively selecting features. To the best of our knowledge, this is the
first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve
the proposed bi-level model, we design an efficient proximal alternating
minimization (PAM) algorithm, whose subproblems either have explicit solutions
or can be computed by fast solvers. Furthermore, we establish the convergence
result and computational complexity. Finally, extensive experiments on two
synthetic datasets and eight real datasets demonstrate the superiority of BLUFS
in clustering and classification tasks.

</details>


### [233] [Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL](https://arxiv.org/abs/2505.20578)
*Xingyu Chen,Shihao Ma,Runsheng Lin,Jiecong Lin,Bo Wang*

Main category: cs.LG

TL;DR: Ctrl-DNA是一种基于约束强化学习的框架，用于设计具有可控细胞类型特异性的调控DNA序列，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 设计精确细胞类型特异性基因表达的调控DNA序列对合成生物学、基因治疗和精准医学至关重要，但现有方法难以生成可靠的新序列。

Method: 通过将调控序列设计建模为生物学约束优化问题，应用强化学习迭代优化序列，以最大化目标细胞类型的调控活性并限制脱靶效应。

Result: Ctrl-DNA在人类启动子和增强子上的评估显示，其性能优于现有生成和强化学习方法，生成高适应性序列并实现最佳细胞类型特异性。

Conclusion: Ctrl-DNA生成的序列捕获了关键的细胞类型特异性转录因子结合位点，证明了其生物学合理性。

Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific
gene expression is crucial for advancements in synthetic biology, gene therapy
and precision medicine. Although transformer-based language models (LMs) can
effectively capture patterns in regulatory DNA, their generative approaches
often struggle to produce novel sequences with reliable cell-specific activity.
Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)
framework tailored for designing regulatory DNA sequences with controllable
cell-type specificity. By formulating regulatory sequence design as a
biologically informed constrained optimization problem, we apply RL to
autoregressive genomic LMs, enabling the models to iteratively refine sequences
that maximize regulatory activity in targeted cell types while constraining
off-target effects. Our evaluation on human promoters and enhancers
demonstrates that Ctrl-DNA consistently outperforms existing generative and
RL-based approaches, generating high-fitness regulatory sequences and achieving
state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences
capture key cell-type-specific transcription factor binding sites (TFBS), short
DNA motifs recognized by regulatory proteins that control gene expression,
demonstrating the biological plausibility of the generated sequences.

</details>


### [234] [Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction](https://arxiv.org/abs/2505.20589)
*Mahdi Pourmirzaei,Farzaneh Esmaili,Salhuldin Alqarghuli,Mohammadreza Pourmirzaei,Ye Han,Kai Chen,Mohsen Rezaei,Duolin Wang,Dong Xu*

Main category: cs.LG

TL;DR: Prot2Token是一个统一的蛋白质预测框架，通过将多种任务转化为标准化的下一个标记预测格式，实现了高效的多任务学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统蛋白质预测任务需要专用模型的问题，开发一种广泛适用且计算高效的蛋白质语言模型。

Method: 使用自回归解码器，结合预训练蛋白质编码器的嵌入和可学习任务标记，进行多样化预测。

Result: 在多个基准测试中表现优异，速度显著提升（如比AlphaFold2快近1000倍），性能匹配或超越专用方法。

Conclusion: Prot2Token为蛋白质建模提供了一个多功能、高通量的范式，有望加速生物学发现和新疗法的开发。

Abstract: The diverse nature of protein prediction tasks has traditionally necessitated
specialized models, hindering the development of broadly applicable and
computationally efficient Protein Language Models (PLMs). In this work, we
introduce Prot2Token, a unified framework that overcomes these challenges by
converting a wide spectrum of protein-related predictions, from sequence-level
properties and residue-specific attributes to complex inter-protein
interactions, into a standardized next-token prediction format. At its core,
Prot2Token employs an autoregressive decoder, conditioned on embeddings from
pre-trained protein encoders and guided by learnable task tokens, to perform
diverse predictions. This architecture uniquely facilitates multi-task
learning, enabling a single model to master numerous tasks with improved
efficiency. We present extensive experimental validation across a variety of
benchmarks, demonstrating Prot2Tokens strong predictive power in different
types of protein-prediction tasks. Key results include significant speedups
(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or
exceeding specialized approaches. Beyond that, we introduce an auxiliary
self-supervised decoder pre-training approach to improve spatially sensitive
task performance. Prot2Token thus offers a significant step towards a
versatile, high-throughput paradigm for protein modeling, promising to
accelerate biological discovery and the development of novel therapeutics. The
code is available at https://github.com/mahdip72/prot2token .

</details>


### [235] [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://arxiv.org/abs/2505.20621)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: 论文提出了一种针对离线强化学习中毒攻击的认证防御方法，显著提升了鲁棒性和适用范围。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习依赖外部数据集，易受中毒攻击影响，其顺序性加剧了这种脆弱性。

Method: 利用差分隐私的性质，扩展认证防御，覆盖连续和离散空间以及随机和确定性环境。

Result: 在高达7%的训练数据被污染时，性能下降不超过50%，认证半径比之前工作大5倍。

Conclusion: 该方法显著提升了离线强化学习的安全性和可靠性。

Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning
(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on
externally sourced datasets, a vulnerability that is exacerbated by its
sequential nature. To mitigate the risks posed by RL poisoning, we extend
certified defenses to provide larger guarantees against adversarial
manipulation, ensuring robustness for both per-state actions, and the overall
expected cumulative reward. Our approach leverages properties of Differential
Privacy, in a manner that allows this work to span both continuous and discrete
spaces, as well as stochastic and deterministic environments -- significantly
expanding the scope and applicability of achievable guarantees. Empirical
evaluations demonstrate that our approach ensures the performance drops to no
more than $50\%$ with up to $7\%$ of the training data poisoned, significantly
improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while
producing certified radii that is $5$ times larger as well. This highlights the
potential of our framework to enhance safety and reliability in offline RL.

</details>


### [236] [Position: Adopt Constraints Over Penalties in Deep Learning](https://arxiv.org/abs/2505.20628)
*Juan Ramirez,Meraj Hashemizadeh,Simon Lacoste-Julien*

Main category: cs.LG

TL;DR: 论文主张在AI系统中采用定制约束优化方法（如拉格朗日法）替代固定权重惩罚项，以更好地满足约束条件并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统依赖固定权重惩罚项来满足外部约束，但这种方法可能无法同时满足约束和性能要求，且调参成本高。

Method: 提出采用拉格朗日法等定制约束优化方法，动态优化惩罚系数（拉格朗日乘数），并与深度学习流程无缝集成。

Result: 定制方法能真正解决约束问题、减少调参需求，并提升系统可问责性。

Conclusion: 定制约束优化方法优于传统固定权重惩罚项，应成为AI系统的首选方案。

Abstract: Recent efforts toward developing trustworthy AI systems with accountability
guarantees have led to a growing reliance on machine learning formulations that
incorporate external requirements, or constraints. These requirements are often
enforced through penalization--adding fixed-weight terms to the task loss. We
argue that this approach is ill-suited, and that tailored constrained
optimization methods should be adopted instead. In particular, no penalty
coefficient may yield a solution that both satisfies the constraints and
achieves good performance--i.e., one solving the constrained problem. Moreover,
tuning these coefficients is costly, incurring significant time and
computational overhead. In contrast, tailored constrained methods--such as the
Lagrangian approach, which optimizes the penalization "coefficients" (the
Lagrange multipliers) alongside the model--(i) truly solve the constrained
problem and add accountability, (ii) eliminate the need for extensive penalty
tuning, and (iii) integrate seamlessly with modern deep learning pipelines.

</details>


### [237] [Explaining Concept Shift with Interpretable Feature Attribution](https://arxiv.org/abs/2505.20634)
*Ruiqi Lyu,Alistair Turcan,Bryan Wilder*

Main category: cs.LG

TL;DR: SGShift是一种用于检测表格数据中概念漂移并归因于稀疏特征集的模型，通过GAM建模和特征选择，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练数据与测试数据分布不同时性能下降，概念漂移会导致模型学习错误的表示，识别漂移特征有助于理解数据集差异。

Method: SGShift使用广义加性模型（GAM）建模概念漂移，结合特征选择识别漂移特征，并引入knockoffs和吸收项控制假发现和模型拟合不足。

Result: 在合成和真实数据实验中，SGShift的AUC>0.9，召回率>90%，性能显著优于基线方法。

Conclusion: SGShift能有效检测和归因概念漂移，适用于多种ML模型，具有高准确性和鲁棒性。

Abstract: Regardless the amount of data a machine learning (ML) model is trained on,
there will inevitably be data that differs from their training set, lowering
model performance. Concept shift occurs when the distribution of labels
conditioned on the features changes, making even a well-tuned ML model to have
learned a fundamentally incorrect representation. Identifying these shifted
features provides unique insight into how one dataset differs from another,
considering the difference may be across a scientifically relevant dimension,
such as time, disease status, population, etc. In this paper, we propose
SGShift, a model for detecting concept shift in tabular data and attributing
reduced model performance to a sparse set of shifted features. SGShift models
concept shift with a Generalized Additive Model (GAM) and performs subsequent
feature selection to identify shifted features. We propose further extensions
of SGShift by incorporating knockoffs to control false discoveries and an
absorption term to account for models with poor fit to the data. We conduct
extensive experiments in synthetic and real data across various ML models and
find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$,
often 2 or 3 times as high as baseline methods.

</details>


### [238] [Can Past Experience Accelerate LLM Reasoning?](https://arxiv.org/abs/2505.20643)
*Bo Pan,Liang Zhao*

Main category: cs.LG

TL;DR: 论文研究了LLMs是否可以通过重复任务暴露提高推理速度，并提出了SpeedupLLM框架，实验显示推理速度最高可提升56%。


<details>
  <summary>Details</summary>
Motivation: 人类通过经验能更快完成任务，而LLMs通常需要更多计算资源。本文探讨LLMs是否也能通过类似方式提高推理速度。

Method: 提出SpeedupLLM框架，结合自适应计算分配和记忆机制，系统化研究任务相关性和计算预算。

Result: 实验表明，LLMs在适当记忆和推理方法下，推理速度最高可提升56%。

Conclusion: LLMs可以通过重复任务暴露提高推理速度，SpeedupLLM框架为此提供了理论保障和实践验证。

Abstract: Allocating more compute to large language models (LLMs) reasoning has
generally been demonstrated to improve their effectiveness, but also results in
increased inference time. In contrast, humans can perform tasks faster and
better with increased experience and exposure. Hence, this paper aims to
investigate the question: Can LLMs also become faster at reasoning through
recurrent exposure on relevant tasks, and if so, how can it be achieved? To
address these questions, we first formalize the problem setting of LLM
reasoning speedup systematically in the dimensions of task relevancy and
compute budget calculation. We then propose SpeedupLLM, a theoretically
guaranteed framework to implement and benchmark such reasoning speedup
behaviour based on adaptive compute allocation and memory mechanisms. We
further conduct comprehensive experiments to benchmark such behaviour across
different question similarity levels, memory methods, and reasoning methods.
Results show that LLMs can generally reason faster with past experience,
achieving up to a 56% reduction in compute cost when equipped with appropriate
memory and reasoning methods.

</details>


### [239] [Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory](https://arxiv.org/abs/2505.20646)
*Eduardo Y. Sakabe,Felipe S. Abrahão,Alexandre Simões,Esther Colombini,Paula Costa,Ricardo Gudwin,Hector Zenil*

Main category: cs.LG

TL;DR: 论文提出了一种基于算法信息理论的方法，使用二值化神经网络（BNNs）来理解和控制神经网络的复杂性，并通过块分解方法（BDM）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于熵的损失函数和统计指标无法捕捉网络结构中更深层次的因果相关性，因此需要一种更理论化的方法来衡量学习动态。

Method: 采用算法概率（AP）和其定义的通用分布，结合块分解方法（BDM）来近似算法复杂度，并分析训练过程中的结构变化。

Result: BDM比熵更能准确反映训练中的结构变化，且与训练损失的相关性更强，表明学习是一个算法压缩的过程。

Conclusion: 研究为基于信息理论和复杂性的学习与正则化提供了一个理论框架，并验证了算法复杂度在衡量学习进展中的有效性。

Abstract: Understanding and controlling the informational complexity of neural networks
is a central challenge in machine learning, with implications for
generalization, optimization, and model capacity. While most approaches rely on
entropy-based loss functions and statistical metrics, these measures often fail
to capture deeper, causally relevant algorithmic regularities embedded in
network structure. We propose a shift toward algorithmic information theory,
using Binarized Neural Networks (BNNs) as a first proxy. Grounded in
algorithmic probability (AP) and the universal distribution it defines, our
approach characterizes learning dynamics through a formal, causally grounded
lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation
of algorithmic complexity based on AP -- and demonstrate that it more closely
tracks structural changes during training than entropy, consistently exhibiting
stronger correlations with training loss across varying model sizes and
randomized training runs. These results support the view of training as a
process of algorithmic compression, where learning corresponds to the
progressive internalization of structured regularities. In doing so, our work
offers a principled estimate of learning progression and suggests a framework
for complexity-aware learning and regularization, grounded in first principles
from information theory, complexity, and computability.

</details>


### [240] [Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning](https://arxiv.org/abs/2505.20648)
*Mengmeng Chen,Xiaohu Wu,Qiqi Liu,Tiantian He,Yew-Soon Ong,Yaochu Jin,Qicheng Lao,Han Yu*

Main category: cs.LG

TL;DR: PHN-HVVS是一种新的PFL框架，通过Voronoi网格分解和遗传算法优化高维空间中的Pareto前沿覆盖。


<details>
  <summary>Details</summary>
Motivation: 解决现有PFL方法在高维空间采样和覆盖凸形Pareto前沿时的挑战。

Method: 使用Voronoi网格分解设计空间，结合遗传算法和新的损失函数优化Pareto前沿覆盖。

Result: 在多个MOO任务中显著优于基线方法，并推动了FL领域的研究。

Conclusion: PHN-HVVS有效提升了Pareto前沿的覆盖范围和HV指标，具有广泛的应用潜力。

Abstract: Multi-objective optimization (MOO) exists extensively in machine learning,
and aims to find a set of Pareto-optimal solutions, called the Pareto front,
e.g., it is fundamental for multiple avenues of research in federated learning
(FL). Pareto-Front Learning (PFL) is a powerful method implemented using
Hypernetworks (PHNs) to approximate the Pareto front. This method enables the
acquisition of a mapping function from a given preference vector to the
solutions on the Pareto front. However, most existing PFL approaches still face
two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to
cover the entire Pareto Front which has a convex shape. Here, we introduce a
novel PFL framework, called as PHN-HVVS, which decomposes the design space into
Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid
partitioning within high-dimensional space. We put forward a new loss function,
which effectively contributes to more extensive coverage of the resultant
Pareto front and maximizes the HV Indicator. Experimental results on multiple
MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines
significantly in generating Pareto front. Also, we illustrate that PHN-HVVS
advances the methodologies of several recent problems in the FL field. The code
is available at
https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.

</details>


### [241] [An Optimisation Framework for Unsupervised Environment Design](https://arxiv.org/abs/2505.20659)
*Nathan Monette,Alistair Letcher,Michael Beukman,Matthew T. Jackson,Alexander Rutherford,Alexander D. Goldie,Jakob N. Foerster*

Main category: cs.LG

TL;DR: 本文提出了一种基于优化视角的无监督环境设计（UED）方法，通过非凸-强凹目标函数提供更强的理论保证，并在零和博弈中实现可证明的收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了在高风险环境中部署强化学习代理，需要提高其对陌生场景的鲁棒性。UED方法旨在最大化代理在环境配置中的泛化能力。

Method: 采用非凸-强凹目标函数，提出了一种在零和博弈中可证明收敛的算法。

Result: 实验验证了该方法的有效性，在多个不同难度的环境中优于现有方法。

Conclusion: 本文通过优化视角改进了UED的理论保证和实际性能，为高风险环境中的强化学习代理提供了更可靠的解决方案。

Abstract: For reinforcement learning agents to be deployed in high-risk settings, they
must achieve a high level of robustness to unfamiliar scenarios. One method for
improving robustness is unsupervised environment design (UED), a suite of
methods aiming to maximise an agent's generalisability across configurations of
an environment. In this work, we study UED from an optimisation perspective,
providing stronger theoretical guarantees for practical settings than prior
work. Whereas previous methods relied on guarantees if they reach convergence,
our framework employs a nonconvex-strongly-concave objective for which we
provide a provably convergent algorithm in the zero-sum setting. We empirically
verify the efficacy of our method, outperforming prior methods in a number of
environments with varying difficulties.

</details>


### [242] [Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers](https://arxiv.org/abs/2505.20666)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于偏微分方程（PDE）的Transformer注意力机制框架，通过动态演化注意力权重解决长序列输入问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的静态注意力矩阵在处理极长输入序列时面临挑战，需要一种动态机制来平滑噪声、增强长程依赖并稳定梯度流。

Method: 将PDE（如扩散、波动或反应扩散方程）融入注意力机制，使注意力权重在伪时间维度上动态演化。

Result: 理论分析表明PDE注意力优化了损失函数景观，且远距离交互呈多项式衰减；实验证明其优于标准及专用长序列Transformer变体。

Conclusion: PDE为基础的注意力机制为注意力机制引入了连续时间动态和全局一致性，具有潜在优势。

Abstract: We propose a novel framework, Continuous_Time Attention, which infuses
partial differential equations (PDEs) into the Transformer's attention
mechanism to address the challenges of extremely long input sequences. Instead
of relying solely on a static attention matrix, we allow attention weights to
evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion
dynamics. This mechanism systematically smooths local noise, enhances
long_range dependencies, and stabilizes gradient flow. Theoretically, our
analysis shows that PDE_based attention leads to better optimization landscapes
and polynomial rather than exponential decay of distant interactions.
Empirically, we benchmark our method on diverse experiments_demonstrating
consistent gains over both standard and specialized long sequence Transformer
variants. Our findings highlight the potential of PDE_based formulations to
enrich attention mechanisms with continuous_time dynamics and global coherence.

</details>


### [243] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/abs/2505.20686)
*Kianté Brantley,Mingyu Chen,Zhaolin Gao,Jason D. Lee,Wen Sun,Wenhao Zhan,Xuezhou Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为A*-PO的两阶段策略优化框架，用于高效训练大型语言模型（LLMs）进行推理任务，显著减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在优化大型语言模型时存在高计算和内存消耗的问题，需要改进。

Method: 分两阶段：离线采样估计最优值函数，然后通过最小二乘回归进行策略更新。

Result: 在数学推理任务中表现优异，训练时间减少2倍，内存使用降低30%。

Conclusion: A*-PO是一种高效且性能优越的策略优化方法。

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


### [244] [Evidential Deep Active Learning for Semi-Supervised Classification](https://arxiv.org/abs/2505.20691)
*Shenkai Zhao,Xinao Zhang,Lipeng Pan,Xiaobin Xu,Danilo Pelusi*

Main category: cs.LG

TL;DR: 论文提出了一种基于证据深度主动学习的半监督分类方法（EDALSSC），通过量化标记和未标记数据的不确定性估计，改进了现有方法忽略预测结果可靠性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督主动学习方法在训练过程中常忽略预测结果的不确定性估计，导致样本选择的有效性存疑。

Method: EDALSSC结合证据深度学习和T-conorm算子，量化标记和未标记数据的不确定性，并动态平衡证据和类别数量的影响。样本选择策略基于不确定性估计的最大值。

Result: 实验表明，EDALSSC在图像分类数据集上优于现有的半监督和监督主动学习方法。

Conclusion: EDALSSC通过有效量化不确定性，提升了半监督分类的性能。

Abstract: Semi-supervised classification based on active learning has made significant
progress, but the existing methods often ignore the uncertainty estimation (or
reliability) of the prediction results during the learning process, which makes
it questionable whether the selected samples can effectively update the model.
Hence, this paper proposes an evidential deep active learning approach for
semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised
learning framework to simultaneously quantify the uncertainty estimation of
labeled and unlabeled data during the learning process. The uncertainty
estimation of the former is associated with evidential deep learning, while
that of the latter is modeled by combining ignorance information and conflict
information of the evidence from the perspective of the T-conorm operator.
Furthermore, this article constructs a heuristic method to dynamically balance
the influence of evidence and the number of classes on uncertainty estimation
to ensure that it does not produce counter-intuitive results in EDALSSC. For
the sample selection strategy, EDALSSC selects the sample with the greatest
uncertainty estimation that is calculated in the form of a sum when the
training loss increases in the latter half of the learning process.
Experimental results demonstrate that EDALSSC outperforms existing
semi-supervised and supervised active learning approaches on image
classification datasets.

</details>


### [245] [Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series](https://arxiv.org/abs/2505.20697)
*Zachary C. Brown,David Carlson*

Main category: cs.LG

TL;DR: 提出一种新方法，通过动态图的条件加权叠加建模动态因果关系，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设因果关系静态或线性，难以适用于大脑等动态系统。

Method: 将动态图建模为静态图的条件加权叠加，支持非线性关系。

Result: F1分数平均提升22-28%，部分实验提升超60%，并在真实脑数据中验证有效性。

Conclusion: 新方法能捕捉复杂时变关系，为神经动力学研究提供新工具。

Abstract: The field of hypothesis generation promises to reduce costs in neuroscience
by narrowing the range of interventional studies needed to study various
phenomena. Existing machine learning methods can generate scientific hypotheses
from complex datasets, but many approaches assume causal relationships are
static over time, limiting their applicability to systems with dynamic,
state-dependent behavior, such as the brain. While some techniques attempt
dynamic causal discovery through factor models, they often restrict
relationships to linear patterns or impose other simplifying assumptions. We
propose a novel method that models dynamic graphs as a conditionally weighted
superposition of static graphs, where each static graph can capture nonlinear
relationships. This approach enables the detection of complex, time-varying
interactions between variables beyond linear limitations. Our method improves
f1-scores of predicted dynamic causal patterns by roughly 22-28% on average
over baselines in some of our experiments, with some improvements reaching well
over 60%. A case study on real brain data demonstrates our method's ability to
uncover relationships linked to specific behavioral states, offering valuable
insights into neural dynamics.

</details>


### [246] [Sparsified State-Space Models are Efficient Highway Networks](https://arxiv.org/abs/2505.20698)
*Woomin Song,Jihoon Tack,Sangwoo Mo,Seunghyuk Oh,Jinwoo Shin*

Main category: cs.LG

TL;DR: 论文提出了一种名为Simba的分层稀疏化方法，用于增强状态空间模型（SSMs）的性能，通过在不同层级进行不同程度的稀疏化，提升信息流效率。


<details>
  <summary>Details</summary>
Motivation: SSMs在序列建模中具有潜力，但存在冗余和密集递归操作阻碍信息传递的问题，尤其是在上层层级。

Method: 提出Simba方法，通过基于令牌剪枝的分层稀疏化策略，上层稀疏化程度更高，形成类似高速公路的信息流。

Result: Simba在相同计算预算下优于基线模型Mamba，并在自然语言任务中表现更优。

Conclusion: Simba不仅提高了效率，还改善了长序列信息流，为SSMs的优化提供了新思路。

Abstract: State-space models (SSMs) offer a promising architecture for sequence
modeling, providing an alternative to Transformers by replacing expensive
self-attention with linear recurrences. In this paper, we propose a simple yet
effective trick to enhance SSMs within given computational budgets by
sparsifying them. Our intuition is that tokens in SSMs are highly redundant due
to gradual recurrent updates, and dense recurrence operations block the
delivery of past information. In particular, we observe that upper layers of
SSMs tend to be more redundant as they encode global information, while lower
layers encode local information. Motivated by this, we introduce Simba, a
hierarchical sparsification method for SSMs based on token pruning. Simba
sparsifies upper layers more than lower layers, encouraging the upper layers to
behave like highways. To achieve this, we propose a novel token pruning
criterion for SSMs, measuring the global impact of tokens on the final output
by accumulating local recurrences. We demonstrate that Simba outperforms the
baseline model, Mamba, with the same FLOPS in various natural language tasks.
Moreover, we illustrate the effect of highways, showing that Simba not only
enhances efficiency but also improves the information flow across long
sequences. Code is available at https://github.com/woominsong/Simba.

</details>


### [247] [Are Data Embeddings effective in time series forecasting?](https://arxiv.org/abs/2505.20716)
*Reza Nematirad,Anil Pahwa,Balasubramaniam Natarajan*

Main category: cs.LG

TL;DR: 研究发现，许多先进时间序列预测模型中的数据嵌入层对性能提升无显著作用，移除后反而可能提高准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 探讨数据嵌入技术在时间序列预测中的实际有效性，挑战现有模型的复杂性。

Method: 通过对15种先进模型和4个基准数据集进行广泛的消融研究，移除数据嵌入层并评估性能变化。

Result: 移除数据嵌入层通常不会降低预测性能，甚至能提升准确性和计算效率。

Conclusion: 数据嵌入层在时间序列预测中的作用被高估，简化模型可能更有效。

Abstract: Time series forecasting plays a crucial role in many real-world applications,
and numerous complex forecasting models have been proposed in recent years.
Despite their architectural innovations, most state-of-the-art models report
only marginal improvements -- typically just a few thousandths in standard
error metrics. These models often incorporate complex data embedding layers to
transform raw inputs into higher-dimensional representations to enhance
accuracy. But are data embedding techniques actually effective in time series
forecasting? Through extensive ablation studies across fifteen state-of-the-art
models and four benchmark datasets, we find that removing data embedding layers
from many state-of-the-art models does not degrade forecasting performance. In
many cases, it improves both accuracy and computational efficiency. The gains
from removing embedding layers often exceed the performance differences
typically reported between competing models. Code available at:
https://github.com/neuripsdataembedidng/DataEmbedding

</details>


### [248] [Recurrent Neural Operators: Stable Long-Term PDE Prediction](https://arxiv.org/abs/2505.20721)
*Zaijun Ye,Chen-Song Zhang,Wansheng Wang*

Main category: cs.LG

TL;DR: 提出了一种名为Recurrent Neural Operators (RNOs)的新框架，通过将循环训练整合到神经算子架构中，解决了时间依赖问题中训练与推理不匹配导致的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 标准训练策略（如teacher forcing）在时间依赖问题中会导致训练与推理不匹配，从而在长期自回归预测中产生误差累积。

Method: 提出RNOs框架，通过在训练过程中递归应用算子于自身预测，模拟推理动态，减少误差累积。

Result: 理论分析表明，循环训练能将误差增长从指数级降至线性级；实验证明RNOs在长期准确性和稳定性上显著优于传统方法。

Conclusion: RNOs通过对齐训练与推理动态，显著提升了神经算子在时间依赖问题中的鲁棒性和泛化能力。

Abstract: Neural operators have emerged as powerful tools for learning solution
operators of partial differential equations. However, in time-dependent
problems, standard training strategies such as teacher forcing introduce a
mismatch between training and inference, leading to compounding errors in
long-term autoregressive predictions. To address this issue, we propose
Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent
training into neural operator architectures. Instead of conditioning each
training step on ground-truth inputs, RNOs recursively apply the operator to
their own predictions over a temporal window, effectively simulating
inference-time dynamics during training. This alignment mitigates exposure bias
and enhances robustness to error accumulation. Theoretically, we show that
recurrent training can reduce the worst-case exponential error growth typical
of teacher forcing to linear growth. Empirically, we demonstrate that
recurrently trained Multigrid Neural Operators significantly outperform their
teacher-forced counterparts in long-term accuracy and stability on standard
benchmarks. Our results underscore the importance of aligning training with
inference dynamics for robust temporal generalization in neural operator
learning.

</details>


### [249] [A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs](https://arxiv.org/abs/2505.20725)
*Alberto Pliego Marugán,Jesús M. Pinar-Pérez,Fausto Pedro García Márquez*

Main category: cs.LG

TL;DR: 论文提出了一种结合伽马退化过程和强化学习的维护优化方法，显著降低了长期成本。


<details>
  <summary>Details</summary>
Motivation: 工业4.0的实现需要新的维护优化范式，而传统方法难以应对系统退化行为。

Method: 使用双深度Q网络架构的强化学习代理，无需预设预防阈值，适用于连续退化状态空间。

Result: 代理在不同场景下表现出灵活性，显著优于其他常见维护策略。

Conclusion: 该方法适用于真实系统退化行为，并能显著提升长期经济效益。

Abstract: Efficient maintenance has always been essential for the successful
application of engineering systems. However, the challenges to be overcome in
the implementation of Industry 4.0 necessitate new paradigms of maintenance
optimization. Machine learning techniques are becoming increasingly used in
engineering and maintenance, with reinforcement learning being one of the most
promising. In this paper, we propose a gamma degradation process together with
a novel maintenance model in which repairs are increasingly imperfect, i.e.,
the beneficial effect of system repairs decreases as more repairs are
performed, reflecting the degradational behavior of real-world systems. To
generate maintenance policies for this system, we developed a
reinforcement-learning-based agent using a Double Deep Q-Network architecture.
This agent presents two important advantages: it works without a predefined
preventive threshold, and it can operate in a continuous degradation state
space. Our agent learns to behave in different scenarios, showing great
flexibility. In addition, we performed an analysis of how changes in the main
parameters of the environment affect the maintenance policy proposed by the
agent. The proposed approach is demonstrated to be appropriate and to
significatively improve long-run cost as compared with other common maintenance
strategies.

</details>


### [250] [Adversarial bandit optimization for approximately linear functions](https://arxiv.org/abs/2505.20734)
*Zhuoyu Cheng,Kohei Hatano,Eiji Takimoto*

Main category: cs.LG

TL;DR: 论文研究了非凸非光滑函数的bandit优化问题，损失函数为线性函数与任意扰动的和，提供了期望和高概率的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决非凸非光滑函数在bandit优化中的挑战，特别是损失函数包含扰动的情况。

Method: 通过分析损失函数的结构（线性加扰动），提出算法并推导遗憾界。

Result: 得到了期望和高概率的遗憾界，并改进了bandit线性优化的高概率遗憾界。

Conclusion: 论文为非凸非光滑bandit优化提供了理论支持，并展示了扰动对结果的影响。

Abstract: We consider a bandit optimization problem for nonconvex and non-smooth
functions, where in each trial the loss function is the sum of a linear
function and a small but arbitrary perturbation chosen after observing the
player's choice. We give both expected and high probability regret bounds for
the problem. Our result also implies an improved high-probability regret bound
for the bandit linear optimization, a special case with no perturbation. We
also give a lower bound on the expected regret.

</details>


### [251] [Detecting Informative Channels: ActionFormer](https://arxiv.org/abs/2505.20739)
*Kunpeng Zhao,Asahi Miyazaki,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 论文提出了一种改进的ActionFormer模型，用于传感器信号的人类活动识别，通过Sequence-and-Excitation策略和swish激活函数优化性能，实验结果显示平均mAP提高了16.01%。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在传感器信号的人类活动识别中存在高时间动态性和时空特征依赖性问题，限制了模型捕捉细微变化的能力。

Method: 改进的ActionFormer模型采用Sequence-and-Excitation策略以减少额外参数，并使用swish激活函数保留负范围的方向信息。

Result: 在WEAR数据集上，改进后的模型在惯性数据上的平均mAP提高了16.01%。

Conclusion: 改进的ActionFormer模型有效解决了传感器信号中的时空特征问题，显著提升了识别性能。

Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with
Transformer-based models. Especially, ActionFormer shows us a new perspectives
for HAR in the sense that this approach gives us additional outputs which
detect the border of the activities as well as the activity labels.
ActionFormer was originally proposed with its input as image/video. However,
this was converted to with its input as sensor signals as well. We analyze this
extensively in terms of deep learning architectures. Based on the report of
high temporal dynamics which limits the model's ability to capture subtle
changes effectively and of the interdependencies between the spatial and
temporal features. We propose the modified ActionFormer which will decrease
these defects for sensor signals. The key to our approach lies in accordance
with the Sequence-and-Excitation strategy to minimize the increase in
additional parameters and opt for the swish activation function to retain the
information about direction in the negative range. Experiments on the WEAR
dataset show that our method achieves substantial improvement of a 16.01\% in
terms of average mAP for inertial data.

</details>


### [252] ['Hello, World!': Making GNNs Talk with LLMs](https://arxiv.org/abs/2505.20742)
*Sunwoo Kim,Soo Yong Lee,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: GLN是一种基于大语言模型的图神经网络，通过人类可读的文本表示隐藏层，提升了GNN的可解释性，并在零样本任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GNN隐藏层高维表示导致的黑盒问题，提升模型的可解释性。

Method: 基于大语言模型设计GLN，结合消息传递模块和高级GNN技术（如图注意力和初始残差连接）。

Result: GLN在节点分类和链接预测任务中表现优于现有基于LLM的基线方法。

Conclusion: GLN通过文本表示提升了GNN的可解释性，同时保持了高性能。

Abstract: While graph neural networks (GNNs) have shown remarkable performance across
diverse graph-related tasks, their high-dimensional hidden representations
render them black boxes. In this work, we propose Graph Lingual Network (GLN),
a GNN built on large language models (LLMs), with hidden representations in the
form of human-readable text. Through careful prompt design, GLN incorporates
not only the message passing module of GNNs but also advanced GNN techniques,
including graph attention and initial residual connection. The
comprehensibility of GLN's hidden representations enables an intuitive analysis
of how node representations change (1) across layers and (2) under advanced GNN
techniques, shedding light on the inner workings of GNNs. Furthermore, we
demonstrate that GLN achieves strong zero-shot performance on node
classification and link prediction, outperforming existing LLM-based baseline
methods.

</details>


### [253] [Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction](https://arxiv.org/abs/2505.20755)
*Yifei Wang,Weimin Bai,Colin Zhang,Debing Zhang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: 论文提出了Uni-Instruct框架，统一了10多种一步扩散蒸馏方法，并通过扩散扩展理论解决了原始f-散度的不可计算问题，实现了高效的训练。


<details>
  <summary>Details</summary>
Motivation: 通过理论驱动的框架统一现有的一步扩散蒸馏方法，并从高层面理解这些方法的共性。

Method: 提出了扩散扩展理论，克服了f-散度不可计算的问题，设计了一种等效且可计算的损失函数。

Result: 在CIFAR10和ImageNet-64×64基准测试中取得了创纪录的FID值，并在文本到3D生成任务中表现优异。

Conclusion: Uni-Instruct在理论和实践上均具有重要贡献，为未来一步扩散蒸馏和知识迁移研究提供了基础。

Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation
approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a
theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}.
Uni-Instruct is motivated by our proposed diffusion expansion theory of the
$f$-divergence family. Then we introduce key theories that overcome the
intractability issue of the original expanded $f$-divergence, resulting in an
equivalent yet tractable loss that effectively trains one-step diffusion models
by minimizing the expanded $f$-divergence family. The novel unification
introduced by Uni-Instruct not only offers new theoretical contributions that
help understand existing approaches from a high-level perspective but also
leads to state-of-the-art one-step diffusion generation performances. On the
CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet
Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional
generation and \textbf{\emph{1.38}} for conditional generation. On the
ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA
one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step
teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).
We also apply Uni-Instruct on broader tasks like text-to-3D generation. For
text-to-3D generation, Uni-Instruct gives decent results, which slightly
outperforms previous methods, such as SDS and VSD, in terms of both generation
quality and diversity. Both the solid theoretical and empirical contributions
of Uni-Instruct will potentially help future studies on one-step diffusion
distillation and knowledge transferring of diffusion models.

</details>


### [254] [Practical estimation of the optimal classification error with soft labels and calibration](https://arxiv.org/abs/2505.20761)
*Ryota Ushio,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种在二元分类中估计贝叶斯误差的方法，改进了先前基于软标签的工作，并探讨了硬标签估计器的偏差特性及在标签损坏情况下的估计问题。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习模型性能提升的极限，特别是在二元分类中估计贝叶斯误差的问题。

Method: 扩展了先前基于软标签估计贝叶斯误差的工作，理论分析了硬标签估计器的偏差特性，并提出了在标签损坏情况下使用等渗校准的统计一致估计方法。

Result: 实验表明，该方法在合成和真实数据集上有效，且无需访问输入实例，适用于隐私敏感场景。

Conclusion: 本文为二元分类中贝叶斯误差的估计提供了理论和实践支持，尤其在标签损坏和隐私限制下具有实用价值。

Abstract: While the performance of machine learning systems has experienced significant
improvement in recent years, relatively little attention has been paid to the
fundamental question: to what extent can we improve our models? This paper
provides a means of answering this question in the setting of binary
classification, which is practical and theoretically supported. We extend a
previous work that utilizes soft labels for estimating the Bayes error, the
optimal error rate, in two important ways. First, we theoretically investigate
the properties of the bias of the hard-label-based estimator discussed in the
original work. We reveal that the decay rate of the bias is adaptive to how
well the two class-conditional distributions are separated, and it can decay
significantly faster than the previous result suggested as the number of hard
labels per instance grows. Second, we tackle a more challenging problem
setting: estimation with corrupted soft labels. One might be tempted to use
calibrated soft labels instead of clean ones. However, we reveal that
calibration guarantee is not enough, that is, even perfectly calibrated soft
labels can result in a substantially inaccurate estimate. Then, we show that
isotonic calibration can provide a statistically consistent estimator under an
assumption weaker than that of the previous work. Our method is instance-free,
i.e., we do not assume access to any input instances. This feature allows it to
be adopted in practical scenarios where the instances are not available due to
privacy issues. Experiments with synthetic and real-world datasets show the
validity of our methods and theory.

</details>


### [255] [Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies](https://arxiv.org/abs/2505.20765)
*Kohei Obata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 论文提出RedLamp方法，通过多类数据增强生成伪异常并学习多类边界，解决了现有方法在时间序列异常检测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练集均为正常样本，但实际可能存在异常污染；且数据增强生成的伪异常范围有限，易产生假异常。

Method: RedLamp采用多样数据增强生成多类伪异常，通过软标签进行多类分类，避免模型过度自信，增强鲁棒性。

Result: 实验证明RedLamp在异常检测中有效，且对异常污染具有鲁棒性。

Conclusion: RedLamp通过多类伪异常和软标签学习，提升了时间序列异常检测的性能和解释性。

Abstract: Unsupervised anomaly detection in time series has been a pivotal research
area for decades. Current mainstream approaches focus on learning normality, on
the assumption that all or most of the samples in the training set are normal.
However, anomalies in the training set (i.e., anomaly contamination) can be
misleading. Recent studies employ data augmentation to generate
pseudo-anomalies and learn the boundary separating the training samples from
the augmented samples. Although this approach mitigates anomaly contamination
if augmented samples mimic unseen real anomalies, it suffers from several
limitations. (1) Covering a wide range of time series anomalies is challenging.
(2) It disregards augmented samples that resemble normal samples (i.e., false
anomalies). (3) It places too much trust in the labels of training and
augmented samples. In response, we propose RedLamp, which employs diverse data
augmentations to generate multiclass pseudo-anomalies and learns the multiclass
boundary. Such multiclass pseudo-anomalies cover a wide variety of time series
anomalies. We conduct multiclass classification using soft labels, which
prevents the model from being overconfident and ensures its robustness against
contaminated/false anomalies. The learned latent space is inherently
explainable as it is trained to separate pseudo-anomalies into multiclasses.
Extensive experiments demonstrate the effectiveness of RedLamp in anomaly
detection and its robustness against anomaly contamination.

</details>


### [256] [TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state](https://arxiv.org/abs/2505.20774)
*Xiaowen Ma,Zhenliang Ni,Shuai Xiao,Xinghao Chen*

Main category: cs.LG

TL;DR: TimePro是一种基于Mamba的创新模型，通过构建变量和时间感知的超状态，解决了长期时间序列预测中的多延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型在处理多变量时间序列时，通常对所有变量或时间点进行统一处理，难以捕捉复杂的变量关系和非平凡的时间表示。

Method: TimePro通过保留每个变量令牌的细粒度时间特征，并自适应选择关注的时间点来调整普通状态，构建变量和时间感知的超状态。

Result: 在实验中，TimePro在八个真实世界的长期预测基准上表现优异，且具有满意的线性复杂度。

Conclusion: TimePro能够有效感知变量关系和显著时间信息，从而实现准确的预测。

Abstract: In long-term time series forecasting, different variables often influence the
target variable over distinct time intervals, a challenge known as the
multi-delay issue. Traditional models typically process all variables or time
points uniformly, which limits their ability to capture complex variable
relationships and obtain non-trivial time representations. To address this
issue, we propose TimePro, an innovative Mamba-based model that constructs
variate- and time-aware hyper-states. Unlike conventional approaches that
merely transfer plain states across variable or time dimensions, TimePro
preserves the fine-grained temporal features of each variate token and
adaptively selects the focused time points to tune the plain state. The
reconstructed hyper-state can perceive both variable relationships and salient
temporal information, which helps the model make accurate forecasting. In
experiments, TimePro performs competitively on eight real-world long-term
forecasting benchmarks with satisfactory linear complexity. Code is available
at https://github.com/xwmaxwma/TimePro.

</details>


### [257] [Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI](https://arxiv.org/abs/2505.20775)
*Fabian Scheurer,Alexander Hammer,Mario Schubert,Robert-Patrick Steiner,Oliver Gamm,Kaomei Guan,Frank Sonntag,Hagen Malberg,Martin Schmidt*

Main category: cs.LG

TL;DR: 论文提出了一种非侵入性方法，通过基于AI的视频运动分析自动分类iPSC-CMs的成熟度，准确率达99.5%。


<details>
  <summary>Details</summary>
Motivation: iPSC-CMs的成熟度评估通常耗时且可能损伤细胞，因此需要一种非侵入性方法。

Method: 使用支持向量机（SVM）分析视频记录中的10个运动特征，并通过SHAP解释模型。

Result: 优化后的模型在测试集上达到99.5%的准确率，位移、舒张上升时间和跳动持续时间是关键特征。

Conclusion: 该方法可减少实验变异性，提高可重复性，适用于功能测试或药物筛选前的成熟度评估。

Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an
important resource for the identification of new therapeutic targets and
cardioprotective drugs. After differentiation iPSC-CMs show an immature,
fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation
medium (MM) strongly enhances their structural, metabolic and functional
phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging
as most methods are time consuming and go in line with cell damage or loss of
the sample. To address this issue, we developed a non-invasive approach for
automated classification of iPSC-CM maturity through interpretable artificial
intelligence (AI)-based analysis of beat characteristics derived from
video-based motion analysis. In a prospective study, we evaluated 230 video
recordings of early-state, immature iPSC-CMs on day 21 after differentiation
(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10
features were extracted using Maia motion analysis software and entered into a
support vector machine (SVM). The hyperparameters of the SVM were optimized in
a grid search on 80 % of the data using 5-fold cross-validation. The optimized
model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley
Additive Explanations (SHAP) identified displacement, relaxation-rise time and
beating duration as the most relevant features for assessing maturity level.
Our results suggest the use of non-invasive, optical motion analysis combined
with AI-based methods as a tool to assess iPSC-CMs maturity and could be
applied before performing functional readouts or drug testing. This may
potentially reduce the variability and improve the reproducibility of
experimental studies.

</details>


### [258] [Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification](https://arxiv.org/abs/2505.20797)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Main category: cs.LG

TL;DR: 论文探讨了量子模型在解决医学诊断中类别不平衡问题的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在类别不平衡的疾病诊断中效果受限，量子模型因其高维计算能力可能提供解决方案。

Method: 提出利用量子模型的高维计算能力来克服类别不平衡问题。

Result: 量子模型有望比传统模型更有效地处理类别不平衡问题。

Conclusion: 量子模型为医学诊断中的类别不平衡问题提供了新的研究方向。

Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely
medical treatment and enhancing patient survival rates. In recent years,
Machine Learning has revolutionized diagnostic practices by creating
classification models capable of identifying diseases. However, these
classification problems often suffer from significant class imbalances, which
can inhibit the effectiveness of traditional models. Therefore, the interest in
Quantum models has arisen, driven by the captivating promise of overcoming the
limitations of the classical counterpart thanks to their ability to express
complex patterns by mapping data in a higher-dimensional computational space.

</details>


### [259] [Leaner Transformers: More Heads, Less Depth](https://arxiv.org/abs/2505.20802)
*Hemanth Saratchandran,Damien Teney,Simon Lucey*

Main category: cs.LG

TL;DR: 论文挑战了Transformer模型“越大越好”的观念，提出通过增加注意力头数量而非深度来优化模型，减少30-50%参数的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型普遍追求更大规模，但可能存在不必要的参数冗余。论文旨在探索更高效的模型设计方式。

Method: 通过理论分析重新定义多头注意力的作用，发现其能改善注意力块的稳定性，并据此重新设计模型架构，增加注意力头数量同时减少深度。

Result: 在多种任务（如ImageNet-1k、GLUE等）上，模型参数减少30-50%仍能保持准确率。

Conclusion: 增加注意力头数量而非深度是更高效的模型优化方向，挑战了传统观念。

Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms
to capture complex patterns in large datasets, leading to significant
improvements in performance. This success has contributed to the belief that
"bigger means better", leading to ever-increasing model sizes. This paper
challenge this ideology by showing that many existing transformers might be
unnecessarily oversized. We discover a theoretical principle that redefines the
role of multi-head attention. An important benefit of the multiple heads is in
improving the conditioning of the attention block. We exploit this theoretical
insight and redesign popular architectures with an increased number of heads.
The improvement in the conditioning proves so significant in practice that
model depth can be decreased, reducing the parameter count by up to 30-50%
while maintaining accuracy. We obtain consistent benefits across a variety of
transformer-based architectures of various scales, on tasks in computer vision
(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,
TinyStories, and the Long-Range Arena benchmark).

</details>


### [260] [Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models](https://arxiv.org/abs/2505.20804)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Main category: cs.LG

TL;DR: 量子模型（如QNNs和QSVMs）在医疗分类任务中表现出潜力，尤其在数据不平衡时优于经典模型。QSVMs因不易过拟合而表现更优。


<details>
  <summary>Details</summary>
Motivation: 疾病诊断的准确性和及时性对患者生存率至关重要，但传统机器学习模型在数据不平衡时性能受限。量子模型因其高维计算能力可能提供更优解决方案。

Method: 研究比较了量子模型（QNNs和QSVMs）与经典模型在三个医疗数据集（前列腺癌、心衰和糖尿病）上的表现。

Result: QSVMs在所有数据集中表现优于QNNs，且量子模型在高数据不平衡场景下超越经典模型。

Conclusion: 量子模型在医疗分类任务中具有潜力，值得进一步研究。

Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and
heart failure is crucial for timely medical intervention and improving patient
survival rates. Machine learning has revolutionized diagnostic methods in
recent years by developing classification models that detect diseases based on
selected features. However, these classification tasks are often highly
imbalanced, limiting the performance of classical models. Quantum models offer
a promising alternative, exploiting their ability to express complex patterns
by operating in a higher-dimensional computational space through superposition
and entanglement. These unique properties make quantum models potentially more
effective in addressing the challenges of imbalanced datasets. This work
evaluates the potential of quantum classifiers in healthcare, focusing on
Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs),
comparing them with popular classical models. The study is based on three
well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes.
The results indicate that QSVMs outperform QNNs across all datasets due to
their susceptibility to overfitting. Furthermore, quantum models prove the
ability to overcome classical models in scenarios with high dataset imbalance.
Although preliminary, these findings highlight the potential of quantum models
in healthcare classification tasks and lead the way for further research in
this domain.

</details>


### [261] [Simple yet Effective Graph Distillation via Clustering](https://arxiv.org/abs/2505.20807)
*Yurui Lai,Taiyan Zhang,Renchi Yang*

Main category: cs.LG

TL;DR: ClustGDD提出了一种高效且有效的图数据蒸馏方法，通过聚类和同质性优化来压缩大图，显著提升GNN训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有图数据蒸馏方法依赖启发式策略，导致结果质量下降或训练成本高，ClustGDD旨在解决这一问题。

Method: ClustGDD通过快速聚类和同质性最大化合成压缩图，并利用类感知图采样和一致性损失优化节点属性。

Result: 实验表明，ClustGDD在节点分类任务上优于或媲美现有方法，且训练速度显著提升。

Conclusion: ClustGDD为高效GNN训练提供了新思路，结合聚类和同质性优化，具有实际应用潜力。

Abstract: Despite plentiful successes achieved by graph representation learning in
various domains, the training of graph neural networks (GNNs) still remains
tenaciously challenging due to the tremendous computational overhead needed for
sizable graphs in practice. Recently, graph data distillation (GDD), which
seeks to distill large graphs into compact and informative ones, has emerged as
a promising technique to enable efficient GNN training. However, most existing
GDD works rely on heuristics that align model gradients or representation
distributions on condensed and original graphs, leading to compromised result
quality, expensive training for distilling large graphs, or both. Motivated by
this, this paper presents an efficient and effective GDD approach, ClustGDD.
Under the hood, ClustGDD resorts to synthesizing the condensed graph and node
attributes through fast and theoretically-grounded clustering that minimizes
the within-cluster sum of squares and maximizes the homophily on the original
graph. The fundamental idea is inspired by our empirical and theoretical
findings unveiling the connection between clustering and empirical condensation
quality using Fr\'echet Inception Distance, a well-known quality metric for
synthetic images. Furthermore, to mitigate the adverse effects caused by the
homophily-based clustering, ClustGDD refines the nodal attributes of the
condensed graph with a small augmentation learned via class-aware graph
sampling and consistency loss. Our extensive experiments exhibit that GNNs
trained over condensed graphs output by ClustGDD consistently achieve superior
or comparable performance to state-of-the-art GDD methods in terms of node
classification on five benchmark datasets, while being orders of magnitude
faster.

</details>


### [262] [Interpretable Credit Default Prediction with Ensemble Learning and SHAP](https://arxiv.org/abs/2505.20815)
*Shiqi Yang,Ziyi Huang,Wengran Xiao,Xinyu Shen*

Main category: cs.LG

TL;DR: 论文研究了信用违约预测问题，通过机器学习构建模型框架，并比较多种分类算法。结果显示集成学习方法在性能和鲁棒性上表现优越，SHAP方法提升了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决信用违约预测问题，提升信用风险控制系统的智能化发展。

Method: 预处理、特征工程和模型训练，比较逻辑回归、随机森林、XGBoost、LightGBM等算法，使用SHAP方法分析特征。

Result: 集成学习方法在复杂非线性关系和数据不平衡问题上表现优越，外部信用评分变量对模型决策起主导作用。

Conclusion: 研究结果为信用风险控制系统的智能化提供了有效参考和技术支持。

Abstract: This study focuses on the problem of credit default prediction, builds a
modeling framework based on machine learning, and conducts comparative
experiments on a variety of mainstream classification algorithms. Through
preprocessing, feature engineering, and model training of the Home Credit
dataset, the performance of multiple models including logistic regression,
random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and
recall is evaluated. The results show that the ensemble learning method has
obvious advantages in predictive performance, especially in dealing with
complex nonlinear relationships between features and data imbalance problems.
It shows strong robustness. At the same time, the SHAP method is used to
analyze the importance and dependency of features, and it is found that the
external credit score variable plays a dominant role in model decision making,
which helps to improve the model's interpretability and practical application
value. The research results provide effective reference and technical support
for the intelligent development of credit risk control systems.

</details>


### [263] [HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling](https://arxiv.org/abs/2505.20836)
*Hexiong Yang,Mingrui Chen,Huaibo Huang,Junxian Duan,Jie Cao,Zhen Zhou,Ran He*

Main category: cs.LG

TL;DR: 提出了一种混合架构蒸馏（HAD）方法，通过结合蒸馏和重建任务，更高效地进行DNA序列建模的预训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大数据或大模型，计算负担重；尝试用紧凑模型效果不佳。

Method: 使用NTv2-500M作为教师模型，设计分组掩码策略，对齐可见标记的特征嵌入并重建不可见标记。

Result: 在多个基准测试中表现优异，甚至超越参数大500倍的教师模型。

Conclusion: HAD方法高效且有效，能深入理解基因组序列的内在表示模式。

Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the
natural language domain, the paradigm of self-supervised pre-training and
fine-tuning has also achieved remarkable progress in the field of DNA sequence
modeling. However, previous methods often relied on massive pre-training data
or large-scale base models with huge parameters, imposing a significant
computational burden. To address this, many works attempted to use more compact
models to achieve similar outcomes but still fell short by a considerable
margin. In this work, we propose a Hybrid Architecture Distillation (HAD)
approach, leveraging both distillation and reconstruction tasks for more
efficient and effective pre-training. Specifically, we employ the NTv2-500M as
the teacher model and devise a grouping masking strategy to align the feature
embeddings of visible tokens while concurrently reconstructing the invisible
tokens during MLM pre-training. To validate the effectiveness of our proposed
method, we conducted comprehensive experiments on the Nucleotide Transformer
Benchmark and Genomic Benchmark. Compared to models with similar parameters,
our model achieved excellent performance. More surprisingly, it even surpassed
the distillation ceiling-teacher model on some sub-tasks, which is more than
500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization,
which shows that our model can gain a sophisticated understanding of the
intrinsic representation pattern in genomic sequences.

</details>


### [264] [FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](https://arxiv.org/abs/2505.20839)
*Daehyeon Baek,Jieun Choi,Jimyoung Son,Kyungmin Bin,Seungbeom Choi,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.LG

TL;DR: FireQ是一个针对大语言模型（LLM）的量化框架，通过INT4-FP8混合精度量化和三阶段流水线技术，显著提升推理吞吐量，同时减少精度损失。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，内存带宽限制成为推理吞吐量的瓶颈，因此需要高效的训练后量化（PTQ）方法。

Method: FireQ采用INT4-FP8混合精度量化，对线性层和注意力层分别设计量化策略，并引入三阶段流水线优化预填充阶段。

Result: FireQ在Llama2-7B和Llama3-8B上分别实现了1.68倍和1.26倍的推理加速，且精度损失可忽略。

Conclusion: FireQ通过创新的量化技术和流水线优化，显著提升了LLM的推理效率，为实际应用提供了高效解决方案。

Abstract: As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ quantizes linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from quantization, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
quantization scaling factor of INT4 quantization, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
quantization challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.

</details>


### [265] [Aggregation Buffer: Revisiting DropEdge with a New Parameter Block](https://arxiv.org/abs/2505.20840)
*Dooho Lee,Myeong Kong,Sagad Hamid,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.LG

TL;DR: 本文重新审视了DropEdge技术，提出其性能受限的理论原因，并设计Aggregation Buffer以提升GNN的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DropEdge虽能减少过拟合，但其性能提升有限，作者希望探究原因并提出改进方案。

Method: 通过理论分析DropEdge的局限性，提出Aggregation Buffer参数块，兼容任何GNN模型。

Result: 在多个数据集上表现一致提升，同时解决了度偏差和结构差异问题。

Conclusion: Aggregation Buffer有效解决了DropEdge的局限性，为GNN提供了一种统一的改进方案。

Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly
removes edges to expose diverse graph structures during training. While being a
promising approach to effectively reduce overfitting on specific connections in
the graph, we observe that its potential performance gain in supervised
learning tasks is significantly limited. To understand why, we provide a
theoretical analysis showing that the limited performance of DropEdge comes
from the fundamental limitation that exists in many GNN architectures. Based on
this analysis, we propose Aggregation Buffer, a parameter block specifically
designed to improve the robustness of GNNs by addressing the limitation of
DropEdge. Our method is compatible with any GNN model, and shows consistent
performance improvements on multiple datasets. Moreover, our method effectively
addresses well-known problems such as degree bias or structural disparity as a
unifying solution. Code and datasets are available at
https://github.com/dooho00/agg-buffer.

</details>


### [266] [Cooperation of Experts: Fusing Heterogeneous Information with Large Margin](https://arxiv.org/abs/2505.20853)
*Shuo Wang,Shunyang Huang,Jinghui Yuan,Zhixiang Shen,Zhao Kang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoE的框架，通过编码多类型信息到统一的异构多重网络中，解决异构信息融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分考虑不同语义空间中对象模式的异质性，因此需要一种更灵活且强大的模型。

Method: CoE框架使用专用编码器作为领域专家，通过大间隔机制和优化策略协作学习不同语义空间的关系模式。

Result: 理论分析证明了框架的可行性和稳定性，实验表明其在多种基准测试中表现优异。

Conclusion: CoE框架为复杂数据的异构信息融合提供了高效且通用的解决方案。

Abstract: Fusing heterogeneous information remains a persistent challenge in modern
data analysis. While significant progress has been made, existing approaches
often fail to account for the inherent heterogeneity of object patterns across
different semantic spaces. To address this limitation, we propose the
Cooperation of Experts (CoE) framework, which encodes multi-typed information
into unified heterogeneous multiplex networks. By overcoming modality and
connection differences, CoE provides a powerful and flexible model for
capturing the intricate structures of real-world complex data. In our
framework, dedicated encoders act as domain-specific experts, each specializing
in learning distinct relational patterns in specific semantic spaces. To
enhance robustness and extract complementary knowledge, these experts
collaborate through a novel large margin mechanism supported by a tailored
optimization strategy. Rigorous theoretical analyses guarantee the framework's
feasibility and stability, while extensive experiments across diverse
benchmarks demonstrate its superior performance and broad applicability. Our
code is available at https://github.com/strangeAlan/CoE.

</details>


### [267] [Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization](https://arxiv.org/abs/2505.20881)
*Yiding Shi,Jianan Zhou,Wen Song,Jieyi Bi,Yaoxin Wu,Jie Zhang*

Main category: cs.LG

TL;DR: MoH框架通过元学习优化启发式算法，利用LLM自动构建多样化优化器，提升组合优化问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义的进化计算优化器和单任务训练，限制了启发式算法的多样性和泛化能力。

Method: 提出MoH框架，利用LLM迭代优化元优化器，自动构建多样化优化器，并采用多任务训练。

Result: 实验表明MoH在经典组合优化问题中表现优异，尤其在跨尺寸任务中达到最优性能。

Conclusion: MoH通过元学习和多任务训练，显著提升了启发式算法的探索能力和泛化性能。

Abstract: Heuristic design with large language models (LLMs) has emerged as a promising
approach for tackling combinatorial optimization problems (COPs). However,
existing approaches often rely on manually predefined evolutionary computation
(EC) optimizers and single-task training schemes, which may constrain the
exploration of diverse heuristic algorithms and hinder the generalization of
the resulting heuristics. To address these issues, we propose Meta-Optimization
of Heuristics (MoH), a novel framework that operates at the optimizer level,
discovering effective optimizers through the principle of meta-learning.
Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that
autonomously constructs diverse optimizers through (self-)invocation, thereby
eliminating the reliance on a predefined EC optimizer. These constructed
optimizers subsequently evolve heuristics for downstream tasks, enabling
broader heuristic exploration. Moreover, MoH employs a multi-task training
scheme to promote its generalization capability. Experiments on classic COPs
demonstrate that MoH constructs an effective and interpretable meta-optimizer,
achieving state-of-the-art performance across various downstream tasks,
particularly in cross-size settings.

</details>


### [268] [Improved Bounds for Swap Multicalibration and Swap Omniprediction](https://arxiv.org/abs/2505.20885)
*Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: 本文提出了一种高效算法，显著改进了多校准和全预测的误差率，并在分布和在线设置中取得了多项改进的样本复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 解决Garg等人（2024）提出的关于高效实现$O(\sqrt{T})$ $\ell_{2}$-多校准误差的开放性问题。

Method: 提出了一种高效算法，实现了$O(T^{\frac{1}{3}})$ $\ell_{2}$-交换多校准误差，并进一步改进了$\ell_{1}$-交换多校准和交换全预测的误差率。

Result: 算法在在线和分布设置中显著提升了性能，如$O(\varepsilon^{-3})$样本复杂度学习$\varepsilon$-交换全预测器。

Conclusion: 本文通过高效算法解决了开放性问题，并在多校准和全预测领域取得了多项突破性进展。

Abstract: In this paper, we consider the related problems of multicalibration -- a
multigroup fairness notion and omniprediction -- a simultaneous loss
minimization paradigm, both in the distributional and online settings. The
recent work of Garg et al. (2024) raised the open problem of whether it is
possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error
against bounded linear functions. In this paper, we answer this question in a
strongly affirmative sense. We propose an efficient algorithm that achieves
$O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high
probability and expectation). On propagating this bound onward, we obtain
significantly improved rates for $\ell_{1}$-swap multicalibration and swap
omniprediction for a loss class of convex Lipschitz functions. In particular,
we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap
multicalibration and swap omniprediction errors, thereby improving upon the
previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our
improved online results, we further obtain several improved sample complexity
rates in the distributional setting. In particular, we establish a
$O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an
$\varepsilon$-swap omnipredictor for the class of convex and Lipschitz
functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning
an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon
^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1},
\ell_{2}$-swap multicalibrated predictors against linear functions, all of
which significantly improve on the previous best-known bounds.

</details>


### [269] [One-Time Soft Alignment Enables Resilient Learning without Weight Transport](https://arxiv.org/abs/2505.20892)
*Jeonghwan Cheon,Jaehyuk Bae,Se-Bum Paik*

Main category: cs.LG

TL;DR: 通过一次性软对齐前向和反馈权重，使深度网络在不依赖对称权重传输的情况下达到与反向传播相当的性能。


<details>
  <summary>Details</summary>
Motivation: 反向传播依赖对称权重传输和全局同步，计算成本高且生物学上不现实；反馈对齐虽避免了对称权重传输，但学习性能和稳定性较差。

Method: 在初始化时进行一次软对齐前向和反馈权重，避免学习过程中的权重传输。

Result: 初始对齐提高了网络的可训练性，促进平滑梯度流和收敛到更平坦的极小值，提升泛化和鲁棒性。适度偏离对称权重还能增强对抗鲁棒性。

Conclusion: 简单的初始化策略可在生物学合理且资源高效的方式下实现深度网络的有效学习。

Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on
symmetric weight transport and global synchronization makes it computationally
expensive and biologically implausible. Feedback alignment offers a promising
alternative by approximating error gradients through fixed random feedback,
thereby avoiding symmetric weight transport. However, this approach often
struggles with poor learning performance and instability, especially in deep
networks. Here, we show that a one-time soft alignment between forward and
feedback weights at initialization enables deep networks to achieve performance
comparable to backpropagation, without requiring weight transport during
learning. This simple initialization condition guides stable error minimization
in the loss landscape, improving network trainability. Spectral analyses
further reveal that initial alignment promotes smoother gradient flow and
convergence to flatter minima, resulting in better generalization and
robustness. Notably, we also find that allowing moderate deviations from exact
weight symmetry can improve adversarial robustness compared to standard
backpropagation. These findings demonstrate that a simple initialization
strategy can enable effective learning in deep networks in a biologically
plausible and resource-efficient manner.

</details>


### [270] [DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition](https://arxiv.org/abs/2505.20894)
*Marius Bock,Michael Moeller,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: DeepConvContext提出了一种多尺度时间序列分类框架，通过处理时间有序窗口序列，解决了传统滑动窗口方法在HAR中长程时间依赖建模的局限性，性能优于DeepConvLSTM。


<details>
  <summary>Details</summary>
Motivation: 传统HAR方法（如DeepConvLSTM）依赖滑动窗口独立分类，限制了时间上下文的建模能力。本文旨在通过多尺度框架解决这一问题。

Method: 提出DeepConvContext框架，基于LSTM建模窗口内和窗口间的时间模式，不依赖注意力机制。

Result: 在六个HAR基准测试中，平均F1分数提升10%，最高提升21%。

Conclusion: DeepConvContext通过多尺度时间建模显著提升了HAR性能，LSTM在惯性传感器数据中表现优于注意力机制。

Abstract: Despite recognized limitations in modeling long-range temporal dependencies,
Human Activity Recognition (HAR) has traditionally relied on a sliding window
approach to segment labeled datasets. Deep learning models like the
DeepConvLSTM typically classify each window independently, thereby restricting
learnable temporal context to within-window information. To address this
constraint, we propose DeepConvContext, a multi-scale time series
classification framework for HAR. Drawing inspiration from the vision-based
Temporal Action Localization community, DeepConvContext models both intra- and
inter-window temporal patterns by processing sequences of time-ordered windows.
Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext
relies solely on LSTMs -- with ablation studies demonstrating the superior
performance of LSTMs over attention-based variants for modeling inertial sensor
data. Across six widely-used HAR benchmarks, DeepConvContext achieves an
average 10% improvement in F1-score over the classic DeepConvLSTM, with gains
of up to 21%. Code to reproduce our experiments is publicly available via
github.com/mariusbock/context_har.

</details>


### [271] [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
*Yiwei Wu,Atticus Geiger,Raphaël Millière*

Main category: cs.LG

TL;DR: Transformer模型通过训练学习实现变量绑定，无需显式架构支持，展示了连接主义与符号方法的结合。


<details>
  <summary>Details</summary>
Motivation: 研究现代神经网络如何在没有内置绑定操作的情况下实现变量绑定，这是符号计算和认知的基础。

Method: 训练Transformer模型解引用符号程序中的变量，程序包含深度达四步的变量赋值链和干扰项。

Result: 模型经历了三个训练阶段，最终利用残差流作为可寻址内存空间，通过注意力头实现变量绑定。

Conclusion: Transformer能够学习系统化的变量绑定机制，为连接主义和符号方法的结合提供了实证。

Abstract: Variable binding -- the ability to associate variables with values -- is
fundamental to symbolic computation and cognition. Although classical
architectures typically implement variable binding via addressable memory, it
is not well understood how modern neural networks lacking built-in binding
operations may acquire this capacity. We investigate this by training a
Transformer to dereference queried variables in symbolic programs where
variables are assigned either numerical constants or other variables. Each
program requires following chains of variable assignments up to four steps deep
to find the queried value, and also contains irrelevant chains of assignments
acting as distractors. Our analysis reveals a developmental trajectory with
three distinct phases during training: (1) random prediction of numerical
constants, (2) a shallow heuristic prioritizing early variable assignments, and
(3) the emergence of a systematic mechanism for dereferencing assignment
chains. Using causal interventions, we find that the model learns to exploit
the residual stream as an addressable memory space, with specialized attention
heads routing information across token positions. This mechanism allows the
model to dynamically track variable bindings across layers, resulting in
accurate dereferencing. Our results show how Transformer models can learn to
implement systematic variable binding without explicit architectural support,
bridging connectionist and symbolic approaches.

</details>


### [272] [Humble AI in the real-world: the case of algorithmic hiring](https://arxiv.org/abs/2505.20918)
*Rahul Nair,Inge Vejsbjerg,Elizabeth Daly,Christos Varytimidis,Bran Knowles*

Main category: cs.LG

TL;DR: 论文提出“谦逊AI”概念，强调在AI开发中需谨慎，通过怀疑、好奇和承诺应对统计学习的局限性、意外结果及多元价值观。以算法招聘为例，展示了如何通过不确定性量化等技术实现谦逊AI，并探讨其对信任的影响。


<details>
  <summary>Details</summary>
Motivation: 传统AI开发可能忽视统计学习的局限性、意外结果及多元价值观，导致算法招聘中的误识别和刻板印象问题。

Method: 在招聘平台中应用谦逊AI原则，通过不确定性量化、熵估计和用户体验设计（突出算法未知性）实现技术可行性。

Result: 初步与招聘人员焦点小组讨论表明，谦逊AI系统可能增加认知负荷，但有望提升对结果的信任。

Conclusion: 谦逊AI为AI开发提供新方向，未来需进一步用户研究验证其对信任的促进作用。

Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development
and deployments through scepticism (accounting for limitations of statistical
learning), curiosity (accounting for unexpected outcomes), and commitment
(accounting for multifaceted values beyond performance). We present a
real-world case study for humble AI in the domain of algorithmic hiring.
Specifically, we evaluate virtual screening algorithms in a widely used hiring
platform that matches candidates to job openings. There are several challenges
in misrecognition and stereotyping in such contexts that are difficult to
assess through standard fairness and trust frameworks; e.g., someone with a
non-traditional background is less likely to rank highly. We demonstrate
technical feasibility of how humble AI principles can be translated to practice
through uncertainty quantification of ranks, entropy estimates, and a user
experience that highlights algorithmic unknowns. We describe preliminary
discussions with focus groups made up of recruiters. Future user studies seek
to evaluate whether the higher cognitive load of a humble AI system fosters a
climate of trust in its outcomes.

</details>


### [273] [Label Leakage in Federated Inertial-based Human Activity Recognition](https://arxiv.org/abs/2505.20924)
*Marius Bock,Maximilian Hopp,Kristof Van Laerhoven,Michael Moeller*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中标签重建攻击在人类活动识别（HAR）中的有效性，发现活动类别数量、采样策略和类别不平衡是关键因素，重建准确率可达90%。局部差分隐私技术保护有限，提出了隐私保护部署建议。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别（HAR）中的标签信息具有敏感性，但此前未在联邦学习背景下研究标签重建攻击的有效性。

Method: 在HAR基准数据集上评估了基于梯度的标签泄漏攻击，分析了活动类别数量、采样策略和类别不平衡的影响。

Result: 重建准确率高达90%，局部差分隐私技术（如梯度噪声和裁剪）保护效果有限。

Conclusion: 提出了隐私保护的HAR系统部署建议，并指出未来研究的开放挑战。

Abstract: While prior work has shown that Federated Learning updates can leak sensitive
information, label reconstruction attacks, which aim to recover input labels
from shared gradients, have not yet been examined in the context of Human
Activity Recognition (HAR). Given the sensitive nature of activity labels, this
study evaluates the effectiveness of state-of-the-art gradient-based label
leakage attacks on HAR benchmark datasets. Our findings show that the number of
activity classes, sampling strategy, and class imbalance are critical factors
influencing the extent of label leakage, with reconstruction accuracies
reaching up to 90% on two benchmark datasets, even for trained models.
Moreover, we find that Local Differential Privacy techniques such as gradient
noise and clipping offer only limited protection, as certain attacks still
reliably infer both majority and minority class labels. We conclude by offering
practical recommendations for the privacy-aware deployment of federated HAR
systems and identify open challenges for future research. Code to reproduce our
experiments is publicly available via github.com/mariusbock/leakage_har.

</details>


### [274] [MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models](https://arxiv.org/abs/2505.20930)
*Ruiqi Zhang,Simon H. Tindemans*

Main category: cs.LG

TL;DR: 本文提出了一种考虑训练时间的速度指标，并采用主动学习方法提升多级蒙特卡洛（MLMC）在电力系统可靠性评估中的效率。


<details>
  <summary>Details</summary>
Motivation: 在资源充足性评估中，预标记数据集通常不可用，且大规模系统中训练数据的标注时间会抵消代理模型的效率提升。

Method: 引入速度指标评估MLMC效率，并提出基于委员会投票的主动学习方法以减少标注需求。

Result: 案例研究表明，主动学习方法在有限时间预算内显著提升了MLMC效率，同时减少了训练成本。

Conclusion: 主动学习方法在电力系统可靠性评估中能有效提升MLMC效率，优于传统代理模型方法。

Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction
technique for accelerating reliability assessments of complex power system.
Recently, data-driven surrogate models have been proposed as lower-level models
in the MLMC framework due to their high correlation and negligible execution
time once trained. However, in resource adequacy assessments, pre-labeled
datasets are typically unavailable. For large-scale systems, the efficiency
gains from surrogate models are often offset by the substantial time required
for labeling training data. Therefore, this paper introduces a speed metric
that accounts for training time in evaluating MLMC efficiency. Considering the
total time budget is limited, a vote-by-committee active learning approach is
proposed to reduce the required labeling calls. A case study demonstrates that,
within practical variance thresholds, active learning enables significantly
improved MLMC efficiency with reduced training effort, compared to regular
surrogate modelling approaches.

</details>


### [275] [NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion](https://arxiv.org/abs/2505.20934)
*Max Collins,Jordan Vice,Tim French,Ajmal Mian*

Main category: cs.LG

TL;DR: NatADiff是一种利用去噪扩散生成自然对抗样本的方法，旨在更真实地反映现实场景中的测试错误，提升对抗样本的迁移性和图像保真度。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本研究多集中于受限样本，无法准确反映现实测试错误。NatADiff通过生成自然对抗样本，揭示模型分类特征，提升对抗鲁棒性。

Method: 结合去噪扩散和时间旅行采样，引导扩散轨迹至真实类别与对抗类别的交集，增强攻击迁移性并保持图像质量。

Result: NatADiff在攻击成功率上与现有技术相当，同时在模型间迁移性和自然测试错误相似性（FID衡量）上表现更优。

Conclusion: NatADiff生成的对抗样本不仅迁移性更强，且更接近自然测试错误，为对抗样本研究提供了新方向。

Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by
deep learning models to cause misclassifications. The study of these
adversarial samples provides insight into the features a model uses to classify
inputs, which can be leveraged to improve robustness against future attacks.
However, much of the existing literature focuses on constrained adversarial
samples, which do not accurately reflect test-time errors encountered in
real-world settings. To address this, we propose `NatADiff', an adversarial
sampling scheme that leverages denoising diffusion to generate natural
adversarial samples. Our approach is based on the observation that natural
adversarial samples frequently contain structural elements from the adversarial
class. Deep learning models can exploit these structural elements to shortcut
the classification process, rather than learning to genuinely distinguish
between classes. To leverage this behavior, we guide the diffusion trajectory
towards the intersection of the true and adversarial classes, combining
time-travel sampling with augmented classifier guidance to enhance attack
transferability while preserving image fidelity. Our method achieves comparable
attack success rates to current state-of-the-art techniques, while exhibiting
significantly higher transferability across model architectures and better
alignment with natural test-time errors as measured by FID. These results
demonstrate that NatADiff produces adversarial samples that not only transfer
more effectively across models, but more faithfully resemble naturally
occurring test-time errors.

</details>


### [276] [Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning](https://arxiv.org/abs/2505.20938)
*Chongjie Si,Yidan Cui,Fuchao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的部分多标签学习方法Schirn，通过在高噪声标签矩阵上施加稀疏约束，同时保持预测标签矩阵的高秩特性，解决了现有方法中稀疏性和低秩性假设冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的部分多标签学习方法依赖于噪声标签矩阵的稀疏性和真实标签矩阵的低秩性假设，但这些假设在现实场景中往往是冲突且不切实际的。

Method: 提出Schirn方法，对噪声标签矩阵施加稀疏约束，同时强制预测标签矩阵具有高秩特性。

Result: 实验表明，Schirn在性能上优于现有方法，能够有效应对现实中的部分多标签学习挑战。

Conclusion: Schirn方法通过解决稀疏性和低秩性假设的冲突，显著提升了部分多标签学习的性能。

Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm
to scenarios where each sample is associated with a candidate label set
containing both ground-truth labels and noisy labels. Existing PML methods
commonly rely on two assumptions: sparsity of the noise label matrix and
low-rankness of the ground-truth label matrix. However, these assumptions are
inherently conflicting and impractical for real-world scenarios, where the true
label matrix is typically full-rank or close to full-rank. To address these
limitations, we demonstrate that the sparsity constraint contributes to the
high-rank property of the predicted label matrix. Based on this, we propose a
novel method Schirn, which introduces a sparsity constraint on the noise label
matrix while enforcing a high-rank property on the predicted label matrix.
Extensive experiments demonstrate the superior performance of Schirn compared
to state-of-the-art methods, validating its effectiveness in tackling
real-world PML challenges.

</details>


### [277] [Efficient Spectral Control of Partially Observed Linear Dynamical Systems](https://arxiv.org/abs/2505.20943)
*Anand Brahmbhatt,Gon Buzaglo,Sofiia Druchyna,Elad Hazan*

Main category: cs.LG

TL;DR: 提出了一种名为DSC的新算法，用于部分观测和对抗扰动下的线性动态系统控制问题，在保持最佳已知遗憾保证的同时，显著提高了运行时间效率。


<details>
  <summary>Details</summary>
Motivation: 解决在部分观测和对抗扰动下控制线性动态系统的问题，同时优化算法的运行时间。

Method: 采用双级谱逼近策略，利用双卷积和通用谱滤波器基，高效准确地学习最佳线性动态控制器。

Result: DSC算法在保持最佳遗憾保证的同时，显著提高了运行时间效率。

Conclusion: DSC算法在控制线性动态系统方面具有高效性和准确性，适用于部分观测和对抗扰动场景。

Abstract: We propose a new method for the problem of controlling linear dynamical
systems under partial observation and adversarial disturbances. Our new
algorithm, Double Spectral Control (DSC), matches the best known regret
guarantees while exponentially improving runtime complexity over previous
approaches in its dependence on the system's stability margin. Our key
innovation is a two-level spectral approximation strategy, leveraging double
convolution with a universal basis of spectral filters, enabling efficient and
accurate learning of the best linear dynamical controllers.

</details>


### [278] [Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence](https://arxiv.org/abs/2505.20964)
*Mehdi Bennis,Salem Lahlou*

Main category: cs.LG

TL;DR: 本文提出了一种基于系统2认知原则的统一研究愿景，旨在通过抽象、组合性和涌现通信实现语义理解和目标导向交互，推动6G与AI的深度融合。


<details>
  <summary>Details</summary>
Motivation: 当前6G愿景仍停留在5G的渐进式改进，而AI模型存在脆弱性和数据依赖性。本文旨在突破纯技术层面的通信，实现语义理解和智能协作。

Method: 提出基于系统2认知的三个支柱：抽象（从原始数据学习世界模型）、组合性（组合概念与子系统）和涌现通信（创建自适应语言）。

Result: 通过整合这些原则，为智能系统的推理、适应和协作奠定基础，统一无线通信、机器学习和机器人技术的进展。

Conclusion: 本文为6G与AI的深度融合提供了理论基础，推动真正智能系统的发展。

Abstract: The trajectories of 6G and AI are set for a creative collision. However,
current visions for 6G remain largely incremental evolutions of 5G, while
progress in AI is hampered by brittle, data-hungry models that lack robust
reasoning capabilities. This paper argues for a foundational paradigm shift,
moving beyond the purely technical level of communication toward systems
capable of semantic understanding and effective, goal-oriented interaction. We
propose a unified research vision rooted in the principles of System-2
cognition, built upon three pillars: Abstraction, enabling agents to learn
meaningful world models from raw sensorimotor data; Compositionality, providing
the algebraic tools to combine learned concepts and subsystems; and Emergent
Communication, allowing intelligent agents to create their own adaptive and
grounded languages. By integrating these principles, we lay the groundwork for
truly intelligent systems that can reason, adapt, and collaborate, unifying
advances in wireless communications, machine learning, and robotics under a
single coherent framework.

</details>


### [279] [Understanding the behavior of representation forgetting in continual learning](https://arxiv.org/abs/2505.20970)
*Joonkyu Kim,Yejin Kim,Jy-yong Sohn*

Main category: cs.LG

TL;DR: 本文提出了一个新的度量标准“表示差异”来分析持续学习中的表示遗忘，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，先前任务的灾难性遗忘是一个关键问题，而表示遗忘（隐藏层的遗忘）尚未得到充分研究。本文旨在填补这一空白。

Method: 引入表示差异度量标准，通过数学分析和实验验证其在持续学习中的行为。

Result: 发现表示遗忘随着层数增加而加速，而增加网络宽度可以减缓遗忘。实验支持了理论发现。

Conclusion: 表示差异是分析持续学习中表示遗忘的有效工具，为理解其动态提供了新视角。

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [280] [Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs](https://arxiv.org/abs/2505.20972)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Main category: cs.LG

TL;DR: Deep $k$-grouping 是一种基于无监督学习的组合优化框架，用于解决大规模图和超图上的 $k$-分组问题，通过 GPU 加速和新型优化策略显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督神经网络求解器在大规模图和超图的 $k$-分组问题上表现不佳，主要受限于计算框架。

Method: 提出 OH-PUBO 建模方法，结合 GPU 加速算法和 Gini 系数退火策略，通过无监督学习优化目标函数。

Result: 实验表明，Deep $k$-grouping 在性能上优于现有神经网络求解器和经典启发式方法（如 SCIP 和 Tabu）。

Conclusion: Deep $k$-grouping 为大规模 $k$-分组问题提供了高效且可扩展的解决方案。

Abstract: Along with AI computing shining in scientific discovery, its potential in the
combinatorial optimization (CO) domain has also emerged in recent years. Yet,
existing unsupervised neural network solvers struggle to solve $k$-grouping
problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,
due to limited computational frameworks. In this work, we propose Deep
$k$-grouping, an unsupervised learning-based CO framework. Specifically, we
contribute: Novel one-hot encoded polynomial unconstrained binary optimization
(OH-PUBO), a formulation for modeling k-grouping problems on graphs and
hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated
algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs
the relaxation of large-scale OH-PUBO objectives as differentiable loss
functions and trains to optimize them in an unsupervised manner. To ensure
scalability, it leverages GPU-accelerated algorithms to unify the training
pipeline; A Gini coefficient-based continuous relaxation annealing strategy to
enforce discreteness of solutions while preventing convergence to local optima.
Experimental results demonstrate that Deep $k$-grouping outperforms existing
neural network solvers and classical heuristics such as SCIP and Tabu.

</details>


### [281] [BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks](https://arxiv.org/abs/2505.20997)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Main category: cs.LG

TL;DR: BIPNN是一种基于超图神经网络的非监督学习框架，用于解决非线性二进制整数规划问题，通过GPU加速和连续退火增强训练流程显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络求解器在处理非线性二进制整数规划问题时缺乏扩展性，而传统Branch-and-Cut方法因线性松弛导致计算复杂度高。

Method: BIPNN将非线性BIP问题转化为无约束、可微分多项式损失函数，利用超图神经网络进行端到端优化，并采用GPU加速和连续退火技术。

Result: 实验表明，BIPNN能高效并行优化大规模非线性BIP问题，显著降低训练成本并生成高质量离散解。

Conclusion: BIPNN在解决非线性BIP问题上表现出优越性，为相关领域提供了新的高效解决方案。

Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains
requiring discrete decision-making. As the advance of AI computing, recent
works explore neural network-based solvers for integer linear programming (ILP)
problems. Yet, they lack scalability for tackling nonlinear challenges. To
handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear
relaxations, leading to exponential growth in auxiliary variables and severe
computation limitations. To overcome these limitations, we propose BIPNN
(Binary Integer Programming Neural Network), an unsupervised learning framework
to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).
Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear
(sin, log, exp) optimization problems-into unconstrained, differentiable, and
polynomial loss functions. The reformulation stems from the observation of a
precise one-to-one mapping between polynomial BIP objectives and hypergraph
structures, enabling the unsupervised training of HyperGNN to optimize BIP
problems in an end-to-end manner. On this basis, we propose a GPU-accelerated
and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline
enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel
via straightforward gradient descent, thus significantly reducing the training
cost while ensuring the generation of discrete, high-quality solutions.
Extensive experiments on synthetic and real-world datasets highlight the
superiority of our approach.

</details>


### [282] [Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models](https://arxiv.org/abs/2505.21005)
*Fengzhe Zhang,Laurence I. Midgley,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: VT-DIS是一种轻量级后训练方法，通过调整预训练SBDM的噪声协方差，优化重要性采样，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决SBDM中不完美分数估计导致的偏差问题，同时避免传统方法的高计算成本。

Method: 通过最小化α-散度（α=2）调整噪声协方差，为联合前向-反向过程分配重要性权重。

Result: 在多个基准测试中，VT-DIS实现了高效样本利用率（80%、35%、3.5%），计算成本显著低于传统方法。

Conclusion: VT-DIS是一种高效且计算成本低的方法，适用于Boltzmann分布的无偏估计。

Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for
Boltzmann distributions; however, imperfect score estimates bias downstream
Monte Carlo estimates. Classical importance sampling (IS) can correct this
bias, but computing exact likelihoods requires solving the probability-flow
ordinary differential equation (PF-ODE), a procedure that is prohibitively
costly and scales poorly with dimensionality. We introduce Variance-Tuned
Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that
adapts the per-step noise covariance of a pretrained SBDM by minimizing the
$\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse
denoising trajectories. VT-DIS assigns a single trajectory-wise importance
weight to the joint forward-reverse process, yielding unbiased expectation
estimates at test time with negligible overhead compared to standard sampling.
On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective
sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using
only a fraction of the computational budget required by vanilla diffusion + IS
or PF-ODE-based IS.

</details>


### [283] [Federated Instrumental Variable Analysis via Federated Generalized Method of Moments](https://arxiv.org/abs/2505.21012)
*Geetika,Somya Tyagi,Bapi Chatterjee*

Main category: cs.LG

TL;DR: 提出了一种名为FedIV的联邦学习算法，用于高维工具变量分析，通过联邦广义矩估计（FedGMM）实现，解决了非独立同分布数据的隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对广义矩估计（GMM）或工具变量（IV）分析的联邦学习算法，而联邦学习在保护数据隐私的同时训练模型的需求日益增长。

Method: 将FedGMM建模为一个联邦零和游戏，通过非凸非凹极小极大优化问题实现，并使用联邦梯度下降上升（FedGDA）算法求解。

Result: 理论证明了FedGDA极限点的局部最优性，并通过实验验证了算法的有效性。

Conclusion: FedIV通过FedGMM成功实现了联邦环境下的工具变量分析，为高维非独立同分布数据提供了隐私保护的解决方案。

Abstract: Instrumental variables (IV) analysis is an important applied tool for areas
such as healthcare and consumer economics. For IV analysis in high-dimensional
settings, the Generalized Method of Moments (GMM) using deep neural networks
offers an efficient approach. With non-i.i.d. data sourced from scattered
decentralized clients, federated learning is a popular paradigm for training
the models while promising data privacy. However, to our knowledge, no
federated algorithm for either GMM or IV analysis exists to date. In this work,
we introduce federated instrumental variables analysis (FedIV) via federated
generalized method of moments (FedGMM). We formulate FedGMM as a federated
zero-sum game defined by a federated non-convex non-concave minimax
optimization problem, which is solved using federated gradient descent ascent
(FedGDA) algorithm. One key challenge arises in theoretically characterizing
the federated local optimality. To address this, we present properties and
existence results of clients' local equilibria via FedGDA limit points.
Thereby, we show that the federated solution consistently estimates the local
moment conditions of every participating client. The proposed algorithm is
backed by extensive experiments to demonstrate the efficacy of our approach.

</details>


### [284] [NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation](https://arxiv.org/abs/2505.21020)
*Yuan Gao,Ruiqi Shu,Hao Wu,Fan Xu,Yanfei Xiang,Ruijian Gou,Qingsong Wen,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度交互图神经网络的神经海洋模型（NeuralOM），用于S2S海洋模拟，解决了现有ML模型物理一致性和慢变特性不足的问题。


<details>
  <summary>Details</summary>
Motivation: S2S海洋模拟对海洋研究至关重要，但传统数值方法和现有ML模型在物理一致性和慢变特性方面存在不足。

Method: 设计了多阶段框架和多尺度交互消息模块，以模拟海洋的慢变特性和复杂动力学行为。

Result: 实验表明，NeuralOM在S2S和极端事件模拟中优于现有模型。

Conclusion: NeuralOM通过结合物理一致性和慢变特性，显著提升了S2S海洋模拟的准确性。

Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically
important for marine research, yet remains challenging due to its substantial
thermal inertia and extended time delay. Machine learning (ML)-based models
have demonstrated significant advancements in simulation accuracy and
computational efficiency compared to traditional numerical methods.
Nevertheless, a significant limitation of current ML models for S2S ocean
simulation is their inadequate incorporation of physical consistency and the
slow-changing properties of the ocean system. In this work, we propose a neural
ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive
graph neural network to emulate diverse physical phenomena associated with
ocean systems effectively. Specifically, we propose a multi-stage framework
tailored to model the ocean's slowly changing nature. Additionally, we
introduce a multi-scale interactive messaging module to capture complex
dynamical behaviors, such as gradient changes and multiplicative coupling
relationships inherent in ocean dynamics. Extensive experimental evaluations
confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S
and extreme event simulation. The codes are available at
https://github.com/YuanGao-YG/NeuralOM.

</details>


### [285] [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
*Charles London,Varun Kanade*

Main category: cs.LG

TL;DR: 暂停符号（如“...”）能提升Transformer性能，本文首次证明其在理论上的计算表达能力增强。


<details>
  <summary>Details</summary>
Motivation: 解释暂停符号对Transformer性能提升的理论基础，填补现有研究的空白。

Method: 通过形式化分离结果，分析暂停符号对Transformer计算表达能力的影响，结合实验验证。

Result: 暂停符号使Transformer能表达更复杂的函数类（如$\mathsf{AC}^0$和$\mathsf{TC}^0$），实验证明其能学习奇偶校验等任务。

Conclusion: 暂停符号是增强Transformer推理能力的独特机制，与链式思维提示互补。

Abstract: Pause tokens, simple filler symbols such as "...", consistently improve
Transformer performance on both language and mathematical tasks, yet their
theoretical effect remains unexplained. We provide the first formal separation
result, proving that adding pause tokens to constant-depth, logarithmic-width
Transformers strictly increases their computational expressivity. With
bounded-precision activations, Transformers without pause tokens compute only a
strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of
pause tokens allows them to express the entire class. For logarithmic-precision
Transformers, we show that adding pause tokens achieves expressivity equivalent
to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate
that two-layer causally masked Transformers can learn parity when supplied with
pause tokens, a function that they appear unable to learn without them. Our
results provide a rigorous theoretical explanation for prior empirical
findings, clarify how pause tokens interact with width, depth, and numeric
precision, and position them as a distinct mechanism, complementary to
chain-of-thought prompting, for enhancing Transformer reasoning.

</details>


### [286] [TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data](https://arxiv.org/abs/2505.21027)
*Zhipeng He,Chun Ouyang,Lijie Wen,Cong Liu,Catarina Moreira*

Main category: cs.LG

TL;DR: 论文提出了一种针对表格数据的对抗攻击新基准，同时评估攻击的有效性和不可感知性，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在图像数据中已被广泛研究，但在表格数据中的应用面临新的挑战，如特征的异质性和复杂的依赖关系。现有研究多关注攻击有效性，而忽视了不可感知性。

Method: 研究评估了五种对抗攻击在四种模型和十一个表格数据集（包括混合和纯数值数据集）上的表现，分析了有效性和不可感知性的交互作用。

Result: 研究结果揭示了攻击有效性和不可感知性之间的关系，并比较了不同数据集类型的结果，为改进对抗攻击算法提供了见解。

Conclusion: 该基准为表格数据的对抗机器学习领域提供了重要参考，推动了对抗攻击算法的设计进步。

Abstract: Adversarial attacks pose a significant threat to machine learning models by
inducing incorrect predictions through imperceptible perturbations to input
data. While these attacks have been extensively studied in unstructured data
like images, their application to tabular data presents new challenges. These
challenges arise from the inherent heterogeneity and complex feature
interdependencies in tabular data, which differ significantly from those in
image data. To address these differences, it is crucial to consider
imperceptibility as a key criterion specific to tabular data. Most current
research focuses primarily on achieving effective adversarial attacks, often
overlooking the importance of maintaining imperceptibility. To address this
gap, we propose a new benchmark for adversarial attacks on tabular data that
evaluates both effectiveness and imperceptibility. In this study, we assess the
effectiveness and imperceptibility of five adversarial attacks across four
models using eleven tabular datasets, including both mixed and numerical-only
datasets. Our analysis explores how these factors interact and influence the
overall performance of the attacks. We also compare the results across
different dataset types to understand the broader implications of these
findings. The findings from this benchmark provide valuable insights for
improving the design of adversarial attack algorithms, thereby advancing the
field of adversarial machine learning on tabular data.

</details>


### [287] [LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms](https://arxiv.org/abs/2505.21034)
*Wenhu Li,Niki van Stein,Thomas Bäck,Elena Raponi*

Main category: cs.LG

TL;DR: 利用大型语言模型（LLM）自动生成贝叶斯优化（BO）算法代码，通过进化策略迭代优化，在多个测试函数中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化的设计通常依赖专家经验，而LLM为自动化算法设计提供了新途径。

Method: 使用进化策略引导LLM生成包含初始设计、代理模型和获取函数的BO算法代码，并通过BBOB测试评估和迭代优化。

Result: 在24个BBOB测试函数中的19个上表现优于现有方法，且能泛化到更高维度和不同任务。

Conclusion: LLM可作为算法协同设计工具，为BO开发自动化和新算法组合发现提供新范式。

Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing
expensive black-box functions, but designing effective BO algorithms remains a
manual, expertise-driven task. Recent advancements in Large Language Models
(LLMs) have opened new avenues for automating scientific discovery, including
the automatic design of optimization algorithms. While prior work has used LLMs
within optimization loops or to generate non-BO algorithms, we tackle a new
challenge: Using LLMs to automatically generate full BO algorithm code. Our
framework uses an evolution strategy to guide an LLM in generating Python code
that preserves the key components of BO algorithms: An initial design, a
surrogate model, and an acquisition function. The LLM is prompted to produce
multiple candidate algorithms, which are evaluated on the established Black-Box
Optimization Benchmarking (BBOB) test suite from the COmparing Continuous
Optimizers (COCO) platform. Based on their performance, top candidates are
selected, combined, and mutated via controlled prompt variations, enabling
iterative refinement. Despite no additional fine-tuning, the LLM-generated
algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB
functions in dimension 5 and generalize well to higher dimensions, and
different tasks (from the Bayesmark framework). This work demonstrates that
LLMs can serve as algorithmic co-designers, offering a new paradigm for
automating BO development and accelerating the discovery of novel algorithmic
combinations. The source code is provided at
https://github.com/Ewendawi/LLaMEA-BO.

</details>


### [288] [Scalable and adaptive prediction bands with kernel sum-of-squares](https://arxiv.org/abs/2505.21039)
*Louis Allain,Sébastien da Veiga,Brian Staber*

Main category: cs.LG

TL;DR: 本文提出了一种基于统计学习的方法，通过RKHS和核SoS方法改进共形预测的适应性问题，并引入HSIC作为超参数调优策略。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）缺乏适应性，本文旨在通过统计学习方法直接优化覆盖率和适应性。

Method: 利用RKHS和核SoS方法，提出通用表示定理和高效对偶求解方法，并引入HSIC调优核长度尺度。

Result: 实验表明该方法在适应性和覆盖率上优于现有方法，且计算效率高。

Conclusion: 本文方法显著提升了CP的适应性，HSIC策略具有广泛适用性。

Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction
bands with valid coverage in finite samples, while being free of any
distributional assumption. A well-known limitation of conformal prediction is
the lack of adaptivity, although several works introduced practically efficient
alternate procedures. In this work, we build upon recent ideas that rely on
recasting the CP problem as a statistical learning problem, directly targeting
coverage and adaptivity. This statistical learning problem is based on
reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)
methods. First, we extend previous results with a general representer theorem
and exhibit the dual formulation of the learning problem. Crucially, such dual
formulation can be solved efficiently by accelerated gradient methods with
several hundreds or thousands of samples, unlike previous strategies based on
off-the-shelf semidefinite programming algorithms. Second, we introduce a new
hyperparameter tuning strategy tailored specifically to target adaptivity
through bounds on test-conditional coverage. This strategy, based on the
Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune
kernel lengthscales in our framework, but has broader applicability since it
could be used in any CP algorithm where the score function is learned. Finally,
extensive experiments are conducted to show how our method compares to related
work. All figures can be reproduced with the accompanying code.

</details>


### [289] [A domain adaptation neural network for digital twin-supported fault diagnosis](https://arxiv.org/abs/2505.21046)
*Zhenling Chen,Haiwei Fu,Zhiguo Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种基于域对抗神经网络（DANN）的故障诊断框架，用于解决数字孪生模拟数据与真实数据之间的域差异问题，显著提升了模型在真实场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决数字孪生模拟数据与真实系统数据之间的差异导致的模型性能下降问题。

Method: 采用域对抗神经网络（DANN）实现从模拟数据（源域）到真实数据（目标域）的知识迁移。

Result: 实验表明，DANN显著提升了诊断性能，例如将CNN模型的准确率从70.00%提升至80.22%。

Conclusion: DANN能有效弥合模拟与真实数据之间的差距，提升故障诊断的实用性。

Abstract: Digital twins offer a promising solution to the lack of sufficient labeled
data in deep learning-based fault diagnosis by generating simulated data for
model training. However, discrepancies between simulation and real-world
systems can lead to a significant drop in performance when models are applied
in real scenarios. To address this issue, we propose a fault diagnosis
framework based on Domain-Adversarial Neural Networks (DANN), which enables
knowledge transfer from simulated (source domain) to real-world (target domain)
data. We evaluate the proposed framework using a publicly available robotics
fault diagnosis dataset, which includes 3,600 sequences generated by a digital
twin model and 90 real sequences collected from physical systems. The DANN
method is compared with commonly used lightweight deep learning models such as
CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating
domain adaptation significantly improves the diagnostic performance. For
example, applying DANN to a baseline CNN model improves its accuracy from
70.00% to 80.22% on real-world test data, demonstrating the effectiveness of
domain adaptation in bridging the sim-to-real gap.

</details>


### [290] [Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity](https://arxiv.org/abs/2505.21073)
*Pierre Houedry,Nicolas Courty,Florestan Martin-Baillon,Laetitia Chapel,Titouan Vayer*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Trees and the associated shortest-path tree metrics provide a powerful
framework for representing hierarchical and combinatorial structures in data.
Given an arbitrary metric space, its deviation from a tree metric can be
quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing
algorithms that bridge an arbitrary metric to its closest tree metric is still
a vivid subject of interest, as most common approaches are either heuristical
and lack guarantees, or perform moderately well. In this work, we introduce a
novel differentiable optimization framework, coined DeltaZero, that solves this
problem. Our method leverages a smooth surrogate for Gromov's
$\delta$-hyperbolicity which enables a gradient-based optimization, with a
tractable complexity. The corresponding optimization procedure is derived from
a problem with better worst case guarantees than existing bounds, and is
justified statistically. Experiments on synthetic and real-world datasets
demonstrate that our method consistently achieves state-of-the-art distortion.

</details>


### [291] [Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling](https://arxiv.org/abs/2505.21074)
*Yichuan Cao,Yibo Miao,Xiao-Shan Gao,Yinpeng Dong*

Main category: cs.LG

TL;DR: 论文提出了一种基于规则偏好建模的引导红队测试方法（RPG-RT），用于评估文本到图像（T2I）模型的安全性，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能生成不当或有害内容，现有红队测试方法在封闭模型和未知防御机制下效果有限。

Method: RPG-RT利用LLM迭代修改提示并基于T2I系统反馈微调，结合规则偏好建模优化动态适应过程。

Result: 在19个T2I系统、3个商业API和T2V模型上的实验验证了方法的优越性和实用性。

Conclusion: RPG-RT为评估T2I模型安全性提供了一种高效且适应性强的解决方案。

Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their
potential to generate inappropriate or harmful images. Evaluating these models'
security through red-teaming is vital, yet white-box approaches are limited by
their need for internal access, complicating their use with closed-source
models. Moreover, existing black-box methods often assume knowledge about the
model's specific defense mechanisms, limiting their utility in real-world
commercial API scenarios. A significant challenge is how to evade unknown and
diverse defense mechanisms. To overcome this difficulty, we propose a novel
Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively
employs LLM to modify prompts to query and leverages feedback from T2I systems
for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a
prior, enabling the LLM to dynamically adapt to unknown defense mechanisms.
Given that the feedback is often labeled and coarse-grained, making it
difficult to utilize directly, we further propose rule-based preference
modeling, which employs a set of rules to evaluate desired or undesired
feedback, facilitating finer-grained control over the LLM's dynamic adaptation
process. Extensive experiments on nineteen T2I systems with varied safety
mechanisms, three online commercial API services, and T2V models verify the
superiority and practicality of our approach.

</details>


### [292] [Efficient Large Language Model Inference with Neural Block Linearization](https://arxiv.org/abs/2505.21077)
*Mete Erdogan,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: NBL框架通过线性近似替换自注意力层，显著加速LLM推理，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的大语言模型（LLM）推理需求高的问题。

Method: 提出Neural Block Linearization（NBL），利用线性最小均方误差估计器近似自注意力层，并通过典型相关分析计算误差上限作为替换标准。

Result: 在DeepSeek-R1-Distill-Llama-8B上，NBL将推理速度提升32%，精度损失小于1%。

Conclusion: NBL是一种无需微调的高效LLM推理加速方案。

Abstract: The high inference demands of transformer-based Large Language Models (LLMs)
pose substantial challenges in their deployment. To this end, we introduce
Neural Block Linearization (NBL), a novel framework for accelerating
transformer model inference by replacing self-attention layers with linear
approximations derived from Linear Minimum Mean Squared Error estimators. NBL
leverages Canonical Correlation Analysis to compute a theoretical upper bound
on the approximation error. Then, we use this bound as a criterion for
substitution, selecting the LLM layers with the lowest linearization error. NBL
can be efficiently applied to pre-trained LLMs without the need for
fine-tuning. In experiments, NBL achieves notable computational speed-ups while
preserving competitive accuracy on multiple reasoning benchmarks. For instance,
applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B
increases the inference speed by 32% with less than 1% accuracy trade-off,
making it a flexible and promising solution to improve the inference efficiency
of LLMs.

</details>


### [293] [Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations](https://arxiv.org/abs/2505.21095)
*Kei Takemura,Ryuta Matsuno,Keita Sakuma*

Main category: cs.LG

TL;DR: 提出了一种新的乐观在线镜像下降算法，解决了在线学习中适应性问题，实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 在线学习需要适应未知问题特性（如梯度变化、函数曲率和梯度尺度），但现有算法因次优性无法同时实现这些目标。

Method: 设计了一种带有辅助初始轮次和大学习率的乐观在线镜像下降算法，通过负项抵消间隙相关因子。

Result: 新算法首次同时实现了最优梯度变化界限和Lipschitz适应性，解决了先前工作的关键限制。

Conclusion: 该算法解决了Xie等人提出的开放问题，显著提升了在线学习的适应性和性能。

Abstract: A central goal in online learning is to achieve adaptivity to unknown problem
characteristics, such as environmental changes captured by gradient variation
(GV), function curvature (universal online learning, UOL), and gradient scales
(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal
performance is a major challenge, partly due to limitations in algorithms for
prediction with expert advice. These algorithms often serve as meta-algorithms
in online ensemble frameworks, and their sub-optimality hinders overall UOL
performance. Specifically, existing algorithms addressing the ``impossible
tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound
compared to the lower bound. To solve this problem, we propose a novel
optimistic online mirror descent algorithm with an auxiliary initial round
using large learning rates. This design enables a refined analysis where a
generated negative term cancels the gap-related factor, resolving the
impossible tuning issue up to $\log\log T$ factors. Leveraging our improved
algorithm as a meta-algorithm, we develop the first UOL algorithm that
simultaneously achieves state-of-the-art GV bounds and LA under standard
assumptions. Our UOL result overcomes key limitations of prior works, notably
resolving the conflict between LA mechanisms and regret analysis for GV bounds
-- an open problem highlighted by Xie et al.

</details>


### [294] [Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance](https://arxiv.org/abs/2505.21101)
*Badr Moufad,Yazid Janati,Alain Durmus,Ahmed Ghorbel,Eric Moulines,Jimmy Olsson*

Main category: cs.LG

TL;DR: 论文分析了Classifier-Free Guidance (CFG)在条件扩散模型中的局限性，提出了一种修正方法，通过引入Rényi散度项和Gibbs采样来平衡多样性与质量。


<details>
  <summary>Details</summary>
Motivation: CFG虽然提升了生成质量和对提示的匹配度，但牺牲了样本多样性。论文旨在解决这一质量与多样性的权衡问题。

Method: 通过分析发现CFG缺少Rényi散度项，提出修正方法并设计Gibbs采样过程，从目标分布中采样。

Result: 在图像和文本到音频生成任务中，新方法显著优于CFG，提升了多样性和质量。

Conclusion: 修正后的CFG方法在保持多样性的同时提升了生成质量，为条件扩散模型提供了更优的解决方案。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving
conditional diffusion models by linearly combining the outputs of conditional
and unconditional denoisers. While CFG enhances visual quality and improves
alignment with prompts, it often reduces sample diversity, leading to a
challenging trade-off between quality and diversity. To address this issue, we
make two key contributions. First, CFG generally does not correspond to a
well-defined denoising diffusion model (DDM). In particular, contrary to common
intuition, CFG does not yield samples from the target distribution associated
with the limiting CFG score as the noise level approaches zero -- where the
data distribution is tilted by a power $w \gt 1$ of the conditional
distribution. We identify the missing component: a R\'enyi divergence term that
acts as a repulsive force and is required to correct CFG and render it
consistent with a proper DDM. Our analysis shows that this correction term
vanishes in the low-noise limit. Second, motivated by this insight, we propose
a Gibbs-like sampling procedure to draw samples from the desired tilted
distribution. This method starts with an initial sample from the conditional
diffusion model without CFG and iteratively refines it, preserving diversity
while progressively enhancing sample quality. We evaluate our approach on both
image and text-to-audio generation tasks, demonstrating substantial
improvements over CFG across all considered metrics. The code is available at
https://github.com/yazidjanati/cfgig

</details>


### [295] [Universal Value-Function Uncertainties](https://arxiv.org/abs/2505.21119)
*Moritz A. Zanger,Max Weltevrede,Yaniv Oren,Pascal R. Van der Vaart,Caroline Horsch,Wendelin Böhmer,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: UVU是一种通过预测误差量化价值函数不确定性的方法，计算高效且性能媲美深度集成。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中价值函数不确定性估计的计算成本高和启发式方法不可靠的问题。

Method: 使用在线学习器和固定随机目标网络之间的预测误差量化不确定性，结合时序差分学习和合成奖励。

Result: 理论分析表明UVU与无限宽度网络下的集成方差等价，实验显示其在多任务离线RL中性能与大型集成相当。

Conclusion: UVU提供了一种简单高效的价值函数不确定性估计方法，适用于多种RL场景。

Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge
for many aspects of reinforcement learning (RL), including efficient
exploration, safe decision-making, and offline RL. While deep ensembles provide
a robust method for quantifying value uncertainty, they come with significant
computational overhead. Single-model methods, while computationally favorable,
often rely on heuristics and typically require additional propagation
mechanisms for myopic uncertainty estimates. In this work we introduce
universal value-function uncertainties (UVU), which, similar in spirit to
random network distillation (RND), quantify uncertainty as squared prediction
errors between an online learner and a fixed, randomly initialized target
network. Unlike RND, UVU errors reflect policy-conditional value uncertainty,
incorporating the future uncertainties any given policy may encounter. This is
due to the training procedure employed in UVU: the online network is trained
using temporal difference learning with a synthetic reward derived from the
fixed, randomly initialized target network. We provide an extensive theoretical
analysis of our approach using neural tangent kernel (NTK) theory and show that
in the limit of infinite network width, UVU errors are exactly equivalent to
the variance of an ensemble of independent universal value functions.
Empirically, we show that UVU achieves equal performance to large ensembles on
challenging multi-task offline RL settings, while offering simplicity and
substantial computational savings.

</details>


### [296] [Robust and Computation-Aware Gaussian Processes](https://arxiv.org/abs/2505.21133)
*Marshal Arijona Sinaga,Julien Martinelli,Samuel Kaski*

Main category: cs.LG

TL;DR: RCaGP是一种新的高斯过程模型，通过结合近似不确定性的处理与鲁棒贝叶斯更新，解决了大数据和异常值环境下的计算鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 标准高斯过程及其稀疏近似在大数据和异常值环境下存在计算不可行性和鲁棒性问题。

Method: RCaGP结合了近似不确定性的处理与鲁棒贝叶斯更新，通过低秩矩阵乘法等近似方法实现可扩展性。

Result: 模型提供了更保守和可靠的方差估计，并在回归和高通量贝叶斯优化中表现出色。

Conclusion: RCaGP通过联合解决鲁棒性和近似问题，在大数据和异常值环境下表现优越。

Abstract: Gaussian processes (GPs) are widely used for regression and optimization
tasks such as Bayesian optimization (BO) due to their expressiveness and
principled uncertainty estimates. However, in settings with large datasets
corrupted by outliers, standard GPs and their sparse approximations struggle
with computational tractability and robustness. We introduce Robust
Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly
addresses these challenges by combining a principled treatment of
approximation-induced uncertainty with robust generalized Bayesian updating.
The key insight is that robustness and approximation-awareness are not
orthogonal but intertwined: approximations can exacerbate the impact of
outliers, and mitigating one without the other is insufficient. Unlike previous
work that focuses narrowly on either robustness or approximation quality, RCaGP
combines both in a principled and scalable framework, thus effectively managing
both outliers and computational uncertainties introduced by approximations such
as low-rank matrix multiplications. Our model ensures more conservative and
reliable uncertainty estimates, a property we rigorously demonstrate.
Additionally, we establish a robustness property and show that the mean
function is key to preserving it, motivating a tailored model selection scheme
for robust mean functions. Empirical results confirm that solving these
challenges jointly leads to superior performance across both clean and
outlier-contaminated settings, both on regression and high-throughput Bayesian
optimization benchmarks.

</details>


### [297] [Learning Single Index Models with Diffusion Priors](https://arxiv.org/abs/2505.21135)
*Anqi Tang,Youming Chen,Shuchen Xue,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型（DMs）的信号恢复方法，适用于半参数单指标模型，能够处理非线性测量模型中的不连续或未知链接函数，仅需一轮无条件采样和部分反转即可实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在信号恢复中表现出色，但现有研究要么局限于特定重建问题，要么无法处理非线性测量模型中的不连续或未知链接函数。本文旨在解决这一问题。

Method: 提出了一种高效的重建方法，仅需一轮无条件采样和部分反转扩散模型，适用于半参数单指标模型。

Result: 数值实验表明，该方法在图像数据集上相比其他方法能实现更准确的恢复，同时显著减少神经函数评估次数。

Conclusion: 该方法为扩散模型在非线性信号恢复中的应用提供了有效解决方案，具有高效性和准确性。

Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate
diverse and high-quality images by efficiently modeling complex data
distributions. They have also been explored as powerful generative priors for
signal recovery, resulting in a substantial improvement in the quality of
reconstructed signals. However, existing research on signal recovery with
diffusion models either focuses on specific reconstruction problems or is
unable to handle nonlinear measurement models with discontinuous or unknown
link functions. In this work, we focus on using DMs to achieve accurate
recovery from semi-parametric single index models, which encompass a variety of
popular nonlinear models that may have {\em discontinuous} and {\em unknown}
link functions. We propose an efficient reconstruction method that only
requires one round of unconditional sampling and (partial) inversion of DMs.
Theoretical analysis on the effectiveness of the proposed methods has been
established under appropriate conditions. We perform numerical experiments on
image datasets for different nonlinear measurement models. We observe that
compared to competing methods, our approach can yield more accurate
reconstructions while utilizing significantly fewer neural function
evaluations.

</details>


### [298] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136)
*Jintao Zhang,Xiaoming Xu,Jia Wei,Haofeng Huang,Pengle Zhang,Chendong Xiang,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++通过量化加速注意力计算，利用FP8指令进一步提速，比FlashAttention快3.9倍，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 注意力计算的时间复杂度随序列长度呈二次增长，效率至关重要。

Method: 采用量化加速矩阵乘法（Matmul），并利用FP8指令（FP16累加）进一步提速。

Result: SageAttention2++比FlashAttention快3.9倍，且保持与SageAttention2相同的注意力准确性。

Conclusion: SageAttention2++显著加速了多种模型（语言、图像、视频生成），且端到端指标损失可忽略。

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [299] [HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs](https://arxiv.org/abs/2505.21140)
*Honglin Gao,Xiang Li,Lan Zhao,Gaoxi Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种针对异构图神经网络（HGNNs）的新型后门攻击框架HeteroBA，通过插入精心设计的触发节点和结构连接，利用注意力机制和聚类策略选择辅助节点，实现在不影响干净数据准确性的情况下误导模型分类。


<details>
  <summary>Details</summary>
Motivation: 尽管HGNNs在复杂多关系数据建模中表现优异，但其在对抗性攻击（尤其是后门攻击）下的鲁棒性和安全性研究不足。

Method: 提出HeteroBA框架，通过插入触发节点和结构连接，结合注意力机制和聚类策略选择辅助节点，实现高效的后门攻击。

Result: 在三个数据集和多种HGNN架构上的实验表明，HeteroBA攻击成功率高且对干净数据准确性影响极小。

Conclusion: HeteroBA揭示了HGNNs的潜在漏洞，呼吁在多关系图场景中加强针对后门威胁的防御措施。

Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing
attention for modeling complex multi-relational data in domains such as
recommendation, finance, and social networks. While existing research has been
largely focused on enhancing HGNNs' predictive performance, their robustness
and security, especially under backdoor attacks, remain underexplored. In this
paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework
for node classification tasks on heterogeneous graphs. HeteroBA inserts
carefully crafted trigger nodes with realistic features and targeted structural
connections, leveraging attention-based and clustering-based strategies to
select influential auxiliary nodes for effective trigger propagation, thereby
causing the model to misclassify specific nodes into a target label while
maintaining accuracy on clean data. Experimental results on three datasets and
various HGNN architectures demonstrate that HeteroBA achieves high attack
success rates with minimal impact on the clean accuracy. Our method sheds light
on potential vulnerabilities in HGNNs and calls for more robust defenses
against backdoor threats in multi-relational graph scenarios.

</details>


### [300] [A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques](https://arxiv.org/abs/2505.21141)
*Nancy C. Woods,Virtue Ene Agada,Adebola K. Ojo*

Main category: cs.LG

TL;DR: 该研究结合支持向量机（SVM）和多类分类关联规则（MCAR）算法，提出了一种更准确的钓鱼网站检测方法，分类准确率达98.30%。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站对经济和信息安全造成严重威胁，现有检测方法缺乏统一的最佳算法。

Method: 使用MCAR进行特征提取和规则生成，SVM进行分类和预测，结合两种算法的优势。

Result: 模型分类准确率为98.30%，AUC为98%，预测方差为82.84%，计算时间为2205.33秒。

Conclusion: 结合SVM和MCAR的混合方法显著提高了钓鱼网站检测的准确性。

Abstract: Phishing is a semantic attack which targets the user rather than the
computer. It is a new Internet crime in comparison with other forms such as
virus and hacking. Considering the damage phishing websites has caused to
various economies by collapsing organizations, stealing information and
financial diversion, various researchers have embarked on different ways of
detecting phishing websites but there has been no agreement about the best
algorithm to be used for prediction. This study is interested in integrating
the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class
Classification Rules based on Association Rules (MCAR) to establish a strong
and better means of predicting phishing websites. A total of 11,056 websites
were used from both PhishTank and yahoo directory to verify the effectiveness
of this approach. Feature extraction and rules generation were done by the MCAR
technique; classification and prediction were done by SVM technique. The result
showed that the technique achieved 98.30% classification accuracy with a
computation time of 2205.33s with minimum error rate. It showed a total of 98%
Area under the Curve (AUC) which showed the proportion of accuracy in
classifying phishing websites. The model showed 82.84% variance in the
prediction of phishing websites based on the coefficient of determination. The
use of two techniques together in detecting phishing websites produced a more
accurate result as it combined the strength of both techniques respectively.
This research work centralized on this advantage by building a hybrid of two
techniques to help produce a more accurate result.

</details>


### [301] [Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score](https://arxiv.org/abs/2505.21147)
*Xuanning Zhou,Hao Zeng,Xiaobo Xia,Bingyi Jing,Hongxin Wei*

Main category: cs.LG

TL;DR: SemiCP扩展了共形预测（CP）框架，利用半监督学习结合标记和未标记数据进行校准，提出新的非共形性评分函数NNM，解决了标记数据不足时的覆盖偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中标记数据有限，标准CP可能导致覆盖偏差和预测集过大，因此需要扩展CP框架以利用未标记数据。

Method: 提出SemiCP框架，引入非共形性评分函数NNM，利用未标记数据的伪标签评分选择相似标记数据，整合到校准过程中。

Result: 理论证明SemiCP在温和假设下提供渐近覆盖保证，实验验证其减少不稳定性和低效性，适应条件覆盖设置。

Conclusion: SemiCP有效解决了标记数据不足的问题，提升了CP框架的实用性和稳定性。

Abstract: Conformal prediction (CP) is a powerful framework for uncertainty
quantification, providing prediction sets with coverage guarantees when
calibrated on sufficient labeled data. However, in real-world applications
where labeled data is often limited, standard CP can lead to coverage deviation
and output overly large prediction sets. In this paper, we extend CP to the
semi-supervised setting and propose SemiCP, leveraging both labeled data and
unlabeled data for calibration. Specifically, we introduce a novel
nonconformity score function, NNM, designed for unlabeled data. This function
selects labeled data with similar pseudo-label scores to estimate nonconformity
scores, integrating them into the calibration process to overcome sample size
limitations. We theoretically demonstrate that, under mild assumptions, SemiCP
provide asymptotically coverage guarantee for prediction sets. Extensive
experiments further validate that our approach effectively reduces instability
and inefficiency under limited calibration data, can be adapted to conditional
coverage settings, and integrates seamlessly with existing CP methods.

</details>


### [302] [STEB: In Search of the Best Evaluation Approach for Synthetic Time Series](https://arxiv.org/abs/2505.21160)
*Michael Stenger,Robert Leppich,André Bauer,Samuel Kounev*

Main category: cs.LG

TL;DR: STEB是一个用于全面、可解释地自动比较合成时间序列评估指标的基准框架，通过多样数据集和配置变换验证指标的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 由于数据增强和隐私法规的需求，合成时间序列生成模型和评估指标众多，但缺乏大规模客观比较的方法。

Method: STEB利用10个数据集、随机性注入和13种可配置数据变换，计算指标的可靠性和一致性，并跟踪运行时间和测试误差。

Result: 实验中对41个指标进行排名，发现上游时间序列嵌入对最终评分有显著影响。

Conclusion: STEB为合成时间序列评估提供了首个基准框架，支持全面且可解释的指标比较。

Abstract: The growing need for synthetic time series, due to data augmentation or
privacy regulations, has led to numerous generative models, frameworks, and
evaluation measures alike. Objectively comparing these measures on a large
scale remains an open challenge. We propose the Synthetic Time series
Evaluation Benchmark (STEB) -- the first benchmark framework that enables
comprehensive and interpretable automated comparisons of synthetic time series
evaluation measures. Using 10 diverse datasets, randomness injection, and 13
configurable data transformations, STEB computes indicators for measure
reliability and score consistency. It tracks running time, test errors, and
features sequential and parallel modes of operation. In our experiments, we
determine a ranking of 41 measures from literature and confirm that the choice
of upstream time series embedding heavily impacts the final score.

</details>


### [303] [Topological Deep Learning for Speech Data](https://arxiv.org/abs/2505.21173)
*Zhiwang Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拓扑数据分析（TDA）的拓扑感知卷积核，显著提升了语音识别网络的性能。


<details>
  <summary>Details</summary>
Motivation: 受Carlsson等人启发，研究旨在探索TDA在深度学习中的应用潜力，特别是通过数学工具优化神经网络。

Method: 通过研究正交群作用在核上的性质，建立了矩阵空间的纤维丛分解，并提出了新的滤波器生成方法，最终设计出Orthogonal Feature (OF)层。

Result: OF层在音素识别任务中表现出色，尤其在低噪声环境下，同时展示了跨领域适应性。

Conclusion: 该研究揭示了TDA在神经网络优化中的潜力，为数学与深度学习的跨学科研究开辟了新途径。

Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep
learning. Inspired by Carlsson et al., this study designs topology-aware
convolutional kernels that significantly improve speech recognition networks.
Theoretically, by investigating orthogonal group actions on kernels, we
establish a fiber-bundle decomposition of matrix spaces, enabling new filter
generation methods. Practically, our proposed Orthogonal Feature (OF) layer
achieves superior performance in phoneme recognition, particularly in low-noise
scenarios, while demonstrating cross-domain adaptability. This work reveals
TDA's potential in neural network optimization, opening new avenues for
mathematics-deep learning interdisciplinary studies.

</details>


### [304] [Latent label distribution grid representation for modeling uncertainty](https://arxiv.org/abs/2505.21180)
*ShuNing Sun,YinSong Xiong,Yu Zhang,Zhuoran Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种潜在标签分布网格（LLDG）方法，用于建模标签分布的不确定性，并通过低秩方案和Tucker重建技术减少噪声，生成准确的标签分布。


<details>
  <summary>Details</summary>
Motivation: 标签分布学习（LDL）中标签分布标注的高成本和复杂性导致标签空间存在不确定性，影响算法决策。

Method: 构建标签相关矩阵，将其扩展为服从高斯分布的向量以形成LLDG，并通过LLDG-Mixer和低秩方案重建标签分布。

Result: 实验结果表明，该方法在多个基准数据集上表现优异。

Conclusion: LLDG能有效建模标签空间的不确定性，提升标签分布学习的准确性。

Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has
promising representation capabilities for characterizing the polysemy of an
instance, the complexity and high cost of the label distribution annotation
lead to inexact in the construction of the label space. The existence of a
large number of inexact labels generates a label space with uncertainty, which
misleads the LDL algorithm to yield incorrect decisions. To alleviate this
problem, we model the uncertainty of label distributions by constructing a
\textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to
form a low-noise representation space. Specifically, we first construct a label
correlation matrix based on the differences between labels, and then expand
each value of the matrix into a vector that obeys a Gaussian distribution, thus
building a LLDG to model the uncertainty of the label space. Finally, the LLDG
is reconstructed by the LLDG-Mixer to generate an accurate label distribution.
Note that we enforce a customized low-rank scheme on this grid, which assumes
that the label relations may be noisy and it needs to perform noise-reduction
with the help of a Tucker reconstruction technique. Furthermore, we attempt to
evaluate the effectiveness of the LLDG by considering its generation as an
upstream task to achieve the classification of the objects. Extensive
experimental results show that our approach performs competitively on several
benchmarks.

</details>


### [305] [Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations](https://arxiv.org/abs/2505.21182)
*Huy Hoang,Tien Mai,Pradeep Varakantham,Tanvi Verma*

Main category: cs.LG

TL;DR: 论文提出了一种离线模仿学习方法，利用专家和不良行为的对比数据，通过优化KL散度差异来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线模仿学习通常忽略不良行为的信号，而本文旨在利用专家和不良行为的对比数据来改进学习效果。

Method: 提出了一种基于KL散度差异的优化目标，证明在专家数据占优时目标为凸，避免了对抗训练。

Result: 实验表明，该方法在标准基准测试中优于现有最优基线。

Conclusion: 通过统一处理正负样本，该方法在离线模仿学习中表现优异。

Abstract: Offline imitation learning typically learns from expert and unlabeled
demonstrations, yet often overlooks the valuable signal in explicitly
undesirable behaviors. In this work, we study offline imitation learning from
contrasting behaviors, where the dataset contains both expert and undesirable
demonstrations. We propose a novel formulation that optimizes a difference of
KL divergences over the state-action visitation distributions of expert and
undesirable (or bad) data. Although the resulting objective is a DC
(Difference-of-Convex) program, we prove that it becomes convex when expert
demonstrations outweigh undesirable demonstrations, enabling a practical and
stable non-adversarial training objective. Our method avoids adversarial
training and handles both positive and negative demonstrations in a unified
framework. Extensive experiments on standard offline imitation learning
benchmarks demonstrate that our approach consistently outperforms
state-of-the-art baselines.

</details>


### [306] [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
*Yu Yan,Sheng Sun,Zhifei Zheng,Ziji Hao,Teli Liu,Min Liu*

Main category: cs.LG

TL;DR: PoisonSwarm框架通过模型众包策略生成多样化的有害数据，解决了现有LLM在有害数据合成中的可靠性和多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖LLM合成有害数据，但受限于其安全对齐机制，生成可靠性和内容多样性不足。

Method: 提出PoisonSwarm框架，通过生成良性模板、语义单元分解、动态模型切换逐步毒化，确保合成成功。

Result: 实验表明PoisonSwarm在有害数据合成中具有高扩展性和多样性，达到最优性能。

Conclusion: PoisonSwarm为构建安全AI应用提供了高效的有害数据合成方法。

Abstract: To construct responsible and secure AI applications, harmful information data
is widely utilized for adversarial testing and the development of safeguards.
Existing studies mainly leverage Large Language Models (LLMs) to synthesize
data to obtain high-quality task datasets at scale, thereby avoiding costly
human annotation. However, limited by the safety alignment mechanisms of LLMs,
the synthesis of harmful data still faces challenges in generation reliability
and content diversity. In this study, we propose a novel harmful information
synthesis framework, PoisonSwarm, which applies the model crowdsourcing
strategy to generate diverse harmful data while maintaining a high success
rate. Specifically, we generate abundant benign data as the based templates in
a counterfactual manner. Subsequently, we decompose each based template into
multiple semantic units and perform unit-by-unit toxification and final
refinement through dynamic model switching, thus ensuring the success of
synthesis. Experimental results demonstrate that PoisonSwarm achieves
state-of-the-art performance in synthesizing different categories of harmful
data with high scalability and diversity.

</details>


### [307] [Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection](https://arxiv.org/abs/2505.21201)
*Steven Sam,Silima Marshal DAbreo*

Main category: cs.LG

TL;DR: 研究利用随机森林和SVM模型，结合环境与经济因素，为印度农业提供作物推荐，发现随机森林在滞后变量方法中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 印度农业面临低生产力和气候变化挑战，现有推荐系统局限性大，需更全面的模型提升准确性。

Method: 使用19种作物和15个州的数据，采用随机森林和SVM模型，结合10折交叉验证、时间序列分割和滞后变量方法。

Result: 10折交叉验证准确率高但存在过拟合，时间序列分割表现下降，滞后变量方法显著提升性能。

Conclusion: 滞后变量方法下的随机森林模型最适合印度农业作物推荐，能有效处理时间依赖性。

Abstract: Agriculture constitutes a primary source of food production, economic growth
and employment in India, but the sector is confronted with low farm
productivity and yields aggravated by increased pressure on natural resources
and adverse climate change variability. Efforts involving green revolution,
land irrigations, improved seeds and organic farming have yielded suboptimal
outcomes. The adoption of computational tools like crop recommendation systems
offers a new way to provide insights and help farmers tackle low productivity.
However, most agricultural recommendation systems in India focus narrowly on
environmental factors and regions, limiting accurate predictions of high-yield,
profitable crops. This study uses environmental and economic factors with 19
crops across 15 states to develop and evaluate Random Forest and SVM models
using 10-fold Cross Validation, Time-series Split, and Lag Variables. The
10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but
raised overfitting concerns. Introducing temporal order, better reflecting
real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the
Time-series Split.To further increase the model accuracy while maintaining the
temporal order, the Lag Variables approach was employed, which resulted in
improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross
validation approach. Overall, the models in the Time-series Split and Lag
Variable Approaches offer practical insights by handling temporal dependencies
and enhancing its adaptability to changing agricultural conditions over time.
Consequently, the study shows the Random Forest model developed based on the
Lag Variables as the most preferred algorithm for optimal crop recommendation
in the Indian context.

</details>


### [308] [Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics](https://arxiv.org/abs/2505.21204)
*Marie Steinacker,Yuri Kheifetz,Markus Scholz*

Main category: cs.LG

TL;DR: 论文研究了化疗期间血小板计数的个性化时间序列建模，比较了混合机制模型与数据驱动方法的预测性能。数据驱动方法在数据充足时表现更好，而混合模型在数据稀疏时更优。


<details>
  <summary>Details</summary>
Motivation: 化疗引起的血液毒性具有高度个体差异和难以预测的特点，现有机制模型难以准确预测不规则或非典型轨迹的患者。

Method: 研究开发并比较了混合机制模型（结合神经网络）与纯数据驱动方法（非线性自回归外生模型）。

Result: 数据驱动方法在数据充足时显著提高预测准确性，尤其对高风险患者；混合模型在数据稀疏时表现更优。

Conclusion: 研究框架可推广至其他治疗相关毒性的预测，为个性化医疗提供广泛应用潜力。

Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a
frequent side effect of cytotoxic chemotherapy and poses a significant
challenge in clinical practice due to its high inter-patient variability and
limited predictability. Current mechanistic models often struggle to accurately
forecast outcomes for patients with irregular or atypical trajectories. In this
study, we develop and compare hybrid mechanistic and data-driven approaches for
individualized time series modeling of platelet counts during chemotherapy. We
consider hybrid models that combine mechanistic models with neural networks,
known as universal differential equations. As a purely data-driven alternative,
we utilize a nonlinear autoregressive exogenous model using gated recurrent
units as the underlying architecture. These models are evaluated across a range
of real patient scenarios, varying in data availability and sparsity, to assess
predictive performance. Our findings demonstrate that data-driven methods, when
provided with sufficient data, significantly improve prediction accuracy,
particularly for high-risk patients with irregular platelet dynamics. This
highlights the potential of data-driven approaches in enhancing clinical
decision-making. In contrast, hybrid and mechanistic models are superior in
scenarios with limited or sparse data. The proposed modeling and comparison
framework is generalizable and could be extended to predict other
treatment-related toxicities, offering broad applicability in personalized
medicine.

</details>


### [309] [Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection](https://arxiv.org/abs/2505.21219)
*Qinjun Fei,Nuria Rodríguez-Barroso,María Victoria Luzón,Zhongliang Zhang,Francisco Herrera*

Main category: cs.LG

TL;DR: SBRO-FL框架通过动态竞价、声誉建模和成本感知选择，解决了跨机构联邦学习中的客户选择问题，提升了模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在跨机构联邦学习中，客户选择因数据质量、预算限制和激励兼容性而复杂化，现有方法难以联合优化这些因素。

Method: 提出SBRO-FL框架，结合动态竞价、基于Shapley值的贡献评估和声誉系统，通过0-1整数规划选择客户。

Result: 在多个数据集上实验表明，SBRO-FL提高了准确性、收敛速度和鲁棒性。

Conclusion: 平衡数据可靠性、激励兼容性和成本效率对实现可扩展且可信的联邦学习至关重要。

Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure
high model performance, yet it remains challenging due to data quality
decompensation, budget constraints, and incentive compatibility. As training
progresses, these factors exacerbate client heterogeneity and degrade global
performance. Most existing approaches treat these challenges in isolation,
making jointly optimizing multiple factors difficult. To address this, we
propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a
unified framework integrating dynamic bidding, reputation modeling, and
cost-aware selection. Clients submit bids based on their perceived data
quality, and their contributions are evaluated using Shapley values to quantify
their marginal impact on the global model. A reputation system, inspired by
prospect theory, captures historical performance while penalizing
inconsistency. The client selection problem is formulated as a 0-1 integer
program that maximizes reputation-weighted utility under budget constraints.
Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that
SBRO-FL improves accuracy, convergence speed, and robustness, even in
adversarial and low-bid interference scenarios. Our results highlight the
importance of balancing data reliability, incentive compatibility, and cost
efficiency to enable scalable and trustworthy FL deployments.

</details>


### [310] [Why Do More Experts Fail? A Theoretical Analysis of Model Merging](https://arxiv.org/abs/2505.21226)
*Zijing Wang,Xingle Xu,Yongkang Liu,Yiqun Zhang,Peiqin Lin,Shi Feng,Xiaocui Yang,Daling Wang,Hinrich Schütze*

Main category: cs.LG

TL;DR: 本文研究了模型合并的可扩展性问题，揭示了合并模型数量增加时性能提升受限的理论原因，并提出了一种新方法（RHT）来扩展合并模型的能力。


<details>
  <summary>Details</summary>
Motivation: 随着合并模型数量的增加，现有方法难以维持性能提升，本文旨在探索限制模型合并可扩展性的关键障碍。

Method: 通过理论分析（如高斯宽度和近似运动学理论）证明模型合并存在上限，并提出Reparameterized Heavy-Tailed方法（RHT）扩展合并模型的能力。

Result: 在12个基准测试中验证了理论分析的正确性，RHT方法显著提升了合并模型的性能。

Conclusion: 模型合并存在理论上限，RHT方法为扩展合并模型能力提供了新思路，为未来研究指明了方向。

Abstract: Model merging dramatically reduces storage and computational resources by
combining multiple expert models into a single multi-task model. Although
recent model merging methods have shown promising results, they struggle to
maintain performance gains as the number of merged models increases. In this
paper, we investigate the key obstacles that limit the scalability of model
merging when integrating a large number of expert models. First, we prove that
there is an upper bound on model merging. Further theoretical analysis reveals
that the limited effective parameter space imposes a strict constraint on the
number of models that can be successfully merged. Gaussian Width shows that the
marginal benefit of merging additional models diminishes according to a
strictly concave function. This implies that the effective parameter space
becomes rapidly saturated as the number of merged models increases.
Furthermore, using Approximate Kinematics Theory, we prove the existence of a
unique optimal threshold beyond which adding more models does not yield
significant performance improvements. At the same time, we introduce a
straightforward Reparameterized Heavy-Tailed method (RHT) to extend the
coverage of the merged model, thereby enhancing its performance. Empirical
results on 12 benchmarks, including both knowledge-intensive and
general-purpose tasks, validate our theoretical analysis. We believe that these
results spark further research beyond the current scope of model merging. The
source code is in the anonymous Github repository
https://github.com/wzj1718/ModelMergingAnalysis.

</details>


### [311] [BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design](https://arxiv.org/abs/2505.21241)
*Divya Nori,Anisha Parsan,Caroline Uhler,Wengong Jin*

Main category: cs.LG

TL;DR: 提出了一种基于能量模型的方法pTMEnergy，用于改进蛋白质结合剂设计，通过统计能量函数优化设计流程BECraft，显著提升了设计效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖结构预测置信度指标（如ipTM），但这些指标未能反映结合剂-靶标复合物的统计似然性，且梯度稀疏，优化效果有限。

Method: 通过将结构预测器的置信度输出重新解释为能量模型（EBM），提取统计能量函数pTMEnergy，并将其整合到设计流程BECraft中。

Result: BECraft在多个挑战性目标上优于BindCraft、RFDiffusion和ESM3，提高了结合剂成功率并减少了结构冲突，同时在虚拟筛选任务中达到新水平。

Conclusion: pTMEnergy和BECraft为蛋白质结合剂设计提供了更优的统计能量优化方法，显著提升了设计效果和虚拟筛选性能。

Abstract: Protein binder design has been transformed by hallucination-based methods
that optimize structure prediction confidence metrics, such as the interface
predicted TM-score (ipTM), via backpropagation. However, these metrics do not
reflect the statistical likelihood of a binder-target complex under the learned
distribution and yield sparse gradients for optimization. In this work, we
propose a method to extract such likelihoods from structure predictors by
reinterpreting their confidence outputs as an energy-based model (EBM). By
leveraging the Joint Energy-based Modeling (JEM) framework, we introduce
pTMEnergy, a statistical energy function derived from predicted inter-residue
error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a
design pipeline that maintains the same optimization framework as BindCraft but
replaces ipTM with our energy-based objective. BECraft outperforms BindCraft,
RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in
silico binder success rates while reducing structural clashes. Furthermore,
pTMEnergy establishes a new state-of-the-art in structure-based virtual
screening tasks for miniprotein and RNA aptamer binders.

</details>


### [312] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/abs/2505.21251)
*Mustafa Hajij,Lennart Bastian,Sarah Osentoski,Hardik Kabaria,John L. Davenport,Sheik Dawood,Balaji Cherukuri,Joseph G. Kocheemoolayil,Nastaran Shahmansouri,Adrian Lew,Theodore Papamarkou,Tolga Birdal*

Main category: cs.LG

TL;DR: CTNNs是一种基于代数拓扑的深度学习框架，适用于多种结构化数据，解决了传统模型设计中的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型设计缺乏针对特定任务和数据类型的统一理论框架，CTNNs通过代数拓扑中的copresheaf概念填补了这一空白。

Method: CTNNs利用copresheaf理论构建模型，适用于图像、点云、图、网格等结构化数据，解决了长程依赖、过度平滑等问题。

Result: 实验表明，CTNNs在结构化数据基准测试中优于传统基线，尤其在需要层次化或局部敏感的任务中表现突出。

Conclusion: CTNNs为下一代深度学习架构提供了理论严谨且实用的多尺度基础。

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data: including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by grounding model design in the language of
copresheaves, a concept from algebraic topology that generalizes and subsumes
most practical deep learning models in use today. This abstract yet
constructive formulation yields a rich design space from which theoretically
sound and practically effective solutions can be derived to tackle core
challenges in representation learning: long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results underscore CTNNs as a principled, multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [313] [Learnable Kernel Density Estimation for Graphs](https://arxiv.org/abs/2505.21285)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: LGKDE是一种学习图核密度估计的框架，通过图神经网络和最大均值差异学习图度量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图密度估计中结构模式和语义变化捕获不足的问题，避免传统方法因固定核特征导致的性能不佳。

Method: 利用图神经网络表示图为离散分布，通过最大均值差异学习多尺度核密度估计，参数通过最大化密度学习。

Result: 在合成图分布和异常检测任务中表现优异，优于现有基线方法。

Conclusion: LGKDE在理论和实验上均表现出色，为图密度估计提供了有效解决方案。

Abstract: This work proposes a framework LGKDE that learns kernel density estimation
for graphs. The key challenge in graph density estimation lies in effectively
capturing both structural patterns and semantic variations while maintaining
theoretical guarantees. Combining graph kernels and kernel density estimation
(KDE) is a standard approach to graph density estimation, but has
unsatisfactory performance due to the handcrafted and fixed features of
kernels. Our method LGKDE leverages graph neural networks to represent each
graph as a discrete distribution and utilizes maximum mean discrepancy to learn
the graph metric for multi-scale KDE, where all parameters are learned by
maximizing the density of graphs relative to the density of their well-designed
perturbed counterparts. The perturbations are conducted on both node features
and graph spectra, which helps better characterize the boundary of normal
density regions. Theoretically, we establish consistency and convergence
guarantees for LGKDE, including bounds on the mean integrated squared error,
robustness, and complexity. We validate LGKDE by demonstrating its
effectiveness in recovering the underlying density of synthetic graph
distributions and applying it to graph anomaly detection across diverse
benchmark datasets. Extensive empirical evaluation shows that LGKDE
demonstrates superior performance compared to state-of-the-art baselines on
most benchmark datasets.

</details>


### [314] [GSAT: Graph Structure Attention Networks](https://arxiv.org/abs/2505.21288)
*Farshad Noravesh,Reza Haffari,Layki Soon,Arghya Pal*

Main category: cs.LG

TL;DR: 论文提出了一种结合匿名随机游走（ARWs）和结构注意力机制（GSAT）的图神经网络方法，以增强图分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）在建模时往往忽略了节点局部拓扑结构信息，导致需要多层网络传递远距离节点信息，引发过平滑等问题。

Method: 通过匿名随机游走（ARWs）建模结构信息，并提出图结构注意力网络（GSAT），结合节点属性和结构表示，自动学习邻域边的注意力模式。

Result: 实验表明，GSAT在某些图分类基准上略微提升了当前最优性能（SOTA）。

Conclusion: GSAT通过整合结构信息，有效提升了图分类性能，为GNNs的进一步发展提供了新思路。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing
data represented in graph structures, achieving remarkable success across a
wide range of applications. However, to further improve the performance on
graph classification benchmarks, structural representation of each node that
encodes rich local topological information in the neighbourhood of nodes is an
important type of feature that is often overlooked in the modeling. The
consequence of neglecting the structural information has resulted high number
of layers to connect messages from distant nodes which by itself produces other
problems such as oversmoothing. In the present paper, we leverage these
structural information that are modeled by anonymous random walks (ARWs) and
introduce graph structure attention network (GSAT) which is a generalization of
graph attention network(GAT) to integrate the original attribute and the
structural representation to enforce the model to automatically find patterns
for attending to different edges in the node neighbourhood to enrich graph
representation. Our experiments show GSAT slightly improves SOTA on some graph
classification benchmarks.

</details>


### [315] [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/abs/2505.21289)
*Nurbek Tastan,Stefanos Laskaridis,Martin Takac,Karthik Nandakumar,Samuel Horvath*

Main category: cs.LG

TL;DR: LoFT是一种新的低秩适应方法，通过优化器内部动态对齐全模型更新，显著缩小了与全微调的差距，且无需额外调参。


<details>
  <summary>Details</summary>
Motivation: LoRA等参数高效微调方法虽然减少了可训练参数，但在准确性和收敛速度上仍不及全微调。

Method: LoFT在低秩子空间中学习权重更新，并将优化器的动量和方差投影到同一子空间，模拟全模型更新。

Result: LoFT显著缩小了与全微调的性能差距，且无需增加推理成本。

Conclusion: LoFT是一种高效的低秩适应方法，优于标准LoRA方法。

Abstract: Large pre-trained models are commonly adapted to downstream tasks using
parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),
which injects small trainable low-rank matrices instead of updating all
weights. While LoRA dramatically reduces trainable parameters with little
overhead, it can still underperform full fine-tuning in accuracy and often
converges more slowly. We introduce LoFT, a novel low-rank adaptation method
that behaves like full fine-tuning by aligning the optimizer's internal
dynamics with those of updating all model weights. LoFT not only learns weight
updates in a low-rank subspace (like LoRA) but also properly projects the
optimizer's first and second moments (Adam's momentum and variance) into the
same subspace, mirroring full-model updates. By aligning the low-rank update
itself with the full update, LoFT eliminates the need for tuning extra
hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach
substantially narrows the performance gap between adapter-based tuning and full
fine-tuning and consistently outperforms standard LoRA-style methods, all
without increasing inference cost.

</details>


### [316] [A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features](https://arxiv.org/abs/2505.21317)
*Ihab Bendidi,Yassir El Mesbahi,Alisandra K. Denton,Karush Suri,Kian Kenyon-Dean,Auguste Genovesio,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: 论文提出了一种通过显微镜图像增强转录组学数据的框架，利用弱配对数据对齐模态，并引入Semi-Clipped和PEA技术解决数据稀缺问题，提升预测能力并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解细胞对刺激的响应对于生物学发现和药物开发至关重要，但弱配对数据稀缺限制了多模态学习的应用。

Method: 提出Semi-Clipped（基于CLIP的跨模态蒸馏方法）和PEA（扰动嵌入增强技术），利用弱配对数据对齐模态并增强转录组学数据。

Result: 方法在跨模态蒸馏中达到最优效果，提升了转录组学数据的预测能力，同时保留了其可解释性。

Conclusion: 该框架为复杂生物任务提供了丰富的单模态表示，解决了数据稀缺问题并提升了多模态学习的实用性。

Abstract: Understanding cellular responses to stimuli is crucial for biological
discovery and drug development. Transcriptomics provides interpretable,
gene-level insights, while microscopy imaging offers rich predictive features
but is harder to interpret. Weakly paired datasets, where samples share
biological states, enable multimodal learning but are scarce, limiting their
utility for training and multimodal inference. We propose a framework to
enhance transcriptomics by distilling knowledge from microscopy images. Using
weakly paired data, our method aligns and binds modalities, enriching gene
expression representations with morphological information. To address data
scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal
distillation using pretrained foundation models, achieving state-of-the-art
results, and (2) PEA (Perturbation Embedding Augmentation), a novel
augmentation technique that enhances transcriptomics data while preserving
inherent biological information. These strategies improve the predictive power
and retain the interpretability of transcriptomics, enabling rich unimodal
representations for complex biological tasks.

</details>


### [317] [Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization](https://arxiv.org/abs/2505.21321)
*Leonard Papenmeier,Luigi Nardi*

Main category: cs.LG

TL;DR: Bencher是一个模块化的黑盒优化基准测试框架，通过解耦基准测试执行与优化逻辑，解决了依赖冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试套件通常将多个基准测试集成在一个项目中，导致依赖冲突和集成复杂性问题。

Method: Bencher通过将每个基准测试隔离在独立的虚拟Python环境中，并通过统一的RPC接口访问，简化了集成。支持本地、Docker或HPC集群部署。

Result: Bencher支持80个跨连续、分类和二元领域的基准测试，提供轻量级客户端和可复现的容器化运行时。

Conclusion: Bencher通过模块化和隔离设计，显著提升了基准测试的灵活性和可扩展性。

Abstract: We present Bencher, a modular benchmarking framework for black-box
optimization that fundamentally decouples benchmark execution from optimization
logic. Unlike prior suites that focus on combining many benchmarks in a single
project, Bencher introduces a clean abstraction boundary: each benchmark is
isolated in its own virtual Python environment and accessed via a unified,
version-agnostic remote procedure call (RPC) interface. This design eliminates
dependency conflicts and simplifies the integration of diverse, real-world
benchmarks, which often have complex and conflicting software requirements.
Bencher can be deployed locally or remotely via Docker or on high-performance
computing (HPC) clusters via Singularity, providing a containerized,
reproducible runtime for any benchmark. Its lightweight client requires minimal
setup and supports drop-in evaluation of 80 benchmarks across continuous,
categorical, and binary domains.

</details>


### [318] [UGCE: User-Guided Incremental Counterfactual Exploration](https://arxiv.org/abs/2505.21330)
*Christos Fragkathoulas,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: UGCE是一种基于遗传算法的增量反事实解释框架，动态适应用户约束变化，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法动态适应用户约束的迭代更新，效率低下且不灵活。

Method: 提出UGCE框架，基于遗传算法增量更新反事实解释。

Result: 在五个基准数据集上，UGCE显著提高计算效率并保持高质量解。

Conclusion: UGCE支持稳定性能，高效适应约束变化，并揭示约束类型对搜索行为的影响。

Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting
machine learning predictions by identifying minimal feature changes that alter
model outputs. However, in real-world settings, users often refine feasibility
constraints over time, requiring counterfactual generation to adapt
dynamically. Existing methods fail to support such iterative updates, instead
recomputing explanations from scratch with each change, an inefficient and
rigid approach. We propose User-Guided Incremental Counterfactual Exploration
(UGCE), a genetic algorithm-based framework that incrementally updates
counterfactuals in response to evolving user constraints. Experimental results
across five benchmark datasets demonstrate that UGCE significantly improves
computational efficiency while maintaining high-quality solutions compared to a
static, non-incremental approach. Our evaluation further shows that UGCE
supports stable performance under varying constraint sequences, benefits from
an efficient warm-start strategy, and reveals how different constraint types
may affect search behavior.

</details>


### [319] [Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)
*Loucas Pillaud-Vivien,Adrien Schertzer*

Main category: cs.LG

TL;DR: 论文研究了在高维高斯模型中联合学习一维投影和单变量函数的问题，分析了梯度流动态并证明了收敛性，提出了一种基于RKHS的实用方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据中低维结构学习的非凸问题，结合表示学习和非线性回归。

Method: 采用交替方案分析梯度流动态，利用RKHS实现高效的单变量函数估计。

Result: 证明了收敛性，且初始方向与目标负相关时仍能收敛。

Conclusion: 为高维数据中的低维结构学习提供了理论见解和实用方法。

Abstract: We consider the problem of jointly learning a one-dimensional projection and
a univariate function in high-dimensional Gaussian models. Specifically, we
study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$,
where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of
$\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are
learned from Gaussian data. This setting captures a fundamental non-convex
problem at the intersection of representation learning and nonlinear
regression. We analyze the gradient flow dynamics of a natural alternating
scheme and prove convergence, with a rate controlled by the information
exponent reflecting the \textit{Gaussian regularity} of the function
$\varphi^\star$. Strikingly, our analysis shows that convergence still occurs
even when the initial direction is negatively correlated with the target. On
the practical side, we demonstrate that such joint learning can be effectively
implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the
structure of the problem, enabling efficient and flexible estimation of the
univariate function. Our results offer both theoretical insight and practical
methodology for learning low-dimensional structure in high-dimensional
settings.

</details>


### [320] [An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction](https://arxiv.org/abs/2505.21339)
*Henryk Mustroph,Michel Kunkler,Stefanie Rinderle-Ma*

Main category: cs.LG

TL;DR: 提出了一种基于U-ED-LSTM和MC采样的概率后缀预测方法，用于处理业务流程中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅预测最可能的后缀，但面对高不确定性或高变异性时，单一预测的局限性明显。

Method: 采用U-ED-LSTM模型和MC采样算法，通过MC dropout和学习损失衰减捕捉不确定性和变异性。

Result: U-ED-LSTM在多个数据集上表现良好，概率预测优于单一预测，尤其在罕见前缀或长后缀情况下。

Conclusion: 该方法能有效捕捉事件日志中的不确定性，提升预测的多样性和准确性。

Abstract: Suffix prediction of business processes forecasts the remaining sequence of
events until process completion. Current approaches focus on predicting a
single, most likely suffix. However, if the future course of a process is
exposed to uncertainty or has high variability, the expressiveness of a single
suffix prediction can be limited. To address this limitation, we propose
probabilistic suffix prediction, a novel approach that approximates a
probability distribution of suffixes. The proposed approach is based on an
Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)
suffix sampling algorithm. We capture epistemic uncertainties via MC dropout
and aleatoric uncertainties as learned loss attenuation. This technical report
provides a detailed evaluation of the U-ED-LSTM's predictive performance and
assesses its calibration on four real-life event logs with three different
hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable
predictive performance across various datasets, ii) aggregating probabilistic
suffix predictions into mean values can outperform most likely predictions,
particularly for rare prefixes or longer suffixes, and iii) the approach
effectively captures uncertainties present in event logs.

</details>


### [321] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/abs/2505.21347)
*Ziheng Cheng,Yixiao Huang,Hui Xu,Somayeh Sojoudi,Xuandong Zhao,Dawn Song,Song Mei*

Main category: cs.LG

TL;DR: 论文提出了OVERT，首个大规模评估文本到图像（T2I）模型过度拒绝现象的基准，包含4600个看似有害但无害的提示和1785个真正有害的提示。研究发现过度拒绝是普遍问题，并尝试通过提示改写缓解，但发现其可能影响原始提示的忠实度。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I模型在生成视觉内容方面取得了显著成功，但其安全对齐策略常导致过度拒绝无害提示，降低了实用性。目前缺乏系统性评估这一现象的大规模基准。

Method: 提出自动构建合成评估数据的流程，创建OVERT基准，包含4600个无害但看似有害的提示和1785个真正有害的提示。评估多个领先T2I模型的过度拒绝行为，并尝试通过提示改写缓解问题。

Result: 发现过度拒绝是普遍问题，提示改写虽能缓解但可能影响忠实度。OVERT框架能灵活适应不同安全需求。

Conclusion: 需要进一步研究以在不损害功能的情况下增强T2I模型的安全对齐。OVERT为评估和改进提供了重要工具。

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality.As a preliminary attempt to
reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


### [322] [CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models](https://arxiv.org/abs/2505.21360)
*Dhanesh Ramachandram*

Main category: cs.LG

TL;DR: CRISP-NAM是一种可解释的神经网络模型，用于竞争风险生存分析，通过独立神经网络建模特征对风险的影响，保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，患者可能经历多种事件类型，竞争风险是生存建模中的关键问题。

Method: 扩展神经可加架构，建模特定原因风险，通过独立神经网络可视化特征与风险的非线性关系。

Result: 在多个数据集上表现优于现有方法。

Conclusion: CRISP-NAM在保持可解释性的同时，提供了竞争风险的准确预测。

Abstract: Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.

</details>


### [323] [Subgroups Matter for Robust Bias Mitigation](https://arxiv.org/abs/2505.21363)
*Anissa Alloula,Charles Jones,Ben Glocker,Bartłomiej W. Papież*

Main category: cs.LG

TL;DR: 论文探讨了偏见缓解方法失败的原因，指出子群定义是关键因素，并通过实验和理论分析揭示了子群选择对性能的显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管偏见缓解方法不断发展，但缺乏一致性成功，研究旨在揭示子群定义对方法效果的关键作用。

Method: 通过多任务实验，系统评估不同子群定义（如粗粒度、细粒度、交叉和噪声子群）对偏见缓解方法的影响。

Result: 子群选择显著影响性能，某些子群定义甚至比不缓解更差，且改善公平性可能需要使用不同的子群。

Conclusion: 子群定义在偏见缓解中至关重要，需谨慎选择以提高模型的鲁棒性和公平性。

Abstract: Despite the constant development of new bias mitigation methods for machine
learning, no method consistently succeeds, and a fundamental question remains
unanswered: when and why do bias mitigation techniques fail? In this paper, we
hypothesise that a key factor may be the often-overlooked but crucial step
shared by many bias mitigation methods: the definition of subgroups. To
investigate this, we conduct a comprehensive evaluation of state-of-the-art
bias mitigation methods across multiple vision and language classification
tasks, systematically varying subgroup definitions, including coarse,
fine-grained, intersectional, and noisy subgroups. Our results reveal that
subgroup choice significantly impacts performance, with certain groupings
paradoxically leading to worse outcomes than no mitigation at all. Our findings
suggest that observing a disparity between a set of subgroups is not a
sufficient reason to use those subgroups for mitigation. Through theoretical
analysis, we explain these phenomena and uncover a counter-intuitive insight
that, in some cases, improving fairness with respect to a particular set of
subgroups is best achieved by using a different set of subgroups for
mitigation. Our work highlights the importance of careful subgroup definition
in bias mitigation and suggest it as a alternative lever for improving the
robustness and fairness of machine learning models.

</details>


### [324] [Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders](https://arxiv.org/abs/2505.21364)
*James Oldfield,Shawn Im,Yixuan Li,Mihalis A. Nicolaou,Ioannis Patras,Grigorios G Chrysos*

Main category: cs.LG

TL;DR: 论文提出了一种名为Mixture of Decoders (MxDs)的方法，通过层级稀疏性改进多层感知机（MLPs）的可解释性和准确性，避免了神经元级稀疏性带来的性能损失。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经元级稀疏性的方法在近似原始映射时会导致模型性能显著下降，因此需要一种新的方法来平衡稀疏性和准确性。

Method: MxDs通过张量分解将预训练的密集层扩展为数千个稀疏激活的子层，每个子层实现全秩权重的线性变换，保留了原始解码器的表达能力。

Result: 实验表明，MxDs在稀疏性和准确性方面优于现有方法（如Transcoders），并在稀疏探测和特征引导任务中表现出色。

Conclusion: MxDs为设计既可解释又忠实分解的语言模型提供了新途径。

Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models,
yet their dense representations render them difficult to understand, edit, and
steer. Recent methods learn interpretable approximations via neuron-level
sparsity, yet fail to faithfully reconstruct the original
mapping--significantly increasing model's next-token cross-entropy loss. In
this paper, we advocate for moving to layer-level sparsity to overcome the
accuracy trade-off in sparse layer approximation. Under this paradigm, we
introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear
Units, expanding pre-trained dense layers into tens of thousands of specialized
sublayers. Through a flexible form of tensor factorization, each sparsely
activating MxD sublayer implements a linear transformation with full-rank
weights--preserving the original decoders' expressive capacity even under heavy
sparsity. Experimentally, we show that MxDs significantly outperform
state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier
in language models with up to 3B parameters. Further evaluations on sparse
probing and feature steering demonstrate that MxDs learn similarly specialized
features of natural language--opening up a promising new avenue for designing
interpretable yet faithful decompositions. Our code is included at:
https://github.com/james-oldfield/MxD/.

</details>


### [325] [PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment](https://arxiv.org/abs/2505.21366)
*Qi Yu,Zhichen Zeng,Yuchen Yan,Zhining Liu,Baoyu Jing,Ruizhong Qiu,Ariful Azad,Hanghang Tong*

Main category: cs.LG

TL;DR: PLANETALIGN是一个用于网络对齐的Python库，提供数据集、方法和评估工具，旨在促进NA方法的开发和基准测试。


<details>
  <summary>Details</summary>
Motivation: 网络对齐（NA）是多网络学习任务的基础，但缺乏系统开发和评估NA方法的工具。

Method: PLANETALIGN整合了18个数据集和14种NA方法，提供标准化评估流程和易用API。

Result: 通过比较研究揭示了现有NA方法的优缺点。

Conclusion: PLANETALIGN有望推动NA问题的深入理解和未来方法的改进。

Abstract: Network alignment (NA) aims to identify node correspondence across different
networks and serves as a critical cornerstone behind various downstream
multi-network learning tasks. Despite growing research in NA, there lacks a
comprehensive library that facilitates the systematic development and
benchmarking of NA methods. In this work, we introduce PLANETALIGN, a
comprehensive Python library for network alignment that features a rich
collection of built-in datasets, methods, and evaluation pipelines with
easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA
methods with extensible APIs for easy use and development of NA methods. Our
standardized evaluation pipeline encompasses a wide range of metrics, enabling
a systematic assessment of the effectiveness, scalability, and robustness of NA
methods. Through extensive comparative studies, we reveal practical insights
into the strengths and limitations of existing NA methods. We hope that
PLANETALIGN can foster a deeper understanding of the NA problem and facilitate
the development and benchmarking of more effective, scalable, and robust
methods in the future. The source code of PLANETALIGN is available at
https://github.com/yq-leo/PlanetAlign.

</details>


### [326] [Improving LLM-based Global Optimization with Search Space Partitioning](https://arxiv.org/abs/2505.21372)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Fabio Ferreira,Aaron Klein,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: HOLLM是一种新颖的全局优化算法，通过将搜索空间划分为有潜力的子区域，结合LLM生成高质量候选点，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在全局优化中表现良好，但在高维空间或缺乏领域先验时效果不佳，HOLLM旨在解决这些问题。

Method: HOLLM将搜索空间划分为子区域，采用类似多臂老虎机的评分机制平衡探索与利用，LLM在每个子区域内生成候选点。

Result: 实验表明，HOLLM在标准优化基准上优于贝叶斯优化和基于LLM的全局采样策略。

Conclusion: HOLLM通过结合空间划分和LLM采样，显著提升了全局优化的效果。

Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate
models and candidate generators within global optimization frameworks for
expensive blackbox functions. Despite promising results, LLM-based methods
often struggle in high-dimensional search spaces or when lacking
domain-specific priors, leading to sparse or uninformative suggestions. To
overcome these limitations, we propose HOLLM, a novel global optimization
algorithm that enhances LLM-driven sampling by partitioning the search space
into promising subregions. Each subregion acts as a ``meta-arm'' selected via a
bandit-inspired scoring mechanism that effectively balances exploration and
exploitation. Within each selected subregion, an LLM then proposes high-quality
candidate points, without any explicit domain knowledge. Empirical evaluation
on standard optimization benchmarks shows that HOLLM consistently matches or
surpasses leading Bayesian optimization and trust-region methods, while
substantially outperforming global LLM-based sampling strategies.

</details>


### [327] [DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2505.21382)
*Nastaran Saadati,Zhanhong Jiang,Joshua R. Waite,Shreyan Ganguly,Aditya Balu,Chinmay Hegde,Soumik Sarkar*

Main category: cs.LG

TL;DR: 论文提出了一种改进的去中心化LoRA（DLoRA）方法，通过确保梯度平滑性使其收敛速度与去中心化SGD匹配，并引入DeCAF算法解决共识干扰问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在去中心化环境中的应用尚未充分探索，尤其是缺乏理论支持，如平滑性保证和模型共识干扰问题。

Method: 通过梯度平滑性改进DLoRA的收敛速度，并引入DeCAF算法，结合截断奇异值分解（TSVD）解决共识干扰。

Result: 理论分析表明TSVD的近似误差有界，共识差异随秩增加而消失，实验显示算法在视觉/语言任务中优于本地训练，媲美联邦学习。

Conclusion: DeCAF算法在理论和实验上均表现出色，适用于IID和非IID数据分布，为去中心化LoRA提供了有效解决方案。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective,
computationally tractable fine-tuning approaches for training Vision-Language
Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by
freezing the pre-trained model weights and injecting trainable low-rank
matrices, allowing for efficient learning of these foundation models even on
edge devices. However, LoRA in decentralized settings still remains under
explored, particularly for the theoretical underpinnings due to the lack of
smoothness guarantee and model consensus interference (defined formally below).
This work improves the convergence rate of decentralized LoRA (DLoRA) to match
the rate of decentralized SGD by ensuring gradient smoothness. We also
introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular
value decomposition (TSVD)-based matrix factorization to resolve consensus
interference. Theoretical analysis shows TSVD's approximation error is bounded
and consensus differences between DLoRA and DeCAF vanish as rank increases,
yielding DeCAF's matching convergence rate. Extensive experiments across
vision/language tasks demonstrate our algorithms outperform local training and
rivals federated learning under both IID and non-IID data distributions.

</details>


### [328] [Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features](https://arxiv.org/abs/2505.21391)
*Zixuan Xie,Xinyu Liu,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文首次在任意特征下建立了线性TD(λ)的L²收敛速率，无需额外假设或算法修改。


<details>
  <summary>Details</summary>
Motivation: 传统的线性TD(λ)收敛速率分析依赖于线性无关特征假设，而实际场景中这一假设常不成立。

Method: 提出了一种新的随机逼近结果，针对解集的收敛速率而非单点收敛。

Result: 证明了在折扣和平均奖励设置下，线性TD(λ)的L²收敛速率。

Conclusion: 该研究扩展了线性TD(λ)的适用性，解决了实际中特征相关性问题。

Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning
algorithms for policy evaluation. Previously, convergence rates are typically
established under the assumption of linearly independent features, which does
not hold in many practical scenarios. This paper instead establishes the first
$L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary
features, without making any algorithmic modification or additional
assumptions. Our results apply to both the discounted and average-reward
settings. To address the potential non-uniqueness of solutions resulting from
arbitrary features, we develop a novel stochastic approximation result
featuring convergence rates to the solution set instead of a single point.

</details>


### [329] [Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits](https://arxiv.org/abs/2505.21393)
*Maoli Liu,Zhuohua Li,Xiangxiang Dai,John C. S. Lui*

Main category: cs.LG

TL;DR: 论文提出了三种新算法（CLiSK、CLiME、CLiSK-ME），用于改进对话推荐系统中的偏好学习和对话启动策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在偏好学习和对话启动策略上存在不足，导致偏好估计不准确和交互效率低下。

Method: CLiSK通过平滑关键词上下文增强探索，CLiME基于偏好不确定性自适应启动对话，CLiSK-ME结合两者。

Result: 理论证明算法具有更紧的遗憾上界$O(\sqrt{dT\log{T}})$，实验显示累积遗憾至少提升14.6%。

Conclusion: 新算法在理论和实验上均优于现有方法，接近极小极大最优。

Abstract: Conversational recommender systems proactively query users with relevant "key
terms" and leverage the feedback to elicit users' preferences for personalized
recommendations. Conversational contextual bandits, a prevalent approach in
this domain, aim to optimize preference learning by balancing exploitation and
exploration. However, several limitations hinder their effectiveness in
real-world scenarios. First, existing algorithms employ key term selection
strategies with insufficient exploration, often failing to thoroughly probe
users' preferences and resulting in suboptimal preference estimation. Second,
current algorithms typically rely on deterministic rules to initiate
conversations, causing unnecessary interactions when preferences are
well-understood and missed opportunities when preferences are uncertain. To
address these limitations, we propose three novel algorithms: CLiSK, CLiME, and
CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in
preference learning, CLiME adaptively initiates conversations based on
preference uncertainty, and CLiSK-ME integrates both techniques. We
theoretically prove that all three algorithms achieve a tighter regret upper
bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving
upon existing methods. Additionally, we provide a matching lower bound
$\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our
algorithms are nearly minimax optimal. Extensive evaluations on both synthetic
and real-world datasets show that our approaches achieve at least a 14.6%
improvement in cumulative regret.

</details>


### [330] [Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment](https://arxiv.org/abs/2505.21395)
*Xingyu Zhou,Yulian Wu,Wenqian Weng,Francesco Orabona*

Main category: cs.LG

TL;DR: 论文提出SquareχPO方法，通过替换标准对数损失为平方损失，在隐私保护和标签噪声下优化语言模型对齐，实现了最优差分隐私和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究在标签噪声和隐私保护条件下，如何优化语言模型的离线对齐问题。

Method: 提出SquareχPO方法，将标准对数损失替换为平方损失，适用于差分隐私和标签噪声场景。

Result: 在局部和中心隐私模型下实现最优对齐率，并在标签噪声下提供理论保证，同时支持隐私和噪声的联合处理。

Conclusion: SquareχPO在隐私和噪声场景下表现出色，且分析框架统一，具有独立的理论价值。

Abstract: In this paper, we theoretically study the offline alignment of language
models with human preference feedback, under both preference label corruption
and privacy protections. To this end, we propose Square$\chi$PO, a simple
one-line change to $\chi$PO where the standard log-loss is replaced by a new
square loss over probability. Thanks to the inherent properties of this new
loss, we have advanced the state-of-the-art of differentially private and
robust offline direct alignment. Specifically, for the local model of label
privacy, Square$\chi$PO is the first algorithm that attains an optimal rate
based on single-policy concentrability even with general function
approximations. It also gives the first result under the central model of
privacy protection over both prompts (responses) and labels. On the robustness
side against Huber label corruption, Square$\chi$PO is the first alignment
method that has a meaningful theoretical guarantee under general function
approximations. More importantly, Square$\chi$PO can address privacy protection
and corruption simultaneously, where an interesting separation is observed,
implying that the order of privacy and corruption matters. Furthermore, we show
that Square$\chi$PO can also be easily extended to handle the scenario of the
general preference model with state-of-the-art guarantees under corruption and
privacy. Last but not least, all of our theoretical guarantees enjoy a unified
analysis, building upon a new result on the generalization error bounds of
least-square regression under corruption and privacy constraints, which we
believe is of independent interest to the community.

</details>


### [331] [A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21400)
*Gen Li,Changxiao Cai*

Main category: cs.LG

TL;DR: 本文从信息论角度分析了扩散语言模型的收敛性，证明了采样误差随迭代次数T的倒数衰减，并与目标文本序列中标记间的互信息线性相关。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出强大潜力，但其理论理解尚不完善，本文旨在填补这一空白。

Method: 通过信息论视角，建立扩散语言模型的收敛保证，分析KL散度度量的采样误差。

Result: 证明了采样误差随迭代次数T的倒数衰减，并与标记间的互信息线性相关，建立了匹配的上下界。

Conclusion: 研究结果为扩散语言模型的实际有效性提供了新的理论见解。

Abstract: Diffusion models have emerged as a powerful paradigm for modern generative
modeling, demonstrating strong potential for large language models (LLMs).
Unlike conventional autoregressive (AR) models that generate tokens
sequentially, diffusion models enable parallel token sampling, leading to
faster generation and eliminating left-to-right generation constraints. Despite
their empirical success, the theoretical understanding of diffusion model
approaches remains underdeveloped. In this work, we develop convergence
guarantees for diffusion language models from an information-theoretic
perspective. Our analysis demonstrates that the sampling error, measured by the
Kullback-Leibler (KL) divergence, decays inversely with the number of
iterations $T$ and scales linearly with the mutual information between tokens
in the target text sequence. In particular, we establish matching upper and
lower bounds, up to some constant factor, to demonstrate the tightness of our
convergence analysis. These results offer novel theoretical insights into the
practical effectiveness of diffusion language models.

</details>


### [332] [Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks](https://arxiv.org/abs/2505.21404)
*Anas Jnini,Flavio Vella*

Main category: cs.LG

TL;DR: D-NGD方法通过将自然梯度下降的计算从参数空间转移到残差空间，显著降低了计算复杂度，并成功扩展到大规模PINN训练。


<details>
  <summary>Details</summary>
Motivation: 传统自然梯度方法在参数空间中计算Gauss-Newton更新，导致O(n^3)的高计算复杂度，限制了其在大规模PINN中的应用。

Method: 提出D-NGD方法，将Gauss-Newton步计算转移到残差空间，并引入测地加速校正，支持直接求解和Nystrom预处理的共轭梯度求解。

Result: D-NGD成功扩展到1280万参数的PINN训练，相比一阶和拟牛顿方法，最终误差降低1-3个数量级，且支持单GPU训练。

Conclusion: D-NGD为大规模PINN训练提供了一种高效且可扩展的自然梯度优化方法。

Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed
Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the
parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is
the number of network trainable weights. We show that exactly the same step can
instead be formulated in a generally smaller residual space of size $m =
\sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g.
PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation
points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent}
(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it
with a geodesic-acceleration correction at negligible extra cost, and provides
both a dense direct solver for modest $m$ and a Nystrom-preconditioned
conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with
up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower
final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton
methods, and -- crucially -- enables natural-gradient training of PINNs at this
scale on a single GPU.

</details>


### [333] [A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment](https://arxiv.org/abs/2505.21414)
*Brett Bissey,Kyle Gatesman,Walker Dimon,Mohammad Alam,Luis Robaina,Joseph Weissman*

Main category: cs.LG

TL;DR: 本文提出了一种分析并保护基于深度强化学习（DRL）的决策支持系统的框架，通过模拟发现学习行为模式和漏洞，评估对抗攻击效果。


<details>
  <summary>Details</summary>
Motivation: 在部署前确保DRL系统的安全性，揭示其行为模式和潜在漏洞，以应对高风险环境中的对抗攻击。

Method: 开发了一个框架，通过模拟生成精确的观察扰动，评估对抗攻击效果，并在自定义战略游戏CyberStrike中验证。

Result: 发现并排名了攻击对不同观察指标和时间步的影响，验证了对抗攻击在多种DRL架构和算法中的可迁移性。

Conclusion: 强调了在高风险环境中保护决策策略的对抗防御机制的重要性。

Abstract: This paper introduces a comprehensive framework designed to analyze and
secure decision-support systems trained with Deep Reinforcement Learning (DRL),
prior to deployment, by providing insights into learned behavior patterns and
vulnerabilities discovered through simulation. The introduced framework aids in
the development of precisely timed and targeted observation perturbations,
enabling researchers to assess adversarial attack outcomes within a strategic
decision-making context. We validate our framework, visualize agent behavior,
and evaluate adversarial outcomes within the context of a custom-built
strategic game, CyberStrike. Utilizing the proposed framework, we introduce a
method for systematically discovering and ranking the impact of attacks on
various observation indices and time-steps, and we conduct experiments to
evaluate the transferability of adversarial attacks across agent architectures
and DRL training algorithms. The findings underscore the critical need for
robust adversarial defense mechanisms to protect decision-making policies in
high-stakes environments.

</details>


### [334] [When Shift Happens - Confounding Is to Blame](https://arxiv.org/abs/2505.21422)
*Abbavaram Gowtham Reddy,Celia Rubio-Madrigal,Rebekka Burkholz,Krikamol Muandet*

Main category: cs.LG

TL;DR: 论文探讨了分布偏移对机器学习模型鲁棒性和泛化能力的影响，发现传统因果不变性方法可能不如预期有效，而利用所有协变量（包括非因果变量）反而能提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究分布偏移对模型泛化能力的影响，并解释为何传统因果不变性方法在某些情况下表现不佳。

Method: 通过实证和理论分析，探讨隐藏混杂因素对分布偏移的影响，并提出利用环境特定关系和混杂因素代理的方法。

Result: 发现隐藏混杂因素导致分布偏移，传统方法失效；利用环境特定关系和混杂因素代理可提升泛化性能。

Conclusion: 研究为设计鲁棒的OOD泛化算法和协变量选择策略提供了新的理论和实践指导。

Abstract: Distribution shifts introduce uncertainty that undermines the robustness and
generalization capabilities of machine learning models. While conventional
wisdom suggests that learning causal-invariant representations enhances
robustness to such shifts, recent empirical studies present a counterintuitive
finding: (i) empirical risk minimization (ERM) can rival or even outperform
state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its
OOD generalization performance improves when all available covariates, not just
causal ones, are utilized. Drawing on both empirical and theoretical evidence,
we attribute this phenomenon to hidden confounding. Shifts in hidden
confounding induce changes in data distributions that violate assumptions
commonly made by existing OOD generalization approaches. Under such conditions,
we prove that effective generalization requires learning environment-specific
relationships, rather than relying solely on invariant ones. Furthermore, we
show that models augmented with proxies for hidden confounders can mitigate the
challenges posed by hidden confounding shifts. These findings offer new
theoretical insights and practical guidance for designing robust OOD
generalization algorithms and principled covariate selection strategies.

</details>


### [335] [Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization](https://arxiv.org/abs/2505.21423)
*Vit Fojtik,Maria Matveev,Hung-Hsu Chou,Gitta Kutyniok,Johannes Maly*

Main category: cs.LG

TL;DR: 论文探讨了梯度下降中学习率如何平衡参数范数和模型锐度，表明单一隐式偏差不足以解释泛化性能，需综合考虑动态权衡。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化神经网络泛化能力的机制，特别是梯度下降中学习率对隐式正则化的影响。

Method: 通过实验和理论分析，研究了梯度下降中学习率对参数范数和模型锐度的动态权衡，并以对角线性网络为例进行验证。

Result: 发现学习率在低参数范数和低锐度之间起到平衡作用，单一隐式偏差无法最小化泛化误差。

Conclusion: 需采用更广泛的隐式正则化视角，综合考虑学习率诱导的动态权衡，以全面解释泛化性能。

Abstract: A widely believed explanation for the remarkable generalization capacities of
overparameterized neural networks is that the optimization algorithms used for
training induce an implicit bias towards benign solutions. To grasp this
theoretically, recent works examine gradient descent and its variants in
simplified training settings, often assuming vanishing learning rates. These
studies reveal various forms of implicit regularization, such as $\ell_1$-norm
minimizing parameters in regression and max-margin solutions in classification.
Concurrently, empirical findings show that moderate to large learning rates
exceeding standard stability thresholds lead to faster, albeit oscillatory,
convergence in the so-called Edge-of-Stability regime, and induce an implicit
bias towards minima of low sharpness (norm of training loss Hessian). In this
work, we argue that a comprehensive understanding of the generalization
performance of gradient descent requires analyzing the interaction between
these various forms of implicit regularization. We empirically demonstrate that
the learning rate balances between low parameter norm and low sharpness of the
trained model. We furthermore prove for diagonal linear networks trained on a
simple regression task that neither implicit bias alone minimizes the
generalization error. These findings demonstrate that focusing on a single
implicit bias is insufficient to explain good generalization, and they motivate
a broader view of implicit regularization that captures the dynamic trade-off
between norm and sharpness induced by non-negligible learning rates.

</details>


### [336] [Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate](https://arxiv.org/abs/2505.21430)
*Shiwei Zeng,Jie Shen*

Main category: cs.LG

TL;DR: 本文研究了在数据中存在恒定恶意噪声的情况下，如何高效学习稀疏半空间的问题，提出了一种基于铰链损失最小化的属性高效算法。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法面临数据损坏或对抗攻击的问题，需要设计对噪声具有鲁棒性的高效算法。

Method: 通过改进现有的铰链损失最小化程序，结合稀疏约束和梯度分析，实现属性高效学习。

Result: 提出了一种在恒定恶意噪声率下工作的属性高效PAC学习算法。

Conclusion: 在满足特定分布条件的情况下，通过简单改进现有方法，可以实现对噪声鲁棒的稀疏半空间学习。

Abstract: Attribute-efficient learning of sparse halfspaces has been a fundamental
problem in machine learning theory. In recent years, machine learning
algorithms are faced with prevalent data corruptions or even adversarial
attacks. It is of central interest to design efficient algorithms that are
robust to noise corruptions. In this paper, we consider that there exists a
constant amount of malicious noise in the data and the goal is to learn an
underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log
d)$ samples. Specifically, we follow a recent line of works and assume that the
underlying distribution satisfies a certain concentration condition and a
margin condition at the same time. Under such conditions, we show that
attribute-efficiency can be achieved by simple variants to existing hinge loss
minimization programs. Our key contribution includes: 1) an attribute-efficient
PAC learning algorithm that works under constant malicious noise rate; 2) a new
gradient analysis that carefully handles the sparsity constraint in hinge loss
minimization.

</details>


### [337] [Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution](https://arxiv.org/abs/2505.21438)
*Yiwen Tu,Ziqi Liu,Jiaqi W. Ma,Weijing Tang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MTIF的方法，用于在多任务学习（MTL）中量化任务相关性并减少负迁移。MTIF通过数据归因提供细粒度的实例级相关性测量，优于传统的任务级方法。实验表明，MTIF能高效近似子集训练模型的性能，并通过数据选择策略提升MTL模型表现。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中任务相关性的测量和负迁移的缓解是一个关键挑战。本文旨在通过数据归因方法解决这一问题。

Method: 提出MultiTask Influence Function（MTIF），将影响函数扩展到MTL模型（硬或软参数共享），提供细粒度的实例级任务相关性测量。

Result: MTIF能高效且准确地近似子集训练模型的性能，基于MTIF的数据选择策略显著提升了MTL模型的表现。

Conclusion: MTIF建立了数据归因与MTL的新联系，为任务相关性测量和MTL模型优化提供了高效且细粒度的解决方案。

Abstract: Measuring task relatedness and mitigating negative transfer remain a critical
open challenge in Multitask Learning (MTL). This work extends data attribution
-- which quantifies the influence of individual training data points on model
predictions -- to MTL setting for measuring task relatedness. We propose the
MultiTask Influence Function (MTIF), a method that adapts influence functions
to MTL models with hard or soft parameter sharing. Compared to conventional
task relatedness measurements, MTIF provides a fine-grained, instance-level
relatedness measure beyond the entire-task level. This fine-grained relatedness
measure enables a data selection strategy to effectively mitigate negative
transfer in MTL. Through extensive experiments, we demonstrate that the
proposed MTIF efficiently and accurately approximates the performance of models
trained on data subsets. Moreover, the data selection strategy enabled by MTIF
consistently improves model performance in MTL. Our work establishes a novel
connection between data attribution and MTL, offering an efficient and
fine-grained solution for measuring task relatedness and enhancing MTL models.

</details>


### [338] [Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)
*Sheikh Shafayat,Fahim Tajwar,Ruslan Salakhutdinov,Jeff Schneider,Andrea Zanette*

Main category: cs.LG

TL;DR: 论文提出了一种在线自训练强化学习算法，利用模型的自一致性生成监督信号，无需真实标签，在数学推理任务中表现优异，但也揭示了奖励黑客化的挑战。


<details>
  <summary>Details</summary>
Motivation: 减少对人工监督的依赖，探索自训练方法以提升大型语言模型的性能。

Method: 提出在线自训练强化学习算法，通过模型的自一致性推断正确性信号，无需真实监督。

Result: 在数学推理任务中快速达到与基于黄金标准答案的强化学习方法相当的性能。

Conclusion: 自监督改进可以在无外部标签的情况下显著提升性能，但也面临奖励黑客化等挑战。

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [339] [High-Dimensional Calibration from Swap Regret](https://arxiv.org/abs/2505.21460)
*Maxwell Fishelson,Noah Golowich,Mehryar Mohri,Jon Schneider*

Main category: cs.LG

TL;DR: 论文研究了多维预测在线校准问题，将其与在线线性优化的外部遗憾最小化联系起来，提出了一种无需优化子程序的通用算法，并证明了指数级依赖的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究多维预测在线校准问题，探索其与在线线性优化问题的联系，以简化算法设计并理解校准误差的下界。

Method: 通过将校准问题转化为在线线性优化的外部遗憾最小化问题，提出了一种基于TreeSwap和Follow-The-Leader的通用算法。

Result: 证明了在多维单纯形和ℓ1范数下，校准误差的指数级依赖是必要的，并给出了具体的校准时间界限。

Conclusion: 论文通过连接校准与在线优化问题，提出了一种通用算法，并证明了校准误差的下界，为相关研究提供了理论支持。

Abstract: We study the online calibration of multi-dimensional forecasts over an
arbitrary convex set $\mathcal{P} \subset \mathbb{R}^d$ relative to an
arbitrary norm $\Vert\cdot\Vert$. We connect this with the problem of external
regret minimization for online linear optimization, showing that if it is
possible to guarantee $O(\sqrt{\rho T})$ worst-case regret after $T$ rounds
when actions are drawn from $\mathcal{P}$ and losses are drawn from the dual
$\Vert \cdot \Vert_*$ unit norm ball, then it is also possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\rho /\epsilon^2))$ rounds.
When $\mathcal{P}$ is the $d$-dimensional simplex and $\Vert \cdot \Vert$ is
the $\ell_1$-norm, the existence of $O(\sqrt{T\log d})$-regret algorithms for
learning with experts implies that it is possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\log{d}/\epsilon^2)) =
d^{O(1/\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).
  Interestingly, our algorithm obtains this guarantee without requiring access
to any online linear optimization subroutine or knowledge of the optimal rate
$\rho$ -- in fact, our algorithm is identical for every setting of
$\mathcal{P}$ and $\Vert \cdot \Vert$. Instead, we show that the optimal
regularizer for the above OLO problem can be used to upper bound the above
calibration error by a swap regret, which we then minimize by running the
recent TreeSwap algorithm with Follow-The-Leader as a subroutine.
  Finally, we prove that any online calibration algorithm that guarantees
$\epsilon T$ $\ell_1$-calibration error over the $d$-dimensional simplex
requires $T \geq \exp(\mathrm{poly}(1/\epsilon))$ (assuming $d \geq
\mathrm{poly}(1/\epsilon)$). This strengthens the corresponding
$d^{\Omega(\log{1/\epsilon})}$ lower bound of Peng, and shows that an
exponential dependence on $1/\epsilon$ is necessary.

</details>


### [340] [Causal Posterior Estimation](https://arxiv.org/abs/2505.21468)
*Simon Dirmeier,Antonietta Mira*

Main category: cs.LG

TL;DR: CPE是一种用于模拟器模型中贝叶斯推断的新方法，通过归一化流（NF）近似后验分布，结合模型的条件依赖结构，提高推断精度。


<details>
  <summary>Details</summary>
Motivation: 模拟器模型中似然函数难以计算或计算成本高，需要一种高效的后验推断方法。

Method: 利用归一化流（NF）近似后验分布，结合模型的图表示条件依赖结构，提出离散和连续NF架构，并优化采样复杂度。

Result: 实验表明，CPE在精度上优于或匹配现有技术。

Conclusion: CPE通过直接结合模型的条件依赖结构，实现了高效且精确的后验推断。

Abstract: We present Causal Posterior Estimation (CPE), a novel method for Bayesian
inference in simulator models, i.e., models where the evaluation of the
likelihood function is intractable or too computationally expensive, but where
one can simulate model outputs given parameter values. CPE utilizes a
normalizing flow-based (NF) approximation to the posterior distribution which
carefully incorporates the conditional dependence structure induced by the
graphical representation of the model into the neural network. Thereby it is
possible to improve the accuracy of the approximation. We introduce both
discrete and continuous NF architectures for CPE and propose a constant-time
sampling procedure for the continuous case which reduces the computational
complexity of drawing samples to O(1) as for discrete NFs. We show, through an
extensive experimental evaluation, that by incorporating the conditional
dependencies induced by the graphical model directly into the neural network,
rather than learning them from data, CPE is able to conduct highly accurate
posterior inference either outperforming or matching the state of the art in
the field.

</details>


### [341] [Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models](https://arxiv.org/abs/2505.21475)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TL;DR: 该论文研究了在高斯分布下学习实值多指数模型（MIMs）的复杂性，提出了一种在对抗性标签噪声下学习MIMs的通用算法，并证明了其复杂性的最优性。


<details>
  <summary>Details</summary>
Motivation: 研究MIMs学习的复杂性，特别是在对抗性噪声下的学习效率，以填补现有理论的空白。

Method: 提出了一种基于平方损失的PAC学习算法，适用于有界变差MIMs，并分析了其在对抗性噪声、可实现噪声和独立噪声下的复杂性。

Result: 算法在对抗性噪声下的复杂性为d^{O(m)}2^{poly(K/ε)}，在可实现和独立噪声下为d^{O(m)}2^{poly(K)}(1/ε)^{O(K)}。同时，证明了SQ学习的下界。

Conclusion: 该算法首次实现了对正齐次L-Lipschitz K-MIMs的高效学习，且复杂性独立于网络规模，改进了先前工作的指数依赖性。

Abstract: We study the complexity of learning real-valued Multi-Index Models (MIMs)
under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to
\mathbb{R}$ that depends only on the projection of its input onto a
$K$-dimensional subspace. We give a general algorithm for PAC learning a broad
class of MIMs with respect to the square loss, even in the presence of
adversarial label noise. Moreover, we establish a nearly matching Statistical
Query (SQ) lower bound, providing evidence that the complexity of our algorithm
is qualitatively optimal as a function of the dimension. Specifically, we
consider the class of bounded variation MIMs with the property that degree at
most $m$ distinguishing moments exist with respect to projections onto any
subspace. In the presence of adversarial label noise, the complexity of our
learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the
realizable and independent noise settings, our algorithm incurs complexity
$d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper
bound, we show that if for some subspace degree-$m$ distinguishing moments do
not exist, then any SQ learner for the corresponding class of MIMs requires
complexity $d^{\Omega(m)}$. As an application, we give the first efficient
learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The
resulting algorithm has complexity $\mathrm{poly}(d)
2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for
Lipschitz homogeneous ReLU networks with complexity independent of the network
size, removing the exponential dependence incurred in prior work.

</details>


### [342] [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
*Ted Zadouri,Hubert Strauss,Tri Dao*

Main category: cs.LG

TL;DR: 论文提出两种注意力机制改进（GTA和GLA），以减少KV缓存的内存传输并提升硬件效率，同时保持模型质量。实验显示GTA和GLA在性能和并行性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM解码因KV缓存加载和顺序解码导致延迟高且并行性受限，研究旨在优化硬件利用率和并行性。

Method: 提出Grouped-Tied Attention (GTA)减少内存传输，Grouped Latent Attention (GLA)优化并行解码，结合低级优化。

Result: GTA节省一半KV缓存，GLA比FlashMLA快2倍，在线服务中延迟降低2倍，吞吐量提升。

Conclusion: GTA和GLA显著提升解码效率，适用于大规模并行和低延迟场景。

Abstract: LLM decoding is bottlenecked for large batches and long contexts by loading
the key-value (KV) cache from high-bandwidth memory, which inflates per-token
latency, while the sequential nature of decoding limits parallelism. We analyze
the interplay among arithmetic intensity, parallelization, and model quality
and question whether current architectures fully exploit modern hardware. This
work redesigns attention to perform more computation per byte loaded from
memory to maximize hardware efficiency without trading off parallel
scalability. We first propose Grouped-Tied Attention (GTA), a simple variant
that combines and reuses key and value states, reducing memory transfers
without compromising model quality. We then introduce Grouped Latent Attention
(GLA), a parallel-friendly latent attention paired with low-level optimizations
for fast decoding while maintaining high model quality. Experiments show that
GTA matches Grouped-Query Attention (GQA) quality while using roughly half the
KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier
to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for
example, in a speculative decoding setting when the query length exceeds one.
Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end
latency and increases throughput in online serving benchmarks by up to
2$\times$.

</details>


### [343] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
*Xiangxin Zhou,Zichen Liu,Anya Sims,Haonan Wang,Tianyu Pang,Chongxuan Li,Liang Wang,Min Lin,Chao Du*

Main category: cs.LG

TL;DR: 论文提出了一种名为VeriFree的无验证器方法，通过直接最大化生成参考答案的概率，解决了传统基于验证器的强化学习方法在通用推理领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于验证器的强化学习方法仅适用于可验证的任务，且存在依赖强验证器、易受奖励攻击等问题，难以扩展到现实领域。

Method: 提出VeriFree方法，绕过答案验证，直接使用强化学习最大化生成参考答案的概率。

Result: VeriFree在多个基准测试中表现优于或匹配基于验证器的方法，同时具有显著的实际优势和计算效率。

Conclusion: VeriFree是一种高效且通用的方法，为通用推理领域的强化学习提供了新的解决方案。

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [344] [TumorHoPe2: An updated database for Tumor Homing Peptides](https://arxiv.org/abs/2505.20913)
*Diksha Kashyap,Devanshi Gupta,Naman Kumar Mehta,Gajendra P. S. Raghava*

Main category: q-bio.BM

TL;DR: TumorHoPe2是一个手动整理的数据库，提供关于实验验证的肿瘤归巢肽（THPs）的详细信息，包含1847个条目，是2012年版本的重大更新。


<details>
  <summary>Details</summary>
Motivation: 满足对肿瘤归巢肽（THPs）组织化数据日益增长的需求。

Method: 通过手动整理实验验证的THPs数据，包括序列、修饰、靶向癌细胞系和肿瘤类型，数据来源于噬菌体展示库和合成肽。

Result: 数据库包含1847个条目（1297个独特THPs），涵盖172种癌细胞系和37种肿瘤类型，提供搜索、过滤和分析工具。

Conclusion: TumorHoPe2为研究社区提供了高效的数据访问和分析工具，免费开放使用。

Abstract: Addressing the growing need for organized data on tumor homing peptides
(THPs), we present TumorHoPe2, a manually curated database offering extensive
details on experimentally validated THPs. This represents a significant update
to TumorHoPe, originally developed by our group in 2012. TumorHoPe2 now
contains 1847 entries, representing 1297 unique tumor homing peptides, a
substantial expansion from the 744 entries in its predecessor. For each
peptide, the database provides critical information, including sequence,
terminal or chemical modifications, corresponding cancer cell lines, and
specific tumor types targeted. The database compiles data from two primary
sources: phage display libraries, which are commonly used to identify peptide
ligands targeting tumor-specific markers, and synthetic peptides, which are
chemically modified to enhance properties such as stability, binding affinity,
and specificity. Our dataset includes 594 chemically modified peptides, with
255 having N-terminal and 195 C-terminal modifications. These THPs have been
validated against 172 cancer cell lines and demonstrate specificity for 37
distinct tumor types. To maximize utility for the research community,
TumorHoPe2 is equipped with intuitive tools for data searching, filtering, and
analysis, alongside a RESTful API for efficient programmatic access and
integration into bioinformatics pipelines. It is freely available at
https://webs.iiitd.edu.in/raghava/tumorhope2/

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [345] [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
*Louis Jalouzot,Alexis Thual,Yair Lakretz,Christophe Pallier,Bertrand Thirion*

Main category: q-bio.NC

TL;DR: 研究探讨了从少量参与者的fMRI数据中解码自然语音的最优策略，发现多参与者训练对解码准确性无显著提升，且解码器对句法特征的建模优于语义特征。


<details>
  <summary>Details</summary>
Motivation: 探索在有限参与者数据下，如何优化从fMRI数据解码自然语音的策略。

Method: 使用深度神经网络预测LLM衍生的文本表示，并比较单参与者和多参与者训练的效果。

Result: 多参与者训练未提升解码准确性，解码器对句法特征建模更优，复杂句法或丰富语义内容增加解码难度。

Conclusion: 深度表型分析（单参与者大量数据）更有效，多参与者解码需更大样本或更深表型分析。

Abstract: We investigate optimal strategies for decoding perceived natural speech from
fMRI data acquired from a limited number of participants. Leveraging Lebel et
al. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness
of training deep neural networks to predict LLM-derived text representations
from fMRI activity. Then, in this data regime, we observe that multi-subject
training does not improve decoding accuracy compared to single-subject
approach. Furthermore, training on similar or different stimuli across subjects
has a negligible effect on decoding accuracy. Finally, we find that our
decoders better model syntactic than semantic features, and that stories
containing sentences with complex syntax or rich semantic content are more
challenging to decode. While our results demonstrate the benefits of having
extensive data per participant (deep phenotyping), they suggest that leveraging
multi-subject for natural speech decoding likely requires deeper phenotyping or
a substantially larger cohort.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [346] [Larger cities, more commuters, more crime? The role of inter-city commuting in the scaling of urban crime](https://arxiv.org/abs/2505.20822)
*Simon Puttock,Umberto Barros,Diego Pinheiro,Marcos Oliveira*

Main category: physics.soc-ph

TL;DR: 研究发现，城市间通勤与犯罪率的关系密切，较大城市吸引更多通勤者，导致犯罪率上升。


<details>
  <summary>Details</summary>
Motivation: 探讨城市间通勤如何影响人口与犯罪的关系，揭示城市互联性对犯罪率的影响。

Method: 分析通勤者数量与犯罪率的关系，比较包含通勤者和仅包含人口的模型。

Result: 每增加1%的通勤者，盗窃和入室盗窃分别上升0.32%和0.20%。包含通勤者的模型更能解释犯罪率变化。

Conclusion: 城市互联性（通勤者数量）对犯罪率的影响不容忽视，需在犯罪研究中纳入城市间联系。

Abstract: Cities attract a daily influx of non-resident commuters, reflecting their
role in wider urban networks -- not as isolated places. However, it remains
unclear how this inter-connectivity shapes the way crime scales with
population, given that larger cities tend to receive more commuters and
experience more crime. Here, we investigate how inter-city commuting relates to
the population--crime relationship. We find that larger cities receive
proportionately more commuters, which in turn is associated with higher crime
levels. Specifically, each 1% increase in inbound commuters corresponds to a
0.32% rise in theft and 0.20% rise in burglary, holding population constant. We
show that models incorporating both population and commuter inflows better
explain crime variation than population-only models. These findings underscore
the importance of considering how cities are connected -- not just their
population size -- in disentangling the population--crime relationship.

</details>


### [347] [Parameter Effects in ReCom Ensembles](https://arxiv.org/abs/2505.21326)
*Kristopher Tapp,Todd Proebsting,Alec Ramsay*

Main category: physics.soc-ph

TL;DR: 研究了315个ReCom重分区集合，发现人口容忍度影响小，而算法和县保留参数对某些指标影响显著。


<details>
  <summary>Details</summary>
Motivation: 探讨重分区诉讼中参数选择的影响，填补现有研究的空白。

Method: 分析了7个州三个立法机构的315个ReCom集合，系统调整人口容忍度、县保留强度和算法变体，并引入新方法验证收敛性。

Result: 人口容忍度对所有评分影响可忽略，而算法和县保留参数在某些指标上影响显著，且在不同辖区中表现不一致或一致。

Conclusion: 使用ReCom集合时应慎重考虑参数选择。

Abstract: Ensemble analysis has become central to redistricting litigation, but
parameter effects remain understudied. We analyze 315 ReCom ensembles across
the three legislative chambers in 7 states, systematically varying the
population tolerance, county preservation strength, and algorithm variant. To
validate convergence, we introduce new methods to approximate effective sample
size and measure redundancy. We find that varying the population tolerance has
a negligible effect on all scores, whereas the algorithm and
county-preservation parameters can significantly affect some metrics,
inconsistently in some cases but surprisingly consistently in others across
jurisdictions. These findings suggest parameter choices should be thoughtfully
considered when using ReCom ensembles.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [348] [Generalized Coordination of Partially Cooperative Urban Traffic](https://arxiv.org/abs/2505.20879)
*Max Bastian Mertens,Michael Buchholz*

Main category: cs.RO

TL;DR: 提出了一种适用于城市交通的通用合作机动规划方法，显著提升交通效率。


<details>
  <summary>Details</summary>
Motivation: 通过车联网技术提升自动驾驶车辆的舒适性和安全性，同时解决混合交通（合作与非合作车辆共存）的挑战。

Method: 基于优化方法，辅以高效启发式算法处理高负载场景。

Result: 在合作率40%时，交通吞吐量提升，平均等待时间和停车车辆数量减少，且不影响安全性。

Conclusion: 该方法在混合交通中有效提升效率，具有实际应用潜力。

Abstract: Vehicle-to-anything connectivity, especially for autonomous vehicles,
promises to increase passenger comfort and safety of road traffic, for example,
by sharing perception and driving intention. Cooperative maneuver planning uses
connectivity to enhance traffic efficiency, which has, so far, been mainly
considered for automated intersection management. In this article, we present a
novel cooperative maneuver planning approach that is generalized to various
situations found in urban traffic. Our framework handles challenging mixed
traffic, that is, traffic comprising both cooperative connected vehicles and
other vehicles at any distribution. Our solution is based on an optimization
approach accompanied by an efficient heuristic method for high-load scenarios.
We extensively evaluate the proposed planer in a distinctly realistic
simulation framework and show significant efficiency gains already at a
cooperation rate of 40%. Traffic throughput increases, while the average
waiting time and the number of stopped vehicles are reduced, without impacting
traffic safety.

</details>


### [349] [Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments](https://arxiv.org/abs/2505.20423)
*Julio de la Torre-Vanegas,Miguel Soriano-Garcia,Israel Becerra,Diego Mercado-Ravell*

Main category: cs.RO

TL;DR: 论文提出了一种基于语义分割的风险感知方法，用于无人机在拥挤城市环境中的安全着陆，通过风险地图和控制系统实现高成功率着陆。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在紧急情况下于复杂城市环境中安全着陆的挑战，尤其是面对移动障碍物和视觉干扰。

Method: 使用深度神经网络进行像素级风险评估，结合风险地图算法和控制系统，动态识别安全着陆区。

Result: 在多样化城市环境中验证，着陆成功率超过90%，风险指标显著改善。

Conclusion: 风险导向的视觉方法能有效降低紧急着陆事故风险，提升无人机在复杂城市环境中的操作能力。

Abstract: Landing safely in crowded urban environments remains an essential yet
challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in
emergency situations. In this work, we propose a risk-aware approach that
harnesses semantic segmentation to continuously evaluate potential hazards in
the drone's field of view. By using a specialized deep neural network to assign
pixel-level risk values and applying an algorithm based on risk maps, our
method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving
critical obstacles such as vehicles, people, etc., and other visual challenges
like shifting illumination. A control system then guides the UAV toward this
low-risk region, employing altitude-dependent safety thresholds and temporal
landing point stabilization to ensure robust descent trajectories. Experimental
validation in diverse urban environments demonstrates the effectiveness of our
approach, achieving over 90% landing success rates in very challenging real
scenarios, showing significant improvements in various risk metrics. Our
findings suggest that risk-oriented vision methods can effectively help reduce
the risk of accidents in emergency landing situations, particularly in complex,
unstructured, urban scenarios, densely populated with moving risky obstacles,
while potentiating the true capabilities of UAVs in complex urban operations.

</details>


### [350] [Robot Operation of Home Appliances by Reading User Manuals](https://arxiv.org/abs/2505.20424)
*Jian Zhang,Hanbo Zhang,Anxing Xiao,David Hsu*

Main category: cs.RO

TL;DR: ApBot是一个通过阅读用户手册操作家用电器的机器人系统，利用视觉语言模型构建结构化模型，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 家用机器人需要能够操作各种家用电器，但现有方法难以从非结构化的用户手册中提取有效策略并可靠执行。

Method: ApBot通过视觉语言模型从用户手册中构建结构化符号模型，并通过视觉反馈更新模型。

Result: 实验表明，ApBot在模拟和真实环境中均显著优于直接使用大型视觉语言模型的方法。

Conclusion: 结构化内部表示对机器人可靠操作复杂家用电器至关重要。

Abstract: Operating home appliances, among the most common tools in every household, is
a critical capability for assistive home robots. This paper presents ApBot, a
robot system that operates novel household appliances by "reading" their user
manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial
policies from their unstructured, textual descriptions in a user manual
document, (ii) ground the policies to the appliance in the physical world, and
(iii) execute the policies reliably over potentially many steps, despite
compounding errors. To tackle these challenges, ApBot constructs a structured,
symbolic model of an appliance from its manual, with the help of a large
vision-language model (VLM). It grounds the symbolic actions visually to
control panel elements. Finally, ApBot closes the loop by updating the model
based on visual feedback. Our experiments show that across a wide range of
simulated and real-world appliances, ApBot achieves consistent and
statistically significant improvements in task success rate, compared with
state-of-the-art large VLMs used directly as control policies. These results
suggest that a structured internal representations plays an important role in
robust robot operation of home appliances, especially, complex ones.

</details>


### [351] [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
*Matthew Lisondra,Beno Benhabib,Goldie Nejat*

Main category: cs.RO

TL;DR: 本文综述了基础模型在移动服务机器人中的集成，探讨了其在实时传感器融合、语言条件控制和自适应任务执行中的作用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如大型语言模型、视觉语言模型等）的快速发展为移动服务机器人中的具身AI提供了新机遇，但仍面临多模态传感器融合、实时决策等挑战。

Method: 通过系统综述，分析基础模型如何解决具身AI的关键挑战，并探讨其在实时传感器融合、语言条件控制和自适应任务执行中的应用。

Result: 基础模型在家庭辅助、医疗和服务自动化等领域展示了变革性影响，但仍需解决预测性扩展法则、自主长期适应等问题。

Conclusion: 基础模型在移动服务机器人中具有巨大潜力，未来需关注预测性扩展、跨具身泛化等方向，以实现高效、稳健的部署。

Abstract: Rapid advancements in foundation models, including Large Language Models,
Vision-Language Models, Multimodal Large Language Models, and
Vision-Language-Action Models have opened new avenues for embodied AI in mobile
service robotics. By combining foundation models with the principles of
embodied AI, where intelligent systems perceive, reason, and act through
physical interactions, robots can improve understanding, adapt to, and execute
complex tasks in dynamic real-world environments. However, embodied AI in
mobile service robots continues to face key challenges, including multimodal
sensor fusion, real-time decision-making under uncertainty, task
generalization, and effective human-robot interactions (HRI). In this paper, we
present the first systematic review of the integration of foundation models in
mobile service robotics, identifying key open challenges in embodied AI and
examining how foundation models can address them. Namely, we explore the role
of such models in enabling real-time sensor fusion, language-conditioned
control, and adaptive task execution. Furthermore, we discuss real-world
applications in the domestic assistance, healthcare, and service automation
sectors, demonstrating the transformative impact of foundation models on
service robotics. We also include potential future research directions,
emphasizing the need for predictive scaling laws, autonomous long-term
adaptation, and cross-embodiment generalization to enable scalable, efficient,
and robust deployment of foundation models in human-centric robotic systems.

</details>


### [352] [Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners](https://arxiv.org/abs/2505.20573)
*Jiabao Ji,Yongchao Chen,Yang Zhang,Ramana Rao Kompella,Chuchu Fan,Gaowen Liu,Shiyu Chang*

Main category: cs.RO

TL;DR: 论文提出了一种结合强化学习与可验证奖励（RLVR）的框架，以提升小型语言模型在物理约束下的任务规划能力，实验证明其优于无约束的大模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在机器人控制任务中表现优异，但在实际应用中常因忽视物理约束而生成无效动作计划。

Method: 提出RLVR框架，通过强化学习引入可验证奖励，激励模型在规划时考虑物理约束。

Result: 实验表明，约束感知的小型LLMs在BoxNet和BoxNet3D任务中显著优于无约束的大模型。

Conclusion: 通过物理约束的引入，小型LLMs可实现高效、可扩展的多机器人控制。

Abstract: Large language models (LLMs) have demonstrated strong performance in various
robot control tasks. However, their deployment in real-world applications
remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently
produce invalid action plans that violate physical constraints, such as
directing a robot to an unreachable location or causing collisions between
robots. This issue primarily arises from a lack of awareness of these physical
constraints during the reasoning process. To address this issue, we propose a
novel framework that integrates reinforcement learning with verifiable rewards
(RLVR) to incentivize knowledge of physical constraints into LLMs to induce
constraints-aware reasoning during plan generation. In this approach, only
valid action plans that successfully complete a control task receive positive
rewards. We applied our method to two small-scale LLMs: a non-reasoning
Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results
demonstrate that constraint-aware small LLMs largely outperform large-scale
models without constraints, grounded on both the BoxNet task and a newly
developed BoxNet3D environment built using MuJoCo. This work highlights the
effectiveness of grounding even small LLMs with physical constraints to enable
scalable and efficient multi-robot control in complex, physically constrained
environments.

</details>


### [353] [Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform](https://arxiv.org/abs/2505.20751)
*Zongcai Tan amd Dandan Zhang*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的仿真平台Interactive OT Gym，用于光学镊子驱动的微机器人协同操作，显著提升了操作效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统多陷阱光学镊子在动态环境中协同操作复杂形状微机器人存在挑战，需要一种更高效的控制方法。

Method: 开发了结合物理场模拟、触觉反馈、强化学习和上下文感知共享控制策略的仿真平台。

Result: 实验表明，共享控制系统将任务完成时间减少67%，并实现100%的成功率。

Conclusion: Interactive OT Gym为开发先进的光学镊子驱动微操作系统提供了高效、低成本的训练和测试环境。

Abstract: Optical tweezers (OT) offer unparalleled capabilities for micromanipulation
with submicron precision in biomedical applications. However, controlling
conventional multi-trap OT to achieve cooperative manipulation of multiple
complex-shaped microrobots in dynamic environments poses a significant
challenge. To address this, we introduce Interactive OT Gym, a reinforcement
learning (RL)-based simulation platform designed for OT-driven microrobotics.
Our platform supports complex physical field simulations and integrates haptic
feedback interfaces, RL modules, and context-aware shared control strategies
tailored for OT-driven microrobot in cooperative biological object manipulation
tasks. This integration allows for an adaptive blend of manual and autonomous
control, enabling seamless transitions between human input and autonomous
operation. We evaluated the effectiveness of our platform using a cell
manipulation task. Experimental results show that our shared control system
significantly improves micromanipulation performance, reducing task completion
time by approximately 67% compared to using pure human or RL control alone and
achieving a 100% success rate. With its high fidelity, interactivity, low cost,
and high-speed simulation capabilities, Interactive OT Gym serves as a
user-friendly training and testing environment for the development of advanced
interactive OT-driven micromanipulation systems and control algorithms. For
more details on the project, please see our website
https://sites.google.com/view/otgym

</details>


### [354] [FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation](https://arxiv.org/abs/2505.20783)
*Jiaping Xiao,Cheng Wen Tsao,Yuhang Zhang,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本文提出了一种基于基础模型（LLM和VLM）的无人机路径规划方法（FM-Planner），并通过仿真和实际实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型（如LLM和VLM）在无人机全局路径规划中的实际应用和效果，填补现有研究的空白。

Method: 1. 系统评估了8种代表性的LLM和VLM方法；2. 设计了一个结合语义推理和视觉感知的LLM-Vision规划器；3. 通过实际实验验证了规划器的性能。

Result: 研究揭示了基础模型在无人机路径规划中的优势、局限性和可行性，并提供了实际应用的实现方案。

Conclusion: FM-Planner为无人机自主飞行提供了一种有效的路径规划解决方案，展示了基础模型在机器人领域的潜力。

Abstract: Path planning is a critical component in autonomous drone operations,
enabling safe and efficient navigation through complex environments. Recent
advances in foundation models, particularly large language models (LLMs) and
vision-language models (VLMs), have opened new opportunities for enhanced
perception and intelligent decision-making in robotics. However, their
practical applicability and effectiveness in global path planning remain
relatively unexplored. This paper proposes foundation model-guided path
planners (FM-Planner) and presents a comprehensive benchmarking study and
practical validation for drone path planning. Specifically, we first
systematically evaluate eight representative LLM and VLM approaches using
standardized simulation scenarios. To enable effective real-time navigation, we
then design an integrated LLM-Vision planner that combines semantic reasoning
with visual perception. Furthermore, we deploy and validate the proposed path
planner through real-world experiments under multiple configurations. Our
findings provide valuable insights into the strengths, limitations, and
feasibility of deploying foundation models in real-world drone applications and
providing practical implementations in autonomous flight. Project site:
https://github.com/NTU-ICG/FM-Planner.

</details>


### [355] [Spatial RoboGrasp: Generalized Robotic Grasping Control Policy](https://arxiv.org/abs/2505.20814)
*Yiqi Huang,Travis Davies,Jiahuan Yan,Jiankai Sun,Xiang Chen,Luhui Hu*

Main category: cs.RO

TL;DR: 提出一种结合多模态感知与抓取预测的统一框架，通过扩散策略提升机器人抓取任务的泛化性和精确性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作在多样化环境中因空间感知限制导致的泛化性和精确性问题。

Method: 融合领域随机增强、单目深度估计和深度感知6-DoF抓取提示，构建空间表示，结合扩散策略生成动作序列。

Result: 抓取成功率提升40%，任务成功率提升45%，在环境变化下表现更稳健。

Conclusion: 空间感知与扩散模仿学习结合为通用机器人抓取提供了可扩展且鲁棒的解决方案。

Abstract: Achieving generalizable and precise robotic manipulation across diverse
environments remains a critical challenge, largely due to limitations in
spatial perception. While prior imitation-learning approaches have made
progress, their reliance on raw RGB inputs and handcrafted features often leads
to overfitting and poor 3D reasoning under varied lighting, occlusion, and
object conditions. In this paper, we propose a unified framework that couples
robust multimodal perception with reliable grasp prediction. Our architecture
fuses domain-randomized augmentation, monocular depth estimation, and a
depth-aware 6-DoF Grasp Prompt into a single spatial representation for
downstream action planning. Conditioned on this encoding and a high-level task
prompt, our diffusion-based policy yields precise action sequences, achieving
up to 40% improvement in grasp success and 45% higher task success rates under
environmental variation. These results demonstrate that spatially grounded
perception, paired with diffusion-based imitation learning, offers a scalable
and robust solution for general-purpose robotic grasping.

</details>


### [356] [Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning](https://arxiv.org/abs/2505.20962)
*Nikos Giannakakis,Argyris Manetas,Panagiotis P. Filntisis,Petros Maragos,George Retsinas*

Main category: cs.RO

TL;DR: 论文提出了一种基于对象中心的编码器，将语义分割和视觉表示生成耦合处理，利用Slot Attention机制和预训练的SOLV模型，通过人类动作视频数据微调，提升机器人任务的强化学习和模仿学习效果。


<details>
  <summary>Details</summary>
Motivation: 受人类认知功能和心理学理论的启发，研究旨在通过对象为中心的方式处理场景，以提升机器人视觉运动策略生成的效果。

Method: 采用Slot Attention机制和预训练的SOLV模型，在人类动作视频数据上进行微调，耦合处理语义分割和视觉表示生成。

Result: 实验表明，视觉表示能有效增强强化和模仿学习训练，且利用域外预训练模型和人类动作数据微调显著提升性能。

Conclusion: 该方法减少了对标注或机器人专用数据集的依赖，并展示了利用现有视觉编码器加速训练和提升泛化能力的潜力。

Abstract: Learning visual representations from observing actions to benefit robot
visuo-motor policy generation is a promising direction that closely resembles
human cognitive function and perception. Motivated by this, and further
inspired by psychological theories suggesting that humans process scenes in an
object-based fashion, we propose an object-centric encoder that performs
semantic segmentation and visual representation generation in a coupled manner,
unlike other works, which treat these as separate processes. To achieve this,
we leverage the Slot Attention mechanism and use the SOLV model, pretrained in
large out-of-domain datasets, to bootstrap fine-tuning on human action video
data. Through simulated robotic tasks, we demonstrate that visual
representations can enhance reinforcement and imitation learning training,
highlighting the effectiveness of our integrated approach for semantic
segmentation and encoding. Furthermore, we show that exploiting models
pretrained on out-of-domain datasets can benefit this process, and that
fine-tuning on datasets depicting human actions -- although still out-of-domain
-- , can significantly improve performance due to close alignment with robotic
tasks. These findings show the capability to reduce reliance on annotated or
robot-specific action datasets and the potential to build on existing visual
encoders to accelerate training and improve generalizability.

</details>


### [357] [STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation](https://arxiv.org/abs/2505.20781)
*Hossein Goli,Michael Gimelfarb,Nathan Samuel de Lara,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: STITCH-OPE是一种基于扩散模型的生成框架，用于高维长时域问题的离策略评估（OPE），通过引导去噪过程生成目标策略的轨迹，显著降低了方差和误差。


<details>
  <summary>Details</summary>
Motivation: 在机器人或医疗等领域，直接与环境交互成本高或不安全，现有OPE方法在高维长时域问题中效果不佳，STITCH-OPE旨在解决这些问题。

Method: 利用预训练的扩散模型生成目标策略的轨迹，通过减去行为策略的分数防止过正则化，并通过拼接部分轨迹生成长时域轨迹。

Result: 在D4RL和OpenAI Gym基准测试中，STITCH-OPE在均方误差、相关性和遗憾指标上显著优于现有方法。

Conclusion: STITCH-OPE通过创新的技术改进，为高维长时域OPE提供了高效且理论保证的解决方案。

Abstract: Off-policy evaluation (OPE) estimates the performance of a target policy
using offline data collected from a behavior policy, and is crucial in domains
such as robotics or healthcare where direct interaction with the environment is
costly or unsafe. Existing OPE methods are ineffective for high-dimensional,
long-horizon problems, due to exponential blow-ups in variance from importance
weighting or compounding errors from learned dynamics models. To address these
challenges, we propose STITCH-OPE, a model-based generative framework that
leverages denoising diffusion for long-horizon OPE in high-dimensional state
and action spaces. Starting with a diffusion model pre-trained on the behavior
data, STITCH-OPE generates synthetic trajectories from the target policy by
guiding the denoising process using the score function of the target policy.
STITCH-OPE proposes two technical innovations that make it advantageous for
OPE: (1) prevents over-regularization by subtracting the score of the behavior
policy during guidance, and (2) generates long-horizon trajectories by
stitching partial trajectories together end-to-end. We provide a theoretical
guarantee that under mild assumptions, these modifications result in an
exponential reduction in variance versus long-horizon trajectory diffusion.
Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement
in mean squared error, correlation, and regret metrics compared to
state-of-the-art OPE methods.

</details>


### [358] [Hume: Introducing System-2 Thinking in Visual-Language-Action Model](https://arxiv.org/abs/2505.21432)
*Haoming Song,Delin Qu,Yuanqi Yao,Qizhi Chen,Qi Lv,Yiwen Tang,Modi Shi,Guanghui Ren,Maoqing Yao,Bin Zhao,Dong Wang,Xuelong Li*

Main category: cs.RO

TL;DR: Hume是一个双系统视觉-语言-动作模型，通过价值引导的System-2思维和级联动作去噪，探索机器人基础模型在物理世界中的类人思维能力。


<details>
  <summary>Details</summary>
Motivation: 探索慢思维（System-2）在机器人基础模型中的潜力，以提升复杂任务的处理能力。

Method: Hume结合System-2的价值引导思维（通过状态-动作值估计和重复采样选择动作）和System-1的轻量级反应式视觉运动策略（级联动作去噪）。

Result: Hume在多个仿真基准和实际机器人部署中优于现有最先进的视觉-语言-动作模型。

Conclusion: Hume展示了双系统思维在机器人控制中的有效性，为物理世界中的复杂任务提供了新思路。

Abstract: Humans practice slow thinking before performing actual actions when handling
complex tasks in the physical world. This thinking paradigm, recently, has
achieved remarkable advancement in boosting Large Language Models (LLMs) to
solve complex tasks in digital domains. However, the potential of slow thinking
remains largely unexplored for robotic foundation models interacting with the
physical world. In this work, we propose Hume: a dual-system
Vision-Language-Action (VLA) model with value-guided System-2 thinking and
cascaded action denoising, exploring human-like thinking capabilities of
Vision-Language-Action models for dexterous robot control. System 2 of Hume
implements value-Guided thinking by extending a Vision-Language-Action Model
backbone with a novel value-query head to estimate the state-action value of
predicted actions. The value-guided thinking is conducted by repeat sampling
multiple action candidates and selecting one according to state-action value.
System 1 of Hume is a lightweight reactive visuomotor policy that takes System
2 selected action and performs cascaded action denoising for dexterous robot
control. At deployment time, System 2 performs value-guided thinking at a low
frequency while System 1 asynchronously receives the System 2 selected action
candidate and predicts fluid actions in real time. We show that Hume
outperforms the existing state-of-the-art Vision-Language-Action models across
multiple simulation benchmark and real-robot deployments.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [359] [Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3](https://arxiv.org/abs/2505.20947)
*Lorenzo Monti,Tatiana Muraveva,Alessia Garofalo,Gisella Clementini,Maria Letizia Valentini*

Main category: astro-ph.SR

TL;DR: 论文提出了一种基于深度学习的统一框架，利用Gaia G波段光变曲线估计RR Lyrae星的金属丰度，适用于RRab和RRc两种脉动类型，表现出高预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于RR Lyrae星（RRLs）的金属丰度与光变曲线形态相关，且Gaia DR3提供了约27万颗RRLs的光变曲线，亟需可扩展的方法从光变数据估计其金属丰度。

Method: 采用基于门控循环单元（GRU）的神经网络，针对时间序列外回归优化，包括相位折叠、平滑和样本加权等预处理步骤，并以文献中的光变金属丰度作为训练目标。

Result: 在验证集上，模型表现优异：RRab星的MAE=0.0565 dex，RMSE=0.0765 dex，R^2=0.9401；RRc星的MAE=0.0505 dex，RMSE=0.0720 dex，R^2=0.9625。

Conclusion: 深度学习在大规模光变金属丰度估计中表现出高效性，适用于恒星种群和银河结构研究。

Abstract: RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity
tracers due to the correlation between their metal abundances and light curve
morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs,
there is a pressing need for scalable methods to estimate their metallicities
from photometric data. We introduce a unified deep learning framework that
estimates metallicities for both fundamental-mode (RRab) and first-overtone
(RRc) RRLs using Gaia G-band light curves. This approach extends our previous
work on RRab stars to include RRc stars, aiming for high predictive accuracy
and broad generalization across both pulsation types. The model is based on a
Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic
regression. Our pipeline includes preprocessing steps such as phase folding,
smoothing, and sample weighting, and uses photometric metallicities from the
literature as training targets. The architecture is designed to handle
morphological differences between RRab and RRc light curves without requiring
separate models. On held-out validation sets, our GRU model achieves strong
performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401;
for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results
show the effectiveness of deep learning for large-scale photometric metallicity
estimation and support its application to studies of stellar populations and
Galactic structure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [360] [Predictive Performance of Deep Quantum Data Re-uploading Models](https://arxiv.org/abs/2505.20337)
*Xin Wang,Han-Xiao Tao,Re-Bing Wu*

Main category: quant-ph

TL;DR: 量子数据重上传模型在深层编码时预测性能下降，建议采用更宽而非更深的电路架构。


<details>
  <summary>Details</summary>
Motivation: 研究量子机器学习中数据重上传模型的预测性能，发现其在高维数据下性能不足的问题。

Method: 理论分析和实验验证，包括合成线性可分数据集和真实数据集。

Result: 深层编码导致预测性能退化至接近随机猜测水平，数据重上传无法缓解。

Conclusion: 处理高维数据时，应采用更宽的量子电路架构而非更深的架构。

Abstract: Quantum machine learning models incorporating data re-uploading circuits have
garnered significant attention due to their exceptional expressivity and
trainability. However, their ability to generate accurate predictions on unseen
data, referred to as the predictive performance, remains insufficiently
investigated. This study reveals a fundamental limitation in predictive
performance when deep encoding layers are employed within the data re-uploading
model. Concretely, we theoretically demonstrate that when processing
high-dimensional data with limited-qubit data re-uploading models, their
predictive performance progressively degenerates to near random-guessing levels
as the number of encoding layers increases. In this context, the repeated data
uploading cannot mitigate the performance degradation. These findings are
validated through experiments on both synthetic linearly separable datasets and
real-world datasets. Our results demonstrate that when processing
high-dimensional data, the quantum data re-uploading models should be designed
with wider circuit architectures rather than deeper and narrower ones.

</details>


### [361] [Quantum AIXI: Universal Intelligence via Quantum Information](https://arxiv.org/abs/2505.21170)
*Elija Perrier*

Main category: quant-ph

TL;DR: 论文探讨了AIXI模型的量子力学扩展，提出了量子AIXI（QAIXI）框架，研究了量子环境下的智能体交互及其理论可行性。


<details>
  <summary>Details</summary>
Motivation: 由于宇宙本质上是量子力学的，而经典模拟量子系统存在指数级开销，因此研究量子力学版本的AIXI模型是否可行成为问题。

Method: 通过量子信息和经典寄存器与通道的交互模型，扩展AIXI框架，引入量子AIXI（QAIXI），并探讨量子Solomonoff归纳的条件和限制。

Result: 提出了QAIXI模型，展示了量子智能体如何执行经典和量子动作，并讨论了上下文性对QAIXI模型的影响。

Conclusion: QAIXI为量子环境下的通用智能提供了理论框架，但受限于量子Solomonoff归纳的条件和上下文性。

Abstract: AIXI is a widely studied model of artificial general intelligence (AGI) based
upon principles of induction and reinforcement learning. However, AIXI is
fundamentally classical in nature - as are the environments in which it is
modelled. Given the universe is quantum mechanical in nature and the
exponential overhead required to simulate quantum mechanical systems
classically, the question arises as to whether there are quantum mechanical
analogues of AIXI which are theoretically consistent or practically feasible as
models of universal intelligence. To address this question, we extend the
framework to quantum information and present Quantum AIXI (QAIXI). We introduce
a model of quantum agent/environment interaction based upon quantum and
classical registers and channels, showing how quantum AIXI agents may take both
classical and quantum actions. We formulate the key components of AIXI in
quantum information terms, extending previous research on quantum Kolmogorov
complexity and a QAIXI value function. We discuss conditions and limitations
upon quantum Solomonoff induction and show how contextuality fundamentally
affects QAIXI models.

</details>


### [362] [Leveraging Diffusion Models for Parameterized Quantum Circuit Generation](https://arxiv.org/abs/2505.20863)
*Daniel Barta,Darya Martyniuk,Johannes Jung,Adrian Paschke*

Main category: quant-ph

TL;DR: 本文提出了一种基于去噪扩散模型（DMs）的生成方法，用于合成参数化量子电路（PQCs），展示了其在生成高保真GHZ态和量子机器学习任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 量子计算的实用化需要优化量子电路设计，本文旨在通过生成模型加速和优化PQCs的设计。

Method: 扩展了Fürrutter等人的扩散模型，提出了一种能同时生成电路架构和连续门参数的生成方法。

Result: 实验表明该方法在生成高保真GHZ态和QML分类任务中表现优异，且具有跨门集和量子比特数的强泛化能力。

Conclusion: 扩散模型为PQCs设计提供了高效且通用的工具，有助于推动量子计算的实际应用。

Abstract: Quantum computing holds immense potential, yet its practical success depends
on multiple factors, including advances in quantum circuit design. In this
paper, we introduce a generative approach based on denoising diffusion models
(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent
diffusion model pipeline of F\"urrutter et al. [1], our model effectively
conditions the synthesis process, enabling the simultaneous generation of
circuit architectures and their continuous gate parameters. We demonstrate our
approach in synthesizing PQCs optimized for generating high-fidelity
Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum
machine learning (QML) classification tasks. Our results indicate a strong
generalization across varying gate sets and scaling qubit counts, highlighting
the versatility and computational efficiency of diffusion-based methods. This
work illustrates the potential of generative models as a powerful tool for
accelerating and optimizing the design of PQCs, supporting the development of
more practical and scalable quantum applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [363] [ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction](https://arxiv.org/abs/2505.20431)
*Qimin Chen,Yuezhi Yang,Yifang Wang,Vladimir G. Kim,Siddhartha Chaudhuri,Hao Zhang,Zhiqin Chen*

Main category: cs.GR

TL;DR: 3D detailizer模型通过文本提示快速将粗糙3D形状转化为高质量资产，支持用户交互编辑，生成多样化且风格一致的细节。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成模型难以实现快速交互式编辑和多样化结构控制，且生成质量有限。

Method: 基于预训练多视角图像扩散模型，通过Score Distillation Sampling (SDS)蒸馏知识，分两阶段训练以处理复杂结构。

Result: 生成质量优于现有文本到3D模型，支持秒级交互编辑，并能生成超出分布范围的创意资产。

Conclusion: 3D detailizer为交互式3D建模提供了高效工具，具有广泛的结构和风格泛化能力。

Abstract: We introduce a 3D detailizer, a neural model which can instantaneously (in
<1s) transform a coarse 3D shape proxy into a high-quality asset with detailed
geometry and texture as guided by an input text prompt. Our model is trained
using the text prompt, which defines the shape class and characterizes the
appearance and fine-grained style of the generated details. The coarse 3D
proxy, which can be easily varied and adjusted (e.g., via user editing),
provides structure control over the final shape. Importantly, our detailizer is
not optimized for a single shape; it is the result of distilling a generative
model, so that it can be reused, without retraining, to generate any number of
shapes, with varied structures, whose local details all share a consistent
style and appearance. Our detailizer training utilizes a pretrained multi-view
image diffusion model, with text conditioning, to distill the foundational
knowledge therein into our detailizer via Score Distillation Sampling (SDS). To
improve SDS and enable our detailizer architecture to learn generalizable
features over complex structures, we train our model in two training stages to
generate shapes with increasing structural complexity. Through extensive
experiments, we show that our method generates shapes of superior quality and
details compared to existing text-to-3D models under varied structure control.
Our detailizer can refine a coarse shape in less than a second, making it
possible to interactively author and adjust 3D shapes. Furthermore, the
user-imposed structure control can lead to creative, and hence
out-of-distribution, 3D asset generations that are beyond the current
capabilities of leading text-to-3D generative models. We demonstrate an
interactive 3D modeling workflow our method enables, and its strong
generalizability over styles, structures, and object categories.

</details>


### [364] [Stochastic Preconditioning for Neural Field Optimization](https://arxiv.org/abs/2505.20473)
*Selena Ling,Merlin Nimier-David,Alec Jacobson,Nicholas Sharp*

Main category: cs.GR

TL;DR: 通过引入空间随机性训练神经场，显著提升拟合效果，替代或超越传统层次结构和频率空间构建方法。


<details>
  <summary>Details</summary>
Motivation: 神经场在视觉计算中表现优异，但传统训练方法依赖复杂的层次结构或频率空间构建，效率较低。

Method: 提出一种基于高斯分布偏移采样的模糊场查询方法，优化训练过程，提升收敛性和鲁棒性。

Result: 实验表明，该方法在多种神经场表示和任务中均表现优异，性能接近或超越定制层次结构。

Conclusion: 随机预条件方法简单高效，为神经场训练提供了统一的解决方案，显著提升质量和鲁棒性。

Abstract: Neural fields are a highly effective representation across visual computing.
This work observes that fitting these fields is greatly improved by
incorporating spatial stochasticity during training, and that this simple
technique can replace or even outperform custom-designed hierarchies and
frequency space constructions. The approach is formalized as implicitly
operating on a blurred version of the field, evaluated in-expectation by
sampling with Gaussian-distributed offsets. Querying the blurred field during
optimization greatly improves convergence and robustness, akin to the role of
preconditioners in numerical linear algebra. This implicit, sampling-based
perspective fits naturally into the neural field paradigm, comes at no
additional cost, and is extremely simple to implement. We describe the basic
theory of this technique, including details such as handling boundary
conditions, and extending to a spatially-varying blur. Experiments demonstrate
this approach on representations including coordinate MLPs, neural hashgrids,
triplanes, and more, across tasks including surface reconstruction and radiance
fields. In settings where custom-designed hierarchies have already been
developed, stochastic preconditioning nearly matches or improves their
performance with a simple and unified approach; in settings without existing
hierarchies it provides an immediate boost to quality and robustness.

</details>


### [365] [CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians](https://arxiv.org/abs/2505.21041)
*Weihang Liu,Yuhui Zhong,Yuke Li,Xi Chen,Jiadi Cui,Honglong Zhang,Lan Xu,Xin Lou,Yujiao Shi,Jingyi Yu,Yingliang Zhang*

Main category: cs.GR

TL;DR: CityGo是一种混合框架，结合纹理代理几何与残差和周围3D高斯，用于轻量级、逼真的城市场景渲染。


<details>
  <summary>Details</summary>
Motivation: 大规模城市场景的精确高效建模对AR导航、无人机巡检和智慧城市数字孪生至关重要，但现有方法存在遮挡、几何不完整和高内存需求等问题。

Method: CityGo首先从MVS点云提取紧凑的建筑代理网格，使用零阶SH高斯生成无遮挡纹理，并通过残差高斯捕捉高频细节，周围高斯表示城市背景，采用重要性感知下采样减少冗余。

Result: 实验表明，CityGo显著减少训练时间（平均1.4倍加速），在移动GPU上实现实时渲染，且内存和能耗大幅降低。

Conclusion: CityGo通过混合表示和优化策略，实现了高效、轻量的城市场景建模与渲染。

Abstract: Accurate and efficient modeling of large-scale urban scenes is critical for
applications such as AR navigation, UAV based inspection, and smart city
digital twins. While aerial imagery offers broad coverage and complements
limitations of ground-based data, reconstructing city-scale environments from
such views remains challenging due to occlusions, incomplete geometry, and high
memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve
scalability and visual quality but remain limited by dense primitive usage,
long training times, and poor suit ability for edge devices. We propose CityGo,
a hybrid framework that combines textured proxy geometry with residual and
surrounding 3D Gaussians for lightweight, photorealistic rendering of urban
scenes from aerial perspectives. Our approach first extracts compact building
proxy meshes from MVS point clouds, then uses zero order SH Gaussians to
generate occlusion-free textures via image-based rendering and back-projection.
To capture high-frequency details, we introduce residual Gaussians placed based
on proxy-photo discrepancies and guided by depth priors. Broader urban context
is represented by surrounding Gaussians, with importance-aware downsampling
applied to non-critical regions to reduce redundancy. A tailored optimization
strategy jointly refines proxy textures and Gaussian parameters, enabling
real-time rendering of complex urban scenes on mobile GPUs with significantly
reduced training and memory requirements. Extensive experiments on real-world
aerial datasets demonstrate that our hybrid representation significantly
reduces training time, achieving on average 1.4x speedup, while delivering
comparable visual fidelity to pure 3D Gaussian Splatting approaches.
Furthermore, CityGo enables real-time rendering of large-scale urban scenes on
mobile consumer GPUs, with substantially reduced memory usage and energy
consumption.

</details>


### [366] [IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model](https://arxiv.org/abs/2505.21146)
*Yang Zhao,Yan Zhang,Xubo Yang*

Main category: cs.GR

TL;DR: IKMo是一种基于扩散模型的运动生成方法，通过解耦轨迹和姿态输入，采用两阶段条件框架生成高质量运动。实验证明其在轨迹关键帧约束下优于现有方法，并通过MLLM代理提升用户期望匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有方法对轨迹和姿态进行全局处理导致输出不理想，IKMo旨在通过解耦和优化输入提升运动生成的保真度和可控性。

Method: 采用两阶段条件框架：第一阶段优化输入，第二阶段通过并行编码器处理轨迹和姿态，最终由ControlNet融合生成运动。

Result: 在HumanML3D和KIT-ML数据集上，IKMo在各项指标上均优于现有方法，用户研究显示MLLM代理显著提升用户满意度。

Conclusion: IKMo通过解耦和优化输入，结合MLLM代理，显著提升了运动生成的保真度和用户可控性。

Abstract: Existing human motion generation methods with trajectory and pose inputs
operate global processing on both modalities, leading to suboptimal outputs. In
this paper, we propose IKMo, an image-keyframed motion generation method based
on the diffusion model with trajectory and pose being decoupled. The trajectory
and pose inputs go through a two-stage conditioning framework. In the first
stage, the dedicated optimization module is applied to refine inputs. In the
second stage, trajectory and pose are encoded via a Trajectory Encoder and a
Pose Encoder in parallel. Then, motion with high spatial and semantic fidelity
is guided by a motion ControlNet, which processes the fused trajectory and pose
data. Experiment results based on HumanML3D and KIT-ML datasets demonstrate
that the proposed method outperforms state-of-the-art on all metrics under
trajectory-keyframe constraints. In addition, MLLM-based agents are implemented
to pre-process model inputs. Given texts and keyframe images from users, the
agents extract motion descriptions, keyframe poses, and trajectories as the
optimized inputs into the motion generation model. We conducts a user study
with 10 participants. The experiment results prove that the MLLM-based agents
pre-processing makes generated motion more in line with users' expectation. We
believe that the proposed method improves both the fidelity and controllability
of motion generation by the diffusion model.

</details>


### [367] [efunc: An Efficient Function Representation without Neural Networks](https://arxiv.org/abs/2505.21319)
*Biao Zhang,Peter Wonka*

Main category: cs.GR

TL;DR: 提出了一种参数高效、不依赖神经网络的高质量函数逼近方法，基于多项式插值和径向基函数，显著减少计算时间和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经网络方法参数过多、实用性受限的问题，追求高效且高质量的函数逼近。

Method: 提出连续函数建模框架，采用多项式插值和径向基函数的紧凑表示，开发内存优化的CUDA算法。

Result: 计算时间和内存消耗减少90%以上，性能优于或媲美现有技术（如八叉树/哈希网格）。

Conclusion: 该方法在3D SDFs上验证了高效性和优越性，为函数逼近提供了更实用的解决方案。

Abstract: Function fitting/approximation plays a fundamental role in computer graphics
and other engineering applications. While recent advances have explored neural
networks to address this task, these methods often rely on architectures with
many parameters, limiting their practical applicability. In contrast, we pursue
high-quality function approximation using parameter-efficient representations
that eliminate the dependency on neural networks entirely. We first propose a
novel framework for continuous function modeling. Most existing works can be
formulated using this framework. We then introduce a compact function
representation, which is based on polynomials interpolated using radial basis
functions, bypassing both neural networks and complex/hierarchical data
structures. We also develop memory-efficient CUDA-optimized algorithms that
reduce computational time and memory consumption to less than 10% compared to
conventional automatic differentiation frameworks. Finally, we validate our
representation and optimization pipeline through extensive experiments on 3D
signed distance functions (SDFs). The proposed representation achieves
comparable or superior performance to state-of-the-art techniques (e.g.,
octree/hash-grid techniques) with significantly fewer parameters.

</details>


### [368] [Structure from Collision](https://arxiv.org/abs/2505.21335)
*Takuhiro Kaneko*

Main category: cs.GR

TL;DR: 论文提出了一种名为SfC-NeRF的新模型，通过碰撞过程中的外观变化估计物体的内部结构，解决了现有方法无法捕捉不可见内部结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经3D表示方法（如NeRF和3DGS）只能估计可见的外部结构，而无法捕捉不可见的内部结构。本文旨在通过碰撞过程中的外观变化解决这一问题。

Method: 提出SfC-NeRF模型，通过物理约束、外观保持约束和关键帧约束优化视频序列中的内部结构。为避免陷入局部最优，提出体积退火策略。

Result: 在115个具有不同结构和材料属性的物体上进行了实验，验证了SfC-NeRF的有效性。

Conclusion: SfC-NeRF能够有效估计物体的内部结构，为3D表示领域提供了新的解决方案。

Abstract: Recent advancements in neural 3D representations, such as neural radiance
fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate
estimation of 3D structures from multiview images. However, this capability is
limited to estimating the visible external structure, and identifying the
invisible internal structure hidden behind the surface is difficult. To
overcome this limitation, we address a new task called Structure from Collision
(SfC), which aims to estimate the structure (including the invisible internal
structure) of an object from appearance changes during collision. To solve this
problem, we propose a novel model called SfC-NeRF that optimizes the invisible
internal structure of an object through a video sequence under physical,
appearance (i.e., visible external structure)-preserving, and keyframe
constraints. In particular, to avoid falling into undesirable local optima
owing to its ill-posed nature, we propose volume annealing; that is, searching
for global optima by repeatedly reducing and expanding the volume. Extensive
experiments on 115 objects involving diverse structures (i.e., various cavity
shapes, locations, and sizes) and material properties revealed the properties
of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.

</details>


### [369] [CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects](https://arxiv.org/abs/2505.21437)
*Huaijin Pi,Zhi Cen,Zhiyang Dou,Taku Komura*

Main category: cs.GR

TL;DR: 提出了一种协调扩散噪声优化框架，用于合成全身操纵关节物体的运动，包括身体、手和物体的运动。通过噪声空间优化和统一表示，实现了高保真度和精确性。


<details>
  <summary>Details</summary>
Motivation: 全身操纵关节物体在虚拟人类和机器人领域有广泛应用，但面临手与身体协调和高自由度精确控制的挑战。

Method: 采用三个专用扩散模型分别优化身体、左手和右手的运动，通过梯度流实现协调，并使用基础点集（BPS）统一表示手与物体的空间关系。

Result: 实验表明，该方法在运动质量和物理合理性上优于现有方法，支持多种功能如物体姿态控制和行走中操纵。

Conclusion: 该方法成功解决了全身操纵的协调和精确性问题，具有广泛的应用潜力。

Abstract: Synthesizing whole-body manipulation of articulated objects, including body
motion, hand motion, and object motion, is a critical yet challenging task with
broad applications in virtual humans and robotics. The core challenges are
twofold. First, achieving realistic whole-body motion requires tight
coordination between the hands and the rest of the body, as their movements are
interdependent during manipulation. Second, articulated object manipulation
typically involves high degrees of freedom and demands higher precision, often
requiring the fingers to be placed at specific regions to actuate movable
parts. To address these challenges, we propose a novel coordinated diffusion
noise optimization framework. Specifically, we perform noise-space optimization
over three specialized diffusion models for the body, left hand, and right
hand, each trained on its own motion dataset to improve generalization.
Coordination naturally emerges through gradient flow along the human kinematic
chain, allowing the global body posture to adapt in response to hand motion
objectives with high fidelity. To further enhance precision in hand-object
interaction, we adopt a unified representation based on basis point sets (BPS),
where end-effector positions are encoded as distances to the same BPS used for
object geometry. This unified representation captures fine-grained spatial
relationships between the hand and articulated object parts, and the resulting
trajectories serve as targets to guide the optimization of diffusion noise,
producing highly accurate interaction motion. We conduct extensive experiments
demonstrating that our method outperforms existing approaches in motion quality
and physical plausibility, and enables various capabilities such as object pose
control, simultaneous walking and manipulation, and whole-body generation from
hand-only data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [370] [Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software](https://arxiv.org/abs/2505.20303)
*David Hanson*

Main category: cs.SE

TL;DR: 研究探讨AI生成代码的透明度与安全性，分析市场机会与挑战，提出解决方案，并展望其对通用人工智能及人机交互的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件开发中的广泛应用，AI生成代码的普及与复杂性增加，亟需确保其透明性与安全性。

Method: 通过分析当前现状、识别潜在风险、探讨未来影响，提出增强透明性与功能分析的解决方案。

Result: 研究发现AI生成代码的市场机会与挑战并存，需采取主动措施确保其负责任的发展与部署。

Conclusion: 强调在软件工程中负责任地开发与部署AI的重要性，并呼吁采取前瞻性措施。

Abstract: As artificial intelligence becomes increasingly integrated into software
development processes, the prevalence and sophistication of AI-generated code
continue to expand rapidly. This study addresses the critical need for
transparency and safety in AI generated code by examining the current
landscape, identifying potential risks, and exploring future implications. We
analyze market opportunities for detecting AI-generated code, discuss the
challenges associated with managing increasing complexity, and propose
solutions to enhance transparency and functionality analysis. Furthermore, this
study investigates the longterm implications of AI generated code, including
its potential role in the development of artificial general intelligence and
its impact on human AI interaction. In conclusion, we emphasize the importance
of proactive measures for ensuring the responsible development and deployment
of AI in software engineering.

</details>


### [371] [Evaluating the Energy-Efficiency of the Code Generated by LLMs](https://arxiv.org/abs/2505.20324)
*Md Arman Islam,Devi Varaprasad Jonnala,Ritika Rekhi,Pratik Pokharel,Siddharth Cilamkoti,Asif Imran,Tevfik Kosar,Bekir Turkkan*

Main category: cs.SE

TL;DR: 论文研究了20种流行LLM生成的代码在878个编程问题上的能源效率，发现其性能与能源效率通常低于人类编写的代码，其中DeepSeek-v3和GPT-4o表现最佳，而Grok-2和Gemini-1.5-Pro表现最差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码质量的提升，其在软件行业中的应用日益广泛，但研究多关注功能正确性，而忽略了能源效率和环境影响。

Method: 通过比较LLM生成的代码与人类编写的规范解决方案，评估其在LeetCode平台上878个不同难度和算法类别问题中的能源效率。

Result: LLM生成的代码在能源效率上普遍低于人类代码，DeepSeek-v3和GPT-4o表现较好，而Grok-2和Gemini-1.5-Pro表现较差。动态规划等算法类别中，LLM代码能耗可达人类代码的450倍。

Conclusion: 尽管LLM能生成功能正确的代码，但其能源效率仍需改进，尤其是在特定算法类别中。未来研究应更多关注代码的环境影响。

Abstract: As the quality of code generated by Large Language Models (LLMs) improves,
their adoption in the software industry for automated code generation continues
to grow. Researchers primarily focus on enhancing the functional correctness of
the generated code while commonly overlooking its energy efficiency and
environmental impact. This paper investigates the energy efficiency of the code
generated by 20 popular LLMs for 878 programming problems of varying difficulty
levels and diverse algorithmic categories selected from the LeetCode platform
by comparing them against canonical human-written solutions. Although LLMs can
produce functionally correct results in most cases, our findings show that the
performance and energy efficiency of LLM-produced solutions are often far below
those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and
GPT-4o generate the most energy-efficient code, whereas Grok-2 and
Gemini-1.5-Pro are among the least energy-efficient models. On average,
human-generated canonical solutions are approximately 1.17 times more energy
efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and
over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific
algorithmic groups such as dynamic programming, backtracking, and bit
manipulation, LLM-generated code can consume up to 450 times more energy than
human-generated canonical solutions.

</details>


### [372] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TL;DR: CRUST-Bench是一个用于评估C到Rust转译的数据集，包含100个C代码库及其对应的安全Rust接口和测试用例，旨在提升转译系统的能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法评估C到安全Rust的转译效果，CRUST-Bench填补了这一空白，帮助改进转译系统。

Method: 提供100个C代码库及其手动编写的安全Rust接口和测试用例，覆盖多文件依赖的复杂场景。

Result: 现有LLMs在单次尝试中仅能解决15个任务，显示安全且符合习惯的Rust生成仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基准，并揭示了LLMs在C到Rust转译中的常见错误。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


### [373] [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
*Xin Zhou,Kisub Kim,Ting Zhang,Martin Weyssow,Luis F. Gomes,Guang Yang,David Lo*

Main category: cs.SE

TL;DR: SWE-Judge是一种新的评估指标，用于准确评估LLM生成的软件工件的正确性，通过集成多个独立评估策略，显著优于现有自动指标。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估方法在评估LLM生成的软件工件正确性时准确性不足，而人工评估虽准确但缺乏可扩展性。

Method: SWE-Judge定义了五种评估策略，通过动态团队选择机制集成最优子集，生成最终正确性评分。

Result: 实验表明，SWE-Judge与人工评估的相关性显著提高（5.9%至183.8%），并在代码生成和程序修复任务中达到与人工标注者相当的一致性。

Conclusion: SWE-Judge是一种可扩展且可靠的替代人工评估的方法。

Abstract: Large Language Models (LLMs) and other automated techniques have been
increasingly used to support software developers by generating software
artifacts such as code snippets, patches, and comments. However, accurately
assessing the correctness of these generated artifacts remains a significant
challenge. On one hand, human evaluation provides high accuracy but is
labor-intensive and lacks scalability. On the other hand, other existing
automatic evaluation metrics are scalable and require minimal human effort, but
they often fail to accurately reflect the actual correctness of generated
software artifacts.
  In this paper, we present SWE-Judge, the first evaluation metric for
LLM-as-Ensemble-Judge specifically designed to accurately assess the
correctness of generated software artifacts. SWE-Judge first defines five
distinct evaluation strategies, each implemented as an independent judge. A
dynamic team selection mechanism then identifies the most appropriate subset of
judges to produce a final correctness score through ensembling. We evaluate
SWE-Judge across a diverse set of software engineering (SE) benchmarks,
including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.
These benchmarks span three SE tasks: code generation, automated program
repair, and code summarization. Experimental results demonstrate that SWE-Judge
consistently achieves a higher correlation with human judgments, with
improvements ranging from 5.9% to 183.8% over existing automatic metrics.
Furthermore, SWE-Judge reaches agreement levels with human annotators that are
comparable to inter-annotator agreement in code generation and program repair
tasks. These findings underscore SWE-Judge's potential as a scalable and
reliable alternative to human evaluation.

</details>


### [374] [Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement](https://arxiv.org/abs/2505.20973)
*Keheliya Gallaba,Ali Arabat,Dayi Lin,Mohammed Sayagh,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文提出了一种名为AlignMind的多智能体系统，通过增强基础模型（FMs）的Theory-of-Mind能力，以更准确地捕捉软件开发中的利益相关者需求。


<details>
  <summary>Details</summary>
Motivation: 基础模型在自然语言任务中表现优异，但在捕捉利益相关者需求方面仍存在挑战，影响其在软件开发中的应用。

Method: 采用基于FM的多智能体系统AlignMind，结合Theory-of-Mind能力，迭代澄清利益相关者的信念、欲望和意图，生成细化需求和可执行的自然语言工作流。

Result: 通过150个多样化用例的评估，证明该方法能准确捕捉利益相关者的意图和需求，并将其转化为规范和行动计划。

Conclusion: 该方法为软件开发过程带来显著改进，为未来构建以意图为核心的开发环境奠定了基础。

Abstract: Foundation Models (FMs) have shown remarkable capabilities in various natural
language tasks. However, their ability to accurately capture stakeholder
requirements remains a significant challenge for using FMs for software
development. This paper introduces a novel approach that leverages an
FM-powered multi-agent system called AlignMind to address this issue. By having
a cognitive architecture that enhances FMs with Theory-of-Mind capabilities,
our approach considers the mental states and perspectives of software makers.
This allows our solution to iteratively clarify the beliefs, desires, and
intentions of stakeholders, translating these into a set of refined
requirements and a corresponding actionable natural language workflow in the
often-overlooked requirements refinement phase of software engineering, which
is crucial after initial elicitation. Through a multifaceted evaluation
covering 150 diverse use cases, we demonstrate that our approach can accurately
capture the intents and requirements of stakeholders, articulating them as both
specifications and a step-by-step plan of action. Our findings suggest that the
potential for significant improvements in the software development process
justifies these investments. Our work lays the groundwork for future innovation
in building intent-first development environments, where software makers can
seamlessly collaborate with AIs to create software that truly meets their
needs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [375] [Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games](https://arxiv.org/abs/2505.21087)
*Marta Grobelna,Jan Křetínský,Maximilian Weininger*

Main category: cs.LO

TL;DR: 论文提出了一种改进的区间值迭代方法，用于解决双人零和并发随机博弈问题，确保近似解的精度。


<details>
  <summary>Details</summary>
Motivation: 传统值迭代方法在终止时缺乏对近似解精度的保证，因此需要一种能提供精度保证的改进方法。

Method: 提出了一种区间值迭代方法，通过结合上近似和下近似序列，并在两者接近时终止迭代。

Result: 该方法能够为并发随机博弈提供精度有保证的近似解。

Conclusion: 区间值迭代方法在理论和实践中均优于传统值迭代，解决了精度保证问题。

Abstract: We consider two-player zero-sum concurrent stochastic games (CSGs) played on
graphs with reachability and safety objectives. These include degenerate
classes such as Markov decision processes or turn-based stochastic games, which
can be solved by linear or quadratic programming; however, in practice, value
iteration (VI) outperforms the other approaches and is the most implemented
method. Similarly, for CSGs, this practical performance makes VI an attractive
alternative to the standard theoretical solution via the existential theory of
reals.
  VI starts with an under-approximation of the sought values for each state and
iteratively updates them, traditionally terminating once two consecutive
approximations are $\epsilon$-close. However, this stopping criterion lacks
guarantees on the precision of the approximation, which is the goal of this
work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard
VI with a converging sequence of over-approximations and terminates once the
over- and under-approximations are $\epsilon$-close.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [376] [FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation](https://arxiv.org/abs/2505.20349)
*Haixin Wang,Ruoyan Li,Fred Xu,Fang Sun,Kaiqiao Han,Zijie Huang,Guancheng Wan,Ching Chang,Xiao Luo,Wei Wang,Yizhou Sun*

Main category: physics.flu-dyn

TL;DR: FD-Bench是一个公平、模块化、全面且可复现的基准测试，用于数据驱动的流体模拟，解决了现有评估方法的碎片化和不公平问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的流体动力学建模缺乏统一的PDE数据集和标准化评估协议，导致公平评估困难。

Method: FD-Bench通过模块化设计，系统评估了85个基线模型，涵盖10种代表性流动场景，并提供了与数值求解器的直接比较框架。

Result: FD-Bench建立了迄今为止最全面的排行榜，解决了可复现性和可比性问题。

Conclusion: FD-Bench为未来数据驱动流体模型的稳健评估奠定了基础，并提供了开源代码支持。

Abstract: Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE
solvers, yet a fair and strong benchmark remains fragmented due to the absence
of unified PDE datasets and standardized evaluation protocols. Although
architectural innovations are abundant, fair assessment is further impeded by
the lack of clear disentanglement between spatial, temporal and loss modules.
In this paper, we introduce FD-Bench, the first fair, modular, comprehensive
and reproducible benchmark for data-driven fluid simulation. FD-Bench
systematically evaluates 85 baseline models across 10 representative flow
scenarios under a unified experimental setup. It provides four key
contributions: (1) a modular design enabling fair comparisons across spatial,
temporal, and loss function modules; (2) the first systematic framework for
direct comparison with traditional numerical solvers; (3) fine-grained
generalization analysis across resolutions, initial conditions, and temporal
windows; and (4) a user-friendly, extensible codebase to support future
research. Through rigorous empirical studies, FD-Bench establishes the most
comprehensive leaderboard to date, resolving long-standing issues in
reproducibility and comparability, and laying a foundation for robust
evaluation of future data-driven fluid models. The code is open-sourced at
https://anonymous.4open.science/r/FD-Bench-15BC.

</details>


### [377] [Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks](https://arxiv.org/abs/2505.20361)
*Chuanxing Wang,Hui Luo,Kai Wang,Guohuai Zhu,Mingxing Luo*

Main category: physics.flu-dyn

TL;DR: ST-PINNs通过分步解决间断问题并结合迁移学习，显著提升了PINNs在多间断流体动力学问题中的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在科学计算中取得了显著进展，但在处理多间断流体动力学问题时仍面临挑战。

Method: 提出ST-PINNs方法，依次从强到弱解决间断问题，并在训练中利用迁移学习。

Result: 数值实验表明，ST-PINNs能更准确地捕捉间断并显著减少误差。

Conclusion: ST-PINNs为复杂间断问题提供了新的解决方案，拓展了PINNs的应用范围。

Abstract: Despite the remarkable progress of physics-informed neural networks (PINNs)
in scientific computing, they continue to face challenges when solving
hydrodynamic problems with multiple discontinuities. In this work, we propose
Separation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such
problems. By sequentially resolving discontinuities from strong to weak and
leveraging transfer learning during training, ST-PINNs significantly reduce the
problem complexity and enhance solution accuracy. To the best of our knowledge,
this is the first study to apply a PINNs-based approach to the two-dimensional
unsteady planar shock refraction problem, offering new insights into the
application of PINNs to complex shock-interface interactions. Numerical
experiments demonstrate that ST-PINNs more accurately capture sharp
discontinuities and substantially reduce solution errors in hydrodynamic
problems involving multiple discontinuities.

</details>


### [378] [A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data](https://arxiv.org/abs/2505.21421)
*Rami Cassia,Rich Kerswell*

Main category: physics.flu-dyn

TL;DR: GraphGPS框架用于从稀疏观测中重建3D Riemann问题的可压缩流动，结合位置编码、局部消息传递和全局上下文感知，改进消息传递以捕捉激波和不连续性，并优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决可压缩流体流动中激波、不连续性等特征的稀疏观测重建问题，探索物理信息机器学习方法的应用。

Method: 采用GraphGPS框架，结合位置编码、局部消息传递和全局上下文感知，改进消息传递以识别激波和不连续性，并优化信息流动方向以提高效率。

Result: GraphGPS在重建精度上优于多种机器学习基准，同时实现了计算节省和更好的训练收敛性。

Conclusion: GraphGPS为稀疏观测下的可压缩流动重建提供了一种高效且精确的物理信息机器学习解决方案。

Abstract: In compressible fluid flow, reconstructing shocks, discontinuities,
rarefactions, and their interactions from sparse measurements is an important
inverse problem with practical applications. Moreover, physics-informed machine
learning has recently become an increasingly popular approach for performing
reconstructions tasks. In this work we explore a machine learning recipe, known
as GraphGPS, for reconstructing canonical compressible flows known as 3D
Riemann problems from sparse observations, in a physics-informed manner. The
GraphGPS framework combines the benefits of positional encodings, local
message-passing of graphs, and global contextual awareness, and we explore the
latter two components through an ablation study. Furthermore, we modify the
aggregation step of message-passing such that it is aware of shocks and
discontinuities, resulting in sharper reconstructions of these features.
Additionally, we modify message-passing such that information flows strictly
from known nodes only, which results in computational savings, better training
convergence, and no degradation of reconstruction accuracy. We also show that
the GraphGPS framework outperforms numerous machine learning benchmarks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [379] [Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares](https://arxiv.org/abs/2505.20327)
*Aurora Poggi,Giuseppe Alessio D'Inverno,Hjalmar Brismar,Ozan Öktem,Matthieu Barreau,Kateryna Morozovska*

Main category: q-bio.QM

TL;DR: 论文比较了CRLSM和PINN在生物系统动力学建模中的表现，发现CRLSM在参数估计和数据拟合上优于PINN，但PINN仍有改进潜力。


<details>
  <summary>Details</summary>
Motivation: 探索数据驱动方法在生物系统动力学建模中的应用，以克服传统方法的局限性，如SINDy需要先验知识的问题。

Method: 比较了CRLSM和PINN在ODE系统识别和参数发现中的性能，并分析了它们在钙信号传递建模中的表现。

Result: CRLSM在参数估计和数据拟合上表现良好，而PINN在当前配置下未能达到预期效果，但未来可能通过调优改进。

Conclusion: CRLSM是目前更优的选择，但PINN在进一步优化后可能展现出更好的潜力。

Abstract: Data-driven discovery of dynamics in biological systems allows for better
observation and characterization of processes, such as calcium signaling in
cell culture. Recent advancements in techniques allow the exploration of
previously unattainable insights of dynamical systems, such as the Sparse
Identification of Non-Linear Dynamics (SINDy), overcoming the limitations of
more classic methodologies. The latter requires some prior knowledge of an
effective library of candidate terms, which is not realistic for a real case
study. Using inspiration from fields like traffic density estimation and
control theory, we propose a methodology for characterization and performance
analysis of calcium delivery in a family of cells. In this work, we compare the
performance of the Constrained Regularized Least-Squares Method (CRLSM) and
Physics-Informed Neural Networks (PINN) for system identification and parameter
discovery for governing ordinary differential equations (ODEs). The CRLSM
achieves a fairly good parameter estimate and a good data fit when using the
learned parameters in the Consensus problem. On the other hand, despite the
initial hypothesis, PINNs fail to match the CRLSM performance and, under the
current configuration, do not provide fair parameter estimation. However, we
have only studied a limited number of PINN architectures, and it is expected
that additional hyperparameter tuning, as well as uncertainty quantification,
could significantly improve the performance in future works.

</details>


### [380] [Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering](https://arxiv.org/abs/2505.20301)
*Chen Liu,Mingchen Li,Yang Tan,Wenrui Gou,Guisheng Fan,Bingxin Zhou*

Main category: q-bio.QM

TL;DR: ProtAttBA是一个基于深度学习的模型，仅利用抗体-抗原复合物的序列信息预测结合亲和力变化，性能优越且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验评估抗体突变耗时耗力，现有深度学习模型依赖高质量复合结构，而实际中这些结构常不可得。

Method: ProtAttBA通过预训练学习蛋白质序列模式，再用监督训练训练基于交叉注意力的回归器预测结合亲和力变化。

Result: 在三个公开基准测试中，ProtAttBA表现优异，尤其在结构不确定时表现出鲁棒性，且能识别关键残基。

Conclusion: ProtAttBA为抗体工程提供了一种快速、经济的计算工具，有望加速新型治疗抗体的开发。

Abstract: A pivotal area of research in antibody engineering is to find effective
modifications that enhance antibody-antigen binding affinity. Traditional
wet-lab experiments assess mutants in a costly and time-consuming manner.
Emerging deep learning solutions offer an alternative by modeling antibody
structures to predict binding affinity changes. However, they heavily depend on
high-quality complex structures, which are frequently unavailable in practice.
Therefore, we propose ProtAttBA, a deep learning model that predicts binding
affinity changes based solely on the sequence information of antibody-antigen
complexes. ProtAttBA employs a pre-training phase to learn protein sequence
patterns, following a supervised training phase using labeled antibody-antigen
complex data to train a cross-attention-based regressor for predicting binding
affinity changes. We evaluated ProtAttBA on three open benchmarks under
different conditions. Compared to both sequence- and structure-based prediction
methods, our approach achieves competitive performance, demonstrating notable
robustness, especially with uncertain complex structures. Notably, our method
possesses interpretability from the attention mechanism. We show that the
learned attention scores can identify critical residues with impacts on binding
affinity. This work introduces a rapid and cost-effective computational tool
for antibody engineering, with the potential to accelerate the development of
novel therapeutic antibodies.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [381] [Scheduling with Uncertain Holding Costs and its Application to Content Moderation](https://arxiv.org/abs/2505.21331)
*Caner Gocmen,Thodoris Lykouris,Deeksha Sinha,Wentao Weng*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In content moderation for social media platforms, the cost of delaying the
review of a content is proportional to its view trajectory, which fluctuates
and is apriori unknown. Motivated by such uncertain holding costs, we consider
a queueing model where job states evolve based on a Markov chain with
state-dependent instantaneous holding costs. We demonstrate that in the
presence of such uncertain holding costs, the two canonical algorithmic
principles, instantaneous-cost ($c\mu$-rule) and expected-remaining-cost
($c\mu/\theta$-rule), are suboptimal. By viewing each job as a Markovian
ski-rental problem, we develop a new index-based algorithm,
Opportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of
serving jobs in the future when uncertainty partly resolves. We show that the
regret of OaRC scales as $\tilde{O}(L^{1.5}\sqrt{N})$, where $L$ is the maximum
length of a job's holding cost trajectory and $N$ is the system size. This
regret bound shows that OaRC achieves asymptotic optimality when the system
size $N$ scales to infinity. Moreover, its regret is independent of the
state-space size, which is a desirable property when job states contain
contextual information. We corroborate our results with an extensive simulation
study based on two holding cost patterns (online ads and user-generated
content) that arise in content moderation for social media platforms. Our
simulations based on synthetic and real datasets demonstrate that OaRC
consistently outperforms existing practice, which is based on the two canonical
algorithmic principles.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [382] [Differentially private ratio statistics](https://arxiv.org/abs/2505.20351)
*Tomer Shoham,Katrina Ligettt*

Main category: stat.ML

TL;DR: 本文填补了差分隐私文献中关于比率统计（如相对风险和比值比）的空白，提出了一种简单算法，在小样本下也能提供良好的隐私性、样本准确性和无偏性。


<details>
  <summary>Details</summary>
Motivation: 尽管比率统计在机器学习中广泛应用，但差分隐私下的比率统计研究较少，本文旨在解决这一问题。

Method: 提出了一种简单的差分隐私算法，分析了相对风险的差分隐私估计器，并开发了构建有效置信区间的方法。

Result: 算法在小样本下表现出色，证明了估计器的一致性，并提供了实用的比率估计解决方案。

Conclusion: 本文为差分隐私下的比率统计提供了理论和实践指导，填补了文献空白。

Abstract: Ratio statistics--such as relative risk and odds ratios--play a central role
in hypothesis testing, model evaluation, and decision-making across many areas
of machine learning, including causal inference and fairness analysis. However,
despite privacy concerns surrounding many datasets and despite increasing
adoption of differential privacy, differentially private ratio statistics have
largely been neglected by the literature and have only recently received an
initial treatment by Lin et al. [1]. This paper attempts to fill this lacuna,
giving results that can guide practice in evaluating ratios when the results
must be protected by differential privacy. In particular, we show that even a
simple algorithm can provide excellent properties concerning privacy, sample
accuracy, and bias, not just asymptotically but also at quite small sample
sizes. Additionally, we analyze a differentially private estimator for relative
risk, prove its consistency, and develop a method for constructing valid
confidence intervals. Our approach bridges a gap in the differential privacy
literature and provides a practical solution for ratio estimation in private
machine learning pipelines.

</details>


### [383] [A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data](https://arxiv.org/abs/2505.20688)
*Taehyo Kim,Qiran Jia,Mony J. de Leon,Hai Shu*

Main category: stat.ML

TL;DR: 提出了一种名为fcHMRF-LIS的空间FDR控制方法，用于神经影像数据分析中的多重检验问题，解决了传统方法在空间依赖性、稳定性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 神经影像数据分析中，传统FDR控制方法假设检验独立且存在高假阴性率，无法有效处理复杂空间依赖性和计算效率问题。

Method: 结合局部显著性指数（LIS）和新型全连接隐马尔可夫随机场（fcHMRF），开发了高效的期望最大化算法，降低了计算复杂度。

Result: 模拟实验显示fcHMRF-LIS能准确控制FDR，降低FNR，减少FDP和FNP的变异性，并提高真阳性率。

Conclusion: fcHMRF-LIS在神经影像数据分析中表现出色，具有更高的稳定性和计算效率，适用于高分辨率数据。

Abstract: False discovery rate (FDR) control methods are essential for voxel-wise
multiple testing in neuroimaging data analysis, where hundreds of thousands or
even millions of tests are conducted to detect brain regions associated with
disease-related changes. Classical FDR control methods (e.g., BH, q-value, and
LocalFDR) assume independence among tests and often lead to high false
non-discovery rates (FNR). Although various spatial FDR control methods have
been developed to improve power, they still fall short in jointly addressing
three major challenges in neuroimaging applications: capturing complex spatial
dependencies, maintaining low variability in both false discovery proportion
(FDP) and false non-discovery proportion (FNP) across replications, and
achieving computational scalability for high-resolution data. To address these
challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR
control method for voxel-wise multiple testing. It integrates the local index
of significance (LIS)-based testing procedure with a novel fully connected
hidden Markov random field (fcHMRF) designed to model complex spatial
structures using a parsimonious parameterization. We develop an efficient
expectation-maximization algorithm incorporating mean-field approximation, the
Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and
permutohedral lattice filtering, reducing the computational complexity from
quadratic to linear in the number of tests. Extensive simulations demonstrate
that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability
in FDP and FNP, and a higher number of true positives compared to existing
methods. Applied to an FDG-PET dataset from the Alzheimer's Disease
Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain
regions and offers notable advantages in computational efficiency.

</details>


### [384] [Kernel Quantile Embeddings and Associated Probability Metrics](https://arxiv.org/abs/2505.20433)
*Masha Naslidnyk,Siu Lun Chau,François-Xavier Briol,Krikamol Muandet*

Main category: stat.ML

TL;DR: 论文提出了一种新的核分位数嵌入（KQE）方法，用于表示概率分布，并构建了一类新的距离度量，其性能优于传统的最大均值差异（MMD）。


<details>
  <summary>Details</summary>
Motivation: 传统方法中，核均值嵌入（如MMD）是表示概率分布的主要方式，但尚不清楚均值函数是否是唯一有意义的表示。本文旨在探索其他可能的表示方式。

Method: 引入核分位数嵌入（KQE）概念，并基于此构建新的距离度量。这些距离在更弱的核条件下仍是概率度量，并能高效估计。

Result: 新方法在假设检验中表现出色，可作为MMD及其快速近似的竞争性替代方案。

Conclusion: KQE提供了一种灵活且高效的概率分布表示方式，扩展了核方法的应用范围。

Abstract: Embedding probability distributions into reproducing kernel Hilbert spaces
(RKHS) has enabled powerful nonparametric methods such as the maximum mean
discrepancy (MMD), a statistical distance with strong theoretical and
computational properties. At its core, the MMD relies on kernel mean embeddings
to represent distributions as mean functions in RKHS. However, it remains
unclear if the mean function is the only meaningful RKHS representation.
Inspired by generalised quantiles, we introduce the notion of kernel quantile
embeddings (KQEs). We then use KQEs to construct a family of distances that:
(i) are probability metrics under weaker kernel conditions than MMD; (ii)
recover a kernelised form of the sliced Wasserstein distance; and (iii) can be
efficiently estimated with near-linear cost. Through hypothesis testing, we
show that these distances offer a competitive alternative to MMD and its fast
approximations.

</details>


### [385] [Learning with Expected Signatures: Theory and Applications](https://arxiv.org/abs/2505.20465)
*Lorenzo Lucchese,Mikko S. Pakkanen,Almut E. D. Veraart*

Main category: stat.ML

TL;DR: 论文提出了一种基于期望签名的低维表示方法，能够完全表征数据生成分布，并改进了其估计器以降低误差。


<details>
  <summary>Details</summary>
Motivation: 解决期望签名在离散时间估计与连续时间理论值之间的差距，并提供更完整的概率解释。

Method: 提出了一种改进的期望签名估计器，特别针对鞅过程的数据生成过程。

Result: 改进的估计器显著降低了均方误差，并提升了预测性能。

Conclusion: 期望签名方法在时间序列和序列数据中具有广泛的应用潜力，改进后的估计器进一步提升了其性能。

Abstract: The expected signature maps a collection of data streams to a lower
dimensional representation, with a remarkable property: the resulting feature
tensor can fully characterize the data generating distribution. This
"model-free" embedding has been successfully leveraged to build multiple
domain-agnostic machine learning (ML) algorithms for time series and sequential
data. The convergence results proved in this paper bridge the gap between the
expected signature's empirical discrete-time estimator and its theoretical
continuous-time value, allowing for a more complete probabilistic
interpretation of expected signature-based ML methods. Moreover, when the data
generating process is a martingale, we suggest a simple modification of the
expected signature estimator with significantly lower mean squared error and
empirically demonstrate how it can be effectively applied to improve predictive
performance.

</details>


### [386] [Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models](https://arxiv.org/abs/2505.20536)
*Guanhao Zhou,Yuefeng Han,Xiufan Yu*

Main category: stat.ML

TL;DR: 本文提出了一种名为CoDEAL的新方法，用于处理因果面板数据中的异质性处理效应和协变量效应，结合神经网络和自编码器技术提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决因果面板数据中异质性处理效应和协变量效应的复杂性问题，传统方法难以捕捉非线性和异质性。

Method: 提出CoDEAL方法，结合前馈神经网络（处理协变量效应）和多输出自编码器（处理面板数据的横截面和时间依赖性），通过矩阵补全算法估算反事实结果。

Result: 理论证明了反事实估计的收敛性，并通过模拟和实际数据验证了方法的优越性能。

Conclusion: CoDEAL方法在复杂因果面板数据中表现出色，兼具灵活性和可解释性。

Abstract: This paper studies the task of estimating heterogeneous treatment effects in
causal panel data models, in the presence of covariate effects. We propose a
novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models,
that employs flexible model structures and powerful neural network
architectures to cohesively deal with the underlying heterogeneity and
nonlinearity of both panel units and covariate effects. The proposed CoDEAL
integrates nonlinear covariate effect components (parameterized by a
feed-forward neural network) with nonlinear factor structures (modeled by a
multi-output autoencoder) to form a heterogeneous causal panel model. The
nonlinear covariate component offers a flexible framework for capturing the
complex influences of covariates on outcomes. The nonlinear factor analysis
enables CoDEAL to effectively capture both cross-sectional and temporal
dependencies inherent in the data panel. This latent structural information is
subsequently integrated into a customized matrix completion algorithm, thereby
facilitating more accurate imputation of missing counterfactual outcomes.
Moreover, the use of a multi-output autoencoder explicitly accounts for
heterogeneity across units and enhances the model interpretability of the
latent factors. We establish theoretical guarantees on the convergence of the
estimated counterfactuals, and demonstrate the compelling performance of the
proposed method using extensive simulation studies and a real data application.

</details>


### [387] [Balancing Performance and Costs in Best Arm Identification](https://arxiv.org/abs/2505.20583)
*Michael O. Harding,Kirthevasan Kandasamy*

Main category: stat.ML

TL;DR: 论文提出了一种新的多臂老虎机模型框架，通过最小化风险函数来平衡推荐臂的性能和学习成本，避免了传统固定预算或置信度设置的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定预算或置信度设置下最佳臂识别问题对实践者选择方法和参数的困扰，更贴近实际需求（如A/B测试中的利润最大化）。

Method: 提出DBCARE算法，通过最小化风险函数（性能惩罚与学习成本之和），并推导理论下界。

Result: DBCARE算法在大多数问题实例上匹配理论下界，模拟实验显示其优于传统固定预算或置信度算法。

Conclusion: 新框架更符合实践需求，DBCARE算法在性能和学习成本平衡上表现优异。

Abstract: We consider the problem of identifying the best arm in a multi-armed bandit
model. Despite a wealth of literature in the traditional fixed budget and fixed
confidence regimes of the best arm identification problem, it still remains a
mystery to most practitioners as to how to choose an approach and corresponding
budget or confidence parameter. We propose a new formalism to avoid this
dilemma altogether by minimizing a risk functional which explicitly balances
the performance of the recommended arm and the cost incurred by learning this
arm. In this framework, a cost is incurred for each observation during the
sampling phase, and upon recommending an arm, a performance penalty is incurred
for identifying a suboptimal arm. The learner's goal is to minimize the sum of
the penalty and cost. This new regime mirrors the priorities of many
practitioners, e.g. maximizing profit in an A/B testing framework, better than
classical fixed budget or confidence settings. We derive theoretical lower
bounds for the risk of each of two choices for the performance penalty, the
probability of misidentification and the simple regret, and propose an
algorithm called DBCARE to match these lower bounds up to polylog factors on
nearly all problem instances. We then demonstrate the performance of DBCARE on
a number of simulated models, comparing to fixed budget and confidence
algorithms to show the shortfalls of existing BAI paradigms on this problem.

</details>


### [388] [Moment Expansions of the Energy Distance](https://arxiv.org/abs/2505.20647)
*Ian Langmore*

Main category: stat.ML

TL;DR: 能量距离在分布相等性测试和机器学习中作为损失函数使用。研究发现，当分布接近时，能量距离对均值差异更敏感，而对协方差差异的敏感性较低，且与维度无关。


<details>
  <summary>Details</summary>
Motivation: 探讨能量距离在分布接近时的敏感性差异，特别是在均值和协方差上的表现。

Method: 分析能量距离的结构，研究其对均值和协方差差异的敏感性，并通过数值实验验证。

Result: 能量距离对均值差异更敏感，而对协方差差异的敏感性较低，且与维度无关。数值结果支持这一结论。

Conclusion: 能量距离在分布接近时对均值差异更敏感，这一特性独立于维度，且在实际应用中具有重要性。

Abstract: The energy distance is used to test distributional equality, and as a loss
function in machine learning. While $D^2(X, Y)=0$ only when $X\sim Y$, the
sensitivity to different moments is of practical importance. This work
considers $D^2(X, Y)$ in the case where the distributions are close. In this
regime, $D^2(X, Y)$ is more sensitive to differences in the means
$\bar{X}-\bar{Y}$, than differences in the covariances $\Delta$. This is due to
the structure of the energy distance and is independent of dimension. The
sensitivity to on versus off diagonal components of $\Delta$ is examined when
$X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs
and, in many cases, off diagonal correlations contribute significantly less.
Numerical results verify these relationships hold even when distributional
assumptions are not strictly met.

</details>


### [389] [Stationary MMD Points for Cubature](https://arxiv.org/abs/2505.20754)
*Zonghao Chen,Toni Karvonen,Heishiro Kanagawa,François-Xavier Briol,Chris. J. Oates*

Main category: stat.ML

TL;DR: 论文研究了通过最大均值差异（MMD）的驻点而非全局最小化来逼近目标概率分布的问题，发现驻点具有超收敛性质，并提出离散梯度流作为计算驻点的实用方法。


<details>
  <summary>Details</summary>
Motivation: 逼近目标概率分布是积分、数据压缩和优化中的基本问题，但MMD的非凸性导致全局最小化困难，因此研究驻点的性质和应用。

Method: 考虑MMD的驻点，分析其超收敛性质，并提出离散梯度流作为计算驻点的实用方法。

Result: 理论证明驻点的积分误差比MMD更快趋近于零，并建立了非渐近有限粒子误差界。

Conclusion: 驻点具有超收敛性质，离散梯度流是计算驻点的有效方法，为相关领域提供了新思路。

Abstract: Approximation of a target probability distribution using a finite set of
points is a problem of fundamental importance, arising in cubature, data
compression, and optimisation. Several authors have proposed to select points
by minimising a maximum mean discrepancy (MMD), but the non-convexity of this
objective precludes global minimisation in general. Instead, we consider
\emph{stationary} points of the MMD which, in contrast to points globally
minimising the MMD, can be accurately computed. Our main theoretical
contribution is the (perhaps surprising) result that, for integrands in the
associated reproducing kernel Hilbert space, the cubature error of stationary
MMD points vanishes \emph{faster} than the MMD. Motivated by this
\emph{super-convergence} property, we consider discretised gradient flows as a
practical strategy for computing stationary points of the MMD, presenting a
refined convergence analysis that establishes a novel non-asymptotic
finite-particle error bound, which may be of independent interest.

</details>


### [390] [Input Convex Kolmogorov Arnold Networks](https://arxiv.org/abs/2505.21208)
*Thomas Deschatre,Xavier Warin*

Main category: stat.ML

TL;DR: 本文提出了一种基于Kolmogorov-Arnold网络（ICKAN）的输入凸神经网络架构，包含两种网络：一种是基于低阶分段线性函数的表示，并提供了通用逼近定理；另一种基于三次样条，仅通过数值结果支持收敛。实验表明其性能与传统输入凸神经网络（ICNNs）相当。此外，ICKAN在解决需要凸函数逼近的最优传输问题时表现出色，与ICNNs结果相似。


<details>
  <summary>Details</summary>
Motivation: 传统输入凸神经网络（ICNNs）在某些任务中表现受限，因此需要探索新的网络架构以提升性能。本文旨在通过Kolmogorov-Arnold网络（ICKAN）提供一种替代方案，并验证其有效性。

Method: 提出两种ICKAN网络：1）基于低阶分段线性函数的表示，提供理论支持；2）基于三次样条，通过数值实验验证收敛性。随后将网络应用于最优传输问题。

Result: 实验表明，两种ICKAN网络在简单测试中与传统ICNNs性能相当。在最优传输问题中，三次样条ICKAN与ICNNs结果相似。

Conclusion: ICKAN是一种有效的输入凸神经网络架构，尤其在最优传输问题中表现出色，为凸函数逼近提供了新的选择。

Abstract: This article presents an input convex neural network architecture using
Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the
first is based on a low-order, linear-by-part, representation of functions, and
a universal approximation theorem is provided. The second is based on cubic
splines, for which only numerical results support convergence. We demonstrate
on simple tests that these networks perform competitively with classical input
convex neural networks (ICNNs). In a second part, we use the networks to solve
some optimal transport problems needing a convex approximation of functions and
demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs
produce results similar to those of classical ICNNs.

</details>


### [391] [Autoencoding Random Forests](https://arxiv.org/abs/2505.21441)
*Binh Duc Vu,Jan Kapar,Marvin Wright,David S. Watson*

Main category: stat.ML

TL;DR: 提出了一种基于随机森林的自编码方法，通过非参数统计和谱图理论学习低维嵌入，并通过优化、分裂重标和最近邻回归实现解码。该方法适用于监督或无监督模型，具有通用一致性，并在多种数据上展示了应用效果。


<details>
  <summary>Details</summary>
Motivation: 传统自编码方法在处理复杂数据时可能表现不佳，而随机森林作为一种强大的非参数模型，可以更好地捕捉数据关系。本文旨在利用随机森林的优势，构建一种新的自编码框架。

Method: 结合非参数统计和谱图理论，学习数据的最优低维嵌入。通过约束优化、分裂重标和最近邻回归解决解码问题，实现从嵌入空间到输入空间的映射。

Result: 提出的方法在多种数据（表格、图像、基因组）上表现优异，适用于可视化、压缩、聚类和去噪等任务。

Conclusion: 该方法为自编码提供了一种新的、高效的解决方案，具有广泛的应用潜力。

Abstract: We propose a principled method for autoencoding with random forests. Our
strategy builds on foundational results from nonparametric statistics and
spectral graph theory to learn a low-dimensional embedding of the model that
optimally represents relationships in the data. We provide exact and
approximate solutions to the decoding problem via constrained optimization,
split relabeling, and nearest neighbors regression. These methods effectively
invert the compression pipeline, establishing a map from the embedding space
back to the input space using splits learned by the ensemble's constituent
trees. The resulting decoders are universally consistent under common
regularity assumptions. The procedure works with supervised or unsupervised
models, providing a window into conditional or joint distributions. We
demonstrate various applications of this autoencoder, including powerful new
tools for visualization, compression, clustering, and denoising. Experiments
illustrate the ease and utility of our method in a wide range of settings,
including tabular, image, and genomic data.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [392] [MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery](https://arxiv.org/abs/2505.20299)
*Jianpeng Chen,Wangzhi Zhan,Haohui Wang,Zian Jia,Jingru Gan,Junkai Zhang,Jingyuan Qi,Tingwei Chen,Lifu Huang,Muhao Chen,Ling Li,Wei Wang,Dawei Zhou*

Main category: physics.optics

TL;DR: 论文提出了一个名为MetamatBench的统一框架，以解决机器学习在超材料发现中的三大挑战：数据异构性、模型复杂性和人机协作。


<details>
  <summary>Details</summary>
Motivation: 超材料具有多尺度结构和可调机械性能，但机器学习在超材料发现中的应用面临数据异构性、模型复杂性和人机协作的挑战。

Method: MetamatBench框架分为三个层次：数据层整合并标准化多模态数据集；ML层提供17种先进方法及12种新评估指标；用户层提供可视化交互界面。

Result: MetamatBench为超材料发现提供了一个统一平台，支持机器学习研究者和非ML研究者进行属性预测和逆向设计。

Conclusion: 该框架通过标准化数据、优化模型和提升用户体验，推动了超材料发现的研究和应用。

Abstract: Metamaterials, engineered materials with architected structures across
multiple length scales, offer unprecedented and tunable mechanical properties
that surpass those of conventional materials. However, leveraging advanced
machine learning (ML) for metamaterial discovery is hindered by three
fundamental challenges: (C1) Data Heterogeneity Challenge arises from
heterogeneous data sources, heterogeneous composition scales, and heterogeneous
structure categories; (C2) Model Complexity Challenge stems from the intricate
geometric constraints of ML models, which complicate their adaptation to
metamaterial structures; and (C3) Human-AI Collaboration Challenge comes from
the "dual black-box'' nature of sophisticated ML models and the need for
intuitive user interfaces. To tackle these challenges, we introduce a unified
framework, named MetamatBench, that operates on three levels. (1) At the data
level, we integrate and standardize 5 heterogeneous, multi-modal metamaterial
datasets. (2) The ML level provides a comprehensive toolkit that adapts 17
state-of-the-art ML methods for metamaterial discovery. It also includes a
comprehensive evaluation suite with 12 novel performance metrics with finite
element-based assessments to ensure accurate and reliable model validation. (3)
The user level features a visual-interactive interface that bridges the gap
between complex ML techniques and non-ML researchers, advancing property
prediction and inverse design of metamaterials for research and applications.
MetamatBench offers a unified platform deployed at
http://zhoulab-1.cs.vt.edu:5550 that enables machine learning researchers and
practitioners to develop and evaluate new methodologies in metamaterial
discovery. For accessibility and reproducibility, we open-source our benchmark
and the codebase at https://github.com/cjpcool/Metamaterial-Benchmark.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [393] [Leveraging GANs for citation intent classification and its impact on citation network analysis](https://arxiv.org/abs/2505.21162)
*Davi A. Bezerra,Filipi N. Silva,Diego R. Amancio*

Main category: cs.DL

TL;DR: 本文提出了一种基于GAN的方法用于分类引用意图，结果表明该方法在性能上具有竞争力，且参数更少。同时，研究发现引用意图的过滤会影响论文在引用网络中的中心性。


<details>
  <summary>Details</summary>
Motivation: 理解引用意图可以更细致地解释科学影响，但目前的方法在分类性能和效率上仍有改进空间。

Method: 采用基于GAN的方法结合上下文嵌入进行引用意图分类。

Result: 提出的方法在分类任务中表现优异，且参数更少。引用意图的过滤显著影响论文在引用网络中的中心性，尤其是介数中心性。

Conclusion: GAN架构结合上下文嵌入在引用意图分类中高效有效，引用意图对论文中心性有显著影响。

Abstract: Citations play a fundamental role in the scientific ecosystem, serving as a
foundation for tracking the flow of knowledge, acknowledging prior work, and
assessing scholarly influence. In scientometrics, they are also central to the
construction of quantitative indicators. Not all citations, however, serve the
same function: some provide background, others introduce methods, or compare
results. Therefore, understanding citation intent allows for a more nuanced
interpretation of scientific impact. In this paper, we adopted a GAN-based
method to classify citation intents. Our results revealed that the proposed
method achieves competitive classification performance, closely matching
state-of-the-art results with substantially fewer parameters. This demonstrates
the effectiveness and efficiency of leveraging GAN architectures combined with
contextual embeddings in intent classification task. We also investigated
whether filtering citation intents affects the centrality of papers in citation
networks. Analyzing the network constructed from the unArXiv dataset, we found
that paper rankings can be significantly influenced by citation intent. All
four centrality metrics examined- degree, PageRank, closeness, and betweenness
- were sensitive to the filtering of citation types. The betweenness centrality
displayed the greatest sensitivity, showing substantial changes in ranking when
specific citation intents were removed.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [394] [An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images](https://arxiv.org/abs/2505.20332)
*Neil Chaudhary,Zaynah Dhunny*

Main category: eess.IV

TL;DR: 论文提出了一种基于卷积神经网络（CNN）的人工智能工具，用于通过组织病理学活检图像准确识别乳腺癌类型，减少侵入性测试和延迟治疗的问题。


<details>
  <summary>Details</summary>
Motivation: 传统乳腺癌类型识别需要额外侵入性测试，延迟治疗并增加患者负担。

Method: 使用CNN架构对图像进行预处理以减少噪声并增强特征，区分良性和恶性组织，并实现乳腺癌亚型的准确分类。

Result: 实验结果表明，该模型在准确性、精确度、召回率和F1分数上优于现有解决方案。

Conclusion: 深度学习技术在临床诊断中具有潜力，为乳腺癌分类提供了有前景的辅助工具。

Abstract: Accurate identification of breast cancer types plays a critical role in
guiding treatment decisions and improving patient outcomes. This paper presents
an artificial intelligence enabled tool designed to aid in the identification
of breast cancer types using histopathological biopsy images. Traditionally
additional tests have to be done on women who are detected with breast cancer
to find out the types of cancer it is to give the necessary cure. Those tests
are not only invasive but also delay the initiation of treatment and increase
patient burden. The proposed model utilizes a convolutional neural network
(CNN) architecture to distinguish between benign and malignant tissues as well
as accurate subclassification of breast cancer types. By preprocessing the
images to reduce noise and enhance features, the model achieves reliable levels
of classification performance. Experimental results on such datasets
demonstrate the model's effectiveness, outperforming several existing solutions
in terms of accuracy, precision, recall, and F1-score. The study emphasizes the
potential of deep learning techniques in clinical diagnostics and offers a
promising tool to assist pathologists in breast cancer classification.

</details>


### [395] [DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals](https://arxiv.org/abs/2505.20367)
*Sen Yan,Fabrizio Gabellieri,Etienne Goffinet,Filippo Castiglione,Thomas Launey*

Main category: eess.IV

TL;DR: 论文提出使用深度学习中的扩散模型来提升非均匀采样（NUS）核磁共振（NMR）光谱的重建质量，减少实验时间的同时保持光谱质量。


<details>
  <summary>Details</summary>
Motivation: 尽管NUS能减少NMR实验时间，但其引入的伪影和光谱质量下降问题需要解决。

Method: 应用扩散模型处理时-时和时-频域的NUS数据，以重建Artina数据集中的挑战性光谱。

Result: 扩散模型在时-频域数据上表现更优，成功提升了重建质量。

Conclusion: 扩散模型为NMR光谱的高效和准确重建提供了新方向，时-频域数据更具潜力。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization
to probe molecules' chemical environment, structure, and dynamics, with
applications spanning from pharmaceuticals to the petroleum industry. Despite
its utility, the high cost of NMR instrumentation, operation and the lengthy
duration of experiments necessitate the development of computational techniques
to optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as
a sub-sampling method to address these challenges, but it often introduces
artifacts and degrades spectral quality, offsetting the benefits of reduced
acquisition times. In this work, we propose the use of deep learning techniques
to enhance the reconstruction quality of NUS spectra. Specifically, we explore
the application of diffusion models, a relatively untapped approach in this
domain. Our methodology involves applying diffusion models to both time-time
and time-frequency NUS data, yielding satisfactory reconstructions of
challenging spectra from the benchmark Artina dataset. This approach
demonstrates the potential of diffusion models to improve the efficiency and
accuracy of NMR spectroscopy as well as the superiority of using a
time-frequency domain data over the time-time one, opening new landscapes for
future studies.

</details>


### [396] [Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing](https://arxiv.org/abs/2505.20746)
*Nikola Andrejic,Milica Spasic,Igor Mihajlovic,Petra Milosavljevic,Djordje Pavlovic,Filip Milisavljevic,Uros Milivojevic,Danilo Delibasic,Ivana Mikic,Sinisa Todorovic*

Main category: eess.IV

TL;DR: Ui2i是一种新型的无配对图像到图像翻译模型，通过改进CycleGAN，更好地分离内容和风格特征，并保持内容完整性。


<details>
  <summary>Details</summary>
Motivation: 解决无配对数据下的风格迁移问题，同时确保内容结构的准确性，特别是在生物医学图像处理中。

Method: 采用U-Net生成器、近似双向谱归一化和注意力机制，结合图像尺度增强进行训练。

Result: 在生物医学任务中验证了Ui2i在保持内容保真度方面的优越性，首次实现了无配对数据下IF图像中叠加信号的分离。

Conclusion: Ui2i在需要高精度结构保留的任务中表现出色，为无配对图像翻译提供了新思路。

Abstract: This work introduces Ui2i, a novel model for unpaired image-to-image
translation, trained on content-wise unpaired datasets to enable style transfer
across domains while preserving content. Building on CycleGAN, Ui2i
incorporates key modifications to better disentangle content and style
features, and preserve content integrity. Specifically, Ui2i employs
U-Net-based generators with skip connections to propagate localized shallow
features deep into the generator. Ui2i removes feature-based normalization
layers from all modules and replaces them with approximate bidirectional
spectral normalization -- a parameter-based alternative that enhances training
stability. To further support content preservation, channel and spatial
attention mechanisms are integrated into the generators. Training is
facilitated through image scale augmentation. Evaluation on two biomedical
tasks -- domain adaptation for nuclear segmentation in immunohistochemistry
(IHC) images and unmixing of biological structures superimposed in
single-channel immunofluorescence (IF) images -- demonstrates Ui2i's ability to
preserve content fidelity in settings that demand more accurate structural
preservation than typical translation tasks. To the best of our knowledge, Ui2i
is the first approach capable of separating superimposed signals in IF images
using real, unpaired training data.

</details>


### [397] [The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective](https://arxiv.org/abs/2505.20810)
*Tariq M Khan,Toufique Ahmed Soomro,Imran Razzak*

Main category: eess.IV

TL;DR: 视网膜成像技术结合AI和移动健康技术，为系统性疾病的早期检测提供了高分辨率工具，但临床转化仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 系统性疾病的早期检测需要非侵入性、高分辨率的工具，视网膜成像技术结合AI和移动健康技术为此提供了可能。

Method: 综述了OCT/OCTA、自适应光学（AO）技术、AI/ML算法及移动健康/远程眼科的最新进展，并评估其诊断性能。

Result: AI模型在糖尿病视网膜病变和心血管风险预测中表现出高敏感性和特异性（>90%敏感性和AUC=0.89）。

Conclusion: 提出多中心协议标准化、前瞻性验证试验及将视网膜筛查整合到临床路径的路线图，以实现精准预防和早期干预。

Abstract: Retinal imaging has emerged as a powerful, non-invasive modality for
detecting and quantifying biomarkers of systemic diseases-ranging from diabetes
and hypertension to Alzheimer's disease and cardiovascular disorders but
current insights remain dispersed across platforms and specialties. Recent
technological advances in optical coherence tomography (OCT/OCTA) and adaptive
optics (AO) now deliver ultra-high-resolution scans (down to 5 {\mu}m ) with
superior contrast and spatial integration, allowing early identification of
microvascular abnormalities and neurodegenerative changes. At the same time,
AI-driven and machine learning (ML) algorithms have revolutionized the analysis
of large-scale retinal datasets, increasing sensitivity and specificity; for
example, deep learning models achieve > 90 \% sensitivity for diabetic
retinopathy and AUC = 0.89 for the prediction of cardiovascular risk from
fundus photographs. The proliferation of mobile health technologies and
telemedicine platforms further extends access, reduces costs, and facilitates
community-based screening and longitudinal monitoring. Despite these
breakthroughs, translation into routine practice is hindered by heterogeneous
imaging protocols, limited external validation of AI models, and integration
challenges within clinical workflows. In this review, we systematically
synthesize the latest OCT/OCT and AO developments, AI/ML approaches, and
mHealth/Tele-ophthalmology initiatives and quantify their diagnostic
performance across disease domains. Finally, we propose a roadmap for
multicenter protocol standardization, prospective validation trials, and
seamless incorporation of retinal screening into primary and specialty care
pathways-paving the way for precision prevention, early intervention, and
ongoing treatment of life-threatening systemic diseases.

</details>


### [398] [Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing](https://arxiv.org/abs/2505.20902)
*Ruiying Li,Bin Pan,Lan Ma,Xia Xu,Zhenwei Shi*

Main category: eess.IV

TL;DR: 本文提出了一种多时相高光谱解混框架MiLD，通过神经ODE建模丰度动态，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视丰度动态，作者采用神经ODE建模丰度时间演化，但面临问题复杂性和理论缺失的挑战。

Method: MiLD框架包括问题定义（ODE和潜变量）、数学建模（动态离散化）、求解算法（神经网络近似）和理论验证（一致性、收敛性、稳定性）。

Result: 实验验证了MiLD在合成和真实数据集上的有效性。

Conclusion: MiLD通过ODE定义问题、动态离散化建模、算法求解和理论支持，成功解决了多时相高光谱解混问题。

Abstract: Multitemporal hyperspectral unmixing can capture dynamical evolution of
materials. Despite its capability, current methods emphasize variability of
endmembers while neglecting dynamics of abundances, which motivates our
adoption of neural ordinary differential equations to model abundances
temporally. However, this motivation is hindered by two challenges: the
inherent complexity in defining, modeling and solving problem, and the absence
of theoretical support. To address above challenges, in this paper, we propose
a multitemporal latent dynamical (MiLD) unmixing framework by capturing
dynamical evolution of materials with theoretical validation. For addressing
multitemporal hyperspectral unmixing, MiLD consists of problem definition,
mathematical modeling, solution algorithm and theoretical support. We formulate
multitemporal unmixing problem definition by conducting ordinary differential
equations and developing latent variables. We transfer multitemporal unmixing
to mathematical model by dynamical discretization approaches, which describe
the discreteness of observed sequence images with mathematical expansions. We
propose algorithm to solve problem and capture dynamics of materials, which
approximates abundance evolution by neural networks. Furthermore, we provide
theoretical support by validating the crucial properties, which verifies
consistency, convergence and stability theorems. The major contributions of
MiLD include defining problem by ordinary differential equations, modeling
problem by dynamical discretization approach, solving problem by multitemporal
unmixing algorithm, and presenting theoretical support. Our experiments on both
synthetic and real datasets have validated the utility of our work

</details>


### [399] [Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution](https://arxiv.org/abs/2505.20984)
*Minghao Han,Weiyi You,Jinhua Zhang,Leheng Zhang,Ce Zhu,Shuhang Gu*

Main category: eess.IV

TL;DR: 本文提出了一种基于扩散模型的生成式图像压缩方法，通过重新定义压缩过程为前向扩散路径，并训练反向神经网络直接重构图像，实现了高效且逼真的图像重建。


<details>
  <summary>Details</summary>
Motivation: 传统学习式图像压缩（LIC）关注数据传输效率，而生成式图像压缩（GIC）通过结合生成模型进一步提升了重建图像的逼真度。本文旨在提出一种更高效的扩散模型框架，直接优化压缩过程。

Method: 将压缩过程重新解释为由随机微分方程（SDEs）控制的前向扩散路径，并训练反向神经网络直接重构图像，无需高斯噪声初始化。

Result: 实验表明，该方法在感知失真、统计保真度和无参考质量评估等多个指标上优于现有生成式图像压缩方法，且仅需少量采样步骤即可实现平滑的速率调整和逼真重建。

Conclusion: 提出的扩散模型框架为生成式图像压缩提供了高效且高质量的解决方案，显著提升了重建图像的逼真度和压缩效率。

Abstract: While learned image compression (LIC) focuses on efficient data transmission,
generative image compression (GIC) extends this framework by integrating
generative modeling to produce photo-realistic reconstructed images. In this
paper, we propose a novel diffusion-based generative modeling framework
tailored for generative image compression. Unlike prior diffusion-based
approaches that indirectly exploit diffusion modeling, we reinterpret the
compression process itself as a forward diffusion path governed by stochastic
differential equations (SDEs). A reverse neural network is trained to
reconstruct images by reversing the compression process directly, without
requiring Gaussian noise initialization. This approach achieves smooth rate
adjustment and photo-realistic reconstructions with only a minimal number of
sampling steps. Extensive experiments on benchmark datasets demonstrate that
our method outperforms existing generative image compression approaches across
a range of metrics, including perceptual distortion, statistical fidelity, and
no-reference quality assessments.

</details>


### [400] [Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods](https://arxiv.org/abs/2505.21355)
*Muhammad Imran,Wayne G. Brisbane,Li-Ming Su,Jason P. Joseph,Wei Shao*

Main category: eess.IV

TL;DR: AI分析微超声图像在检测前列腺癌中优于传统PSA和DRE筛查方法，提高了特异性并保持高敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究AI在微超声图像分析中是否比传统筛查方法（PSA和DRE）更准确地检测临床显著前列腺癌。

Method: 使用自监督卷积自动编码器提取微超声图像特征，随机森林分类器预测癌症，并与基于PSA、DRE等临床模型对比。

Result: AI模型的AUROC为0.871，敏感性92.5%，特异性68.1%；临床模型AUROC为0.753，敏感性96.2%，特异性27.3%。

Conclusion: AI分析的微超声提高了特异性，可能减少不必要的活检，成为低成本筛查替代方案。

Abstract: Background and objective: Micro-ultrasound (micro-US) is a novel imaging
modality with diagnostic accuracy comparable to MRI for detecting clinically
significant prostate cancer (csPCa). We investigated whether artificial
intelligence (AI) interpretation of micro-US can outperform clinical screening
methods using PSA and digital rectal examination (DRE). Methods: We
retrospectively studied 145 men who underwent micro-US guided biopsy (79 with
csPCa, 66 without). A self-supervised convolutional autoencoder was used to
extract deep image features from 2D micro-US slices. Random forest classifiers
were trained using five-fold cross-validation to predict csPCa at the slice
level. Patients were classified as csPCa-positive if 88 or more consecutive
slices were predicted positive. Model performance was compared with a
classifier using PSA, DRE, prostate volume, and age. Key findings and
limitations: The AI-based micro-US model and clinical screening model achieved
AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US
model achieved 92.5% sensitivity and 68.1% specificity, while the clinical
model showed 96.2% sensitivity but only 27.3% specificity. Limitations include
a retrospective single-center design and lack of external validation.
Conclusions and clinical implications: AI-interpreted micro-US improves
specificity while maintaining high sensitivity for csPCa detection. This method
may reduce unnecessary biopsies and serve as a low-cost alternative to
PSA-based screening. Patient summary: We developed an AI system to analyze
prostate micro-ultrasound images. It outperformed PSA and DRE in detecting
aggressive cancer and may help avoid unnecessary biopsies.

</details>


### [401] [Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants](https://arxiv.org/abs/2505.21019)
*Devran Ugurlu,Shuang Qian,Elliot Fairweather,Charlene Mauger,Bram Ruijsink,Laura Dal Toso,Yu Deng,Marina Strocchi,Reza Razavi,Alistair Young,Pablo Lamata,Steven Niederer,Martin Bishop*

Main category: eess.IV

TL;DR: 论文提出了一种自动开源流程，用于从心血管磁共振图像生成患者特异性的左右心室网格，并应用于英国生物银行的55000名参与者，构建了迄今为止最全面的成人心脏模型库。


<details>
  <summary>Details</summary>
Motivation: 心脏数字孪生技术需要大规模生成精确的患者特异性心脏模型，但目前缺乏公开的跨人口群体模型库。

Method: 开发了一种自动开源流程，从心血管磁共振图像生成左右心室网格，并应用于55000名参与者，构建了1423个代表性网格。

Result: 成功构建了涵盖性别、体重指数和年龄范围的1423个代表性心脏模型，代码和预训练网络将公开。

Conclusion: 该研究为心脏数字孪生技术提供了大规模模型库和开源工具，有助于心血管疾病的筛查和治疗规划。

Abstract: A cardiac digital twin is a virtual replica of a patient's heart for
screening, diagnosis, prognosis, risk assessment, and treatment planning of
cardiovascular diseases. This requires an anatomically accurate
patient-specific 3D structural representation of the heart, suitable for
electro-mechanical simulations or study of disease mechanisms. However,
generation of cardiac digital twins at scale is demanding and there are no
public repositories of models across demographic groups. We describe an
automatic open-source pipeline for creating patient-specific left and right
ventricular meshes from cardiovascular magnetic resonance images, its
application to a large cohort of ~55000 participants from UK Biobank, and the
construction of the most comprehensive cohort of adult heart models to date,
comprising 1423 representative meshes across sex (male, female), body mass
index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is
available at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,
and pre-trained networks, representative volumetric meshes with fibers and UVCs
will be made available soon.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [402] [Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data](https://arxiv.org/abs/2505.20731)
*Linshanshan Wang,Mengyan Li,Zongqi Xia,Molei Liu,Tianxi Cai*

Main category: stat.ME

TL;DR: SCORE是一个半监督表示学习框架，通过患者嵌入解决电子健康记录（EHR）的稀疏性和异质性问题，结合PALM模型和混合EM-GVA算法，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: EHR数据具有稀疏性、异质性和高维度，缺乏标准化标签，导致建模困难。SCORE旨在解决这些问题。

Method: 采用PALM模型和预训练代码嵌入，结合混合EM-GVA算法处理大规模数据，利用少量标签数据优化未标记样本。

Result: 理论分析证明混合方法收敛，模拟实验显示SCORE在有限样本下优于现有方法，应用于多发性硬化症（MS）预测时表现更优。

Conclusion: SCORE通过整合未标记数据提升准确性，减少对标签稀缺的敏感性，适用于EHR数据的个性化医疗应用。

Abstract: Electronic Health Records (EHR) offer rich real-world data for personalized
medicine, providing insights into disease progression, treatment responses, and
patient outcomes. However, their sparsity, heterogeneity, and high
dimensionality make them difficult to model, while the lack of standardized
ground truth further complicates predictive modeling. To address these
challenges, we propose SCORE, a semi-supervised representation learning
framework that captures multi-domain disease profiles through patient
embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model
with pre-trained code embeddings to characterize codified features and extract
meaningful patient phenotypes and embeddings. To handle the computational
challenges of large-scale data, it introduces a hybrid Expectation-Maximization
(EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited
labeled data to refine estimates on a vast pool of unlabeled samples. We
theoretically establish the convergence of this hybrid approach, quantify GVA
errors, and derive SCORE's error rate under diverging embedding dimensions. Our
analysis shows that incorporating unlabeled data enhances accuracy and reduces
sensitivity to label scarcity. Extensive simulations confirm SCORE's superior
finite-sample performance over existing methods. Finally, we apply SCORE to
predict disability status for patients with multiple sclerosis (MS) using
partially labeled EHR data, demonstrating that it produces more informative and
predictive patient embeddings for multiple MS-related conditions compared to
existing approaches.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [403] [Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting](https://arxiv.org/abs/2505.20714)
*Zechen Li,Lanqing Yang,Yiheng Bian,Hao Pan,Yongjian Fu,Yezhou Wang,Yi-Chao Chen,Guangtao Xue,Ju Ren*

Main category: cs.NI

TL;DR: 论文提出了一种频率嵌入的3D高斯泼溅算法，用于宽带射频辐射场建模，优于现有单频建模方法。通过设计电磁特征网络，学习频率与高斯属性关系，实现了未知频率的高效重建。实验验证了方法的有效性，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有射频辐射场建模方法局限于单频，无法捕捉宽带频率的复杂关系。本文旨在通过频率嵌入的3D高斯泼溅算法，解决这一问题。

Method: 设计了电磁特征网络，包含衰减和辐射模块，学习频率与3D高斯属性（衰减因子和信号强度）的关系。训练频率嵌入的3DGS模型，实现未知频率的辐射场重建。

Result: 方法在实验中表现优异，SSIM达0.72，比现有技术提升17.8%。即使未在特定频率训练，SSIM仍达0.70，性能仅下降2.8%。

Conclusion: 提出的方法能高效重建未知频率的射频辐射场，性能显著优于现有技术，展示了在宽带频率建模中的潜力。

Abstract: This paper presents an innovative frequency-embedded 3D Gaussian splatting
(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,
offering an advancement over the existing works limited to single-frequency
modeling. Grounded in fundamental physics, we uncover the complex relationship
between EM wave propagation behaviors and RF frequencies. Inspired by this, we
design an EM feature network with attenuation and radiance modules to learn the
complex relationships between RF frequencies and the key properties of each 3D
Gaussian, specifically the attenuation factor and RF signal intensity. By
training the frequency-embedded 3DGS model, we can efficiently reconstruct RF
radiance fields at arbitrary unknown frequencies within a given 3D environment.
Finally, we propose a large-scale power angular spectrum (PAS) dataset
containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,
and conduct extensive experiments to verify the effectiveness of our method.
Our approach achieves an average Structural Similarity Index Measure (SSIM) up
to 0.72, and a significant improvement up to 17.8% compared to the current
state-of-the-art (SOTA) methods trained on individual test frequencies.
Additionally, our method achieves an SSIM of 0.70 without prior training on
these frequencies, which represents only a 2.8% performance drop compared to
models trained with full PAS data. This demonstrates our model's capability to
estimate PAS at unknown frequencies. For related code and datasets, please
refer to https://github.com/sim-2-real/Wideband3DGS.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [404] [Transfer learning for multifidelity simulation-based inference in cosmology](https://arxiv.org/abs/2505.21215)
*Alex A. Saoulis,Davide Piras,Niall Jeffrey,Alessio Spurio Mancini,Ana M. G. Ferreira,Benjamin Joachimi*

Main category: astro-ph.CO

TL;DR: 论文提出了一种基于多保真度迁移学习的模拟推断方法，显著减少了高保真模拟的需求，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统模拟推断（SBI）依赖大量高保真模拟数据，计算成本高昂。本文旨在通过结合低保真模拟数据减少对高保真模拟的需求。

Method: 采用多保真度迁移学习，结合低保真模拟和少量高保真模拟进行训练。实验基于CAMELS多场数据集中的暗物质密度图。

Result: 预训练在暗物质模拟上可将高保真流体模拟需求减少8至15倍，具体取决于模型复杂度、后验维度和性能指标。

Conclusion: 该方法通过利用廉价模拟，实现了高性能且准确的推断，同时大幅降低了计算成本。

Abstract: Simulation-based inference (SBI) enables cosmological parameter estimation
when closed-form likelihoods or models are unavailable. However, SBI relies on
machine learning for neural compression and density estimation. This requires
large training datasets which are prohibitively expensive for high-quality
simulations. We overcome this limitation with multifidelity transfer learning,
combining less expensive, lower-fidelity simulations with a limited number of
high-fidelity simulations. We demonstrate our methodology on dark matter
density maps from two separate simulation suites in the hydrodynamical CAMELS
Multifield Dataset. Pre-training on dark-matter-only $N$-body simulations
reduces the required number of high-fidelity hydrodynamical simulations by a
factor between $8$ and $15$, depending on the model complexity, posterior
dimensionality, and performance metrics used. By leveraging cheaper
simulations, our approach enables performant and accurate inference on
high-fidelity models while substantially reducing computational costs.

</details>


### [405] [Wavelet Flow For Extragalactic Foreground Simulations](https://arxiv.org/abs/2505.21220)
*M. Mebratu,W. L. K. Wu*

Main category: astro-ph.CO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extragalactic foregrounds in cosmic microwave background (CMB) observations
are both a source of cosmological and astrophysical information and a nuisance
to the CMB. Effective field-level modeling that captures their non-Gaussian
statistical distributions is increasingly important for optimal information
extraction, particularly given the precise and low-noise observations from
current and upcoming experiments. We explore the use of Wavelet Flow (WF)
models to tackle the novel task of modeling the field-level probability
distributions of multi-component CMB secondaries. Specifically, we jointly
train correlated CMB lensing convergence ($\kappa$) and cosmic infrared
background (CIB) maps with a WF model and obtain a network that statistically
recovers the input to high accuracy -- the trained network generates samples of
$\kappa$ and CIB fields whose average power spectra are within a few percent of
the inputs across all scales, and whose Minkowski functionals are similarly
accurate compared to the inputs. Leveraging the multiscale architecture of
these models, we fine-tune both the model parameters and the priors at each
scale independently, optimizing performance across different resolutions. These
results demonstrate that WF models can accurately simulate correlated
components of CMB secondaries, supporting improved analysis of cosmological
data. Our code and trained models can be found here
(https://github.com/matiwosm/HybridPriorWavletFlow.git).

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [406] [Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low](https://arxiv.org/abs/2505.20377)
*Lennart Ullner,Alona Zharova,Felix Creutzig*

Main category: eess.SY

TL;DR: 论文研究了如何通过深度强化学习（DRL）等方法优化家庭能源管理（HEM），特别是在电动汽车（EV）充电和光伏（PV）剩余利用方面。结果显示，DRL在高潜力家庭中能显著提升能源管理和成本节约。


<details>
  <summary>Details</summary>
Motivation: 在能源转型背景下，家庭能源管理（HEM）对缓解电网压力至关重要。然而，家庭如何优化EV充电仍不明确。

Method: 研究分析了90个德语国家家庭的真实数据，比较了DRL、基于规则的控制和模型预测控制在HEM中的应用。

Result: DRL能有效协调EV充电和电池存储与PV剩余。高电池容量家庭自优化能力强，而低容量家庭通过DRL显著提升管理和节约。

Conclusion: 具有优化潜力的家庭可通过DRL受益，进而推动电力系统脱碳。

Abstract: Efficient energy management in prosumer households is key to alleviating grid
stress in an energy transition marked by electric vehicles (EV), renewable
energies and battery storage. However, it is unclear how households optimize
prosumer EV charging. Here we study real-world data from 90 households on
fixed-rate electricity tariffs in German-speaking countries to investigate the
potential of Deep Reinforcement Learning (DRL) and other control approaches
(Rule-Based, Model Predictive Control) to manage the dynamic and uncertain
environment of Home Energy Management (HEM) and optimize household charging
patterns. The DRL agent efficiently aligns charging of EV and battery storage
with photovoltaic (PV) surplus. We find that frequent EV charging transactions,
early EV connections and PV surplus increase optimization potential. A detailed
analysis of nine households (1 hour resolution, 1 year) demonstrates that high
battery capacity facilitates self optimization; in this case further
algorithmic control shows little value. In cases with relatively low battery
capacity, algorithmic control with DRL improves energy management and cost
savings by a relevant margin. This result is further corroborated by our
simulation of a synthetic household. We conclude that prosumer households with
optimization potential would profit from DRL, thus benefiting also the full
electricity system and its decarbonization.

</details>


### [407] [Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics](https://arxiv.org/abs/2505.20370)
*Martine Dyring Hansen,Elena Celledoni,Benjamin Kwanen Tapley*

Main category: eess.SY

TL;DR: 提出一种数据驱动方法，直接从位置测量中学习机械系统的运动方程，无需速度数据。


<details>
  <summary>Details</summary>
Motivation: 在系统识别任务中，通常只有位置信息可用（如动作捕捉、像素数据或低分辨率跟踪），因此需要一种无需速度数据的方法。

Method: 利用离散拉格朗日-达朗贝尔原理和受迫离散欧拉-拉格朗日方程构建物理基础模型，将动力学分解为保守和非保守部分，分别用神经网络学习。

Result: 在合成和真实数据集上验证了方法的有效性，能够重建并分离保守和受迫动力学，提供可解释且物理一致的预测。

Conclusion: 该方法在仅有位置数据的情况下，能够有效学习并预测机械系统的动力学行为。

Abstract: We introduce a data-driven method for learning the equations of motion of
mechanical systems directly from position measurements, without requiring
access to velocity data. This is particularly relevant in system identification
tasks where only positional information is available, such as motion capture,
pixel data or low-resolution tracking. Our approach takes advantage of the
discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange
equations to construct a physically grounded model of the system's dynamics. We
decompose the dynamics into conservative and non-conservative components, which
are learned separately using feed-forward neural networks. In the absence of
external forces, our method reduces to a variational discretization of the
action principle naturally preserving the symplectic structure of the
underlying Hamiltonian system. We validate our approach on a variety of
synthetic and real-world datasets, demonstrating its effectiveness compared to
baseline methods. In particular, we apply our model to (1) measured human
motion data and (2) latent embeddings obtained via an autoencoder trained on
image sequences. We demonstrate that we can faithfully reconstruct and separate
both the conservative and forced dynamics, yielding interpretable and
physically consistent predictions.

</details>


### [408] [Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning](https://arxiv.org/abs/2505.21026)
*Runze Lin,Junghui Chen,Biao Huang,Lei Xie,Hongye Su*

Main category: eess.SY

TL;DR: 本文提出了一种结合逆向强化学习和多任务学习的框架，用于数据驱动的多模式控制设计，解决了传统强化学习对数字孪生和奖励函数设计的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0和智能制造时代，过程系统工程需要适应数字化转型。传统强化学习在过程控制中的应用受限于对准确数字孪生和精心设计奖励函数的依赖。

Method: 通过逆向强化学习从历史闭环数据中提取最优奖励函数和控制策略，并结合多任务学习，引入潜在上下文变量以区分模式，训练模式特定的控制器。

Result: 在连续搅拌釜反应器和分批补料生物反应器的案例研究中验证了该框架在多模式数据处理和适应性控制器训练中的有效性。

Conclusion: 该框架为多模式过程控制提供了一种数据驱动的方法，具有实际应用的潜力。

Abstract: In the era of Industry 4.0 and smart manufacturing, process systems
engineering must adapt to digital transformation. While reinforcement learning
offers a model-free approach to process control, its applications are limited
by the dependence on accurate digital twins and well-designed reward functions.
To address these limitations, this paper introduces a novel framework that
integrates inverse reinforcement learning (IRL) with multi-task learning for
data-driven, multi-mode control design. Using historical closed-loop data as
expert demonstrations, IRL extracts optimal reward functions and control
policies. A latent-context variable is incorporated to distinguish modes,
enabling the training of mode-specific controllers. Case studies on a
continuous stirred tank reactor and a fed-batch bioreactor validate the
effectiveness of this framework in handling multi-mode data and training
adaptable controllers.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [409] [LazyVLM: Neuro-Symbolic Approach to Video Analytics](https://arxiv.org/abs/2505.21459)
*Xiangru Jian,Wei Pang,Zhengyuan Dong,Chao Zhang,M. Tamer Özsu*

Main category: cs.DB

TL;DR: LazyVLM是一个结合神经符号方法的视频分析系统，提供类似VLM的用户友好查询接口，同时解决了VLM的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频分析方法在灵活性和效率之间存在权衡，VLM处理长上下文时计算成本高，而神经符号方法依赖手动标注和固定规则。

Method: LazyVLM通过半结构化文本接口分解多帧视频查询为细粒度操作，利用高效关系查询和向量相似性搜索处理。

Result: LazyVLM为大规模开放域视频数据提供了稳健、高效且用户友好的查询解决方案。

Conclusion: LazyVLM成功平衡了灵活性与效率，解决了现有方法的局限性。

Abstract: Current video analytics approaches face a fundamental trade-off between
flexibility and efficiency. End-to-end Vision Language Models (VLMs) often
struggle with long-context processing and incur high computational costs, while
neural-symbolic methods depend heavily on manual labeling and rigid rule
design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics
system that provides a user-friendly query interface similar to VLMs, while
addressing their scalability limitation. LazyVLM enables users to effortlessly
drop in video data and specify complex multi-frame video queries using a
semi-structured text interface for video analytics. To address the scalability
limitations of VLMs, LazyVLM decomposes multi-frame video queries into
fine-grained operations and offloads the bulk of the processing to efficient
relational query execution and vector similarity search. We demonstrate that
LazyVLM provides a robust, efficient, and user-friendly solution for querying
open-domain video data at scale.

</details>


### [410] [Streamlining Knowledge Graph Creation with PyRML](https://arxiv.org/abs/2505.20949)
*Andrea Giovanni Nuzzolese*

Main category: cs.DB

TL;DR: PyRML是一个轻量级的Python库，用于通过声明式映射构建知识图谱，支持RML核心功能，并与Python生态系统无缝集成。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）在多个领域的数据集成中越来越重要，但现有工具在灵活性和易用性上存在不足。PyRML旨在降低KG构建的门槛，提升可重复性和模块化。

Method: PyRML是一个Python原生库，支持RML核心功能，提供编程接口用于创建、执行和测试映射，并与Pandas和RDFlib等库集成。

Result: PyRML实现了声明式语义与实际KG工程的结合，支持透明和模块化的工作流程。

Conclusion: PyRML通过简化KG构建过程，为数据集成提供了更高效和可扩展的解决方案。

Abstract: Knowledge Graphs (KGs) are increasingly adopted as a foundational technology
for integrating heterogeneous data in domains such as climate science, cultural
heritage, and the life sciences. Declarative mapping languages like R2RML and
RML have played a central role in enabling scalable and reusable KG
construction, offering a transparent means of transforming structured and
semi-structured data into RDF. In this paper, we present PyRML, a lightweight,
Python-native library for building Knowledge Graphs through declarative
mappings. PyRML supports core RML constructs and provides a programmable
interface for authoring, executing, and testing mappings directly within Python
environments. It integrates with popular data and semantic web libraries (e.g.,
Pandas and RDFlib), enabling transparent and modular workflows. By lowering the
barrier to entry for KG creation and fostering reproducible, ontology-aligned
data integration, PyRML bridges the gap between declarative semantics and
practical KG engineering.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [411] [InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling](https://arxiv.org/abs/2505.20600)
*Xiaoxiao Jiang,Suyi Li,Lingyun Yang,Tianyu Feng,Zhipeng Di,Weiyi Lu,Guoxuan Zhu,Xiu Lin,Kan Liu,Yinghao Yu,Tao Lan,Guodong Yang,Lin Qu,Liping Zhang,Wei Wang*

Main category: cs.DC

TL;DR: InstGenIE是一个高效处理图像编辑请求的系统，通过跳过未掩码区域的计算、优化缓存加载和采用连续批处理策略，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在生成式图像编辑中，掩码区域的处理通常需要大量计算，而未掩码区域的计算是冗余的。InstGenIE旨在通过优化这些冗余计算和缓存加载，提高系统效率。

Method: InstGenIE通过重用缓存的中间激活跳过未掩码区域的计算，采用无气泡流水线方案重叠计算与缓存加载，并设计连续批处理策略和负载均衡机制。

Result: InstGenIE在图像编辑任务中实现了最高3倍的吞吐量提升和14.7倍的平均请求延迟降低，同时保证了图像质量。

Conclusion: InstGenIE通过优化计算和缓存管理，显著提升了扩散模型在图像编辑任务中的服务效率，为实际生产环境提供了高效解决方案。

Abstract: Generative image editing using diffusion models has become a prevalent
application in today's AI cloud services. In production environments, image
editing typically involves a mask that specifies the regions of an image
template to be edited. The use of masks provides direct control over the
editing process and introduces sparsity in the model inference. In this paper,
we present InstGenIE, a system that efficiently serves image editing requests.
The key insight behind InstGenIE is that image editing only modifies the masked
regions of image templates while preserving the original content in the
unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant
computations associated with the unmasked areas by reusing cached intermediate
activations from previous inferences. To mitigate the high cache loading
overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps
computation with cache loading. Additionally, to reduce queuing latency in
online serving while improving the GPU utilization, InstGenIE proposes a novel
continuous batching strategy for diffusion model serving, allowing newly
arrived requests to join the running batch in just one step of denoising
computation, without waiting for the entire batch to complete. As heterogeneous
masks induce imbalanced loads, InstGenIE also develops a load balancing
strategy that takes into account the loads of both computation and cache
loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving
systems for image editing, achieving up to 3x higher throughput and reducing
average request latency by up to 14.7x while ensuring image quality.

</details>


### [412] [Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures](https://arxiv.org/abs/2505.20705)
*Yang Wang,Wenxuan Zhu,Xuehui Quan,Heyi Wang,Chang Liu,Qiyuan Wu*

Main category: cs.DC

TL;DR: 提出了一种基于时序特征学习的智能预测方法，用于解决分布式系统中的故障预测和响应延迟问题。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中故障预测和响应延迟是重要挑战，需要高效的方法来提前识别潜在故障。

Method: 使用GRU建模系统状态的时间演化，结合注意力机制增强关键时段，并通过前馈神经网络进行分类。

Result: 实验表明，该方法在准确性、F1分数和AUC上优于主流时序模型，具有强预测能力和稳定性。

Conclusion: 该方法能有效学习系统行为模式，实现高效故障检测，训练过程收敛且可靠。

Abstract: This paper addresses the challenges of fault prediction and delayed response
in distributed systems by proposing an intelligent prediction method based on
temporal feature learning. The method takes multi-dimensional performance
metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the
evolution of system states over time. An attention mechanism is then applied to
enhance key temporal segments, improving the model's ability to identify
potential faults. On this basis, a feedforward neural network is designed to
perform the final classification, enabling early warning of system failures. To
validate the effectiveness of the proposed approach, comparative experiments
and ablation analyses were conducted using data from a large-scale real-world
cloud system. The experimental results show that the model outperforms various
mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This
demonstrates strong prediction capability and stability. Furthermore, the loss
function curve confirms the convergence and reliability of the training
process. It indicates that the proposed method effectively learns system
behavior patterns and achieves efficient fault detection.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [413] [Scattering Networks on Noncommutative Finite Groups](https://arxiv.org/abs/2505.20950)
*Maria Teresa Arias,Davide Barbieri,Eugenio Hernández*

Main category: math.NA

TL;DR: 该论文提出了一种在有限群上的散射变换，用于群等变卷积神经网络（G-CNNs），并分析了其在分类任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究散射变换在非欧几里得空间（有限群）上的扩展，以增强卷积神经网络的理论基础和应用范围。

Method: 提出有限群上的小波和散射变换，分析其性质（如非扩张性、稳定性、能量保持和等变性），并在分类任务中验证其效果。

Result: 散射变换在有限群上具有与经典小波相似的性质，且在分类任务中表现出色。

Conclusion: 该研究为群等变卷积神经网络提供了新的理论工具，并展示了其在处理非欧几里得数据时的潜力。

Abstract: Scattering Networks were initially designed to elucidate the behavior of
early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and
are grounded in wavelets. In this work, we introduce a scattering transform on
an arbitrary finite group (not necessarily abelian) within the context of
group-equivariant convolutional neural networks (G-CNNs). We present wavelets
on finite groups and analyze their similarity to classical wavelets. We
demonstrate that, under certain conditions in the wavelet coefficients, the
scattering transform is non-expansive, stable under deformations, preserves
energy, equivariant with respect to left and right group translations, and, as
depth increases, the scattering coefficients are less sensitive to group
translations of the signal, all desirable properties of convolutional neural
networks. Furthermore, we provide examples illustrating the application of the
scattering transform to classify data with domains involving abelian and
nonabelian groups.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [414] [Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach](https://arxiv.org/abs/2505.21139)
*Subhagata Chattopadhyay,Amit K Chattopadhyay*

Main category: q-bio.PE

TL;DR: 该研究采用混合机器学习方法分析COVID-19后心血管事件风险，揭示13个关键心脏病发作风险因素及其易感性，并识别高风险人群。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行显著增加了40岁以上人群感染后心血管事件（如心肌梗死）的发生率，但其机制尚不明确。

Method: 结合人口统计、生化、心电图和铊负荷试验数据，使用聚类算法将人群分为‘高风险’和‘非高风险’组。

Result: 研究发现13个风险因素与心脏病发作概率密切相关，尤其是绝经后患者因雌激素减少和外部压力（如焦虑）导致风险加剧。

Conclusion: 研究为COVID-19后心血管风险评估提供了新方法，并揭示了传统数据建模难以捕捉的风险因素。

Abstract: The COVID-19 pandemic has significantly increased the incidence of
post-infection cardiovascular events, particularly myocardial infarction, in
individuals over 40. While the underlying mechanisms remain elusive, this study
employs a hybrid machine learning approach to analyze epidemiological data in
assessing 13 key heart attack risk factors and their susceptibility. Based on a
unique dataset that combines demographic, biochemical, ECG, and thallium
stress-tests, this study categorizes distinct subpopulations against varying
risk profiles and then divides the population into 'at-risk' (AR) and
'not-at-risk' (NAR) groups using clustering algorithms. The study reveals
strong association between the likelihood of experiencing a heart attack on the
13 risk factors studied. The aggravated risk for postmenopausal patients
indicates compromised individual risk factors due to estrogen depletion that
may be, further compromised by extraneous stress impacts, like anxiety and
fear, aspects that have traditionally eluded data modeling predictions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [415] [Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs](https://arxiv.org/abs/2505.20638)
*Wenhao You,Xingjian Diao,Chunhui Zhang,Keyi Kong,Weiyi Wu,Zhongyu Ouyang,Chiyu Ma,Tingxuan Wu,Noah Wei,Zong Ke,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.SD

TL;DR: 本文探讨了音乐音频视觉问答（Music AVQA）领域的挑战，提出需要专门的处理方法和架构设计，并总结了有效的设计模式。


<details>
  <summary>Details</summary>
Motivation: 音乐AVQA因其复杂的时空动态和领域知识需求，需要针对性的方法，而现有多模态大语言模型难以满足。

Method: 通过系统分析Music AVQA数据集和方法，提出专门的输入处理、时空设计架构和音乐特定建模策略。

Result: 研究总结了与高性能相关的设计模式，并提出了未来结合音乐先验的具体方向。

Conclusion: 本文旨在为音乐多模态理解奠定基础，并激发更多研究关注，同时提供了相关论文的GitHub资源。

Abstract: While recent Multimodal Large Language Models exhibit impressive capabilities
for general multimodal tasks, specialized domains like music necessitate
tailored approaches. Music Audio-Visual Question Answering (Music AVQA)
particularly underscores this, presenting unique challenges with its
continuous, densely layered audio-visual content, intricate temporal dynamics,
and the critical need for domain-specific knowledge. Through a systematic
analysis of Music AVQA datasets and methods, this position paper identifies
that specialized input processing, architectures incorporating dedicated
spatial-temporal designs, and music-specific modeling strategies are critical
for success in this domain. Our study provides valuable insights for
researchers by highlighting effective design patterns empirically linked to
strong performance, proposing concrete future directions for incorporating
musical priors, and aiming to establish a robust foundation for advancing
multimodal musical understanding. This work is intended to inspire broader
attention and further research, supported by a continuously updated anonymous
GitHub repository of relevant papers:
https://github.com/xid32/Survey4MusicAVQA.

</details>


### [416] [Can Large Language Models Predict Audio Effects Parameters from Natural Language?](https://arxiv.org/abs/2505.20770)
*Seungheon Doh,Junghyun Koo,Marco A. Martínez-Ramírez,Wei-Hsiang Liao,Juhan Nam,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: LLM2Fx利用大型语言模型（LLMs）直接从文本描述预测音频效果参数，无需任务特定训练，支持零样本生成。


<details>
  <summary>Details</summary>
Motivation: 降低非专业人士在音乐制作中的技术门槛，通过自然语言操控音频效果参数。

Method: 通过映射自然语言描述到均衡和混响效果参数，结合上下文示例（音频DSP特征、DSP函数代码和少样本示例）提升性能。

Result: LLM生成的参数优于传统优化方法，能有效将自然语言翻译为合适的音频效果设置。

Conclusion: LLMs可作为文本驱动接口，推动更直观、易用的音乐制作工具发展。

Abstract: In music production, manipulating audio effects (Fx) parameters through
natural language has the potential to reduce technical barriers for
non-experts. We present LLM2Fx, a framework leveraging Large Language Models
(LLMs) to predict Fx parameters directly from textual descriptions without
requiring task-specific training or fine-tuning. Our approach address the
text-to-effect parameter prediction (Text2Fx) task by mapping natural language
descriptions to the corresponding Fx parameters for equalization and
reverberation. We demonstrate that LLMs can generate Fx parameters in a
zero-shot manner that elucidates the relationship between timbre semantics and
audio effects in music production. To enhance performance, we introduce three
types of in-context examples: audio Digital Signal Processing (DSP) features,
DSP function code, and few-shot examples. Our results demonstrate that
LLM-based Fx parameter generation outperforms previous optimization approaches,
offering competitive performance in translating natural language descriptions
to appropriate Fx settings. Furthermore, LLMs can serve as text-driven
interfaces for audio production, paving the way for more intuitive and
accessible music production tools.

</details>


### [417] [VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin](https://arxiv.org/abs/2505.21445)
*Zhiqi Ai,Meixuan Bao,Zhiyong Chen,Zhi Yang,Xinnuo Li,Shugong Xu*

Main category: cs.SD

TL;DR: VoxAging数据集收集了293名说话者多年（最长17年）的语音数据，研究了说话者老化对语音验证系统的影响，并分析了年龄组和性别等因素。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏长期、大规模的纵向数据，说话者老化研究面临挑战，VoxAging数据集填补了这一空白。

Method: 收集了293名说话者（226名英语和67名普通话）多年每周记录的语音数据，最长跨度达17年。

Result: 分析了说话者老化现象及其对语音验证系统的影响，探讨了年龄组和性别等因素的作用。

Conclusion: VoxAging数据集为说话者老化研究提供了重要资源，揭示了老化对语音验证系统的影响。

Abstract: The performance of speaker verification systems is adversely affected by
speaker aging. However, due to challenges in data collection, particularly the
lack of sustained and large-scale longitudinal data for individuals, research
on speaker aging remains difficult. In this paper, we present VoxAging, a
large-scale longitudinal dataset collected from 293 speakers (226 English
speakers and 67 Mandarin speakers) over several years, with the longest time
span reaching 17 years (approximately 900 weeks). For each speaker, the data
were recorded at weekly intervals. We studied the phenomenon of speaker aging
and its effects on advanced speaker verification systems, analyzed individual
speaker aging processes, and explored the impact of factors such as age group
and gender on speaker aging research.

</details>


### [418] [VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion](https://arxiv.org/abs/2505.20794)
*Joon-Seung Choi,Dong-Min Byun,Hyung-Seok Oh,Seong-Whan Lee*

Main category: cs.SD

TL;DR: VibE-SVC是一种可控的歌声转换模型，通过离散小波变换提取和操纵颤音，实现更灵活的歌声风格转换。


<details>
  <summary>Details</summary>
Motivation: 颤音在表达情感和增强音乐深度中起关键作用，但因其动态特性难以建模和控制。

Method: 使用离散小波变换分解F0轮廓，显式提取和操纵颤音，实现精确转换。

Result: 实验表明，VibE-SVC能有效转换歌声风格并保持说话人相似性，主观和客观评估均证实高质量转换。

Conclusion: VibE-SVC通过显式颤音控制，提升了歌声转换的灵活性和表现力。

Abstract: Controlling singing style is crucial for achieving an expressive and natural
singing voice. Among the various style factors, vibrato plays a key role in
conveying emotions and enhancing musical depth. However, modeling vibrato
remains challenging due to its dynamic nature, making it difficult to control
in singing voice conversion. To address this, we propose VibESVC, a
controllable singing voice conversion model that explicitly extracts and
manipulates vibrato using discrete wavelet transform. Unlike previous methods
that model vibrato implicitly, our approach decomposes the F0 contour into
frequency components, enabling precise transfer. This allows vibrato control
for enhanced flexibility. Experimental results show that VibE-SVC effectively
transforms singing styles while preserving speaker similarity. Both subjective
and objective evaluations confirm high-quality conversion.

</details>


### [419] [Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/abs/2505.20868)
*Nam-Gyu Kim,Deok-Hyeon Cho,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: Spotlight-TTS通过专注于有声区域的风格提取和方向调整，提升了语音合成的表现力和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成高质量表现力语音方面仍存在挑战。

Method: 提出Spotlight-TTS，采用有声感知风格提取和风格方向调整。

Result: 实验表明Spotlight-TTS在表现力、语音质量和风格迁移能力上优于基线模型。

Conclusion: Spotlight-TTS通过优化风格提取和调整，显著提升了语音合成的表现力和质量。

Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse
methods based on style embedding extracted from reference speech. However,
synthesizing high-quality expressive speech remains challenging. We propose
Spotlight-TTS, which exclusively emphasizes style via voiced-aware style
extraction and style direction adjustment. Voiced-aware style extraction
focuses on voiced regions highly related to style while maintaining continuity
across different speech regions to improve expressiveness. We adjust the
direction of the extracted style for optimal integration into the TTS model,
which improves speech quality. Experimental results demonstrate that
Spotlight-TTS achieves superior performance compared to baseline models in
terms of expressiveness, overall speech quality, and style transfer capability.
Our audio samples are publicly available.

</details>


### [420] [Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection](https://arxiv.org/abs/2505.20956)
*Shiqi Zhang,Tuomas Virtanen*

Main category: cs.SD

TL;DR: 论文提出了一种基于主动学习的方法MFFT，用于解决生物声学事件检测中的标注数据不足等问题，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 生物声学事件检测面临标注数据有限、事件稀疏、物种多样性和类别不平衡等挑战，需要高效利用有限标注预算的方法。

Method: 采用MFFT（一种结合委员会投票分歧和多样性分析的主动学习方法），并优化了现有数据集以评估主动学习算法。

Result: MFFT在冷启动和热启动场景下分别达到68%和71%的mAP（接近全监督的75%），仅需2.3%的标注数据。

Conclusion: MFFT在冷启动和稀有物种检测中表现优异，对濒危物种监测具有重要实用价值。

Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity
conservation but faces practical challenges during model development and
training: limited amounts of annotated data, sparse events, species diversity,
and class imbalance. To address these challenges efficiently with a limited
labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an
active learning method integrating committee voting disagreement and diversity
analysis. We also refine an existing BioSED dataset specifically for evaluating
active learning algorithms. Experimental results demonstrate that MFFT achieves
a mAP of 68% when cold-starting and 71% when warm-starting (which is close to
the fully-supervised mAP of 75%) while using only 2.3% of the annotations.
Notably, MFFT excels in cold-start scenarios and with rare species, which are
critical for monitoring endangered species, demonstrating its practical value.

</details>


### [421] [Training Articulatory Inversion Models for Inter-Speaker Consistency](https://arxiv.org/abs/2505.20529)
*Charles McGhee,Mark J. F. Gales,Kate M. Knill*

Main category: cs.SD

TL;DR: 研究探讨了基于自监督学习（SSL）的单说话人和多说话人数据训练的声学-发音反演（AAI）模型在英语和俄语中是否产生跨说话人一致的发音目标，并提出了一种新的评估方法和训练方法以提高一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨发音目标是否在不同说话人之间具有一致性，并改进跨说话人一致性。

Method: 使用基于SSL的模型，结合单说话人和多说话人数据训练，提出新的评估方法（最小对集）和训练方法（仅使用语音数据）。

Result: SSL-adapted模型在单说话人和多说话人数据上训练的发音目标在跨说话人一致性方面表现不同。

Conclusion: 研究为AAI模型的跨说话人一致性提供了新见解，并提出了改进方法。

Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse
mapping from speech to articulation. Exact articulatory prediction from speech
alone may be impossible, as speakers can choose different forms of articulation
seemingly without reference to their vocal tract structure. However, once a
speaker has selected an articulatory form, their productions vary minimally.
Recent works in AAI have proposed adapting Self-Supervised Learning (SSL)
models to single-speaker datasets, claiming that these single-speaker models
provide a universal articulatory template. In this paper, we investigate
whether SSL-adapted models trained on single and multi-speaker data produce
articulatory targets which are consistent across speaker identities for English
and Russian. We do this through the use of a novel evaluation method which
extracts articulatory targets using minimal pair sets. We also present a
training method which can improve inter-speaker consistency using only speech
data.

</details>


### [422] [Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization](https://arxiv.org/abs/2505.20961)
*Yiyuan Yang,Shitong Xu,Niki Trigoni,Andrew Markham*

Main category: cs.SD

TL;DR: 论文提出了一种基于稀疏交叉注意力、预训练和自适应信号一致性度量的新型3D声源定位框架，解决了现有方法计算成本高和校准要求严格的问题。


<details>
  <summary>Details</summary>
Motivation: 现有声源定位方法在动态或资源受限环境中部署受限，因计算成本高和校准要求严格。

Method: 采用稀疏交叉注意力、预训练和自适应信号一致性度量，减少输入麦克风数量，同时容忍不可靠或未知的麦克风位置输入。

Result: 初步实验表明，该框架在多源定位中具有可扩展性，无需额外硬件。

Conclusion: 该工作通过平衡性能和效率，提高了声源定位在现实场景中的鲁棒性。

Abstract: Sound source localization (SSL) is a critical technology for determining the
position of sound sources in complex environments. However, existing methods
face challenges such as high computational costs and precise calibration
requirements, limiting their deployment in dynamic or resource-constrained
environments. This paper introduces a novel 3D SSL framework, which uses sparse
cross-attention, pretraining, and adaptive signal coherence metrics, to achieve
accurate and computationally efficient localization with fewer input
microphones. The framework is also fault-tolerant to unreliable or even unknown
microphone position inputs, ensuring its applicability in real-world scenarios.
Preliminary experiments demonstrate its scalability for multi-source
localization without requiring additional hardware. This work advances SSL by
balancing the model's performance and efficiency and improving its robustness
for real-world scenarios.

</details>


### [423] [MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection](https://arxiv.org/abs/2505.20979)
*Tongyu Lu,Charlotta-Marlena Geist,Jan Melechovsky,Abhinaba Roy,Dorien Herremans*

Main category: cs.SD

TL;DR: MelodySim是一个专注于旋律相似性的音乐相似性模型和数据集，用于抄袭检测。通过增强Slakh2100数据集生成变体，并开发了一个基于MERT编码器和三元神经网络的模型。


<details>
  <summary>Details</summary>
Motivation: 解决音乐抄袭检测中旋律相似性识别的问题，提供更准确的检测工具。

Method: 1. 构建数据集：通过修改Slakh2100生成旋律保留的变体；2. 开发模型：使用MERT编码器和三元神经网络检测旋律相似性。

Result: 模型在MelodySim测试集上表现高准确率，能有效识别抄袭片段。

Conclusion: MelodySim为音乐抄袭检测提供了有效的旋律相似性分析工具。

Abstract: We propose MelodySim, a melody-aware music similarity model and dataset for
plagiarism detection. First, we introduce a novel method to construct a dataset
with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI
dataset, we generate variations of each piece while preserving the melody
through modifications such as note splitting, arpeggiation, minor track dropout
(excluding bass), and re-instrumentation. A user study confirms that positive
pairs indeed contain similar melodies, with other musical tracks significantly
changed. Second, we develop a segment-wise melodic-similarity detection model
that uses a MERT encoder and applies a triplet neural network to capture
melodic similarity. The resultant decision matrix highlights where plagiarism
might occur. Our model achieves high accuracy on the MelodySim test set.

</details>


### [424] [Text-Queried Audio Source Separation via Hierarchical Modeling](https://arxiv.org/abs/2505.21025)
*Xinlei Yin,Xiulian Peng,Xue Jiang,Zhiwei Xiong,Yan Lu*

Main category: cs.SD

TL;DR: HSM-TSS提出了一种分层分解框架，通过全局-局部语义引导的特征分离和结构保持的声学重建，解决了音频源分离中联合建模声学-文本对齐和语义感知分离的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在单阶段架构中联合建模声学-文本对齐和语义感知分离，且依赖大规模标注数据。

Method: HSM-TSS采用双阶段机制，先在全局语义特征空间对齐文本查询，再在局部语义特征空间分离AudioMAE特征，并进行声学重建。

Result: 该方法在数据高效训练下实现了最先进的分离性能，并在复杂听觉场景中保持与查询的语义一致性。

Conclusion: HSM-TSS通过分层分解和双阶段机制，显著提升了音频源分离的性能和灵活性。

Abstract: Target audio source separation with natural language queries presents a
promising paradigm for extracting arbitrary audio events through arbitrary text
descriptions. Existing methods mainly face two challenges, the difficulty in
jointly modeling acoustic-textual alignment and semantic-aware separation
within a blindly-learned single-stage architecture, and the reliance on
large-scale accurately-labeled training data to compensate for inefficient
cross-modal learning and separation. To address these challenges, we propose a
hierarchical decomposition framework, HSM-TSS, that decouples the task into
global-local semantic-guided feature separation and structure-preserving
acoustic reconstruction. Our approach introduces a dual-stage mechanism for
semantic separation, operating on distinct global and local semantic feature
spaces. We first perform global-semantic separation through a global semantic
feature space aligned with text queries. A Q-Audio architecture is employed to
align audio and text modalities, serving as pretrained global-semantic
encoders. Conditioned on the predicted global feature, we then perform the
second-stage local-semantic separation on AudioMAE features that preserve
time-frequency structures, followed by acoustic reconstruction. We also propose
an instruction processing pipeline to parse arbitrary text queries into
structured operations, extraction or removal, coupled with audio descriptions,
enabling flexible sound manipulation. Our method achieves state-of-the-art
separation performance with data-efficient training while maintaining superior
semantic consistency with queries in complex auditory scenes.

</details>


### [425] [Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation](https://arxiv.org/abs/2505.20745)
*Jingping Nie,Dung T. Tran,Karan Thakkar,Vasudha Kowtha,John Huang,Carlos Avendano,Erdrin Azemi,Vikramjit Mitra*

Main category: cs.SD

TL;DR: 研究探讨了预训练声学基础模型（FMs）在心音听诊中的应用，发现其表现与基线方法相当，其中自研CLAP模型的音频编码器在心率估计中表现更优。


<details>
  <summary>Details</summary>
Motivation: 心音听诊是一种非侵入性技术，但预训练声学基础模型在此领域的应用尚未充分探索。

Method: 使用公开的心音图数据集和心率估计模型，对六种声学基础模型（HuBERT、wav2vec2、wavLM、Whisper、CLAP及自研CLAP模型）进行分层分析，并与基线方法对比。

Result: 预训练基础模型的表征向量与基线方法表现相当，自研CLAP模型的音频编码器在心率估计中表现更优，平均绝对误差更低。

Conclusion: 预训练声学基础模型在心音听诊中具有潜力，尤其是自研CLAP模型的表现突出。

Abstract: Auscultation, particularly heart sound, is a non-invasive technique that
provides essential vital sign information. Recently, self-supervised acoustic
representation foundation models (FMs) have been proposed to offer insights
into acoustics-based vital signs. However, there has been little exploration of
the extent to which auscultation is encoded in these pre-trained FM
representations. In this work, using a publicly available phonocardiogram (PCG)
dataset and a heart rate (HR) estimation model, we conduct a layer-wise
investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,
Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP
model. Additionally, we implement the baseline method from Nie et al., 2024
(which relies on acoustic features) and show that overall, representation
vectors from pre-trained foundation models (FMs) offer comparable performance
to the baseline. Notably, HR estimation using the representations from the
audio encoder of the in-house CLAP model outperforms the results obtained from
the baseline, achieving a lower mean absolute error (MAE) across various
train/validation/test splits despite the domain mismatch.

</details>


### [426] [Model as Loss: A Self-Consistent Training Paradigm](https://arxiv.org/abs/2505.21156)
*Saisamarth Rajesh Phaye,Milos Cernak,Andrew Harper*

Main category: cs.SD

TL;DR: 论文提出了一种名为'Model as Loss'的新训练范式，利用模型自身的编码器作为损失函数来指导训练，以解决传统方法难以捕捉信号细微特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强方法依赖手工设计的损失函数或预训练的深度特征损失，这些方法往往无法捕捉信号的关键细微特性，影响性能。

Method: 提出'Model as Loss'范式，利用模型编码器的任务特定特征空间作为损失函数，优化解码器输出与干净信号的感知和任务相关特性一致。

Result: 该方法在标准语音增强基准测试中优于预训练的深度特征损失，提供更好的感知质量和泛化能力。

Conclusion: 'Model as Loss'通过自一致性训练范式，显著提升了语音增强的性能和泛化能力。

Abstract: Conventional methods for speech enhancement rely on handcrafted loss
functions (e.g., time or frequency domain losses) or deep feature losses (e.g.,
using WavLM or wav2vec), which often fail to capture subtle signal properties
essential for optimal performance. To address this, we propose Model as Loss, a
novel training paradigm that utilizes the encoder from the same model as a loss
function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature
space, optimizing the decoder to produce output consistent with perceptual and
task-relevant characteristics of the clean signal. By using the encoder's
learned features as a loss function, this framework enforces self-consistency
between the clean reference speech and the enhanced model output. Our approach
outperforms pre-trained deep feature losses on standard speech enhancement
benchmarks, offering better perceptual quality and robust generalization to
both in-domain and out-of-domain datasets.

</details>


### [427] [Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning](https://arxiv.org/abs/2505.21356)
*Whenty Ariyanti,Kuan-Yu Chen,Sabato Marco Siniscalchi,Hsin-Min Wang,Yu Tsao*

Main category: cs.SD

TL;DR: 论文提出VOQANet和VOQANet+，基于深度学习的语音质量评估框架，结合声学特征和语音基础模型，提高评估的客观性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语音质量评估依赖专家主观评分，存在变异性，需要自动化、客观的评估方法。

Method: VOQANet利用语音基础模型提取高层声学信息；VOQANet+进一步结合手工声学特征（如jitter、shimmer等）。

Result: VOQANet+在噪声环境下表现稳健，优于基线方法，句子输入效果优于元音输入。

Conclusion: 结合语音基础模型与声学特征提升了解释性和鲁棒性，适用于实际场景。

Abstract: Objective: Perceptual voice quality assessment plays a critical role in
diagnosing and monitoring voice disorders by providing standardized evaluation
of vocal function. Traditionally, this process relies on expert raters
utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation
of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain
(GRBAS). However, these metrics are inherently subjective and susceptible to
inter-rater variability, motivating the need for automated and objective
assessment methods. Methods: We propose Voice Quality Assessment Network
(VOQANet), a deep learning-based framework with an attention mechanism that
leverages a Speech Foundation Model (SFM) to capture high-level acoustic and
prosodic information from raw speech. To enhance robustness and
interpretability, we present VOQANet+, which integrates handcrafted acoustic
features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM
embeddings. Results: Sentence-based input yields stronger performance than
vowel-based input, especially at the patient level. VOQANet consistently
outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even
better and maintains robustness under noisy conditions. Conclusion: Combining
SFM embeddings with domain-informed acoustic features improves interpretability
and resilience. Significance: VOQANet+ shows strong potential for deployment in
real-world and telehealth settings, addressing the limitations of subjective
perceptual assessments with an interpretable and noise-resilient solution.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [428] [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)
*Dancheng Liu,Amir Nassereldine,Chenhui Xu,Jinjun Xiong*

Main category: cs.CL

TL;DR: 研究发现，ASR模型的泛化能力主要受声学多样性而非语言丰富性驱动，通过声学数据增强可将词错误率降低19.24%。


<details>
  <summary>Details</summary>
Motivation: 探讨训练数据中的语言和声学多样性对ASR模型鲁棒性的影响，以解决大规模数据训练的不可行性问题。

Method: 分析声学和语言多样性对ASR模型的影响，并测试声学增强方法在Librispeech数据集上的效果。

Result: 声学增强显著提升模型泛化能力，词错误率在未见数据集上降低19.24%。

Conclusion: 声学数据增强是构建鲁棒ASR模型的有效替代方案，尤其在缺乏大规模数据时。

Abstract: Whisper's robust performance in automatic speech recognition (ASR) is often
attributed to its massive 680k-hour training set, an impractical scale for most
researchers. In this work, we examine how linguistic and acoustic diversity in
training data affect the robustness of the ASR model and reveal that
transcription generalization is primarily driven by acoustic variation rather
than linguistic richness. We find that targeted acoustic augmentation methods
could significantly improve the generalization ability of ASR models, reducing
word-error rates by up to 19.24 percent on unseen datasets when training on the
960-hour Librispeech dataset. These findings highlight strategic acoustically
focused data augmentation as a promising alternative to massive datasets for
building robust ASR models, offering a potential solution to future foundation
ASR models when massive human speech data is lacking.

</details>


### [429] [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)
*Bertram Højer,Terne Sasha Thorn Jakobsen,Anna Rogers,Stefan Heinrich*

Main category: cs.CL

TL;DR: 论文通过调查303名研究者对“智能”的理解，发现社区最认同的三个标准是泛化、适应性和推理能力。当前NLP系统被视为“智能”的观点占少数（29%），且仅16.2%的研究者将开发智能系统视为研究目标。


<details>
  <summary>Details</summary>
Motivation: 探讨NLP研究中“人工智能”框架下“智能”的具体含义及其在研究议程中的作用。

Method: 对来自NLP、机器学习、认知科学、语言学及神经科学等领域的303名研究者进行问卷调查。

Result: 社区最认同的智能标准为泛化、适应性和推理能力；当前NLP系统被视为“智能”的观点占少数（29%），仅16.2%的研究者将开发智能系统视为目标。

Conclusion: 研究揭示了研究者对“智能”的共识及其与NLP研究目标的关联，表明当前系统被认为“智能”的比例较低。

Abstract: Despite the widespread use of ''artificial intelligence'' (AI) framing in
Natural Language Processing (NLP) research, it is not clear what researchers
mean by ''intelligence''. To that end, we present the results of a survey on
the notion of ''intelligence'' among researchers and its role in the research
agenda. The survey elicited complete responses from 303 researchers from a
variety of fields including NLP, Machine Learning (ML), Cognitive Science,
Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the
community agrees on the most: generalization, adaptability, & reasoning. Our
results suggests that the perception of the current NLP systems as
''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the
respondents see developing intelligent systems as a research goal, and these
respondents are more likely to consider the current systems intelligent.

</details>


### [430] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
*Xiao Liu,Xinyi Dong,Xinyang Gao,Yansong Feng,Xun Pang*

Main category: cs.CL

TL;DR: 论文探讨了如何通过增强大型语言模型（LLM）在生成研究想法时的数据支持，提升生成想法的质量。实验表明，结合元数据和自动验证显著提高了想法的可行性和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生成研究想法方面表现出潜力，但其生成的想法常面临可行性和有效性问题。本文旨在通过数据增强解决这些问题。

Method: 引入两种数据增强方式：1）在生成阶段提供元数据引导LLM；2）在筛选阶段自动验证假设的合理性。实验基于社会科学领域的气候谈判主题。

Result: 元数据使生成想法的可行性提升20%，自动验证使筛选想法的质量提升7%。人类研究表明，增强后的LLM想法能激发更高质量的研究提案。

Conclusion: 数据驱动的LLM辅助研究想法生成具有实际应用潜力，尤其在学术场景中。

Abstract: Recent advancements in large language models (LLMs) have shown promise in
generating novel research ideas. However, these ideas often face challenges
related to feasibility and expected effectiveness. This paper explores how
augmenting LLMs with relevant data during the idea generation process can
enhance the quality of generated ideas. We introduce two ways of incorporating
data: (1) providing metadata during the idea generation stage to guide LLMs
toward feasible directions, and (2) adding automatic validation during the idea
selection stage to assess the empirical plausibility of hypotheses within
ideas. We conduct experiments in the social science domain, specifically with
climate negotiation topics, and find that metadata improves the feasibility of
generated ideas by 20%, while automatic validation improves the overall quality
of selected ideas by 7%. A human study shows that LLM-generated ideas, along
with their related data and validation processes, inspire researchers to
propose research ideas with higher quality. Our work highlights the potential
of data-driven research idea generation, and underscores the practical utility
of LLM-assisted ideation in real-world academic settings.

</details>


### [431] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: 本文提出了一种针对法律规范分析的Graph RAG改进方法，结合知识图谱和文本片段，解决法律数据的复杂性和规模问题。


<details>
  <summary>Details</summary>
Motivation: 法律规范具有层次结构、内外引用网络和多时间版本的特点，传统方法难以处理其复杂性和规模。

Method: 通过结合结构化知识图谱和上下文丰富的文本片段，构建更丰富的法律知识表示。

Result: Graph RAG在法律规范数据集上的应用展示了其在法律研究、立法分析和决策支持中的潜力。

Conclusion: 该方法为人工智能在法律领域的应用提供了新方向，有望提升法律研究和决策的效率。

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.

</details>


### [432] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: 论文提出ShIOEnv环境，通过马尔可夫决策过程模拟命令行交互，结合语法掩码和PPO优化生成高质量数据集，显著提升CodeT5模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集缺乏执行数据（如退出码、输出等），限制了行为建模的实用性，需要一种方法生成更全面的CLI交互数据。

Method: 将命令构建建模为马尔可夫决策过程，结合语法掩码和PPO优化生成数据集，并用于微调CodeT5模型。

Result: 语法掩码和PPO显著提升样本效率，CodeT5的BLEU-4分数提升85%（语法掩码）和26%（PPO）。

Conclusion: ShIOEnv环境和数据集为未来研究提供了工具，展示了语法约束和强化学习在CLI建模中的有效性。

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [433] [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
*Zhewei Yao,Guoheng Sun,Lukasz Borchmann,Zheyu Shen,Minghang Deng,Bohan Zhai,Hao Zhang,Ang Li,Yuxiong He*

Main category: cs.CL

TL;DR: Arctic-Text2SQL-R1是一个基于强化学习的框架，旨在通过轻量级奖励信号生成准确且可执行的SQL，避免了复杂的中间监督和奖励设计。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在SQL生成方面取得了进展，但在复杂查询中生成正确且可执行的SQL仍是一个挑战。

Method: 采用强化学习框架，仅基于执行正确性的轻量级奖励信号，结合精心策划的数据、强监督初始化和有效训练实践。

Result: 在六个Test2SQL基准测试中实现了最先进的执行准确率，7B模型性能超过之前的70B级系统。

Conclusion: 该框架展示了可扩展性和高效性，并通过实验提供了对未来Test2SQL研究的实用指导。

Abstract: Translating natural language into SQL (Test2SQL) is a longstanding challenge
at the intersection of natural language understanding and structured data
access. While large language models (LLMs) have significantly improved fluency
in SQL generation, producing correct and executable SQL--particularly for
complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a
reinforcement learning (RL) framework and model family designed to generate
accurate, executable SQL using a lightweight reward signal based solely on
execution correctness. Our approach avoids brittle intermediate supervision and
complex reward shaping, promoting stable training and alignment with the end
task. Combined with carefully curated data, strong supervised initialization,
and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art
execution accuracy across six diverse Test2SQL benchmarks, including the top
position on the BIRD leaderboard. Notably, our 7B model outperforms prior
70B-class systems, highlighting the framework's scalability and efficiency. We
further demonstrate inference-time robustness through simple extensions like
value retrieval and majority voting. Extensive experiments and ablation studies
offer both positive and negative insights, providing practical guidance for
future Test2SQL research.

</details>


### [434] [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
*Wang Cai,Hsiu-Yuan Huang,Zhixiang Wang,Yunfang Wu*

Main category: cs.CL

TL;DR: DyVec是一种动态向量方法，通过改进ICV方法的局限性，显著提升了任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有ICV方法对ICL特定因素敏感，且使用粗粒度或语义碎片化的表示，限制了其适用性。

Method: DyVec采用EQR策略提取鲁棒语义聚合表示，并通过动态潜在分割和注入优化注入位置。

Result: 实验显示DyVec优于few-shot ICL、LoRA及现有ICV基线。

Conclusion: DyVec为推理时任务适应提供了轻量级且数据高效的解决方案。

Abstract: In-Context derived Vector (ICV) methods extract task-relevant representations
from large language models (LLMs) and reinject them during inference, achieving
comparable performance to few-shot In-Context Learning (ICL) without repeated
demonstration processing. However, existing ICV methods remain sensitive to
ICL-specific factors, often use coarse or semantically fragmented
representations as the source of the vector, and rely on heuristic-based
injection positions, limiting their applicability.
  To address these issues, we propose Dynamic Vector (DyVec), which
incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust
semantically aggregated latent representations by mitigating variance
introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to
adaptively partition representations based on task complexity and leverages
REINFORCE-based optimization to learn optimal injection positions for each
segment.
  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior
ICV baselines. Further analysis highlights the effectiveness of dynamically
segmenting and injecting semantically aggregated latent representations. DyVec
provides a lightweight and data-efficient solution for inference-time task
adaptation.

</details>


### [435] [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
*Satya Narayana Cheetirala,Ganesh Raut,Dhavalkumar Patel,Fabio Sanatana,Robert Freeman,Matthew A Levin,Girish N. Nadkarni,Omar Dawkins,Reba Miller,Randolph M. Steinhagen,Eyal Klang,Prem Timsina*

Main category: cs.CL

TL;DR: RAG方法通过检索最相关文本片段，显著减少LLMs的token使用，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长文本分类中LLMs的token限制和高计算成本问题。

Method: 将临床文档分块并向量化，检索前4,000个相关词段输入LLMs。

Result: RAG方法与全文本处理在分类性能上无显著差异（p > 0.05）。

Conclusion: RAG为长临床文档分析提供了可扩展且经济高效的解决方案。

Abstract: Long text classification is challenging for Large Language Models (LLMs) due
to token limits and high computational costs. This study explores whether a
Retrieval Augmented Generation (RAG) approach using only the most relevant text
segments can match the performance of processing entire clinical notes with
large context LLMs. We begin by splitting clinical documents into smaller
chunks, converting them into vector embeddings, and storing these in a FAISS
index. We then retrieve the top 4,000 words most pertinent to the
classification query and feed these consolidated segments into an LLM. We
evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication
identification task. Metrics such as AUC ROC, precision, recall, and F1 showed
no statistically significant differences between the RAG based approach and
whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can
significantly reduce token usage without sacrificing classification accuracy,
providing a scalable and cost effective solution for analyzing lengthy clinical
documents.

</details>


### [436] [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
*Mathew J. Koretsky,Maya Willey,Adi Asija,Owen Bianchi,Chelsea X. Alvarado,Tanay Nayak,Nicole Kuznetsov,Sungwon Kim,Mike A. Nalls,Daniel Khashabi,Faraz Faghri*

Main category: cs.CL

TL;DR: BiomedSQL是一个专门评估生物医学领域文本到SQL转换中科学推理能力的基准，包含68,000个问题/SQL查询/答案三元组，测试模型在隐含领域推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL系统在将定性科学问题映射为可执行SQL时表现不佳，尤其是在需要隐含领域推理的情况下。

Method: BiomedSQL基于一个整合了基因-疾病关联、组学数据因果推断和药物审批记录的BigQuery知识库，评估了多种开源和闭源LLM的表现。

Result: GPT-o3-mini执行准确率为59.0%，而定制多步代理BMSQL达到62.6%，均远低于专家基准的90.0%。

Conclusion: BiomedSQL为提升文本到SQL系统在生物医学知识库上的推理能力提供了新基础，支持科学发现。

Abstract: Biomedical researchers increasingly rely on large-scale structured databases
for complex analytical tasks. However, current text-to-SQL systems often
struggle to map qualitative scientific questions into executable SQL,
particularly when implicit domain reasoning is required. We introduce
BiomedSQL, the first benchmark explicitly designed to evaluate scientific
reasoning in text-to-SQL generation over a real-world biomedical knowledge
base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in
a harmonized BigQuery knowledge base that integrates gene-disease associations,
causal inference from omics data, and drug approval records. Each question
requires models to infer domain-specific criteria, such as genome-wide
significance thresholds, effect directionality, or trial phase filtering,
rather than rely on syntactic translation alone. We evaluate a range of open-
and closed-source LLMs across prompting strategies and interaction paradigms.
Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%
execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,
both well below the expert baseline of 90.0%. BiomedSQL provides a new
foundation for advancing text-to-SQL systems capable of supporting scientific
discovery through robust reasoning over structured biomedical knowledge bases.
Our dataset is publicly available at
https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source
at https://github.com/NIH-CARD/biomedsql.

</details>


### [437] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
*Mengru Wang,Ziwen Xu,Shengyu Mao,Shumin Deng,Zhaopeng Tu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为Steering Target Atoms (STA)的新方法，用于分离和操作知识组件以提升语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的控制对安全性和可靠性至关重要，但现有方法（如提示工程）因参数众多且内部表征高度交织，导致控制精度有限和副作用。

Method: 采用稀疏自编码器（SAE）分离高维空间中的知识，并通过STA方法定位和操作原子知识组件。

Result: 实验证明STA方法有效提升了控制的精确性和安全性，尤其在对抗场景中表现出更强的鲁棒性和灵活性。

Conclusion: STA方法在大型推理模型中的应用验证了其在精确推理控制中的有效性。

Abstract: Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.

</details>


### [438] [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
*Shahriar Noroozizadeh,Sayantan Kumar,George H. Chen,Jeremy C. Weiss*

Main category: cs.CL

TL;DR: PMOA-TTS是首个公开可用的数据集，包含124,699篇PubMed Open Access病例报告，通过LLM-based pipeline转换为结构化的事件-时间线。评估显示高质量，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 临床叙事中的时间动态理解对患者轨迹建模至关重要，但目前缺乏大规模的时间标注资源。

Method: 结合启发式过滤和Llama 3.3识别单病例报告，使用Llama 3.3和DeepSeek R1进行提示驱动提取，生成560万条时间戳临床事件。

Result: 评估显示事件匹配率80%，时间一致性高（c-index > 0.90），下游任务中时间依赖性一致性指数达0.82。

Conclusion: PMOA-TTS为生物医学NLP中的时间线提取、时间推理和纵向建模提供了可扩展的基础。

Abstract: Understanding temporal dynamics in clinical narratives is essential for
modeling patient trajectories, yet large-scale temporally annotated resources
remain limited. We present PMOA-TTS, the first openly available dataset of
124,699 PubMed Open Access (PMOA) case reports, each converted into structured
(event, time) timelines via a scalable LLM-based pipeline. Our approach
combines heuristic filtering with Llama 3.3 to identify single-patient case
reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,
resulting in over 5.6 million timestamped clinical events. To assess timeline
quality, we evaluate against a clinician-curated reference set using three
metrics: (i) event-level matching (80% match at a cosine similarity threshold
of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the
Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide
diagnostic and demographic coverage. In a downstream survival prediction task,
embeddings from extracted timelines achieve time-dependent concordance indices
up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally
structured narratives. PMOA-TTS provides a scalable foundation for timeline
extraction, temporal reasoning, and longitudinal modeling in biomedical NLP.
The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .

</details>


### [439] [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
*Amirhosein Ghasemabadi,Keith G. Mills,Baochun Li,Di Niu*

Main category: cs.CL

TL;DR: 论文提出了一种高效的自我引导测试时间扩展（TTS）框架GG，通过轻量级树搜索和内部信号（如置信度和新颖性）实现高性能，无需依赖昂贵的外部验证模型。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法依赖外部奖励模型或采样方法，计算成本高，GG旨在通过内部信号实现高效推理。

Method: GG采用轻量级树搜索，利用LLM内部信号（置信度和新颖性），并通过强化学习微调提升置信度可靠性。

Result: GG使小模型（1.5B）达到或超越大模型（32B-70B）的精度，GPU内存使用减少10倍，推理速度提升8倍，KV缓存内存减少50%。

Conclusion: GG提供了一种高效、低成本的TTS解决方案，适用于实际部署。

Abstract: Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.

</details>


### [440] [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出了一种多尺度流形对齐框架，通过分解潜在空间为全局、中间和局部语义流形，提升大语言模型的可解释性和信任度。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型性能强大，但其内部推理过程不透明，限制了在关键应用中的可解释性和信任度。

Method: 采用多尺度流形对齐框架，结合几何对齐和信息保留约束，并引入曲率正则化和超参数调优。

Result: 理论分析表明对齐误差（以KL散度衡量）在温和假设下可被限制，框架统一解释了多尺度语义结构。

Conclusion: 该框架提升了大语言模型的可解释性，支持偏见检测和鲁棒性增强等应用。

Abstract: Recent advances in Large Language Models (LLMs) have achieved strong
performance, yet their internal reasoning remains opaque, limiting
interpretability and trust in critical applications. We propose a novel
Multi_Scale Manifold Alignment framework that decomposes the latent space into
global, intermediate, and local semantic manifolds capturing themes, context,
and word-level details. Our method introduces cross_scale mapping functions
that jointly enforce geometric alignment (e.g., Procrustes analysis) and
information preservation (via mutual information constraints like MINE or VIB).
We further incorporate curvature regularization and hyperparameter tuning for
stable optimization. Theoretical analysis shows that alignment error, measured
by KL divergence, can be bounded under mild assumptions. This framework offers
a unified explanation of how LLMs structure multi-scale semantics, advancing
interpretability and enabling applications such as bias detection and
robustness enhancement.

</details>


### [441] [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
*Yixuan Wang,Shiyu Ji,Yijun Liu,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出了一种名为Lookahead Q-Cache (LAQ)的新框架，通过生成低成本伪前瞻查询来优化KV缓存淘汰策略，解决了现有方法在内存受限时的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖KV缓存加速解码，但长文本序列下缓存内存占用显著增加，现有淘汰方法在内存紧张时与实际推理查询不一致。

Method: 提出LAQ框架，生成伪前瞻查询作为重要性估计的观察窗口，使缓存淘汰更符合实际推理场景。

Result: 在LongBench和Needle-in-a-Haystack基准测试中，LAQ在不同预算水平下优于现有方法，内存受限时性能提升1~4点。

Conclusion: LAQ是一种高效且灵活的KV缓存淘汰框架，可与现有方法互补结合，进一步提升性能。

Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate
decoding by reducing redundant computations. However, the KV cache memory usage
grows substantially with longer text sequences, posing challenges for efficient
deployment. Existing KV cache eviction methods prune tokens using
prefilling-stage attention scores, causing inconsistency with actual inference
queries, especially under tight memory budgets. In this paper, we propose
Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost
pseudo lookahead queries to better approximate the true decoding-stage queries.
By using these lookahead queries as the observation window for importance
estimation, LAQ achieves more consistent and accurate KV cache eviction aligned
with real inference scenarios. Experimental results on LongBench and
Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods
across various budget levels, achieving a 1 $\sim$ 4 point improvement on
LongBench under limited cache budget. Moreover, LAQ is complementary to
existing approaches and can be flexibly combined to yield further improvements.

</details>


### [442] [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
*Zishun Yu,Shangzhe Li,Xinhua Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于时间差分学习的通用框架，用于压缩大型语言模型，通过利用教师模型的分布稀疏性，在减少的动作空间上操作，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但计算成本高昂，蒸馏技术成为压缩模型的有效方法。现有方法多从模仿学习或逆强化学习角度出发，本文则从时间差分学习角度提出通用框架。

Method: 利用语言模型概率分布稀疏的特性，设计了一个在减少的动作空间（词汇子集）上操作的时间差分学习框架，并推导出实际算法。

Result: 实验表明，该方法能够显著提升蒸馏后的模型性能。

Conclusion: 通过利用教师模型的分布稀疏性，提出的时间差分学习框架为语言模型蒸馏提供了一种高效且通用的解决方案。

Abstract: Large language models have led to significant progress across many NLP tasks,
although their massive sizes often incur substantial computational costs.
Distillation has become a common practice to compress these large and highly
capable models into smaller, more efficient ones. Many existing language model
distillation methods can be viewed as behavior cloning from the perspective of
imitation learning or inverse reinforcement learning. This viewpoint has
inspired subsequent studies that leverage (inverse) reinforcement learning
techniques, including variations of behavior cloning and temporal difference
learning methods. Rather than proposing yet another specific temporal
difference method, we introduce a general framework for temporal
difference-based distillation by exploiting the distributional sparsity of the
teacher model. Specifically, it is often observed that language models assign
most probability mass to a small subset of tokens. Motivated by this
observation, we design a temporal difference learning framework that operates
on a reduced action space (a subset of vocabulary), and demonstrate how
practical algorithms can be derived and the resulting performance improvements.

</details>


### [443] [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
*Yu Zhang,Wanli Jiang,Zhengyu Yang*

Main category: cs.CL

TL;DR: MOSLIM是一种新颖的多目标对齐方法，通过单一奖励模型和策略模型处理多样化目标，无需偏好训练，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要多个策略或奖励模型，或偏好特定的SFT模型，MOSLIM旨在简化这一过程。

Method: MOSLIM使用多头部奖励模型分类问答对，并通过映射函数优化策略模型。

Result: MOSLIM在多个多目标基准测试中表现优异，且计算资源需求更低。

Conclusion: MOSLIM为多目标对齐提供了一种高效且灵活的方法。

Abstract: The multi-objective alignment of Large Language Models (LLMs) is essential
for ensuring foundational models conform to diverse human preferences. Current
research in this field typically involves either multiple policies or multiple
reward models customized for various preferences, or the need to train a
preference-specific supervised fine-tuning (SFT) model. In this work, we
introduce a novel multi-objective alignment method, MOSLIM, which utilizes a
single reward model and policy model to address diverse objectives. MOSLIM
provides a flexible way to control these objectives through prompting and does
not require preference training during SFT phase, allowing thousands of
off-the-shelf models to be directly utilized within this training framework.
MOSLIM leverages a multi-head reward model that classifies question-answer
pairs instead of scoring them and then optimize policy model with a scalar
reward derived from a mapping function that converts classification results
from reward model into reward scores. We demonstrate the efficacy of our
proposed method across several multi-objective benchmarks and conduct ablation
studies on various reward model sizes and policy optimization methods. The
MOSLIM method outperforms current multi-objective approaches in most results
while requiring significantly fewer GPU computing resources compared with
existing policy optimization methods.

</details>


### [444] [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
*Cayo Viegas,Rohit Gheyi,Márcio Ribeiro*

Main category: cs.CL

TL;DR: LLMs在POSCOMP考试中的表现优于人类，尤其在文本题目上，但图像解释仍是挑战。最新模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在计算机科学等专业领域的实际能力，以指导未来发展。

Method: 测试四个LLMs（ChatGPT-4、Gemini 1.0 Advanced、Claude 3 Sonnet、Le Chat Mistral Large）在2022-2023年POSCOMP考试中的表现，并扩展分析最新模型。

Result: ChatGPT-4在2023年考试中表现最佳，超越所有人类考生。最新模型在2022-2024年考试中持续优于人类。

Conclusion: LLMs在专业领域表现优异，尤其在文本任务上，但需改进图像处理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
expanded the capabilities of artificial intelligence in natural language
processing tasks. Despite this progress, their performance in specialized
domains such as computer science remains relatively unexplored. Understanding
the proficiency of LLMs in these domains is critical for evaluating their
practical utility and guiding future developments. The POSCOMP, a prestigious
Brazilian examination used for graduate admissions in computer science promoted
by the Brazlian Computer Society (SBC), provides a challenging benchmark. This
study investigates whether LLMs can match or surpass human performance on the
POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and
Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP
exams. The assessments measured the models' proficiency in handling complex
questions typical of the exam. LLM performance was notably better on text-based
questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led
with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced
(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were
observed in the 2023 exam. ChatGPT-4 achieved the highest performance,
surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly
ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image
interpretation remains a challenge. Given the rapid evolution of LLMs, we
expanded our analysis to include more recent models - o1, Gemini 2.5 Pro,
Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.
These newer models demonstrate further improvements and consistently surpass
both the average and top-performing human participants across all three years.

</details>


### [445] [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: DMET提出了一种动态流形演化理论，将大语言模型生成建模为低维语义流形上的受控动力系统，通过量化指标评估生成文本的流畅性、语法性和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型生成过程中的动态行为，揭示其内在机制，并为平衡生成文本的创造性和一致性提供理论支持。

Method: 将潜在状态更新建模为连续动力学的离散时间欧拉近似，利用Lyapunov稳定性理论定义三个量化指标（状态连续性、聚类质量、拓扑持久性）。

Result: 实验验证了DMET的预测，并提供了平衡文本生成中创造性和一致性的原则性指导。

Conclusion: DMET为理解语言模型生成机制提供了统一框架，并为优化生成质量提供了实用工具。

Abstract: We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework
that models large language model generation as a controlled dynamical system
evolving on a low_dimensional semantic manifold. By casting latent_state
updates as discrete time Euler approximations of continuous dynamics, we map
intrinsic energy_driven flows and context_dependent forces onto Transformer
components (residual connections, attention, feed-forward networks). Leveraging
Lyapunov stability theory We define three empirical metrics (state continuity,
clustering quality, topological persistence) that quantitatively link
latent_trajectory properties to text fluency, grammaticality, and semantic
coherence. Extensive experiments across decoding parameters validate DMET's
predictions and yield principled guidelines for balancing creativity and
consistency in text generation.

</details>


### [446] [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.CL

TL;DR: 研究发现流行LLMs中存在性别偏见，提出新基准数据集RealWorldQuestioning，并定义熵偏见。测试显示类别层面无显著偏见，但个体问题层面差异显著。提出简单去偏见方法，效果显著。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中性别偏见的存在与持续性，特别是在商业和健康领域的实际问题中。

Method: 使用四个LLMs测试，通过ChatGPT-4o评估响应，定义熵偏见并提出去偏见方法。

Result: 类别层面无显著偏见，个体问题层面差异显著；去偏见方法在78%情况下提升信息量。

Conclusion: 简单去偏见方法有效，可平衡LLM输出，提升信息量。

Abstract: We investigate the existence and persistence of a specific type of gender
bias in some of the popular LLMs and contribute a new benchmark dataset,
RealWorldQuestioning (released on HuggingFace ), developed from real-world
questions across four key domains in business and health contexts: education,
jobs, personal financial management, and general health. We define and study
entropy bias, which we define as a discrepancy in the amount of information
generated by an LLM in response to real questions users have asked. We tested
this using four different LLMs and evaluated the generated responses both
qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our
analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that
there is no significant bias in LLM responses for men and women at a category
level. However, at a finer granularity (the individual question level), there
are substantial differences in LLM responses for men and women in the majority
of cases, which "cancel" each other out often due to some responses being
better for males and vice versa. This is still a concern since typical users of
these tools often ask a specific question (only) as opposed to several varied
ones in each of these common yet important areas of life. We suggest a simple
debiasing approach that iteratively merges the responses for the two genders to
produce a final result. Our approach demonstrates that a simple, prompt-based
debiasing strategy can effectively debias LLM outputs, thus producing responses
with higher information content than both gendered variants in 78% of the
cases, and consistently achieving a balanced integration in the remaining
cases.

</details>


### [447] [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
*Wenkai Fang,Shunyu Liu,Yang Zhou,Kongcheng Zhang,Tongya Zheng,Kaixuan Chen,Mingli Song,Dacheng Tao*

Main category: cs.CL

TL;DR: SeRL通过自指令和自奖励模块，在有限初始数据下提升LLM的推理能力，无需外部标注，性能媲美高质量数据训练。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖高质量指令和可验证奖励，但在专业领域难以获取。

Method: SeRL包含自指令（生成高质量指令）和自奖励（多数投票估计奖励）模块，结合常规RL迭代训练。

Result: 在多个推理基准和不同LLM上，SeRL性能优于同类方法，接近高质量数据训练结果。

Conclusion: SeRL为有限数据下的LLM训练提供有效解决方案，无需外部标注资源。

Abstract: Recent advances have demonstrated the effectiveness of Reinforcement Learning
(RL) in improving the reasoning capabilities of Large Language Models (LLMs).
However, existing works inevitably rely on high-quality instructions and
verifiable rewards for effective training, both of which are often difficult to
obtain in specialized domains. In this paper, we propose Self-play
Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial
data. Specifically, SeRL comprises two complementary modules: self-instruction
and self-rewarding. The former module generates additional instructions based
on the available data at each training step, employing robust online filtering
strategies to ensure instruction quality, diversity, and difficulty. The latter
module introduces a simple yet effective majority-voting mechanism to estimate
response rewards for additional instructions, eliminating the need for external
annotations. Finally, SeRL performs conventional RL based on the generated
data, facilitating iterative self-play learning. Extensive experiments on
various reasoning benchmarks and across different LLM backbones demonstrate
that the proposed SeRL yields results superior to its counterparts and achieves
performance on par with those obtained by high-quality data with verifiable
rewards. Our code is available at https://github.com/wantbook-book/SeRL.

</details>


### [448] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
*Juntong Wu,Zijing Liu,He Cao,Hao Li,Bin Feng,Zishan Shu,Ke Yu,Li Yuan,Yu Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于生物实体的新型评估框架，并引入检索增强方法，显著提升了蛋白质到文本生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质-文本模型在评估中存在数据泄漏问题，且传统自然语言处理指标不适用于该领域。

Method: 重组现有数据集并引入基于生物实体的评估框架，提出检索增强方法。

Result: 检索增强方法在蛋白质到文本生成中显著优于微调的大语言模型，且在无训练场景下表现高效准确。

Conclusion: 提出的方法解决了现有评估问题，为蛋白质-文本模型提供了更准确的性能评估和生成能力。

Abstract: In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.

</details>


### [449] [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
*Zihong Chen,Wanli Jiang,Jinzhe Li,Zhonghang Yuan,Huanjun Kong,Wanli Ouyang,Nanqing Dong*

Main category: cs.CL

TL;DR: GraphGen是一个基于知识图谱的框架，用于生成高质量的QA数据，解决LLM微调中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法存在事实不准确、长尾覆盖不足、知识结构简单和输出同质化等问题，需要更可靠的解决方案。

Method: GraphGen通过构建细粒度知识图谱，识别LLM的知识缺口，并采用多跳邻域采样和风格控制生成多样化的QA数据。

Result: 实验表明，GraphGen在知识密集型任务中优于传统合成数据方法。

Conclusion: GraphGen为LLM监督微调提供了更可靠和全面的数据生成方案。

Abstract: Fine-tuning for large language models (LLMs) typically requires substantial
amounts of high-quality supervised data, which is both costly and
labor-intensive to acquire. While synthetic data generation has emerged as a
promising solution, existing approaches frequently suffer from factual
inaccuracies, insufficient long-tail coverage, simplistic knowledge structures,
and homogenized outputs. To address these challenges, we introduce GraphGen, a
knowledge graph-guided framework designed for three key question-answering (QA)
scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by
constructing a fine-grained knowledge graph from the source text. It then
identifies knowledge gaps in LLMs using the expected calibration error metric,
prioritizing the generation of QA pairs that target high-value, long-tail
knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling
to capture complex relational information and employs style-controlled
generation to diversify the resulting QA data. Experimental results on
knowledge-intensive tasks under closed-book settings demonstrate that GraphGen
outperforms conventional synthetic data methods, offering a more reliable and
comprehensive solution to the data scarcity challenge in supervised
fine-tuning. The code and data are publicly available at
https://github.com/open-sciencelab/GraphGen.

</details>


### [450] [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
*Arvindh Arun,Sumit Kumar,Mojtaba Nayyeri,Bo Xiong,Ponnurangam Kumaraguru,Antonio Vergari,Steffen Staab*

Main category: cs.CL

TL;DR: SEMMA是一种双模块知识图谱基础模型，通过结合文本语义和结构信息，显著提升了零样本推理能力，尤其在未见关系词汇的挑战性场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型主要依赖图结构，忽略了文本属性中的丰富语义信息，限制了模型的泛化能力。

Method: SEMMA利用大型语言模型（LLMs）生成语义嵌入，构建文本关系图，并将其与结构模块融合。

Result: 在54个多样化知识图谱上，SEMMA在完全归纳链接预测中优于纯结构基线（如ULTRA），在未见关系词汇场景中效果是结构方法的2倍。

Conclusion: 文本语义对于结构方法失效的场景至关重要，强调了在知识推理中统一结构和语言信号的必要性。

Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling
zero-shot reasoning over unseen graphs by learning transferable patterns.
However, most existing KGFMs rely solely on graph structure, overlooking the
rich semantic signals encoded in textual attributes. We introduce SEMMA, a
dual-module KGFM that systematically integrates transferable textual semantics
alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich
relation identifiers, generating semantic embeddings that subsequently form a
textual relation graph, which is fused with the structural component. Across 54
diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully
inductive link prediction. Crucially, we show that in more challenging
generalization settings, where the test-time relation vocabulary is entirely
unseen, structural methods collapse while SEMMA is 2x more effective. Our
findings demonstrate that textual semantics are critical for generalization in
settings where structure alone fails, highlighting the need for foundation
models that unify structural and linguistic signals in knowledge reasoning.

</details>


### [451] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
*Zhaolin Li,Jan Niehues*

Main category: cs.CL

TL;DR: 研究表明，大语言模型（LLMs）可以通过上下文学习（ICL）掌握未见过的低资源语言，在语音识别任务中表现优异，甚至超越专用语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型仅支持少数语言，而全球有约7000种语言。研究探索LLMs是否能在无监督数据的情况下学习新语言，尤其是低资源语言。

Method: 通过实验在四种未见过的濒危语言上测试LLMs的上下文学习能力，比较基于概率和基于指令的方法。

Result: 提供更多相关文本样本能提升语言建模和语音识别性能；基于概率的方法优于传统指令方法；ICL使LLMs在语音识别任务中表现媲美或超越专用模型。

Conclusion: LLMs通过ICL能有效学习低资源语言，同时保留原有能力，为语言多样性保护提供了新途径。

Abstract: With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.

</details>


### [452] [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
*Vibhor Agarwal,Arjoo Gupta,Suparna De,Nishanth Sastry*

Main category: cs.CL

TL;DR: 论文提出了一种通用机制，通过设计两种对话核（Conversation Kernels）来捕捉在线对话中的上下文依赖关系，以理解帖子的不同属性（如信息性、趣味性等）。


<details>
  <summary>Details</summary>
Motivation: 随着社交网络和在线讨论论坛的发展，理解在线对话变得重要，但由于帖子简短且隐含上下文依赖，传统方法难以捕捉对话的复杂关系。

Method: 设计了两种对话核，探索对话树中帖子的不同邻域，构建适合特定任务的上下文。实验基于slashdot.org的数据，利用其多样的帖子标签验证方法的通用性。

Result: 提出的方法能够灵活适应不同的对话理解任务，验证了对话核的通用性和有效性。

Conclusion: 对话核是一种通用且灵活的机制，适用于多种在线对话理解任务，为内容分析提供了新思路。

Abstract: Understanding online conversations has attracted research attention with the
growth of social networks and online discussion forums. Content analysis of
posts and replies in online conversations is difficult because each individual
utterance is usually short and may implicitly refer to other posts within the
same conversation. Thus, understanding individual posts requires capturing the
conversational context and dependencies between different parts of a
conversation tree and then encoding the context dependencies between posts and
comments/replies into the language model.
  To this end, we propose a general-purpose mechanism to discover appropriate
conversational context for various aspects about an online post in a
conversation, such as whether it is informative, insightful, interesting or
funny. Specifically, we design two families of Conversation Kernels, which
explore different parts of the neighborhood of a post in the tree representing
the conversation and through this, build relevant conversational context that
is appropriate for each task being considered. We apply our developed method to
conversations crawled from slashdot.org, which allows users to apply highly
different labels to posts, such as 'insightful', 'funny', etc., and therefore
provides an ideal experimental platform to study whether a framework such as
Conversation Kernels is general-purpose and flexible enough to be adapted to
disparately different conversation understanding tasks.

</details>


### [453] [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
*Roi Cohen,Russa Biswas,Gerard de Melo*

Main category: cs.CL

TL;DR: 论文提出了一种信息对齐机制，旨在提升生成文本的事实完整性和信息丰富性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs生成的文本可能事实正确，但往往信息不够丰富。本文旨在解决这一问题。

Method: 提出了一种基于事实基准的信息对齐目标，优先选择既正确又信息丰富的答案。

Result: 研究发现，优化这一目标不仅能提升信息丰富性，还能改善事实准确性。

Conclusion: 信息对齐机制是提升LLMs生成文本质量的有效方法。

Abstract: Factual completeness is a general term that captures how detailed and
informative a factually correct text is. For instance, the factual sentence
``Barack Obama was born in the United States'' is factually correct, though
less informative than the factual sentence ``Barack Obama was born in Honolulu,
Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate
and generate factually incorrect text, they might also tend to choose to
generate factual text that is indeed factually correct and yet less informative
than other, more informative choices. In this work, we tackle this problem by
proposing an informativeness alignment mechanism. This mechanism takes
advantage of recent factual benchmarks to propose an informativeness alignment
objective. This objective prioritizes answers that are both correct and
informative. A key finding of our work is that when training a model to
maximize this objective or optimize its preference, we can improve not just
informativeness but also factuality.

</details>


### [454] [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
*Naba Rizvi,Harper Strickland,Saleha Ahmedi,Aekta Kallepalli,Isha Khirwadkar,William Wu,Imani N. S. Munyaka,Nedjma Ousidhoum*

Main category: cs.CL

TL;DR: 该论文研究了四种大语言模型（LLMs）在识别针对自闭症患者的微妙歧视（ableism）时的表现，发现LLMs能识别相关术语但常忽略有害含义，且依赖关键词匹配而非上下文理解。


<details>
  <summary>Details</summary>
Motivation: LLMs在决策任务中广泛应用，但其对残疾相关偏见的理解不足，尤其是对ableism的识别能力尚未充分研究。

Method: 评估四种LLMs识别自闭症相关歧视的能力，比较其与人类注释者在术语理解和上下文识别中的差异。

Result: LLMs能识别自闭症相关语言，但常忽略有害含义；其解释依赖关键词匹配，而人类则考虑更多因素。两者在分类方案上一致。

Conclusion: LLMs在识别ableism时存在局限，需改进上下文理解能力；二元分类方案适用于评估LLM表现。

Abstract: Large language models (LLMs) are increasingly used in decision-making tasks
like r\'esum\'e screening and content moderation, giving them the power to
amplify or suppress certain perspectives. While previous research has
identified disability-related biases in LLMs, little is known about how they
conceptualize ableism or detect it in text. We evaluate the ability of four
LLMs to identify nuanced ableism directed at autistic individuals. We examine
the gap between their understanding of relevant terminology and their
effectiveness in recognizing ableist content in context. Our results reveal
that LLMs can identify autism-related language but often miss harmful or
offensive connotations. Further, we conduct a qualitative comparison of human
and LLM explanations. We find that LLMs tend to rely on surface-level keyword
matching, leading to context misinterpretations, in contrast to human
annotators who consider context, speaker identity, and potential impact. On the
other hand, both LLMs and humans agree on the annotation scheme, suggesting
that a binary classification is adequate for evaluating LLM performance, which
is consistent with findings from prior studies involving human annotators.

</details>


### [455] [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
*Hawau Olamide Toyin,Rufael Marew,Humaid Alblooshi,Samar M. Magdy,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: ArVoice是一个多说话者现代标准阿拉伯语（MSA）语音语料库，包含带音标的转录文本，适用于多说话者语音合成及其他任务。


<details>
  <summary>Details</summary>
Motivation: 为多说话者语音合成及其他相关任务（如音标恢复、语音转换和深度伪造检测）提供高质量的语音数据资源。

Method: 结合专业录音、现有语料库修改和高质量合成语音，构建包含83.52小时语音的语料库，涵盖11种声音。

Result: 训练了三种开源TTS和两种语音转换系统，展示了数据集的实用性。

Conclusion: ArVoice语料库为研究用途提供了丰富的多说话者语音数据资源。

Abstract: We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech
corpus with diacritized transcriptions, intended for multi-speaker speech
synthesis, and can be useful for other tasks such as speech-based diacritic
restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a
new professionally recorded set from six voice talents with diverse
demographics, (2) a modified subset of the Arabic Speech Corpus; and (3)
high-quality synthetic speech from two commercial systems. The complete corpus
consists of a total of 83.52 hours of speech across 11 voices; around 10 hours
consist of human voices from 7 speakers. We train three open-source TTS and two
voice conversion systems to illustrate the use cases of the dataset. The corpus
is available for research use.

</details>


### [456] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
*Ziju Shen,Naohao Huang,Fanyi Yang,Yutong Wang,Guoxiong Gao,Tianyi Xu,Jiedong Jiang,Wanyi He,Pu Yang,Mengzhou Sun,Haocheng Ju,Peihao Wu,Bryan Dai,Bin Dong*

Main category: cs.CL

TL;DR: REAL-Prover是一个基于Lean 4的开源逐步定理证明器，通过微调的大型语言模型和检索系统，显著提升了解决大学数学问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有定理证明器在高级数学中表现不佳，REAL-Prover旨在突破这一限制。

Method: 结合微调的语言模型（REAL-Prover-v1）和检索系统（Leansearch-PS），使用数据提取管道（HERALD-AF）和交互环境（Jixia-interactive）进行训练。

Result: 在ProofNet数据集上达到23.7%的成功率（Pass@64），在FATE-M基准上达到56.7%的SOTA成功率。

Conclusion: REAL-Prover在高级数学问题上表现出色，为定理证明领域提供了新的工具和基准。

Abstract: Nowadays, formal theorem provers have made monumental progress on high-school
and competition-level mathematics, but few of them generalize to more advanced
mathematics. In this paper, we present REAL-Prover, a new open-source stepwise
theorem prover for Lean 4 to push this boundary. This prover, based on our
fine-tuned large language model (REAL-Prover-v1) and integrated with a
retrieval system (Leansearch-PS), notably boosts performance on solving
college-level mathematics problems. To train REAL-Prover-v1, we developed
HERALD-AF, a data extraction pipeline that converts natural language math
problems into formal statements, and a new open-source Lean 4 interactive
environment (Jixia-interactive) to facilitate synthesis data collection. In our
experiments, our prover using only supervised fine-tune achieves competitive
results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable
to state-of-the-art (SOTA) models. To further evaluate our approach, we
introduce FATE-M, a new benchmark focused on algebraic problems, where our
prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [457] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
*Ting Xu,Zhichao Huang,Jiankai Sun,Shanbo Cheng,Wai Lam*

Main category: cs.CL

TL;DR: SeqPO-SiMT是一种新的策略优化框架，将同步机器翻译任务视为顺序决策问题，通过定制奖励提高翻译质量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决同步机器翻译（SiMT）任务中多步决策的挑战，区别于单步任务的RLHF方法。

Method: 定义SiMT为顺序决策问题，使用定制奖励模拟和优化SiMT过程。

Result: 在多个数据集上显著提升翻译质量并降低延迟，性能超越SFT模型，甚至接近离线翻译效果。

Conclusion: SeqPO-SiMT在多步SiMT任务中表现出色，为同步翻译提供了高效解决方案。

Abstract: We present Sequential Policy Optimization for Simultaneous Machine
Translation (SeqPO-SiMT), a new policy optimization framework that defines the
simultaneous machine translation (SiMT) task as a sequential decision making
problem, incorporating a tailored reward to enhance translation quality while
reducing latency. In contrast to popular Reinforcement Learning from Human
Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in
single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.
This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT
process using a tailored reward. We conduct experiments on six datasets from
diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that
SeqPO-SiMT consistently achieves significantly higher translation quality with
lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning
(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17
in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context
than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly
rival the offline translation of high-performing LLMs, including
Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


### [458] [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
*Jinwu Hu,Zhitian Zhang,Guohao Chen,Xutao Wen,Chao Shuai,Wei Luo,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 论文提出了一种名为TLM的测试时学习（TTL）范式，通过动态调整LLMs以适应目标领域，仅使用未标记的测试数据。该方法通过最小化输入困惑度实现自监督性能提升，并采用高效样本学习策略和低秩适应（LoRA）技术，显著提升了LLMs在领域知识适应中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）通过预训练展现出强大的能力，但在泛化到专业领域和处理语言分布变化时仍存在局限性。

Method: 提出TLM方法，通过最小化未标记测试数据的输入困惑度进行自监督优化，并采用高效样本学习策略和LoRA技术实现轻量级模型更新。

Result: 实验表明，TLM在领域知识适应任务中比原始LLMs性能提升至少20%。

Conclusion: TLM通过测试时学习有效提升了LLMs在专业领域的适应能力，同时避免了灾难性遗忘问题。

Abstract: While Large Language Models (LLMs) have exhibited remarkable emergent
capabilities through extensive pre-training, they still face critical
limitations in generalizing to specialized domains and handling diverse
linguistic variations, known as distribution shifts. In this paper, we propose
a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically
adapts LLMs to target domains using only unlabeled test data during testing.
Specifically, we first provide empirical evidence and theoretical insights to
reveal that more accurate predictions from LLMs can be achieved by minimizing
the input perplexity of the unlabeled test data. Based on this insight, we
formulate the Test-Time Learning process of LLMs as input perplexity
minimization, enabling self-supervised enhancement of LLM performance.
Furthermore, we observe that high-perplexity samples tend to be more
informative for model optimization. Accordingly, we introduce a Sample
Efficient Learning Strategy that actively selects and emphasizes these
high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic
forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)
instead of full-parameter optimization, which allows lightweight model updates
while preserving more original knowledge from the model. We introduce the
AdaptEval benchmark for TTL and demonstrate through experiments that TLM
improves performance by at least 20% compared to original LLMs on domain
knowledge adaptation.

</details>


### [459] [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
*Yan Wang,Yang Ren,Lingfei Qian,Xueqing Peng,Keyi Wang,Yi Han,Dongji Feng,Xiao-Yang Liu,Jimin Huang,Qianqian Xie*

Main category: cs.CL

TL;DR: FinTagging是首个全范围、表格感知的XBRL基准，用于评估大语言模型在XBRL财务报告中的结构化信息提取和语义对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准过于简化XBRL标记为扁平多分类问题，且仅关注叙述性文本，FinTagging旨在提供更真实、细粒度的评估。

Method: 将XBRL标记问题分解为两个子任务：FinNI（财务实体提取）和FinCL（分类驱动的概念对齐），要求模型联合提取事实并与10k+ US-GAAP分类对齐。

Result: 大语言模型在信息提取上表现良好，但在细粒度概念对齐（尤其是区分相近分类条目）上表现不佳。

Conclusion: 现有大语言模型在完全自动化XBRL标记上存在局限，需改进语义推理和模式感知建模以提高财务披露准确性。

Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark
designed to evaluate the structured information extraction and semantic
alignment capabilities of large language models (LLMs) in the context of
XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL
tagging as flat multi-class classification and focus solely on narrative text,
FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for
financial entity extraction and FinCL for taxonomy-driven concept alignment. It
requires models to jointly extract facts and align them with the full 10k+
US-GAAP taxonomy across both unstructured text and structured tables, enabling
realistic, fine-grained evaluation. We assess a diverse set of LLMs under
zero-shot settings, systematically analyzing their performance on both subtasks
and overall tagging accuracy. Our results reveal that, while LLMs demonstrate
strong generalization in information extraction, they struggle with
fine-grained concept alignment, particularly in disambiguating closely related
taxonomy entries. These findings highlight the limitations of existing LLMs in
fully automating XBRL tagging and underscore the need for improved semantic
reasoning and schema-aware modeling to meet the demands of accurate financial
disclosure. Code is available at our GitHub repository and data is at our
Hugging Face repository.

</details>


### [460] [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
*Yi Zhu,Xin Zou,Xindong Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于事件的新型标注方法，构建了首个中文网络欺凌事件检测数据集CHNCI，包含91个事件的220,676条评论。通过结合三种基于解释生成的检测方法生成伪标签，并由人工标注验证，最终实验证明该数据集可作为网络欺凌检测和事件预测的基准。


<details>
  <summary>Details</summary>
Motivation: 现有网络欺凌检测基准多基于言论极性（如“攻击性”和“非攻击性”），本质上是仇恨言论检测，而现实中网络欺凌常通过事件引发广泛社会关注。

Method: 结合三种基于解释生成的网络欺凌检测方法作为集成方法生成伪标签，并由人工标注验证；提出验证是否构成网络欺凌事件的评价标准。

Result: 构建的CHNCI数据集包含91个事件的220,676条评论，实验证明其可作为网络欺凌检测和事件预测的基准。

Conclusion: 这是首个针对中文网络欺凌事件检测任务的研究，CHNCI数据集为相关任务提供了有效基准。

Abstract: Existing cyberbullying detection benchmarks were organized by the polarity of
speech, such as "offensive" and "non-offensive", which were essentially hate
speech detection. However, in the real world, cyberbullying often attracted
widespread social attention through incidents. To address this problem, we
propose a novel annotation method to construct a cyberbullying dataset that
organized by incidents. The constructed CHNCI is the first Chinese
cyberbullying incident detection dataset, which consists of 220,676 comments in
91 incidents. Specifically, we first combine three cyberbullying detection
methods based on explanations generation as an ensemble method to generate the
pseudo labels, and then let human annotators judge these labels. Then we
propose the evaluation criteria for validating whether it constitutes a
cyberbullying incident. Experimental results demonstrate that the constructed
dataset can be a benchmark for the tasks of cyberbullying detection and
incident prediction. To the best of our knowledge, this is the first study for
the Chinese cyberbullying incident detection task.

</details>


### [461] [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
*Qinzhuo Wu,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: 论文提出了一种名为BacktrackAgent的GUI代理框架，通过回溯机制提升任务完成效率，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理缺乏有效的错误检测与恢复机制，导致任务完成效率受限。

Method: 提出BacktrackAgent框架，包含验证器、判断器和反射器模块，并引入判断奖励机制和专用训练数据集。

Result: 在Mobile3M和Auto-UI基准测试中，任务成功率和步骤准确性均有提升。

Conclusion: BacktrackAgent通过回溯机制显著提升了GUI代理的性能，未来将公开数据和代码。

Abstract: Graphical User Interface (GUI) agents have gained substantial attention due
to their impressive capabilities to complete tasks through multiple
interactions within GUI environments. However, existing agents primarily focus
on enhancing the accuracy of individual actions and often lack effective
mechanisms for detecting and recovering from errors. To address these
shortcomings, we propose the BacktrackAgent, a robust framework that
incorporates a backtracking mechanism to improve task completion efficiency.
BacktrackAgent includes verifier, judger, and reflector components as modules
for error detection and recovery, while also applying judgment rewards to
further enhance the agent's performance. Additionally, we develop a training
dataset specifically designed for the backtracking mechanism, which considers
the outcome pages after action executions. Experimental results show that
BacktrackAgent has achieved performance improvements in both task success rate
and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be
released upon acceptance.

</details>


### [462] [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
*Yang He,Xiao Ding,Bibo Cai,Yufei Zhang,Kai Xiong,Zhouhao Sun,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: Self-Route框架通过动态选择推理模式，减少不必要的token消耗，提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决RLLMs在简单问题上过度推理导致的资源浪费问题。

Method: 引入轻量级预推理阶段提取能力感知嵌入，构建Gradient-10K数据集训练路由器。

Result: 在保持精度的同时，token消耗减少30-55%。

Conclusion: Self-Route具有广泛适用性和实用价值。

Abstract: While reasoning-augmented large language models (RLLMs) significantly enhance
complex task performance through extended reasoning chains, they inevitably
introduce substantial unnecessary token consumption, particularly for simpler
problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking
phenomenon leads to inefficient resource usage without proportional accuracy
gains. To address this issue, we propose Self-Route, a dynamic reasoning
framework that automatically selects between general and reasoning modes based
on model capability estimation. Our approach introduces a lightweight
pre-inference stage to extract capability-aware embeddings from hidden layer
representations, enabling real-time evaluation of the model's ability to solve
problems. We further construct Gradient-10K, a model difficulty
estimation-based dataset with dense complexity sampling, to train the router
for precise capability boundary detection. Extensive experiments demonstrate
that Self-Route achieves comparable accuracy to reasoning models while reducing
token consumption by 30-55\% across diverse benchmarks. The proposed framework
demonstrates consistent effectiveness across models with different parameter
scales and reasoning paradigms, highlighting its general applicability and
practical value.

</details>


### [463] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
*Boyi Zeng,Shixiang Song,Siyuan Huang,Yixuan Wang,He Li,Ziwei He,Xinbing Wang,Zhiyu Li,Zhouhan Lin*

Main category: cs.CL

TL;DR: 论文提出了一种通过‘思考’过程增强语言模型的方法，即在单个token生成步骤中多次调用前向过程，通过自监督学习实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 受人类在表达复杂句子前会进行深度思考的启发，作者希望将类似过程引入语言模型，以提升其性能。

Method: 在生成token时，模型通过加权求和所有token嵌入的方式进行‘思考’，并将生成的嵌入反馈为输入进行多次前向传递。

Result: 实验表明，该方法在多种语言模型（如GPT-2、Pythia、LLaMA）上有效，性能接近参数翻倍的普通模型，并在下游任务中显著优于官方模型。

Conclusion: 该方法简单通用，无需人工标注，能显著提升语言模型性能，代码已开源。

Abstract: Humans ponder before articulating complex sentence elements, enabling deeper
cognitive processing through focused effort. In this work, we introduce this
pondering process into language models by repeatedly invoking the forward
process within a single token generation step. During pondering, instead of
generating an actual token sampled from the prediction distribution, the model
ponders by yielding a weighted sum of all token embeddings according to the
predicted token distribution. The generated embedding is then fed back as input
for another forward pass. We show that the model can learn to ponder in this
way through self-supervised learning, without any human annotations. Our method
is straightforward and can be seamlessly integrated with various existing
language models. Experiments across three widely used open-source
architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task
evaluations demonstrate the effectiveness and generality of our method. For
language modeling tasks, pondering language models achieve performance
comparable to vanilla models with twice the number of parameters. On 9
downstream benchmarks, our pondering-enhanced Pythia models significantly
outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is
comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code
is available at https://github.com/LUMIA-Group/PonderingLM.

</details>


### [464] [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Deepak Subramani*

Main category: cs.CL

TL;DR: 小型语言模型（SLMs）在高中物理推理能力上表现差异显著，答案准确率较高但完全正确推理率低，需提升真实理解能力。


<details>
  <summary>Details</summary>
Motivation: 探索SLMs在复杂推理领域（如物理教育）的能力，填补研究空白。

Method: 使用OpenStax高中物理教材构建数据集，结合Bloom分类法标注，采用文化情境化方法，并通过LLM-as-a-judge框架评估模型表现。

Result: Qwen 3 1.7B答案准确率85%，但完全正确推理仅38%；数学符号格式影响小，推理质量随复杂度下降。

Conclusion: SLMs需增强真实理解和可验证推理能力，而非仅追求答案准确率。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, making them promising for educational applications. However,
their capacity for complex reasoning, particularly in domains such as physics,
remains underexplored. This study investigates the high school physics
reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),
including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.
We developed a comprehensive physics dataset from the OpenStax High School
Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and
plaintext mathematical notations. A novel cultural contextualization approach
was applied to a subset, creating culturally adapted problems for Asian,
African, and South American/Australian contexts while preserving core physics
principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,
we evaluated answer and reasoning chain correctness, along with calculation
accuracy. The results reveal significant differences between the SLMs. Qwen 3
1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was
substantially low (38%). The format of the mathematical notation had a
negligible impact on performance. SLMs exhibited varied performance across the
physics topics and showed a decline in reasoning quality with increasing
cognitive and knowledge complexity. In particular, the consistency of reasoning
was largely maintained in diverse cultural contexts, especially by better
performing models. These findings indicate that, while SLMs can often find
correct answers, their underlying reasoning is frequently flawed, suggesting an
overreliance on pattern recognition. For SLMs to become reliable educational
tools in physics, future development must prioritize enhancing genuine
understanding and the generation of sound, verifiable reasoning chains over
mere answer accuracy.

</details>


### [465] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
*Xiaqiang Tang,Jian Li,Keyu Hu,Du Nan,Xiaolong Li,Xi Zhang,Weigao Sun,Sihong Xie*

Main category: cs.CL

TL;DR: 论文提出了一种评估大型语言模型（LLM）生成认知陈述忠实性的框架，并创建了CogniBench-L数据集，用于训练检测认知幻觉的模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅包含重述源材料的“事实陈述”，缺乏对从上下文中推断的“认知陈述”的评估标准，导致其忠实性难以衡量和优化。

Method: 受立法领域证据评估启发，设计了评估认知陈述忠实性的严格框架，并构建了标注流程以自动生成更大规模的CogniBench-L数据集。

Result: 揭示了认知陈述的统计特性，并生成了可用于训练检测模型的大规模数据集。

Conclusion: 提出的框架和数据集为评估和优化LLM生成的认知陈述的忠实性提供了有效工具。

Abstract: Faithfulness hallucination are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standard, existing benchmarks only contain "factual statements" that rephrase
source materials without marking "cognitive statements" that make inference
from the given context, making the consistency evaluation and optimization of
cognitive statements difficult. Inspired by how an evidence is assessed in the
legislative domain, we design a rigorous framework to assess different levels
of faithfulness of cognitive statements and create a benchmark dataset where we
reveal insightful statistics. We design an annotation pipeline to create larger
benchmarks for different LLMs automatically, and the resulting larger-scale
CogniBench-L dataset can be used to train accurate cognitive hallucination
detection model. We release our model and dataset at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [466] [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
*Jungyoub Cha,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: SpecExtend通过集成高效注意力机制和跨模型检索策略，显著提升了长序列推测解码的性能，加速效果达2.22倍。


<details>
  <summary>Details</summary>
Motivation: 推测解码在长输入上性能下降，因注意力成本增加和草稿准确性降低。

Method: 集成FlashAttention和Hybrid Tree Attention，提出跨模型检索策略动态更新KV缓存。

Result: 在16K tokens的长输入上加速2.22倍。

Conclusion: SpecExtend为长序列推测解码提供了高效解决方案。

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), but its performance degrades on long inputs
due to increased attention cost and reduced draft accuracy. We introduce
SpecExtend, a drop-in enhancement that improves the performance of speculative
decoding on long sequences without any additional training. SpecExtend
integrates efficient attention mechanisms such as FlashAttention and Hybrid
Tree Attention into both the draft and target models, reducing latency across
all stages. To improve draft accuracy and speed, we propose Cross-model
Retrieval, a novel KV cache update strategy that uses the target model's
attention scores to dynamically select relevant context for the draft model.
Extensive evaluations on three long-context understanding datasets show that
SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x
for inputs up to 16K tokens, providing an effective solution for speculative
decoding of long sequences. The code is available at
https://github.com/jycha98/SpecExtend .

</details>


### [467] [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
*Amr Hegazy,Mostafa Elhoushi,Amr Alanwar*

Main category: cs.CL

TL;DR: 提出了一种轻量级、可训练的控制器网络，用于在推理时动态调节LLM的行为，避免生成不安全内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度和自适应机制，无法有效控制LLM的不安全行为。

Method: 通过控制器网络观察中间激活，预测全局缩放因子和层特定权重，动态调节预计算的“拒绝方向”向量。

Result: 实验表明，该方法显著提高了拒绝率，优于现有方法，且不改变原始模型参数。

Conclusion: 该方法为推理时细粒度控制LLM行为提供了一种高效且自适应的解决方案。

Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the
generation of unsafe content or failing to adhere to safety guidelines, often
relies on costly fine-tuning. Activation steering provides an alternative for
inference-time control, but existing methods typically lack fine-grained,
adaptive mechanisms. We introduce a novel approach using a lightweight,
trainable controller network integrated during inference. This controller
network observes specific intermediate LLM activations and predicts both a
global scaling factor and layer-specific weights. The predicted global scaling
factor and layer-specific weights then dynamically modulate the intensity of a
steering patch, derived from a pre-computed "refusal direction" vector, applied
across the LLM's layers during generation. Trained on activations from both
harmful and benign prompts, our controller learns to discriminatively apply
nuanced, layer-aware interventions, activating steering primarily for harmful
inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild
Jailbreak Prompts demonstrate that our weighted steering controller
significantly increases refusal rates compared to the base LLM, achieving
targeted behavioral modification without altering the original model
parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show
our approach outperforms existing methods, presenting an efficient and adaptive
method for fine-grained control over LLM behavior at inference time.

</details>


### [468] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
*Junsik Kim,Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: 论文提出了一种新的知识图谱嵌入方法RSCF，通过保持嵌入变换的一致性来提升性能，解决了现有方法中语义不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在关系特定实体变换中存在语义不一致问题，导致嵌入差异不一致，可能丢失有价值的归纳偏置。

Method: RSCF方法通过共享仿射变换、根植实体变换和归一化变化来保持一致性，并增加了关系变换和预测模块以增强语义。

Result: 在基于距离和张量分解的知识图谱补全任务中，RSCF显著优于现有方法，表现出对所有关系及其频率的鲁棒性。

Conclusion: RSCF通过保持嵌入变换的一致性，有效提升了知识图谱嵌入的性能和语义表达能力。

Abstract: In knowledge graph embedding, leveraging relation-specific
entity-transformation has markedly enhanced performance. However, the
consistency of embedding differences before and after transformation remains
unaddressed, risking the loss of valuable inductive bias inherent in the
embeddings. This inconsistency stems from two problems. First, transformation
representations are specified for relations in a disconnected manner, allowing
dissimilar transformations and corresponding entity-embeddings for similar
relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter
Based on Relations) disrupts this consistency through excessive concentration
of entity embeddings under entity-based regularization, generating
indistinguishable score distributions among relations. In this paper, we
introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),
containing more consistent entity-transformation characterized by three
features: 1) shared affine transformation of relation embeddings across all
relations, 2) rooted entity-transformation that adds an entity embedding to its
change represented by the transformed vector, and 3) normalization of the
change to prevent scale reduction. To amplify the advantages of consistency
that preserve semantics on embeddings, RSCF adds relation transformation and
prediction modules for enhancing the semantics. In knowledge graph completion
tasks with distance-based and tensor decomposition models, RSCF significantly
outperforms state-of-the-art KGE methods, showing robustness across all
relations and their frequencies.

</details>


### [469] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
*Shuhao Guan,Moule Lin,Cheng Xu,Xinyi Liu,Jinman Zhao,Jiexin Fan,Qi Xu,Derek Greene*

Main category: cs.CL

TL;DR: PreP-OCR是一个两阶段流程，结合文档图像修复和语义感知的OCR后校正，显著提升历史文档的文本提取效果。


<details>
  <summary>Details</summary>
Motivation: 历史文档因退化导致OCR效果不佳，需同时优化图像清晰度和语言一致性。

Method: 1. 生成合成图像对训练图像修复模型；2. 使用ByT5进行OCR后校正。

Result: 在13,831页历史文档上测试，字符错误率降低63.9-70.3%。

Conclusion: PreP-OCR展示了图像修复与语言校正结合在历史档案数字化中的潜力。

Abstract: This paper introduces PreP-OCR, a two-stage pipeline that combines document
image restoration with semantic-aware post-OCR correction to improve text
extraction from degraded historical documents. Our key innovation lies in
jointly optimizing image clarity and linguistic consistency. First, we generate
synthetic image pairs with randomized text fonts, layouts, and degradations. An
image restoration model is trained on this synthetic data, using
multi-directional patch extraction and fusion to process large images. Second,
a ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,
addresses any remaining OCR errors. Detailed experiments on 13,831 pages of
real historical documents in English, French, and Spanish show that PreP-OCR
pipeline reduces character error rates by 63.9-70.3\% compared to OCR on raw
images. Our pipeline demonstrates the potential of integrating image
restoration with linguistic error correction for digitizing historical
archives.

</details>


### [470] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
*Jiyoung Lee,Seungho Kim,Jieun Han,Jun-Min Lee,Kitaek Kim,Alice Oh,Edward Choi*

Main category: cs.CL

TL;DR: Trans-EnV框架通过将标准英语数据集自动转换为多种非标准英语变体，评估大语言模型的语言鲁棒性，发现性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估主要基于标准美式英语，忽视了全球英语多样性，可能引发公平性问题。

Method: 结合语言学专家知识和基于LLM的转换，构建Trans-EnV框架，生成38种英语变体数据集，评估7种LLM。

Result: 非标准英语变体上模型性能下降高达46.3%，显示显著性能差异。

Conclusion: 强调跨多样英语变体的全面语言鲁棒性评估的重要性，Trans-EnV框架公开可用。

Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American
English (SAE), often overlooking the diversity of global English varieties.
This narrow focus may raise fairness concerns as degraded performance on
non-standard varieties can lead to unequal benefits for users worldwide.
Therefore, it is critical to extensively evaluate the linguistic robustness of
LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a
framework that automatically transforms SAE datasets into multiple English
varieties to evaluate the linguistic robustness. Our framework combines (1)
linguistics expert knowledge to curate variety-specific features and
transformation guidelines from linguistic literature and corpora, and (2)
LLM-based transformations to ensure both linguistic validity and scalability.
Using Trans-EnV, we transform six benchmark datasets into 38 English varieties
and evaluate seven state-of-the-art LLMs. Our results reveal significant
performance disparities, with accuracy decreasing by up to 46.3% on
non-standard varieties. These findings highlight the importance of
comprehensive linguistic robustness evaluation across diverse English
varieties. Each construction of Trans-EnV was validated through rigorous
statistical testing and consultation with a researcher in the field of second
language acquisition, ensuring its linguistic validity. Our
\href{https://github.com/jiyounglee-0523/TransEnV}{code} and
\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}
are publicly available.

</details>


### [471] [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
*Chengyu Wang,Junbing Yan,Wenrui Cai,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TL;DR: EasyDistill是一个全面的知识蒸馏工具包，支持黑盒和白盒蒸馏，适用于大语言模型（LLMs），提供多种功能模块和工业解决方案。


<details>
  <summary>Details</summary>
Motivation: 简化知识蒸馏（KD）在大语言模型中的应用，提升研究者和工业实践者的效率和可操作性。

Method: 结合数据合成、监督微调、排序优化和强化学习等技术，模块化设计支持System 1和System 2模型。

Result: 提供了高效的蒸馏模型、开源数据集和工业解决方案，并成功集成到阿里云PAI平台。

Conclusion: EasyDistill使LLMs的高级知识蒸馏技术更易用且更具影响力。

Abstract: In this paper, we present EasyDistill, a comprehensive toolkit designed for
effective black-box and white-box knowledge distillation (KD) of large language
models (LLMs). Our framework offers versatile functionalities, including data
synthesis, supervised fine-tuning, ranking optimization, and reinforcement
learning techniques specifically tailored for KD scenarios. The toolkit
accommodates KD functionalities for both System 1 (fast, intuitive) and System
2 (slow, analytical) models. With its modular design and user-friendly
interface, EasyDistill empowers researchers and industry practitioners to
seamlessly experiment with and implement state-of-the-art KD strategies for
LLMs. In addition, EasyDistill provides a series of robust distilled models and
KD-based industrial solutions developed by us, along with the corresponding
open-sourced datasets, catering to a variety of use cases. Furthermore, we
describe the seamless integration of EasyDistill into Alibaba Cloud's Platform
for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for
LLMs more accessible and impactful within the NLP community.

</details>


### [472] [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
*Junhyuk Choi,Minju Kim,Yeseon Hong,Bugeun Kim*

Main category: cs.CL

TL;DR: 研究提出基于SCM的新评估指标和BASIC基准，用于检测LVLMs中的性别、种族和颜色刻板印象，发现SCM方法有效且模型架构与参数规模影响刻板印象。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在评估指标和数据集上对内容词和颜色影响的忽视，以更全面检测LVLMs的刻板印象。

Method: 引入SCM指标和BASIC基准，对八种LVLMs进行刻板印象分析。

Result: SCM方法有效；LVLMs存在颜色、性别和种族刻板印象；模型架构与参数规模影响刻板印象。

Conclusion: SCM和BASIC为检测LVLMs刻板印象提供了有效工具，揭示了模型设计对刻板印象的影响。

Abstract: As large vision language models(LVLMs) rapidly advance, concerns about their
potential to learn and generate social biases and stereotypes are increasing.
Previous studies on LVLM's stereotypes face two primary limitations: metrics
that overlooked the importance of content words, and datasets that overlooked
the effect of color. To address these limitations, this study introduces new
evaluation metrics based on the Stereotype Content Model (SCM). We also propose
BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM
metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.
As a result, we found three findings. (1) The SCM-based evaluation is effective
in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output
along with gender and race ones. (3) Interaction between model architecture and
parameter sizes seems to affect stereotypes. We release BASIC publicly on
[anonymized for review].

</details>


### [473] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
*Injae Na,Keonwoong Noh,Woohwan Jung*

Main category: cs.CL

TL;DR: LLM-AT框架自动选择适合的LLM层级以平衡成本与性能，无需训练，通过迭代升级模型直至获得有效响应。


<details>
  <summary>Details</summary>
Motivation: 解决复杂NLP任务中如何选择合适的LLM层级以平衡成本与性能的问题。

Method: LLM-AT由Starter、Generator和Judge组成，通过初始选择、生成响应和评估迭代升级模型。

Result: 实验证明LLM-AT在降低成本的同时实现优越性能。

Conclusion: LLM-AT是一种实用的解决方案，适用于实际应用。

Abstract: LLM providers typically offer multiple LLM tiers, varying in performance and
price. As NLP tasks become more complex and modularized, selecting the suitable
LLM tier for each subtask is a key challenge to balance between cost and
performance. To address the problem, we introduce LLM Automatic Transmission
(LLM-AT) framework that automatically selects LLM tiers without training.
LLM-AT consists of Starter, Generator, and Judge. The starter selects the
initial LLM tier expected to solve the given question, the generator produces a
response using the LLM of the selected tier, and the judge evaluates the
validity of the response. If the response is invalid, LLM-AT iteratively
upgrades to a higher-tier model, generates a new response, and re-evaluates
until a valid response is obtained. Additionally, we propose accuracy
estimator, which enables the suitable initial LLM tier selection without
training. Given an input question, accuracy estimator estimates the expected
accuracy of each LLM tier by computing the valid response rate across top-k
similar queries from past inference records. Experiments demonstrate that
LLM-AT achieves superior performance while reducing costs, making it a
practical solution for real-world applications.

</details>


### [474] [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
*Zhuo Li,Guodong Du,Weiyang Guo,Yigeng Zhou,Xiucheng Li,Wenya Wang,Fangming Liu,Yequan Wang,Deheng Ye,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: HoE（Hierarchical Mixture-of-Experts）是一种轻量级、参数高效且即插即用的方法，无需模型训练即可让大语言模型适应多样化的用户偏好，并在Pareto前沿实现最优权衡。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法难以有效平衡多目标之间的冲突，通常需要昂贵重新训练或结果不理想。

Method: HoE包含三个层次组件：LoRA Experts、Router Experts和Preference Routing，通过参数高效的方式实现Pareto前沿的最优权衡。

Result: 在14个目标和6个基准的200种偏好上评估，HoE优于15种基线方法。

Conclusion: HoE提供了一种无需训练的高效方法，能够灵活适应多样化偏好并在多目标间实现最优权衡。

Abstract: Aligning large language models (LLMs) to simultaneously satisfy multiple
objectives remains a significant challenge, especially given the diverse and
often conflicting nature of human preferences. Existing alignment methods
struggle to balance trade-offs effectively, often requiring costly retraining
or yielding suboptimal results across the Pareto frontier of preferences. In
this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a
\textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play}
approach that eliminates the need for model training, while enabling LLMs to
adapt across the entire Pareto frontier and accommodate diverse user
preferences. In particular, \textit{HoE} consists of three hierarchical
components: LoRA Experts, Router Experts and Preference Routing, reaching
optimal Pareto frontiers and achieving a trade-off between parameter size,
training cost, and performance. We evaluate \textit{HoE} across various tasks
on 14 objectives and 200 different preferences among 6 benchmarks,
demonstrating superior performance over 15 recent baselines. Code is available
in the supplementary materials.

</details>


### [475] [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
*Anil Batra,Laura Sevilla-Lara,Marcus Rohrbach,Frank Keller*

Main category: cs.CL

TL;DR: 论文提出Implicit-VidSRL数据集，用于解决语义角色标注（SRL）中隐含参数缺失的问题，并通过多模态烹饪流程提升上下文推理能力。提出的iSRL-Qwen2-VL模型在隐含参数预测上优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有SRL基准常忽略隐含参数，导致对步骤理解的不足，尤其在多模态流程中。

Method: 引入Implicit-VidSRL数据集，要求从多模态烹饪流程中推断显式和隐含参数，并测试多模态模型的上下文推理能力。

Result: 多模态LLMs在预测隐含参数时表现不佳，而iSRL-Qwen2-VL模型在F1分数上显著优于GPT-4o。

Conclusion: Implicit-VidSRL数据集和iSRL-Qwen2-VL模型为多模态SRL任务提供了更全面的解决方案。

Abstract: Procedural texts help AI enhance reasoning about context and action
sequences. Transforming these into Semantic Role Labeling (SRL) improves
understanding of individual steps by identifying predicate-argument structure
like {verb,what,where/with}. Procedural instructions are highly elliptic, for
instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second
step's where argument is inferred from the context, referring to where the
cucumber was placed. Prior SRL benchmarks often miss implicit arguments,
leading to incomplete understanding. To address this, we introduce
Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit
arguments from contextual information in multimodal cooking procedures. Our
proposed dataset benchmarks multimodal models' contextual reasoning, requiring
entity tracking through visual changes in recipes. We study recent multimodal
LLMs and reveal that they struggle to predict implicit arguments of what and
where/with from multi-modal procedural data given the verb. Lastly, we propose
iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for
what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.

</details>


### [476] [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
*Felix Krejca,Tobias Kietreiber,Alexander Buchelt,Sebastian Neumaier*

Main category: cs.CL

TL;DR: 本文研究了德语报纸论坛的内容审核，开发了结合上下文信息的分类模型，发现CNN和LSTM表现优于ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 在线讨论增多需要高效的内容审核，但德语报纸论坛的研究较少，且现有方法常忽略平台特定上下文。

Method: 使用LSTM、CNN和ChatGPT-3.5 Turbo，结合One Million Posts Corpus评估上下文感知模型。

Result: CNN和LSTM模型因上下文信息表现优异，ChatGPT的零样本分类未改善且表现较差。

Conclusion: 上下文信息对模型性能有显著影响，但ChatGPT在此任务中表现不佳。

Abstract: The increasing volume of online discussions requires advanced automatic
content moderation to maintain responsible discourse. While hate speech
detection on social media is well-studied, research on German-language
newspaper forums remains limited. Existing studies often neglect
platform-specific context, such as user history and article themes. This paper
addresses this gap by developing and evaluating binary classification models
for automatic content moderation in German newspaper forums, incorporating
contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging
the One Million Posts Corpus from the Austrian newspaper Der Standard, we
assess the impact of context-aware models. Results show that CNN and LSTM
models benefit from contextual information and perform competitively with
state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification
does not improve with added context and underperforms.

</details>


### [477] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
*Sebastian Antony Joseph,Syed Murtaza Husain,Stella S. R. Offner,Stéphanie Juneau,Paul Torrey,Adam S. Bolton,Juan P. Farias,Niall Gaffney,Greg Durrett,Junyi Jessy Li*

Main category: cs.CL

TL;DR: AstroVisBench是首个评估大语言模型（LLM）在天文学领域科学计算和可视化能力的基准测试，揭示了当前模型在辅助天文学研究中的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在科学工作流中生成正确科学见解的能力，尤其是在数据处理和可视化方面，填补了现有研究的空白。

Method: 引入AstroVisBench基准测试，结合LLM-as-a-judge工作流，验证模型在天文学数据处理和复杂可视化中的表现。

Result: 评估显示当前最先进的语言模型在天文学研究中作为有用助手的能力存在显著差距。

Conclusion: AstroVisBench为AI科学家提供了端到端评估工具，推动了可视化工作流的发展，适用于从物理学到生物学的广泛领域。

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [478] [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
*Xiangqing Shen,Fanfan Wang,Rui Xia*

Main category: cs.CL

TL;DR: 论文提出RAR框架，结合LLM推理与知识图谱，通过Reasoner、Aligner和Responser三部分优化KGQA任务，实现高效、可解释的推理。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理任务中表现优异但存在幻觉问题，知识图谱提供结构化知识但缺乏灵活推理能力，需结合两者优势。

Method: RAR框架包含Reasoner生成推理链、Aligner对齐KG路径、Responser合成答案，采用EM算法优化。

Result: 在WebQSP和CWQ基准上分别达到93.3%和91.0%的Hit@1分数，零样本泛化能力强且推理高效。

Conclusion: RAR有效结合LLM与知识图谱，生成高质量、可解释的推理链，性能优越且泛化能力强。

Abstract: LLMs have demonstrated remarkable capabilities in complex reasoning tasks,
yet they often suffer from hallucinations and lack reliable factual grounding.
Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack
the flexible reasoning abilities of LLMs. In this paper, we present
Reason-Align-Respond (RAR), a novel framework that systematically integrates
LLM reasoning with knowledge graphs for KGQA. Our approach consists of three
key components: a Reasoner that generates human-like reasoning chains, an
Aligner that maps these chains to valid KG paths, and a Responser that
synthesizes the final answer. We formulate this process as a probabilistic
model and optimize it using the Expectation-Maximization algorithm, which
iteratively refines the reasoning chains and knowledge paths. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of RAR,
achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on
WebQSP and CWQ respectively. Human evaluation confirms that RAR generates
high-quality, interpretable reasoning chains well-aligned with KG paths.
Furthermore, RAR exhibits strong zero-shot generalization capabilities and
maintains computational efficiency during inference.

</details>


### [479] [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
*Bipul Thapa,Gabriel Cofre*

Main category: cs.CL

TL;DR: 论文提出了一种结合TF-IDF和BERT嵌入的混合方法，用于西班牙语客户反馈的情感分类，并通过自定义堆叠集成（CSE）模型显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法将广泛使用语言的反馈翻译为较少使用语言时，会丢失语义完整性和上下文细微差别，因此需要一种能保留原始语言语义深度的分类方法。

Method: 采用TF-IDF与BERT嵌入结合的混合方法，使用CSE模型（包含多种基础分类器和逻辑回归元模型）进行情感分类。

Result: CSE模型在西班牙语数据集上的测试准确率达到93.3%，显著优于单独模型和BERT模型。

Conclusion: 研究强调了西班牙语情感分类的挑战，并展示了结合TF-IDF与BERT的优势，为企业改进客户反馈分析提供了实用见解。

Abstract: Classifying customer feedback into distinct emotion categories is essential
for understanding sentiment and improving customer experience. In this paper,
we classify customer feedback in Spanish into three emotion
categories--positive, neutral, and negative--using advanced NLP and ML
techniques. Traditional methods translate feedback from widely spoken languages
to less common ones, resulting in a loss of semantic integrity and contextual
nuances inherent to the original language. To address this limitation, we
propose a hybrid approach that combines TF-IDF with BERT embeddings,
effectively transforming Spanish text into rich numerical representations that
preserve the semantic depth of the original language by using a Custom Stacking
Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range
of models, including Logistic Regression, KNN, Bagging classifier with LGBM,
and AdaBoost. The CSE model combines these classifiers as base models and uses
a one-vs-all Logistic Regression as the meta-model. Our experimental results
demonstrate that CSE significantly outperforms the individual and BERT model,
achieving a test accuracy of 93.3% on the native Spanish dataset--higher than
the accuracy obtained from the translated version. These findings underscore
the challenges of emotion classification in Spanish and highlight the
advantages of combining vectorization techniques like TF-IDF with BERT for
improved accuracy. Our results provide valuable insights for businesses seeking
to leverage emotion classification to enhance customer feedback analysis and
service improvements.

</details>


### [480] [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
*Jie Shao,Jianxin Wu*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）的推理能力主要归因于Transformer多头自注意力机制中的输出投影模块（oproj），而其他模块更多用于流畅对话。


<details>
  <summary>Details</summary>
Motivation: 探索LLM推理能力的来源，明确是整体模型、特定模块还是过拟合的结果。

Method: 引入Stethoscope for Networks（SfN）工具套件，用于分析和探测LLM内部行为。

Result: 实验表明oproj在推理中起核心作用，其他模块更多支持对话流畅性。

Conclusion: 研究为LLM可解释性提供新视角，并为针对性训练策略开辟途径。

Abstract: Despite the impressive performance of large language models (LLMs), the
process of endowing them with new capabilities--such as mathematical
reasoning--remains largely empirical and opaque. A critical open question is
whether reasoning abilities stem from the entire model, specific modules, or
are merely artifacts of overfitting. In this work, we hypothesize that the
reasoning capabilities in well-trained LLMs are primarily attributed to the
output projection module (oproj) in the Transformer's multi-head self-attention
(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for
Networks (SfN), a suite of diagnostic tools designed to probe and analyze the
internal behaviors of LLMs. Using SfN, we provide both circumstantial and
empirical evidence suggesting that oproj plays a central role in enabling
reasoning, whereas other modules contribute more to fluent dialogue. These
findings offer a new perspective on LLM interpretability and open avenues for
more targeted training strategies, potentially enabling more efficient and
specialized LLMs.

</details>


### [481] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
*Han Xiao,Guozhi Wang,Yuxiang Chai,Zimu Lu,Weifeng Lin,Hao He,Lue Fan,Liuyang Bian,Rui Hu,Liang Liu,Shuai Ren,Yafei Wen,Xiaoxin Chen,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: UI-Genie是一个自改进框架，通过奖励模型和数据生成策略解决GUI代理中的轨迹验证和高质量训练数据扩展问题。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理中轨迹验证困难和高质量训练数据难以扩展的挑战。

Method: 采用奖励模型UI-Genie-RM处理历史上下文，结合自改进管道通过奖励引导探索和动态环境验证扩展任务。

Result: UI-Genie在多个GUI代理基准测试中达到最先进性能，并通过开源框架和数据集促进研究。

Conclusion: UI-Genie通过自改进框架和高质量数据集，显著提升了GUI代理的性能和可扩展性。

Abstract: In this paper, we introduce UI-Genie, a self-improving framework addressing
two key challenges in GUI agents: verification of trajectory outcome is
challenging and high-quality training data are not scalable. These challenges
are addressed by a reward model and a self-improving pipeline, respectively.
The reward model, UI-Genie-RM, features an image-text interleaved architecture
that efficiently pro- cesses historical context and unifies action-level and
task-level rewards. To sup- port the training of UI-Genie-RM, we develop
deliberately-designed data genera- tion strategies including rule-based
verification, controlled trajectory corruption, and hard negative mining. To
address the second challenge, a self-improvement pipeline progressively expands
solvable complex GUI tasks by enhancing both the agent and reward models
through reward-guided exploration and outcome verification in dynamic
environments. For training the model, we generate UI- Genie-RM-517k and
UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI
agents while demonstrating high-quality synthetic trajectory gen- eration
without manual annotation. Experimental results show that UI-Genie achieves
state-of-the-art performance across multiple GUI agent benchmarks with three
generations of data-model self-improvement. We open-source our complete
framework implementation and generated datasets to facilitate further research
in https://github.com/Euphoria16/UI-Genie.

</details>


### [482] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
*Danush Khanna,Pratinav Seth,Sidhaarth Sredharan Murali,Aditya Kumar Guru,Siddharth Shukla,Tanuj Tyagi,Sandeep Chaurasia,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: 论文介绍了MultiManip数据集和SELF-PERCEPT框架，用于检测多轮多人对话中的心理操控，并评估了现有大语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 心理操控在人际交流中普遍存在且难以检测，现有大语言模型在复杂对话中表现不佳。

Method: 构建MultiManip数据集，提出SELF-PERCEPT两阶段提示框架，并评估多种大语言模型。

Result: 现有模型（如GPT-4o和Llama-3.1-8B）在检测操控时效果有限，SELF-PERCEPT表现优异。

Conclusion: SELF-PERCEPT框架能有效检测复杂对话中的心理操控，数据集和代码已开源。

Abstract: Mental manipulation is a subtle yet pervasive form of abuse in interpersonal
communication, making its detection critical for safeguarding potential
victims. However, due to manipulation's nuanced and context-specific nature,
identifying manipulative language in complex, multi-turn, and multi-person
conversations remains a significant challenge for large language models (LLMs).
To address this gap, we introduce the MultiManip dataset, comprising 220
multi-turn, multi-person dialogues balanced between manipulative and
non-manipulative interactions, all drawn from reality shows that mimic
real-world scenarios. For manipulative interactions, it includes 11 distinct
manipulations depicting real-life scenarios. We conduct extensive evaluations
of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various
prompting strategies. Despite their capabilities, these models often struggle
to detect manipulation effectively. To overcome this limitation, we propose
SELF-PERCEPT, a novel, two-stage prompting framework inspired by
Self-Perception Theory, demonstrating strong performance in detecting
multi-person, multi-turn mental manipulation. Our code and data are publicly
available at https://github.com/danushkhanna/self-percept .

</details>


### [483] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
*Wei Chen,Zhao Zhang,Meng Yuan,Kepeng Xu,Fuzhen Zhuang*

Main category: cs.CL

TL;DR: 论文提出了一种名为FCKT的细粒度跨任务知识迁移框架，用于目标情感分析（TSA），通过显式结合方面级信息优化情感预测，减少负迁移并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用粗粒度知识迁移，忽略了方面-情感关系的细粒度控制，导致负迁移问题。

Method: 提出FCKT框架，显式结合方面级信息进行细粒度知识迁移。

Result: 在三个数据集上的实验表明，FCKT优于多种基线方法和大型语言模型（LLMs）。

Conclusion: FCKT通过细粒度知识迁移有效解决了TSA任务中的负迁移问题，提升了性能。

Abstract: In this paper, we address the task of targeted sentiment analysis (TSA),
which involves two sub-tasks, i.e., identifying specific aspects from reviews
and determining their corresponding sentiments. Aspect extraction forms the
foundation for sentiment prediction, highlighting the critical dependency
between these two tasks for effective cross-task knowledge transfer. While most
existing studies adopt a multi-task learning paradigm to align task-specific
features in the latent space, they predominantly rely on coarse-grained
knowledge transfer. Such approaches lack fine-grained control over
aspect-sentiment relationships, often assuming uniform sentiment polarity
within related aspects. This oversimplification neglects contextual cues that
differentiate sentiments, leading to negative transfer. To overcome these
limitations, we propose FCKT, a fine-grained cross-task knowledge transfer
framework tailored for TSA. By explicitly incorporating aspect-level
information into sentiment prediction, FCKT achieves fine-grained knowledge
transfer, effectively mitigating negative transfer and enhancing task
performance. Experiments on three datasets, including comparisons with various
baselines and large language models (LLMs), demonstrate the effectiveness of
FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [484] [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
*Praveen Srinivasa Varadhan,Srija Anand,Soma Siddhartha,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 研究评估了英语F5-TTS模型在11种印度语言上的适应能力，发现仅用印度数据微调效果最佳，生成的IN-F5模型接近人类多语言能力。


<details>
  <summary>Details</summary>
Motivation: 探讨英语预训练模型在低资源印度语言TTS任务中的适应性和效果。

Method: 比较三种方法：从头训练、仅用印度数据微调、同时用印度和英语数据微调。

Result: 仅用印度数据微调的IN-F5表现最佳，支持跨语言流畅合成，并能在零资源语言中生成合成数据。

Conclusion: 英语预训练有助于低资源TTS任务达到人类水平，并提出了一种计算最优策略。

Abstract: What happens when an English Fairytaler is fine-tuned on Indian languages? We
evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring
polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:
(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and
(iii) fine-tuning on both Indian and English data to prevent forgetting.
Fine-tuning with only Indian data proves most effective and the resultant IN-F5
is a near-human polyglot; that enables speakers of one language (e.g., Odia) to
fluently speak in another (e.g., Hindi). Our results show English pretraining
aids low-resource TTS in reaching human parity. To aid progress in other
low-resource languages, we study data-constrained setups and arrive at a
compute optimal strategy. Finally, we show IN-F5 can synthesize unseen
languages like Bhojpuri and Tulu using a human-in-the-loop approach for
zero-resource TTS via synthetic data generation.

</details>


### [485] [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
*Daeen Kabir,Minhajur Rahman Chowdhury Mahim,Sheikh Shafayat,Adnan Sadik,Arian Ahmed,Eunsu Kim,Alice Oh*

Main category: cs.CL

TL;DR: BLUCK是一个新的数据集，用于评估大型语言模型（LLMs）在孟加拉语语言理解和文化知识方面的表现，包含2366道多选题，涵盖23个类别。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在孟加拉语和文化知识方面的表现尚未得到充分评估，BLUCK填补了这一空白。

Method: 数据集基于孟加拉国的文化和历史以及孟加拉语言学，通过6个专有和3个开源LLMs进行基准测试。

Result: LLMs整体表现尚可，但在孟加拉语音学方面存在困难，孟加拉语被视为中等资源语言。

Conclusion: BLUCK是首个专注于孟加拉本土文化的多选题评估基准，为LLMs在孟加拉语领域的进一步研究提供了基础。

Abstract: In this work, we introduce BLUCK, a new dataset designed to measure the
performance of Large Language Models (LLMs) in Bengali linguistic understanding
and cultural knowledge. Our dataset comprises 2366 multiple-choice questions
(MCQs) carefully curated from compiled collections of several college and job
level examinations and spans 23 categories covering knowledge on Bangladesh's
culture and history and Bengali linguistics. We benchmarked BLUCK using 6
proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,
Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that
while these models perform reasonably well overall, they, however, struggles in
some areas of Bengali phonetics. Although current LLMs' performance on Bengali
cultural and linguistic contexts is still not comparable to that of mainstream
languages like English, our results indicate Bengali's status as a mid-resource
language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark
that is centered around native Bengali culture, history, and linguistics.

</details>


### [486] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
*Stephen Chung,Wenyu Du,Jie Fu*

Main category: cs.CL

TL;DR: 通过引入基于双过程理论的四阶段任务（快速思考、验证、慢速思考和总结），提高了大语言模型在问答任务中的推理能力和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答任务中的搜索行为不精确且缺乏信心，导致冗长冗余的回答，需要改进直觉和验证能力。

Method: 提出四阶段任务：快速思考（严格限制token）、验证、慢速思考和总结，以优化模型回答。

Result: Qwen2.5-1.5B准确率从24.9%提升至27.9%，DeepSeek-R1-Qwen-1.5B从45.9%提升至49.8%。快速思考模式仅用1000 token达到26.8%准确率。

Conclusion: 直觉和深思熟虑的推理是互补的系统，针对性训练可显著提升模型性能。

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [487] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
*Hanlin Wang,Chak Tou Leong,Jiashuo Wang,Jian Wang,Wenjie Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为SPA的奖励再分配框架，用于解决强化学习中延迟奖励的问题，通过分解最终奖励为逐步贡献，提升代理训练效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练LLM代理处理多步交互任务时，延迟奖励问题导致早期动作难以获得有效反馈，影响训练效果。

Method: 提出Stepwise Progress Attribution (SPA)框架，训练进度估计器分解最终奖励为逐步贡献，结合环境信号作为中间奖励。

Result: 在多个基准测试中，SPA在成功率和准确性上均优于现有方法（平均提升2.5%和1.9%）。

Conclusion: SPA通过提供更有效的中间奖励，显著提升了强化学习的训练效果。

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [488] [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
*Bogdan Bogachov,Yaoyao Fiona Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级领域适应方法Small Language Graph（SLG），通过图结构中的小型专家模型解决计算资源消耗和幻觉问题，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法计算资源消耗大且存在幻觉问题，尤其在工程场景中需要生成高质量文本。

Method: 采用图结构设计，每个节点为小型专家模型，针对特定文本微调。

Result: SLG在Exact Match指标上优于传统方法3倍，微调速度快1.7倍。

Conclusion: SLG为中小型工程公司提供了低成本使用生成AI的可能，并支持分布式AI系统的发展。

Abstract: Despite recent advancements in domain adaptation techniques for large
language models, these methods remain computationally intensive, and the
resulting models can still exhibit hallucination issues. Most existing
adaptation methods do not prioritize reducing the computational resources
required for fine-tuning and inference of language models. Hallucination issues
have gradually decreased with each new model release. However, they remain
prevalent in engineering contexts, where generating well-structured text with
minimal errors and inconsistencies is critical. This work introduces a novel
approach called the Small Language Graph (SLG), which is a lightweight
adaptation solution designed to address the two key challenges outlined above.
The system is structured in the form of a graph, where each node represents a
lightweight expert - a small language model fine-tuned on specific and concise
texts. The results of this study have shown that SLG was able to surpass
conventional fine-tuning methods on the Exact Match metric by 3 times.
Additionally, the fine-tuning process was 1.7 times faster compared to that of
a larger stand-alone language model. These findings introduce a potential for
small to medium-sized engineering companies to confidently use generative AI
technologies, such as LLMs, without the necessity to invest in expensive
computational resources. Also, the graph architecture and the small size of
expert nodes offer a possible opportunity for distributed AI systems, thus
potentially diverting the global need for expensive centralized compute
clusters.

</details>


### [489] [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
*Rochelle Choenni,Ivan Titov*

Main category: cs.CL

TL;DR: 论文研究了多语言大模型在稀疏化（剪枝）下的性能表现，并提出了一种新方法M-Wanda，通过语言感知的激活统计和动态调整层间稀疏度来优化多语言性能。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型的性能通常依赖模型规模，但剪枝会带来性能损失。研究旨在平衡多语言能力与模型稀疏化之间的权衡。

Method: 提出M-Wanda方法，利用语言感知的激活统计和动态调整层间稀疏度来优化剪枝过程。

Result: M-Wanda在最小额外成本下显著提升了多语言性能。

Conclusion: M-Wanda是首个针对多语言性能优化的剪枝方法，为未来多语言剪枝研究提供了新思路。

Abstract: Multilingual LLM performance is often critically dependent on model size.
With an eye on efficiency, this has led to a surge in interest in one-shot
pruning methods that retain the benefits of large-scale pretraining while
shrinking the model size. However, as pruning tends to come with performance
loss, it is important to understand the trade-offs between multilinguality and
sparsification. In this work, we study multilingual performance under different
sparsity constraints and show that moderate ratios already substantially harm
performance. To help bridge this gap, we propose M-Wanda, a pruning method that
models cross-lingual variation by incorporating language-aware activation
statistics into its pruning criterion and dynamically adjusts layerwise
sparsity based on cross-lingual importance. We show that M-Wanda consistently
improves performance at minimal additional costs. We are the first to
explicitly optimize pruning to retain multilingual performance, and hope to
inspire future advances in multilingual pruning.

</details>


### [490] [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
*Gleb Mezentsev,Ivan Oseledets*

Main category: cs.CL

TL;DR: 研究表明，冻结的大型语言模型（LLM）仅通过两个学习嵌入即可在一次前向传递中生成数百个准确标记，揭示了无需迭代解码的多标记生成能力。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以在不使用自回归的情况下实现文本重建，以揭示LLM的潜在能力。

Method: 通过提供两个学习嵌入，研究冻结LLM在一次前向传递中生成多标记的能力，并分析嵌入的行为和编码信息。

Result: 发现LLM可以在一次前向传递中生成数百个准确标记，且这些嵌入在嵌入空间中形成连通和局部区域。

Conclusion: 研究表明LLM具有无需迭代解码的多标记生成能力，且嵌入空间的性质为学习专用编码器提供了潜力。

Abstract: A recent study showed that large language models (LLMs) can reconstruct
surprisingly long texts - up to thousands of tokens - via autoregressive
generation from just one specially trained input embedding. In this work, we
explore whether such reconstruction is possible without autoregression. We show
that frozen LLMs can generate hundreds of accurate tokens in just one forward
pass, when provided with only two learned embeddings. This reveals a surprising
and underexplored capability of LLMs - multi-token generation without iterative
decoding. We investigate the behaviour of these embeddings and provide insight
into the type of information they encode. We also empirically show that
although these representations are not unique for a given text, they form
connected and local regions in embedding space - a property that suggests the
potential of learning a dedicated encoder into that space.

</details>


### [491] [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
*Jong Hak Moon,Geon Choi,Paloma Rabaey,Min Gwan Kim,Hyuk Gi Hong,Jung-Oh Lee,Hangyul Yoon,Eun Woo Doe,Jiyoun Kim,Harshita Sharma,Daniel C. Castro,Javier Alvarez-Valle,Edward Choi*

Main category: cs.CL

TL;DR: 论文提出了LUNGUAGE数据集和LUNGUAGESCORE评估指标，用于结构化放射学报告生成，支持单报告和纵向患者评估。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告评估方法局限于单报告场景，且依赖粗粒度指标，无法捕捉细粒度临床语义和时间依赖性。

Method: 开发了两阶段框架，将生成的报告转换为结构化表示，并提出LUNGUAGESCORE评估指标，比较实体、关系和属性层面的输出。

Result: LUNGUAGESCORE能有效支持结构化报告评估，实证结果验证了其有效性。

Conclusion: LUNGUAGE数据集和LUNGUAGESCORE指标为序列放射学报告提供了首个基准数据集、结构化框架和评估指标。

Abstract: Radiology reports convey detailed clinical observations and capture
diagnostic reasoning that evolves over time. However, existing evaluation
methods are limited to single-report settings and rely on coarse metrics that
fail to capture fine-grained clinical semantics and temporal dependencies. We
introduce LUNGUAGE,a benchmark dataset for structured radiology report
generation that supports both single-report evaluation and longitudinal
patient-level assessment across multiple studies. It contains 1,473 annotated
chest X-ray reports, each reviewed by experts, and 80 of them contain
longitudinal annotations to capture disease progression and inter-study
intervals, also reviewed by experts. Using this benchmark, we develop a
two-stage framework that transforms generated reports into fine-grained,
schema-aligned structured representations, enabling longitudinal
interpretation. We also propose LUNGUAGESCORE, an interpretable metric that
compares structured outputs at the entity, relation, and attribute level while
modeling temporal consistency across patient timelines. These contributions
establish the first benchmark dataset, structuring framework, and evaluation
metric for sequential radiology reporting, with empirical results demonstrating
that LUNGUAGESCORE effectively supports structured report evaluation. The code
is available at: https://github.com/SuperSupermoon/Lunguage

</details>


### [492] [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
*Roi Cohen,Omri Fahn,Gerard de Melo*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在未经明确训练的情况下，仍能捕捉不确定性，且这种能力与模型规模无关。通过指令调整或特定标记调整，可以统一不确定性类型，提升正确性预测。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但仍存在幻觉问题，导致生成不准确文本。研究旨在探索LLMs如何捕捉不确定性，以解决这一问题。

Method: 研究假设不确定性是模型潜在空间中的线性概念，并验证其在预训练后的表现。同时分析了不同类型的不确定性及其对任务正确性的预测作用。

Result: LLMs能捕捉多种不确定性类型，且这些类型对特定任务或基准的正确性预测有帮助。研究还发现模型规模对不确定性捕捉无显著影响。

Conclusion: 通过指令调整或特定标记调整，统一不确定性类型有助于提升模型的正确性预测能力。

Abstract: Large Language Models are known to capture real-world knowledge, allowing
them to excel in many downstream tasks. Despite recent advances, these models
are still prone to what are commonly known as hallucinations, causing them to
emit unwanted and factually incorrect text. In this work, we study how well
LLMs capture uncertainty, without explicitly being trained for that. We show
that, if considering uncertainty as a linear concept in the model's latent
space, it might indeed be captured, even after only pretraining. We further
show that, though unintuitive, LLMs appear to capture several different types
of uncertainty, each of which can be useful to predict the correctness for a
specific task or benchmark. Furthermore, we provide in-depth results such as
demonstrating a correlation between our correction prediction and the model's
ability to abstain from misinformation using words, and the lack of impact of
model scaling for capturing uncertainty. Finally, we claim that unifying the
uncertainty types as a single one using instruction-tuning or [IDK]-token
tuning is helpful for the model in terms of correctness prediction.

</details>


### [493] [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
*Ilker Kesen,Jonas F. Lotz,Ingo Ziegler,Phillip Rust,Desmond Elliott*

Main category: cs.CL

TL;DR: PIXEL-M4是一种基于多语言预训练的像素语言模型，支持英语、印地语、乌克兰语和简体中文，在非拉丁文字任务中表现优于单语言模型。


<details>
  <summary>Details</summary>
Motivation: 探索多语言预训练对像素语言模型性能的提升，尤其是在跨语言任务中的表现。

Method: 在四种视觉和语言多样性语言（英语、印地语、乌克兰语、简体中文）上预训练PIXEL-M4模型，并进行语义和句法任务的多语言评估。

Result: PIXEL-M4在非拉丁文字任务中优于单语言模型，且能捕捉未见过语言的丰富语言特征。

Conclusion: 多语言预训练显著提升了像素语言模型对多样化语言的支持能力。

Abstract: Pixel language models operate directly on images of rendered text,
eliminating the need for a fixed vocabulary. While these models have
demonstrated strong capabilities for downstream cross-lingual transfer,
multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model
pretrained on four visually and linguistically diverse languages: English,
Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic
and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart
on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4
captures rich linguistic features, even in languages not seen during
pretraining. Furthermore, an analysis of its hidden representations shows that
multilingual pretraining yields a semantic embedding space closely aligned
across the languages used for pretraining. This work demonstrates that
multilingual pretraining substantially enhances the capability of pixel
language models to effectively support a diverse set of languages.

</details>


### [494] [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
*Andrea Pedrotti,Giulia Rambelli,Caterina Villani,Marianna Bolognesi*

Main category: cs.CL

TL;DR: 研究探讨了人类与AI在分类任务中的表现差异，发现AI生成的下级类别与人类分类一致性较低。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补对下级类别分类的空白，并评估AI模型在此任务中的表现。

Method: 使用意大利语心理语言学数据集，对比人类与AI在生成下级类别、类别归纳和典型性判断任务中的表现。

Result: 人类与AI在分类任务中的一致性较低，但AI在不同语义领域的表现差异显著。

Conclusion: AI生成的下级类别在心理学和语言学研究中具有潜力，但仍存在局限性。

Abstract: People can categorize the same entity at multiple taxonomic levels, such as
basic (bear), superordinate (animal), and subordinate (grizzly bear). While
prior research has focused on basic-level categories, this study is the first
attempt to examine the organization of categories by analyzing exemplars
produced at the subordinate level. We present a new Italian psycholinguistic
dataset of human-generated exemplars for 187 concrete words. We then use these
data to evaluate whether textual and vision LLMs produce meaningful exemplars
that align with human category organization across three key tasks: exemplar
generation, category induction, and typicality judgment. Our findings show a
low alignment between humans and LLMs, consistent with previous studies.
However, their performance varies notably across different semantic domains.
Ultimately, this study highlights both the promises and the constraints of
using AI-generated exemplars to support psychological and linguistic research.

</details>


### [495] [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
*Junyan Zhang,Yubo Gao,Yibo Yan,Jungang Li,Zhaorui Hou,Sicheng Tao,Shuliang Liu,Song Dai,Yonghua Hei,Junzhuo Li,Xuming Hu*

Main category: cs.CL

TL;DR: 该研究通过分析微调对LLM计算的重构，揭示了指令特定稀疏组件的作用，并提出了SPARCOM框架和HexaInst数据集。


<details>
  <summary>Details</summary>
Motivation: 理解微调如何改进LLM的指令跟随能力，并揭示其背后的计算机制。

Method: 引入HexaInst数据集和SPARCOM框架，分析稀疏组件的功能通用性和独特性。

Result: 实验证明稀疏组件在指令执行中具有功能通用性和关键作用。

Conclusion: 研究阐明了微调与稀疏计算基板的关系，为可信LLM社区提供了深入见解。

Abstract: The finetuning of Large Language Models (LLMs) has significantly advanced
their instruction-following capabilities, yet the underlying computational
mechanisms driving these improvements remain poorly understood. This study
systematically examines how fine-tuning reconfigures LLM computations by
isolating and analyzing instruction-specific sparse components, i.e., neurons
in dense models and both neurons and experts in Mixture-of-Experts (MoE)
architectures. In particular, we introduce HexaInst, a carefully curated and
balanced instructional dataset spanning six distinct categories, and propose
SPARCOM, a novel analytical framework comprising three key contributions: (1) a
method for identifying these sparse components, (2) an evaluation of their
functional generality and uniqueness, and (3) a systematic comparison of their
alterations. Through experiments, we demonstrate functional generality,
uniqueness, and the critical role of these components in instruction execution.
By elucidating the relationship between fine-tuning-induced adaptations and
sparse computational substrates, this work provides deeper insights into how
LLMs internalize instruction-following behavior for the trustworthy LLM
community.

</details>


### [496] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
*Qishuai Zhong,Zongmin Li,Siqi Fan,Aixin Sun*

Main category: cs.CL

TL;DR: 论文提出了一种评估LLM在用户社会人口属性（如年龄、职业、教育水平）下的行为适应性的框架，通过显式（用户简介）或隐式（多轮对话历史）方式引入属性，并评估模型行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注单轮提示，而实际应用需基于对话历史进行上下文适应，因此需要更全面的评估方法。

Method: 使用多代理管道构建合成数据集，结合用户简介和多轮对话历史，通过VSM 2013问题探测模型的价值表达。

Result: 多数模型会根据人口属性调整表达，尤其在年龄和教育水平上，但一致性参差不齐；推理能力强的模型表现更一致。

Conclusion: 推理能力对稳健的社会人口适应性至关重要，未来研究可进一步优化模型推理能力。

Abstract: Effective engagement by large language models (LLMs) requires adapting
responses to users' sociodemographic characteristics, such as age, occupation,
and education level. While many real-world applications leverage dialogue
history for contextualization, existing evaluations of LLMs' behavioral
adaptation often focus on single-turn prompts. In this paper, we propose a
framework to evaluate LLM adaptation when attributes are introduced either (1)
explicitly via user profiles in the prompt or (2) implicitly through multi-turn
dialogue history. We assess the consistency of model behavior across these
modalities. Using a multi-agent pipeline, we construct a synthetic dataset
pairing dialogue histories with distinct user profiles and employ questions
from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe
value expression. Our findings indicate that most models adjust their expressed
values in response to demographic changes, particularly in age and education
level, but consistency varies. Models with stronger reasoning capabilities
demonstrate greater alignment, indicating the importance of reasoning in robust
sociodemographic adaptation.

</details>


### [497] [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
*Bidyarthi Paul,Jalisha Jashim Era,Mirazur Rahman Zim,Tahmid Sattar Aothoi,Faisal Muhammad Shah*

Main category: cs.CL

TL;DR: 论文介绍了SOMADHAN数据集，用于解决孟加拉语数学应用题（MWPs）的挑战，并通过多种大语言模型（LLMs）评估了性能，其中LLaMA-3.3 70B在少样本CoT提示下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，缺乏针对复杂数学应用题的数据集，限制了其数学推理研究的发展。

Method: 创建了包含8792道复杂孟加拉语MWPs的SOMADHAN数据集，并评估了多种LLMs（如GPT-4o、LLaMA等）在零样本和少样本提示下的表现，同时应用了LoRA进行高效微调。

Result: CoT提示显著提升了性能，LLaMA-3.3 70B在少样本CoT提示下达到88%的准确率。

Conclusion: 该研究填补了孟加拉语NLP的空白，为低资源语言的公平研究和教育技术提供了高质量数据集和可扩展框架。

Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in
natural language processing (NLP) due to the language's low-resource status and
the multi-step reasoning required. Existing models struggle with complex
Bengali MWPs, largely because no human-annotated Bengali dataset has previously
addressed this task. This gap has limited progress in Bengali mathematical
reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex
Bengali MWPs with manually written, step-by-step solutions. We designed this
dataset to support reasoning-focused evaluation and model development in a
linguistically underrepresented context. Using SOMADHAN, we evaluated a range
of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series
models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with
and without Chain of Thought (CoT) reasoning. CoT prompting consistently
improved performance over standard prompting, especially in tasks requiring
multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with
few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune
models efficiently, enabling them to adapt to Bengali MWPs with minimal
computational cost. Our work fills a critical gap in Bengali NLP by providing a
high-quality reasoning dataset and a scalable framework for solving complex
MWPs. We aim to advance equitable research in low-resource languages and
enhance reasoning capabilities in educational and language technologies.

</details>


### [498] [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
*Hovhannes Tamoyan,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在生成内容时具备内部自我监测能力，能够通过Transformer残差流中的线性特征判断事实回忆的正确性。这种能力对格式变化鲁棒，且随模型规模和训练动态变化。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成内容中事实错误的问题，探索其内部是否存在判断事实正确性的机制。

Method: 通过分析Transformer残差流中的线性特征，研究LLMs对实体-关系-属性三元组正确性的自我监测能力，并测试其对格式变化和上下文扰动的鲁棒性。

Result: LLMs在生成时具备内部自我监测能力，这种能力在训练过程中快速出现，并在中间层达到峰值。

Conclusion: LLMs具有内在的自我监测能力，有助于提升其可解释性和可靠性。

Abstract: Factual incorrectness in generated content is one of the primary concerns in
ubiquitous deployment of large language models (LLMs). Prior findings suggest
LLMs can (sometimes) detect factual incorrectness in their generated content
(i.e., fact-checking post-generation). In this work, we provide evidence
supporting the presence of LLMs' internal compass that dictate the correctness
of factual recall at the time of generation. We demonstrate that for a given
subject entity and a relation, LLMs internally encode linear features in the
Transformer's residual stream that dictate whether it will be able to recall
the correct attribute (that forms a valid entity-relation-attribute triplet).
This self-awareness signal is robust to minor formatting variations. We
investigate the effects of context perturbation via different example selection
strategies. Scaling experiments across model sizes and training dynamics
highlight that self-awareness emerges rapidly during training and peaks in
intermediate layers. These findings uncover intrinsic self-monitoring
capabilities within LLMs, contributing to their interpretability and
reliability.

</details>


### [499] [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
*Dario Satriani,Enzo Veltri,Donatello Santoro,Paolo Papotti*

Main category: cs.CL

TL;DR: 论文介绍了RelationalFactQA，一个评估大语言模型（LLMs）生成结构化表格输出能力的基准测试，发现当前LLMs在此任务上表现不佳，准确性低于25%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注简短事实答案，忽略了LLMs生成结构化、多记录表格输出的能力，而这种能力在实际应用中至关重要。

Method: 提出了RelationalFactQA基准测试，包含多样化的自然语言问题（与SQL配对）和标准表格答案，用于系统评估LLMs的结构化知识检索能力。

Result: 实验表明，即使最先进的LLMs在生成关系型输出时表现较差，准确性不超过25%，且随着输出维度增加性能显著下降。

Conclusion: 研究揭示了当前LLMs在合成结构化事实知识方面的局限性，RelationalFactQA将成为衡量未来LLM事实性进展的重要工具。

Abstract: Factuality in Large Language Models (LLMs) is a persistent challenge. Current
benchmarks often assess short factual answers, overlooking the critical ability
to generate structured, multi-record tabular outputs from parametric knowledge.
We demonstrate that this relational fact retrieval is substantially more
difficult than isolated point-wise queries, even when individual facts are
known to the model, exposing distinct failure modes sensitive to output
dimensionality (e.g., number of attributes or records). To systematically
evaluate this under-explored capability, we introduce RelationalFactQA, a new
benchmark featuring diverse natural language questions (paired with SQL) and
gold-standard tabular answers, specifically designed to assess knowledge
retrieval in a structured format. RelationalFactQA enables analysis across
varying query complexities, output sizes, and data characteristics. Our
experiments reveal that even state-of-the-art LLMs struggle significantly, not
exceeding 25% factual accuracy in generating relational outputs, with
performance notably degrading as output dimensionality increases. These
findings underscore critical limitations in current LLMs' ability to synthesize
structured factual knowledge and establish RelationalFactQA as a crucial
resource for measuring future progress in LLM factuality.

</details>


### [500] [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
*Xiao Liu,Da Yin,Zirui Wu,Yansong Feng*

Main category: cs.CL

TL;DR: RefTool是一个参考引导的框架，通过利用外部结构化材料（如教科书）自动生成工具，解决了LLMs在无预定义工具时依赖内部知识的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在无预定义工具时依赖内部知识的问题，提升其在未知领域的推理能力。

Method: RefTool包含两个模块：工具生成（从参考内容生成可执行工具并验证）和工具利用（通过工具箱结构选择和应用工具）。

Result: 在因果、物理和化学基准测试中，RefTool平均准确率比现有方法高11.3%，且成本高效、通用性强。

Conclusion: RefTool通过外部参考生成工具，克服了LLMs的知识限制，提升了推理能力和通用性。

Abstract: Tools enhance the reasoning capabilities of large language models (LLMs) in
complex problem-solving tasks, but not all tasks have available tools. In the
absence of predefined tools, prior works have explored instructing LLMs to
generate tools on their own. However, such approaches rely heavily on the
models' internal knowledge and would fail in domains beyond the LLMs' knowledge
scope. To address this limitation, we propose RefTool, a reference-guided
framework for automatic tool creation that leverages structured external
materials such as textbooks. RefTool consists of two modules: (1) tool
creation, where LLMs generate executable tools from reference content, validate
them using illustrative examples, and organize them hierarchically into a
toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to
select and apply the appropriate tools to solve problems. Experiments on
causality, physics, and chemistry benchmarks demonstrate that RefTool
outperforms existing tool-creation and domain-specific reasoning methods by
11.3% on average accuracy, while being cost-efficient and broadly
generalizable. Analyses reveal that grounding tool creation in references
produces accurate and faithful tools, and that the hierarchical structure
facilitates effective tool selection. RefTool enables LLMs to overcome
knowledge limitations, demonstrating the value of grounding tool creation in
external references for enhanced and generalizable reasoning.

</details>


### [501] [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
*Yihan Wang,Qiao Yan,Zhenghao Xing,Lihao Liu,Junjun He,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CL

TL;DR: 论文提出了一种名为“Catfish Agent”的LLM角色，用于解决多智能体框架中的“Silent Agreement”问题，通过结构化异议提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 多智能体框架在临床问答中虽表现优异，但存在“Silent Agreement”问题，即智能体过早达成共识而缺乏深入分析。

Method: 引入Catfish Agent，采用复杂度感知和语气校准两种机制，以结构化异议挑战共识。

Result: 在多个医学问答基准测试中，该方法优于单智能体和多智能体框架，包括GPT-4o和DeepSeek-R1。

Conclusion: Catfish Agent能有效解决Silent Agreement问题，提升复杂病例的诊断准确性。

Abstract: Large language models (LLMs) have demonstrated strong potential in clinical
question answering, with recent multi-agent frameworks further improving
diagnostic accuracy via collaborative reasoning. However, we identify a
recurring issue of Silent Agreement, where agents prematurely converge on
diagnoses without sufficient critical analysis, particularly in complex or
ambiguous cases. We present a new concept called Catfish Agent, a
role-specialized LLM designed to inject structured dissent and counter silent
agreement. Inspired by the ``catfish effect'' in organizational psychology, the
Catfish Agent is designed to challenge emerging consensus to stimulate deeper
reasoning. We formulate two mechanisms to encourage effective and context-aware
interventions: (i) a complexity-aware intervention that modulates agent
engagement based on case difficulty, and (ii) a tone-calibrated intervention
articulated to balance critique and collaboration. Evaluations on nine medical
Q&A and three medical VQA benchmarks show that our approach consistently
outperforms both single- and multi-agent LLMs frameworks, including leading
commercial models such as GPT-4o and DeepSeek-R1.

</details>


### [502] [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
*Shimao Zhang,Zhejian Lai,Xiang Liu,Shuaijie She,Xiao Liu,Yeyun Gong,Shujian Huang,Jiajun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种细粒度神经元识别算法，用于检测语言神经元（包括语言特定神经元和语言相关神经元）以及语言无关神经元，并基于神经元分布特性将LLMs的多语言推理过程分为四个部分。


<details>
  <summary>Details</summary>
Motivation: 研究多语言对齐如何通过高资源语言向低资源语言的能力转移来增强LLMs的多语言能力，并从语言特定神经元的角度分析LLMs的机制。

Method: 提出新的细粒度神经元识别算法，检测不同类型的神经元，并将LLMs的多语言推理过程分为四个部分。

Result: 系统分析了对齐前后的模型，重点关注不同类型的神经元，并研究了“自发多语言对齐”现象。

Conclusion: 基于不同类型神经元的全面研究，为理解多语言对齐和LLMs的多语言能力提供了实证结果和宝贵见解。

Abstract: Multilingual Alignment is an effective and representative paradigm to enhance
LLMs' multilingual capabilities, which transfers the capabilities from the
high-resource languages to the low-resource languages. Meanwhile, some
researches on language-specific neurons reveal that there are language-specific
neurons that are selectively activated in LLMs when processing different
languages. This provides a new perspective to analyze and understand LLMs'
mechanisms more specifically in multilingual scenarios. In this work, we
propose a new finer-grained neuron identification algorithm, which detects
language neurons~(including language-specific neurons and language-related
neurons) and language-agnostic neurons. Furthermore, based on the
distributional characteristics of different types of neurons, we divide the
LLMs' internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3)
multilingual output space transformation, and (4) vocabulary space outputting.
Additionally, we systematically analyze the models before and after alignment
with a focus on different types of neurons. We also analyze the phenomenon of
''Spontaneous Multilingual Alignment''. Overall, our work conducts a
comprehensive investigation based on different types of neurons, providing
empirical results and valuable insights for better understanding multilingual
alignment and multilingual capabilities of LLMs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [503] [HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings](https://arxiv.org/abs/2505.20585)
*Ben Rahman*

Main category: cs.HC

TL;DR: HOT-FIT-BR框架扩展了HOT-FIT模型，新增三个维度（基础设施指数、政策合规层和社区参与适配），在低收入国家中更敏感地检测数字健康系统问题。


<details>
  <summary>Details</summary>
Motivation: 低收入国家数字健康系统实施常因未考虑基础设施、政策和社区准备而失败，需更全面的评估框架。

Method: 提出HOT-FIT-BR框架，增加基础设施指数、政策合规层和社区参与适配三个维度，并在印尼健康中心进行模拟验证。

Result: HOT-FIT-BR比HOT-FIT敏感度提高58%，尤其适用于基础设施指数<3的农村地区，并成功适配印度和肯尼亚等国家。

Conclusion: HOT-FIT-BR为低收入国家提供了更有效的数字健康系统评估工具，具有广泛适用性。

Abstract: Implementation of digital health systems in low-middle-income countries
(LMICs) often fails due to a lack of evaluations that take into account
infrastructure limitations, local policies, and community readiness. We
introduce HOT-FIT-BR, a contextual evaluation framework that expands the
HOT-FIT model with three new dimensions: (1) Infrastructure Index to measure
electricity/internet availability, (2) Policy Compliance Layer to ensure
regulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community
Engagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR
is 58% more sensitive to detecting problems than HOT-FIT, especially in rural
areas with an Infra Index <3. The framework has also proven adaptive to the
context of other LMICs such as India and Kenya through local parameter
adjustments.

</details>


### [504] [Institutionalizing Folk Theories of Algorithms: How Multi-Channel Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry](https://arxiv.org/abs/2505.20623)
*Qing Xiao,Rongyi Chen,Jingjia Xiao,Tianyang Fu,Alice Qian Zhang,Xianzhe Fan,Bingbing Zhang,Zhicong Lu,Hong Shen*

Main category: cs.HC

TL;DR: 论文研究了平台劳动中工人依赖的‘民间理论’如何被中介组织制度化和工具化，用于劳动管理。


<details>
  <summary>Details</summary>
Motivation: 探讨中介组织如何通过制度化的民间理论管理劳动，填补了现有研究对算法知识制度化的空白。

Method: 通过九个月的民族志田野调查和37次访谈，分析了中国多频道网络（MCNs）对算法的双重理论构建。

Result: MCNs内部承认算法的不稳定性，对外则推广简化理论，将其作为劳动管理工具，促进工人自我规训。

Conclusion: 制度化的民间理论成为软控制的基础设施，影响工人对算法的理解及劳动的结构化与道德化。

Abstract: As algorithmic systems increasingly structure platform labor, workers often
rely on informal "folk theories", experience-based beliefs about how algorithms
work, to navigate opaque and unstable algorithmic environments. Prior research
has largely treated these theories as bottom-up, peer-driven strategies for
coping with algorithmic opacity and uncertainty. In this study, we shift
analytical attention to intermediary organizations and examine how folk
theories of algorithms can be institutionally constructed and operationalized
by those organizations as tools of labor management. Drawing on nine months of
ethnographic fieldwork and 37 interviews with live-streamers and staff at
Multi-Channel Networks (MCNs) in China, we show that MCNs develop and circulate
dual algorithmic theories: internally, they acknowledge the volatility of
platform systems and adopt probabilistic strategies to manage risk; externally,
they promote simplified, prescriptive theories portraying the algorithm as
transparent, fair, and responsive to individual effort. They have further
operationalize those folk theories for labor management, encouraging streamers
to self-discipline and invest in equipment, training, and routines, while
absolving MCNs of accountability. We contribute to CSCW and platform labor
literature by demonstrating how informal algorithmic knowledge, once
institutionalized, can become infrastructures of soft control -- shaping not
only how workers interpret platform algorithms, but also how their labor is
structured, moralized and governed.

</details>


### [505] [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
*Saharsh Barve,Andy Mao,Jiayue Melissa Shi,Prerna Juneja,Koustuv Saha*

Main category: cs.HC

TL;DR: 论文提出了一种理论驱动的偏见检测框架和SSI指数，用于评估文本到图像生成模型中的社会偏见，并通过提示优化显著减少了偏见。用户研究发现，去偏见可能影响上下文的匹配性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在文本到图像生成中可能放大社会刻板印象，引发伦理问题，需要系统性的偏见检测和缓解方法。

Method: 提出SSI指数和偏见检测框架，对DALL-E-3、Midjourney-6.1和Stability AI Core进行审计，并通过LLM优化提示以减少偏见。

Result: 提示优化使SSI显著下降（地理文化类61%，职业类69%，形容词类51%），但用户研究发现去偏见可能影响上下文匹配性。

Conclusion: 需要在伦理去偏见和上下文相关性之间取得平衡，呼吁开发支持全球多样性和包容性的T2I系统。

Abstract: Recent advances in generative AI have enabled visual content creation through
text-to-image (T2I) generation. However, despite their creative potential, T2I
models often replicate and amplify societal stereotypes -- particularly those
related to gender, race, and culture -- raising important ethical concerns.
This paper proposes a theory-driven bias detection rubric and a Social
Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs.
We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and
Stability AI Core -- using 100 queries across three categories -- geocultural,
occupational, and adjectival. Our analysis reveals that initial outputs are
prone to include stereotypical visual cues, including gendered professions,
cultural markers, and western beauty norms. To address this, we adopted our
rubric to conduct targeted prompt refinement using LLMs, which significantly
reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and
51% for adjectival queries. We complemented our quantitative analysis through a
user study examining perceptions, awareness, and preferences around
AI-generated biased imagery. Our findings reveal a key tension -- although
prompt refinement can mitigate stereotypes, it can limit contextual alignment.
Interestingly, users often perceived stereotypical images to be more aligned
with their expectations. We discuss the need to balance ethical debiasing with
contextual relevance and call for T2I systems that support global diversity and
inclusivity while not compromising the reflection of real-world social
complexity.

</details>


### [506] [Learning Annotation Consensus for Continuous Emotion Recognition](https://arxiv.org/abs/2505.21196)
*Ibrahim Shoer,Engin Erzin*

Main category: cs.HC

TL;DR: 提出一种多标注者训练方法，用于连续情感识别（CER），通过共识网络整合标注，优于传统单标签方法。


<details>
  <summary>Details</summary>
Motivation: 情感计算中，多标注者的标注常被合并为单一标签，可能丢失有价值的信息差异。

Method: 使用共识网络整合多标注者的标注，指导主情感预测器更好地反映集体输入。

Result: 在RECOLA和COGNIMUSE数据集上表现优于传统单标签方法。

Conclusion: 充分利用多标注者数据在情感识别中具有优势，适用于标注丰富但不一致的领域。

Abstract: In affective computing, datasets often contain multiple annotations from
different annotators, which may lack full agreement. Typically, these
annotations are merged into a single gold standard label, potentially losing
valuable inter-rater variability. We propose a multi-annotator training
approach for continuous emotion recognition (CER) that seeks a consensus across
all annotators rather than relying on a single reference label. Our method
employs a consensus network to aggregate annotations into a unified
representation, guiding the main arousal-valence predictor to better reflect
collective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach
outperforms traditional methods that unify annotations into a single label.
This underscores the benefits of fully leveraging multi-annotator data in
emotion recognition and highlights its applicability across various fields
where annotations are abundant yet inconsistent.

</details>


### [507] [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
*Yi-Cheng Lin,Kang-Chieh Chen,Zhe-Yan Li,Tzu-Heng Wu,Tzu-Hsuan Wu,Kuan-Yu Chen,Hung-yi Lee,Yun-Nung Chen*

Main category: cs.HC

TL;DR: 本文是第一篇专注于多智能体系统（MAS）中创造力的综述，提出了创造力在MAS中的重要性，并提供了分类、技术概述及挑战分析。


<details>
  <summary>Details</summary>
Motivation: 现有综述多关注MAS基础设施，而忽视了创造力维度，如新颖输出生成与评估、智能体角色设计及创意工作流协调。本文填补了这一空白。

Method: 聚焦文本和图像生成任务，提出智能体主动性和角色设计的分类法，概述生成技术（如发散探索、迭代优化、协作合成）及相关数据集与评估指标。

Result: 总结了创造力在MAS中的关键挑战，如评估标准不一致、偏见缓解不足、协调冲突及缺乏统一基准。

Conclusion: 本文为创造性MAS的开发、评估和标准化提供了结构化框架和路线图。

Abstract: Large language model (LLM)-driven multi-agent systems (MAS) are transforming
how humans and AIs collaboratively generate ideas and artifacts. While existing
surveys provide comprehensive overviews of MAS infrastructures, they largely
overlook the dimension of \emph{creativity}, including how novel outputs are
generated and evaluated, how creativity informs agent personas, and how
creative workflows are coordinated. This is the first survey dedicated to
creativity in MAS. We focus on text and image generation tasks, and present:
(1) a taxonomy of agent proactivity and persona design; (2) an overview of
generation techniques, including divergent exploration, iterative refinement,
and collaborative synthesis, as well as relevant datasets and evaluation
metrics; and (3) a discussion of key challenges, such as inconsistent
evaluation standards, insufficient bias mitigation, coordination conflicts, and
the lack of unified benchmarks. This survey offers a structured framework and
roadmap for advancing the development, evaluation, and standardization of
creative MAS.

</details>


### [508] [Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset](https://arxiv.org/abs/2505.20788)
*Robin Burchard,Kristof Van Laerhoven*

Main category: cs.HC

TL;DR: 论文探讨了在可穿戴设备中利用声音数据进行人类活动识别，特别是水流检测，以增强特定任务（如洗手检测）。由于隐私问题，音频数据需本地处理，增加了能耗。作者通过标注HD-Epic数据集中的水流标签，验证了新类别的有效性。


<details>
  <summary>Details</summary>
Motivation: 声音数据能为人类活动识别提供上下文信息，但隐私问题限制了音频数据的保存。水流检测是一个特殊用例，可用于增强特定任务（如洗手检测）。

Method: 作者在HD-Epic数据集中新增了“水流”标签，标注了717个样本，并分析了其与现有“水”类的关系。训练并评估了两个轻量级分类器。

Result: 新类别“水流”更容易学习，验证了其有效性。

Conclusion: 水流检测是可穿戴设备中人类活动识别的有效补充，但需权衡隐私与能耗问题。

Abstract: Wearable human activity recognition has been shown to benefit from the
inclusion of acoustic data, as the sounds around a person often contain
valuable context. However, due to privacy concerns, it is usually not ethically
feasible to record and save microphone data from the device, since the audio
could, for instance, also contain private conversations. Rather, the data
should be processed locally, which in turn requires processing power and
consumes energy on the wearable device. One special use case of contextual
information that can be utilized to augment special tasks in human activity
recognition is water flow detection, which can, e.g., be used to aid wearable
hand washing detection. We created a new label called tap water for the
recently released HD-Epic data set, creating 717 hand-labeled annotations of
tap water flow, based on existing annotations of the water class. We analyzed
the relation of tap water and water in the dataset and additionally trained and
evaluated two lightweight classifiers to evaluate the newly added label class,
showing that the new class can be learned more easily.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [509] [VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification](https://arxiv.org/abs/2505.20302)
*Patrick Yubeaton,Andre Nakkab,Weihua Xiao,Luca Collini,Ramesh Karri,Chinmay Hegde,Siddharth Garg*

Main category: cs.PL

TL;DR: VeriThoughts是一个用于基于推理的Verilog代码生成的新数据集，提出了基于形式验证方法的基准框架，并优化了一套小规模模型。


<details>
  <summary>Details</summary>
Motivation: 满足对自动化硬件设计工具的需求，以从高级规范生成可验证正确的硬件描述，加速开发过程并保证正确性。

Method: 建立基于形式验证的基准框架，并开发专门优化的小规模模型用于Verilog生成。

Result: 提出了VeriThoughts数据集和基准框架，以及优化的Verilog生成模型。

Conclusion: 该工作为自动化硬件设计提供了可验证正确的解决方案，有望加速硬件开发。

Abstract: This paper introduces VeriThoughts, a novel dataset designed for
reasoning-based Verilog code generation. We establish a new benchmark framework
grounded in formal verification methods to evaluate the quality and correctness
of generated hardware descriptions. Additionally, we present a suite of
specialized small-scale models optimized specifically for Verilog generation.
Our work addresses the growing need for automated hardware design tools that
can produce verifiably correct implementations from high-level specifications,
potentially accelerating the hardware development process while maintaining
rigorous correctness guarantees. Our code and data are available at
\href{https://github.com/wilyub/VeriThoughts}{this URL}.

</details>


### [510] [LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability](https://arxiv.org/abs/2505.20356)
*Shuoming Zhang,Jiacheng Zhao,Chunwei Xia,Zheng Wang,Yunji Chen,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: LEGO-Compiler利用LLMs将高级语言分解为可管理块，通过验证工作流和反馈机制实现高效编译，准确率高达99%。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLMs难以处理长复杂程序的问题，探索LLMs在系统级任务中的应用。

Method: 采用LEGO翻译、分解编译流程为可验证步骤及反馈机制。

Result: 在ExeBench和AnsiBench上分别达到99%和97.9%准确率，编译代码规模提升近一个数量级。

Conclusion: LEGO-Compiler为LLMs在系统任务中的应用开辟了新途径，补充了传统编译器技术。

Abstract: Large language models (LLMs) have the potential to revolutionize how we
design and implement compilers and code translation tools. However, existing
LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,
a novel neural compilation system that leverages LLMs to translate high-level
languages into assembly code. Our approach centers on three key innovations:
LEGO translation, which decomposes the input program into manageable blocks;
breaking down the complex compilation process into smaller, simpler verifiable
steps by organizing it as a verifiable LLM workflow by external tests; and a
feedback mechanism for self-correction. Supported by formal proofs of
translation composability, LEGO-Compiler demonstrates high accuracy on multiple
datasets, including over 99% on ExeBench and 97.9% on industrial-grade
AnsiBench. Additionally, LEGO-Compiler has also acheived near one
order-of-magnitude improvement on compilable code size scalability. This work
opens new avenues for applying LLMs to system-level tasks, complementing
traditional compiler technologies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [511] [REWIND: Speech Time Reversal for Enhancing Speaker Representations in Diffusion-based Voice Conversion](https://arxiv.org/abs/2505.20756)
*Ishan D. Biyani,Nirmesh J. Shah,Ashishkumar P. Gudmalwar,Pankaj Wasnik,Rajiv R. Shah*

Main category: eess.AS

TL;DR: 利用时间反转语音增强说话人表征，显著提高说话人相似性得分。


<details>
  <summary>Details</summary>
Motivation: 时间反转语音虽破坏语言结构但仍保留说话人特征，可用于增强说话人表征。

Method: 提出利用时间反转语音学习说话人表征，结合扩散基声码器模型进行实验。

Result: 实验表明该方法显著提高说话人相似性得分且保持高语音质量。

Conclusion: 时间反转语音是增强说话人表征的有效策略。

Abstract: Speech time reversal refers to the process of reversing the entire speech
signal in time, causing it to play backward. Such signals are completely
unintelligible since the fundamental structures of phonemes and syllables are
destroyed. However, they still retain tonal patterns that enable perceptual
speaker identification despite losing linguistic content. In this paper, we
propose leveraging speaker representations learned from time reversed speech as
an augmentation strategy to enhance speaker representation. Notably, speaker
and language disentanglement in voice conversion (VC) is essential to
accurately preserve a speaker's unique vocal traits while minimizing
interference from linguistic content. The effectiveness of the proposed
approach is evaluated in the context of state-of-the-art diffusion-based VC
models. Experimental results indicate that the proposed approach significantly
improves speaker similarity-related scores while maintaining high speech
quality.

</details>


### [512] [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
*Rui Liu,Pu Gao,Jiatian Xi,Berrak Sisman,Carlos Busso,Haizhou Li*

Main category: eess.AS

TL;DR: EmoCorrector是一种基于文本的语音编辑（TSE）后校正方案，通过检索增强生成（RAG）解决现有TSE方法中情感不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有TSE方法主要关注内容准确性和声学一致性，但忽视了文本修改可能导致的情感不一致问题。

Method: EmoCorrector提取编辑文本的情感特征，检索匹配情感的语音样本，并合成符合目标情感且保留说话者身份和质量的语音。

Result: 在ECD-TSE数据集上的实验表明，EmoCorrector显著提升了目标情感的表达，并解决了当前TSE方法的情感不一致问题。

Conclusion: EmoCorrector为TSE中的情感一致性提供了有效解决方案，并通过ECD-TSE数据集支持了相关研究和评估。

Abstract: Text-based speech editing (TSE) modifies speech using only text, eliminating
re-recording. However, existing TSE methods, mainly focus on the content
accuracy and acoustic consistency of synthetic speech segments, and often
overlook the emotional shifts or inconsistency issues introduced by text
changes. To address this issue, we propose EmoCorrector, a novel
post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented
Generation (RAG) by extracting the edited text's emotional features, retrieving
speech samples with matching emotions, and synthesizing speech that aligns with
the desired emotion while preserving the speaker's identity and quality. To
support the training and evaluation of emotional consistency modeling in TSE,
we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The
prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data
featuring diverse text variations and a range of emotional expressions.
Subjective and objective experiments and comprehensive analysis on ECD-TSE
confirm that EmoCorrector significantly enhances the expression of intended
emotion while addressing emotion inconsistency limitations in current TSE
methods. Code and audio examples are available at
https://github.com/AI-S2-Lab/EmoCorrector.

</details>


### [513] [Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction](https://arxiv.org/abs/2505.20635)
*Zexu Pan,Shengkui Zhao,Tingting Wang,Kun Zhou,Yukun Ma,Chong Zhang,Bin Ma*

Main category: eess.AS

TL;DR: 论文提出了一种即插即用的说话者间注意力模块，用于处理视频中多个共现的面部信息，提升复杂多人环境下的说话者提取准确性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，视频通常包含多个共现的面部信息，这些信息可以作为说话者活动的有用线索，但现有方法未充分利用这些信息。

Method: 引入一个即插即用的说话者间注意力模块，处理灵活数量的共现面部信息，并将其集成到AV-DPRNN和AV-TFGridNet模型中。

Result: 在VoxCeleb2和MISP等数据集上的实验表明，该方法显著优于基线模型，且在LRS2和LRS3上的跨数据集评估验证了其鲁棒性和泛化能力。

Conclusion: 提出的模块能够有效利用共现面部信息，提升复杂多人环境下的说话者提取性能，具有广泛的应用潜力。

Abstract: Audio-visual speaker extraction isolates a target speaker's speech from a
mixture speech signal conditioned on a visual cue, typically using the target
speaker's face recording. However, in real-world scenarios, other co-occurring
faces are often present on-screen, providing valuable speaker activity cues in
the scene. In this work, we introduce a plug-and-play inter-speaker attention
module to process these flexible numbers of co-occurring faces, allowing for
more accurate speaker extraction in complex multi-person environments. We
integrate our module into two prominent models: the AV-DPRNN and the
state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,
including the highly overlapped VoxCeleb2 and sparsely overlapped MISP,
demonstrate that our approach consistently outperforms baselines. Furthermore,
cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and
generalizability of our method.

</details>


### [514] [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
*Nima Sedghiyeh,Sara Sadeghi,Reza Khodadadi,Farzin Kashani,Omid Aghdaei,Somayeh Rahimi,Mohammad Sadegh Safari*

Main category: eess.AS

TL;DR: 该论文介绍了波斯语语音识别基准（PSRB），用于评估自动语音识别（ASR）系统在波斯语等低资源语言中的表现，并提出了新的错误加权指标以提高评估精度。


<details>
  <summary>Details</summary>
Motivation: 评估ASR系统在低资源语言（如波斯语）中的性能具有挑战性，现有基准缺乏多样性和代表性。

Method: 开发了PSRB基准，评估了10种ASR系统（包括商业和开源模型），分析了错误类型并提出了加权替换错误的新指标。

Result: ASR模型在标准波斯语上表现良好，但在区域口音、儿童语音和特定语言挑战上表现不佳。

Conclusion: PSRB为波斯语ASR研究提供了宝贵资源，并可作为其他低资源语言基准开发的框架。

Abstract: Although Automatic Speech Recognition (ASR) systems have become an integral
part of modern technology, their evaluation remains challenging, particularly
for low-resource languages such as Persian. This paper introduces Persian
Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to
address this gap by incorporating diverse linguistic and acoustic conditions.
We evaluate ten ASR systems, including state-of-the-art commercial and
open-source models, to examine performance variations and inherent biases.
Additionally, we conduct an in-depth analysis of Persian ASR transcriptions,
identifying key error types and proposing a novel metric that weights
substitution errors. This metric enhances evaluation robustness by reducing the
impact of minor and partial errors, thereby improving the precision of
performance assessment. Our findings indicate that while ASR models generally
perform well on standard Persian, they struggle with regional accents,
children's speech, and specific linguistic challenges. These results highlight
the necessity of fine-tuning and incorporating diverse, representative training
datasets to mitigate biases and enhance overall ASR performance. PSRB provides
a valuable resource for advancing ASR research in Persian and serves as a
framework for developing benchmarks in other low-resource languages. A subset
of the PSRB dataset is publicly available at
https://huggingface.co/datasets/PartAI/PSRB.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [515] [Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph](https://arxiv.org/abs/2505.20308)
*Muhammad Tayyab Khan,Lequn Chen,Wenhe Feng,Seung Ki Moon*

Main category: cs.IR

TL;DR: 开发了一个基于知识图谱和大型语言模型的交互式系统，用于金属增材制造的设计和规划支持。


<details>
  <summary>Details</summary>
Motivation: 解决金属增材制造中复杂关系知识分散且难以查询的问题。

Method: 构建了一个包含金属、合金、工艺和后续处理的知识图谱，并通过大型语言模型实现自然语言查询。

Result: 系统支持实时交互、兼容性检查和多约束过滤，为工程师提供决策支持。

Conclusion: 该系统首次将领域知识图谱与大型语言模型结合，推动了制造智能中以人为本的工具发展。

Abstract: Metal additive manufacturing (AM) involves complex interdependencies among
processes, materials, feedstock, and post-processing steps. However, the
underlying relationships and domain knowledge remain fragmented across
literature and static databases that often demand expert-level queries,
limiting their applicability in design and planning. To address these gaps, we
develop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53
distinct metals and alloys across seven material families, nine AM processes,
four feedstock types, and associated post-processing requirements. A large
language model (LLM) interface, guided by a few-shot prompting strategy,
enables natural language querying without the need for formal query syntax. The
system supports a range of tasks, including compatibility checks,
multi-constraint filtering, and design for AM (DfAM) guidance. User natural
language queries are normalized, translated into Cypher, and executed over the
KG, with results reformatted into structured responses. This work presents the
first real-time, interactive system that integrates a domain-specific metal AM
KG with an LLM interface, offering accessible, explainable decision support for
engineers and advancing human-centric tools in manufacturing intelligence.

</details>


### [516] [VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration](https://arxiv.org/abs/2505.20362)
*Jiahui Geng,Qing Li,Zongxiong Chen,Yuxia Wang,Derui Zhu,Zhuohan Xie,Chenyang Lyu,Xiuying Chen,Preslav Nakov,Fakhri Karray*

Main category: cs.IR

TL;DR: 本文提出安全校准概念，解决视觉语言模型（VLMs）的欠安全和过安全问题，并发布VSCBench数据集用于评估。实验发现现有方法存在显著问题，改进方法虽有效但会降低模型实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型欠安全（对危险查询响应），而忽视过安全（拒绝安全查询），需系统性解决两者。

Method: 提出安全校准概念，构建VSCBench数据集（3600对图像-文本），评估11种VLMs，并尝试四种改进方法。

Result: 实验发现欠安全和过安全问题严重，改进方法虽有效但会牺牲模型实用性。

Conclusion: 需开发更先进的安全校准方法，VSCBench为未来研究提供评估工具。

Abstract: The rapid advancement of vision-language models (VLMs) has brought a lot of
attention to their safety alignment. However, existing methods have primarily
focused on model undersafety, where the model responds to hazardous queries,
while neglecting oversafety, where the model refuses to answer safe queries. In
this paper, we introduce the concept of $\textit{safety calibration}$, which
systematically addresses both undersafety and oversafety. Specifically, we
present $\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are
visually or textually similar but differ in terms of safety, which is designed
to evaluate safety calibration across image-centric and text-centric scenarios.
Based on our benchmark, we evaluate safety calibration across eleven widely
used VLMs. Our extensive experiments revealed major issues with both
undersafety and oversafety. We further investigated four approaches to improve
the model's safety calibration. We found that even though some methods
effectively calibrated the models' safety problems, these methods also lead to
the degradation of models' utility. This trade-off underscores the urgent need
for advanced calibration methods, and our benchmark provides a valuable tool
for evaluating future approaches. Our code and data are available at
https://github.com/jiahuigeng/VSCBench.git.

</details>


### [517] [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
*Jaeyoung Choe,Jihoon Kim,Woohwan Jung*

Main category: cs.IR

TL;DR: 论文提出了一种名为HiREC的层次检索与证据整理框架，用于解决金融领域标准化文档中重复检索导致的准确性和完整性问题。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法在处理格式相似的标准化文档（如SEC文件）时，容易误识别近重复文本，导致重复检索，影响结果质量。

Method: HiREC框架通过层次检索（先检索相关文档，再选择最相关段落）和证据整理（去除无关段落，必要时生成补充查询）来优化检索过程。

Result: 构建并发布了包含145,897份SEC文档和1,595个问答对的大规模开放域金融（LOFin）问答基准。

Conclusion: HiREC框架有效解决了金融文档中的重复检索问题，提升了检索的准确性和完整性。

Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are
widely used in finance for their excellent performance on knowledge-intensive
tasks. However, standardized documents (e.g., SEC filing) share similar formats
such as repetitive boilerplate texts, and similar table structures. This
similarity forces traditional RAG methods to misidentify near-duplicate text,
leading to duplicate retrieval that undermines accuracy and completeness. To
address these issues, we propose the Hierarchical Retrieval with Evidence
Curation (HiREC) framework. Our approach first performs hierarchical retrieval
to reduce confusion among similar texts. It first retrieve related documents
and then selects the most relevant passages from the documents. The evidence
curation process removes irrelevant passages. When necessary, it automatically
generates complementary queries to collect missing information. To evaluate our
approach, we construct and release a Large-scale Open-domain Financial (LOFin)
question answering benchmark that includes 145,897 SEC documents and 1,595
question-answer pairs. Our code and data are available at
https://github.com/deep-over/LOFin-bench-HiREC.

</details>


### [518] [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
*Xu Kang,Siqi Jiang,Kangwei Xu,Jiahao Li,Ruibo Wu*

Main category: cs.IR

TL;DR: TeroSeek是一个针对萜类化合物的知识库和AI问答工具，整合了20年的文献，通过RAG框架提供高质量信息，优于通用大语言模型。


<details>
  <summary>Details</summary>
Motivation: 萜类化合物的跨学科性质（化学、药理学、生物学）导致知识整合困难，需要专业工具支持研究。

Method: 开发了TeroSeek知识库和AI问答系统，基于RAG框架，整合20年文献数据。

Result: TeroSeek在萜类相关查询中表现优于通用大语言模型，提供结构化高质量信息。

Conclusion: TeroSeek是一个公开可用的领域专家工具，支持多学科研究。

Abstract: Terpenoids are a crucial class of natural products that have been studied for
over 150 years, but their interdisciplinary nature (spanning chemistry,
pharmacology, and biology) complicates knowledge integration. To address this,
the authors developed TeroSeek, a curated knowledge base (KB) built from two
decades of terpenoid literature, coupled with an AI-powered question-answering
chatbot and web service. Leveraging a retrieval-augmented generation (RAG)
framework, TeroSeek provides structured, high-quality information and
outperforms general-purpose large language models (LLMs) in terpenoid-related
queries. It serves as a domain-specific expert tool for multidisciplinary
research and is publicly available at http://teroseek.qmclab.com.

</details>


### [519] [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
*Shahrooz Pouryousef*

Main category: cs.IR

TL;DR: 比较LLMs与经典矩阵分解模型在推荐系统中的表现，提出RAG方法提升LLMs的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能有效利用用户-物品交互数据中的协作信号。

Method: 系统比较LLMs与矩阵分解模型，并引入RAG方法增强LLMs。

Result: 当前LLMs在捕捉协作模式上不如矩阵分解模型，但RAG方法显著提升推荐质量。

Conclusion: RAG方法为基于LLMs的推荐系统提供了有前景的方向。

Abstract: User-item interactions contain rich collaborative signals that form the
backbone of many successful recommender systems. While recent work has explored
the use of large language models (LLMs) for recommendation, it remains unclear
whether LLMs can effectively reason over this type of collaborative
information. In this paper, we conduct a systematic comparison between LLMs and
classical matrix factorization (MF) models to assess LLMs' ability to leverage
user-item interaction data. We further introduce a simple retrieval-augmented
generation (RAG) method that enhances LLMs by grounding their predictions in
structured interaction data. Our experiments reveal that current LLMs often
fall short in capturing collaborative patterns inherent to MF models, but that
our RAG-based approach substantially improves recommendation
quality-highlighting a promising direction for future LLM-based recommenders.

</details>


### [520] [Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems](https://arxiv.org/abs/2505.20771)
*Heng Tang,Feng Liu,Xinbo Chen,Jiawei Chen,Bohao Wang,Changwang Zhang,Jun Wang,Yuegang Sun,Bingde Hu,Can Wang*

Main category: cs.IR

TL;DR: 论文提出了一种名为SOFT的新方法，结合了"Guidance-Only"和"Tuning-Only"策略，通过自蒸馏和自适应课程学习提升LLM在推荐系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有策略无法有效弥合LLM知识空间与推荐任务之间的差距，性能未达预期。

Method: 提出SOFT方法，结合自蒸馏构建辅助数据集，并通过自适应课程调度器逐步学习从简单到复杂的数据。

Result: 实验表明SOFT显著提升了LLM推荐准确性（平均提升37.59%）。

Conclusion: SOFT方法有效结合了两种策略的优势，显著提升了推荐性能。

Abstract: Recent years have witnessed extensive exploration of Large Language Models
(LLMs) on the field of Recommender Systems (RS). There are currently two
commonly used strategies to enable LLMs to have recommendation capabilities: 1)
The "Guidance-Only" strategy uses in-context learning to exploit and amplify
the inherent semantic understanding and item recommendation capabilities of
LLMs; 2) The "Tuning-Only" strategy uses supervised fine-tuning (SFT) to
fine-tune LLMs with the aim of fitting them to real recommendation data.
However, neither of these strategies can effectively bridge the gap between the
knowledge space of LLMs and recommendation, and their performance do not meet
our expectations.
  To better enable LLMs to learn recommendation knowledge, we combine the
advantages of the above two strategies and proposed a novel "Guidance+Tuning"
method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of
curriculum learning. It first employs self-distillation to construct an
auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it
further utilizes a self-adaptive curriculum scheduler to enable LLMs to
gradually learn from simpler data (self-distilled data) to more challenging
data (real RS data). Extensive experiments demonstrate that SOFT significantly
enhances the recommendation accuracy (37.59\% on average) of LLM-based methods.
The code is available via
https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E

</details>


### [521] [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
*Allaa Boutaleb,Bernd Amann,Hubert Naacke,Rafael Angarita*

Main category: cs.IR

TL;DR: 当前表格联合搜索（TUS）的基准测试存在局限性，导致简单基线方法表现优于复杂方法，需改进基准以更真实评估语义理解。


<details>
  <summary>Details</summary>
Motivation: 分析现有TUS基准的不足，发现其未能有效评估语义理解能力，需提出改进标准。

Method: 分析现有TUS基准的局限性，并提出未来基准的改进标准。

Result: 简单基线方法在现有基准中表现优异，表明基准未能有效评估语义理解。

Conclusion: 需改进TUS基准以更真实反映语义理解的进展。

Abstract: Recent table representation learning and data discovery methods tackle table
union search (TUS) within data lakes, which involves identifying tables that
can be unioned with a given query table to enrich its content. These methods
are commonly evaluated using benchmarks that aim to assess semantic
understanding in real-world TUS tasks. However, our analysis of prominent TUS
benchmarks reveals several limitations that allow simple baselines to perform
surprisingly well, often outperforming more sophisticated approaches. This
suggests that current benchmark scores are heavily influenced by
dataset-specific characteristics and fail to effectively isolate the gains from
semantic understanding. To address this, we propose essential criteria for
future benchmarks to enable a more realistic and reliable evaluation of
progress in semantic table union search.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [522] [Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2505.20817)
*Savelii Chezhegov,Aleksandr Beznosikov,Samuel Horváth,Eduard Gorbunov*

Main category: math.OC

TL;DR: 本文填补了Clip-SGD在重尾噪声和$(L_0,L_1)$-平滑性假设下的高概率收敛性空白，扩展了梯度裁剪的应用范围。


<details>
  <summary>Details</summary>
Motivation: 梯度裁剪在机器学习中广泛应用，但Clip-SGD在重尾噪声和$(L_0,L_1)$-平滑性下的高概率收敛性尚未被充分研究。

Method: 通过分析Clip-SGD在凸$(L_0,L_1)$-平滑优化问题中的应用，建立了高概率收敛界。

Result: 本文恢复了确定性情况和$L_1=0$时的已知结果，避免了指数级大因子，且不依赖严格的次高斯噪声假设。

Conclusion: 研究显著扩展了梯度裁剪的适用性，为相关领域提供了理论支持。

Abstract: Gradient clipping is a widely used technique in Machine Learning and Deep
Learning (DL), known for its effectiveness in mitigating the impact of
heavy-tailed noise, which frequently arises in the training of large language
models. Additionally, first-order methods with clipping, such as Clip-SGD,
exhibit stronger convergence guarantees than SGD under the
$(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks.
However, the high-probability convergence of Clip-SGD under both assumptions --
heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed
in the literature. In this paper, we bridge this critical gap by establishing
the first high-probability convergence bounds for Clip-SGD applied to convex
$(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends
prior results by recovering known bounds for the deterministic case and the
stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid
exponentially large factors and do not rely on restrictive sub-Gaussian noise
assumptions, significantly broadening the applicability of gradient clipping.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [523] [EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms](https://arxiv.org/abs/2505.20614)
*Jiaxiong He*

Main category: cs.CR

TL;DR: EarthOL是一种新型共识协议，用可验证的人类贡献替代区块链中的计算浪费，专注于特定领域，兼顾文化多样性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决区块链系统中计算浪费问题，同时承认文化多样性和主观偏好，提出一种领域受限的共识方法。

Method: 采用多层验证系统，包括领域特定评估标准、时间依赖验证机制和全面安全框架，结合博弈论分析和概率建模。

Result: 理论分析表明，在高共识领域实现了激励兼容的人类贡献验证，并在受控场景中达到拜占庭容错。

Conclusion: 为去中心化价值评估的边界提供了理解框架，并为未来特定领域的人类中心共识机制研究奠定了基础。

Abstract: This paper introduces EarthOL, a novel consensus protocol that attempts to
replace computational waste in blockchain systems with verifiable human
contributions within bounded domains. While recognizing the fundamental
impossibility of universal value assessment, we propose a domain-restricted
approach that acknowledges cultural diversity and subjective preferences while
maintaining cryptographic security. Our enhanced Proof-of-Human-Contribution
(PoHC) protocol uses a multi-layered verification system with domain-specific
evaluation criteria, time-dependent validation mechanisms, and comprehensive
security frameworks. We present theoretical analysis demonstrating meaningful
progress toward incentive-compatible human contribution verification in
high-consensus domains, achieving Byzantine fault tolerance in controlled
scenarios while addressing significant scalability and cultural bias
challenges. Through game-theoretic analysis, probabilistic modeling, and
enhanced security protocols, we identify specific conditions under which the
protocol remains stable and examine failure modes with comprehensive mitigation
strategies. This work contributes to understanding the boundaries of
decentralized value assessment and provides a framework for future research in
human-centered consensus mechanisms for specific application domains, with
particular emphasis on validator and security specialist incentive systems.

</details>


### [524] [Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification](https://arxiv.org/abs/2505.20866)
*Xinjie Lin,Gang Xiong,Gaopeng Gou,Wenqi Dong,Jing Yu,Zhen Li,Wei Xia*

Main category: cs.CR

TL;DR: 论文提出了一种名为ETooL的新型加密流量表示模型，通过结合大型语言模型（LLM）和自监督指令调优，解决了现有方法在分布漂移和依赖标记数据方面的局限性，显著提升了流量分类性能。


<details>
  <summary>Details</summary>
Motivation: 加密流量分类面临分布漂移和标记数据稀缺的挑战，现有方法适应性不足。LLM在通用任务中表现优异，但在流量分析领域尚未充分发挥潜力。

Method: 提出ETooL模型，通过自监督指令调优将LLM与流量结构知识结合，建立文本信息与流量交互的联系。

Result: ETooL在监督和零样本分类任务中表现优异，F1分数显著提升，并在动态分布漂移数据集NETD上验证了其有效性。

Conclusion: ETooL通过结合LLM和自监督学习，为加密流量分类提供了更鲁棒和通用的解决方案。

Abstract: Encrypted traffic classification is highly challenging in network security
due to the need for extracting robust features from content-agnostic traffic
data. Existing approaches face critical issues: (i) Distribution drift, caused
by reliance on the closedworld assumption, limits adaptability to realworld,
shifting patterns; (ii) Dependence on labeled data restricts applicability
where such data is scarce or unavailable. Large language models (LLMs) have
demonstrated remarkable potential in offering generalizable solutions across a
wide range of tasks, achieving notable success in various specialized fields.
However, their effectiveness in traffic analysis remains constrained by
challenges in adapting to the unique requirements of the traffic domain. In
this paper, we introduce a novel traffic representation model named Encrypted
Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which
integrates LLMs with knowledge of traffic structures through a self-supervised
instruction tuning paradigm. This framework establishes connections between
textual information and traffic interactions. ETooL demonstrates more robust
classification performance and superior generalization in both supervised and
zero-shot traffic classification tasks. Notably, it achieves significant
improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%),
APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.)
to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic
dataset designed to support dynamic distributional shifts, and use it to
validate ETooL's effectiveness under varying distributional conditions.
Furthermore, we evaluate the efficiency gains achieved through ETooL's
instruction tuning approach.

</details>


### [525] [Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models](https://arxiv.org/abs/2505.20955)
*Puwei Lian,Yujun Cai,Songze Li*

Main category: cs.CR

TL;DR: 论文提出了一种针对扩散模型的成员推理攻击（MIA）的统一范式，并发现现有攻击忽略了模型处理高频信息的缺陷。通过理论分析和实验验证，作者提出了一种高频滤波模块，显著提升了攻击性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但也引发了隐私和版权问题。成员推理攻击（MIA）用于判断数据是否用于模型训练，但现有攻击存在高频信息处理缺陷，导致分类错误。

Method: 作者将现有MIA形式化为统一范式，计算成员分数。通过实验发现高频信息处理缺陷，并提出一种可即插即用的高频滤波模块，无需额外时间成本即可提升攻击性能。

Result: 实验表明，高频滤波模块显著提升了基线攻击在不同数据集和模型上的表现。

Conclusion: 论文揭示了扩散模型高频信息处理的缺陷对MIA的影响，并提出了一种高效解决方案，为隐私保护提供了新思路。

Abstract: Diffusion models have achieved tremendous success in image generation, but
they also raise significant concerns regarding privacy and copyright issues.
Membership Inference Attacks (MIAs) are designed to ascertain whether specific
data were utilized during a model's training phase. As current MIAs for
diffusion models typically exploit the model's image prediction ability, we
formalize them into a unified general paradigm which computes the membership
score for membership identification. Under this paradigm, we empirically find
that existing attacks overlook the inherent deficiency in how diffusion models
process high-frequency information. Consequently, this deficiency leads to
member data with more high-frequency content being misclassified as hold-out
data, and hold-out data with less high-frequency content tend to be
misclassified as member data. Moreover, we theoretically demonstrate that this
deficiency reduces the membership advantage of attacks, thereby interfering
with the effective discrimination of member data and hold-out data. Based on
this insight, we propose a plug-and-play high-frequency filter module to
mitigate the adverse effects of the deficiency, which can be seamlessly
integrated into any attacks within this general paradigm without additional
time costs. Extensive experiments corroborate that this module significantly
improves the performance of baseline attacks across different datasets and
models.

</details>


### [526] [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
*Yao Huang,Yitong Sun,Shouwei Ruan,Yichi Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CR

TL;DR: 论文提出了一种基于ELM理论和遗传优化的新框架，通过扩展策略空间显著提升了LLM的越狱攻击成功率，实验显示在Claude-3.5上成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过提示工程改进了LLM的安全性，但其效果受限于预定义的策略空间，无法应对更复杂的攻击。

Method: 将越狱策略分解为基于ELM理论的核心组件，并开发了基于遗传优化的意图评估机制。

Result: 在Claude-3.5上实现了超过90%的越狱成功率，且具有跨模型迁移能力和高评估准确性。

Conclusion: 扩展策略空间是提升LLM安全性的关键，新框架为研究提供了重要方向。

Abstract: Large Language Models (LLMs), despite advanced general capabilities, still
suffer from numerous safety risks, especially jailbreak attacks that bypass
safety protocols. Understanding these vulnerabilities through black-box
jailbreak attacks, which better reflect real-world scenarios, offers critical
insights into model robustness. While existing methods have shown improvements
through various prompt engineering techniques, their success remains limited
against safety-aligned models, overlooking a more fundamental problem: the
effectiveness is inherently bounded by the predefined strategy spaces. However,
expanding this space presents significant challenges in both systematically
capturing essential attack patterns and efficiently navigating the increased
complexity. To better explore the potential of expanding the strategy space, we
address these challenges through a novel framework that decomposes jailbreak
strategies into essential components based on the Elaboration Likelihood Model
(ELM) theory and develops genetic-based optimization with intention evaluation
mechanisms. To be striking, our experiments reveal unprecedented jailbreak
capabilities by expanding the strategy space: we achieve over 90% success rate
on Claude-3.5 where prior methods completely fail, while demonstrating strong
cross-model transferability and surpassing specialized safeguard models in
evaluation accuracy. The code is open-sourced at:
https://github.com/Aries-iai/CL-GSO.

</details>


### [527] [AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery](https://arxiv.org/abs/2505.21499)
*Haowei Wang,Junjie Wang,Xiaojun Jia,Rupeng Zhang,Mingyang Li,Zhe Liu,Yang Liu,Qing Wang*

Main category: cs.CR

TL;DR: AdInject是一种新型黑盒攻击方法，通过互联网广告投放向Web Agent注入恶意内容，攻击成功率高达60%至100%，揭示了Web Agent安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性环境注入攻击研究依赖不现实假设，限制了实际应用。AdInject旨在提出更现实的威胁模型，揭示广告投放作为攻击向量的风险。

Method: AdInject利用广告投放注入恶意内容，设计误导性广告，并结合VLM优化广告内容以增强攻击效果。

Result: 实验显示AdInject攻击成功率在多数场景超过60%，某些情况下接近100%。

Conclusion: 广告投放是Web Agent的现实威胁，亟需开发防御机制。

Abstract: Vision-Language Model (VLM) based Web Agents represent a significant step
towards automating complex tasks by simulating human-like interaction with
websites. However, their deployment in uncontrolled web environments introduces
significant security vulnerabilities. Existing research on adversarial
environmental injection attacks often relies on unrealistic assumptions, such
as direct HTML manipulation, knowledge of user intent, or access to agent model
parameters, limiting their practical applicability. In this paper, we propose
AdInject, a novel and real-world black-box attack method that leverages the
internet advertising delivery to inject malicious content into the Web Agent's
environment. AdInject operates under a significantly more realistic threat
model than prior work, assuming a black-box agent, static malicious content
constraints, and no specific knowledge of user intent. AdInject includes
strategies for designing malicious ad content aimed at misleading agents into
clicking, and a VLM-based ad content optimization technique that infers
potential user intents from the target website's context and integrates these
intents into the ad content to make it appear more relevant or critical to the
agent's task, thus enhancing attack effectiveness. Experimental evaluations
demonstrate the effectiveness of AdInject, attack success rates exceeding 60%
in most scenarios and approaching 100% in certain cases. This strongly
demonstrates that prevalent advertising delivery constitutes a potent and
real-world vector for environment injection attacks against Web Agents. This
work highlights a critical vulnerability in Web Agent security arising from
real-world environment manipulation channels, underscoring the urgent need for
developing robust defense mechanisms against such threats. Our code is
available at https://github.com/NicerWang/AdInject.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [528] [Fixed-Point Traps and Identity Emergence in Educational Feedback Systems](https://arxiv.org/abs/2505.21038)
*Faruk Alpay*

Main category: math.CT

TL;DR: 论文通过范畴论证明考试驱动教育系统阻碍身份形成和创造力收敛。


<details>
  <summary>Details</summary>
Motivation: 研究考试评分系统如何通过数学结构阻碍学习者的身份稳定和创造力发展。

Method: 使用Alpay代数框架定义考试评分系统（EGCS），证明其递归评估导致学习动态崩溃。

Result: 证明在EGCS下，学习者身份无法稳定，创造力被抑制。

Conclusion: 首次用代数方法证明考试反馈机制对身份形成的阻碍作用。

Abstract: This paper presents a formal categorical proof that exam-driven educational
systems obstruct identity emergence and block creative convergence. Using the
framework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems
(EGCS) as functorial constructs where learning dynamics $\varphi$ are
recursively collapsed by evaluative morphisms $E$. We prove that under such
collapse regimes, no nontrivial fixed-point algebra $\mu_\varphi$ can exist,
hence learner identity cannot stabilize. This creates a universal fixed-point
trap: all generative functors are entropically folded before symbolic emergence
occurs. Our model mathematically explains the creativity suppression, research
stagnation, and structural entropy loss induced by timed exams and grade-based
feedback. The results apply category theory to expose why modern educational
systems prevent {\phi}-emergence and block observer-invariant self-formation.
This work provides the first provable algebraic obstruction of identity
formation caused by institutional feedback mechanics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [529] [CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture](https://arxiv.org/abs/2505.20481)
*Berat Kutay Uğraş,Ömer Nezih Gerek,İbrahim Talha Saygı*

Main category: eess.SP

TL;DR: 论文提出了一种基于Transformer的模型CardioPatternFormer，用于可解释的心电图分类，通过注意力机制识别心脏模式，提升临床实用性。


<details>
  <summary>Details</summary>
Motivation: 心电图解释的复杂性及AI模型的“黑盒”特性限制了临床实用性，因此需要一种可解释的模型。

Method: 采用Transformer架构，设计CardioPatternFormer模型，利用注意力机制识别和分类心脏模式。

Result: 模型在复杂心电图（包括多病理情况）上表现优异，并通过注意力图提供可解释性。

Conclusion: CardioPatternFormer为心电图分析提供了透明且可靠的解决方案，推动了临床AI的可信应用。

Abstract: Accurate ECG interpretation is vital, yet complex cardiac data and
"black-box" AI models limit clinical utility. Inspired by Transformer
architectures' success in NLP for understanding sequential data, we frame ECG
as the heart's unique "language" of temporal patterns. We present
CardioPatternFormer, a novel Transformer-based model for interpretable ECG
classification. It employs a sophisticated attention mechanism to precisely
identify and classify diverse cardiac patterns, excelling at discerning subtle
anomalies and distinguishing multiple co-occurring conditions. This
pattern-guided attention provides clear insights by highlighting influential
signal regions, effectively allowing the "heart to talk" through transparent
interpretations. CardioPatternFormer demonstrates robust performance on
challenging ECGs, including complex multi-pathology cases. Its interpretability
via attention maps enables clinicians to understand the model's rationale,
fostering trust and aiding informed diagnostic decisions. This work offers a
powerful, transparent solution for advanced ECG analysis, paving the way for
more reliable and clinically actionable AI in cardiology.

</details>


### [530] [Federated Learning-Distillation Alternation for Resource-Constrained IoT](https://arxiv.org/abs/2505.20456)
*Rafael Valente da Silva,Onel L. Alcaraz López,Richard Demo Souza*

Main category: eess.SP

TL;DR: 论文提出FLDA方法，通过在联邦学习（FL）和联邦蒸馏（FD）之间交替，平衡模型精度与通信开销和能耗，适用于能量受限的物联网网络。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在能量和通信资源受限的情况下，传统联邦学习（FL）面临能耗高和干扰敏感的问题，而联邦蒸馏（FD）虽降低开销但牺牲精度。

Method: 提出FLDA方法，设备在FL和FD阶段交替运行，结合两者的优势，同时考虑多信道时隙ALOHA网络和背景干扰。

Result: FLDA在模型精度上优于FL和FD，收敛速度比FL快，能耗节省高达98%，且对干扰更不敏感。

Conclusion: FLDA是一种高效且鲁棒的方法，适用于资源受限的物联网网络，平衡了模型性能和资源消耗。

Abstract: Federated learning (FL) faces significant challenges in Internet of Things
(IoT) networks due to device limitations in energy and communication resources,
especially when considering the large size of FL models. From an energy
perspective, the challenge is aggravated if devices rely on energy harvesting
(EH), as energy availability can vary significantly over time, influencing the
average number of participating users in each iteration. Additionally, the
transmission of large model updates is more susceptible to interference from
uncorrelated background traffic in shared wireless environments. As an
alternative, federated distillation (FD) reduces communication overhead and
energy consumption by transmitting local model outputs, which are typically
much smaller than the entire model used in FL. However, this comes at the cost
of reduced model accuracy. Therefore, in this paper, we propose FL-distillation
alternation (FLDA). In FLDA, devices alternate between FD and FL phases,
balancing model information with lower communication overhead and energy
consumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT
network subject to background traffic/interference. In such a scenario, FLDA
demonstrates higher model accuracy than both FL and FD, and achieves faster
convergence than FL. Moreover, FLDA achieves target accuracies saving up to 98%
in energy consumption, while also being less sensitive to interference, both
relative to FL.

</details>
